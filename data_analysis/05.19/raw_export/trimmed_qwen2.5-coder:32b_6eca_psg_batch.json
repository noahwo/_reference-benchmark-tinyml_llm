{
  "data": [
    {
      "id": "131fa856",
      "timestamp": "2025-05-20T00:01:31.143000+00:00",
      "name": "131f_py_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import numpy as np\nimport os\nimport time\nfrom ai_edge_litert.interpreter import Interpreter\nimport cv2\n\n# Phase 1: Setup\n## 1.1 Imports: Already done above\n\n## 1.2 Paths/Parameters\nmodel_path = 'models/ssd-mobilenet_v1/detect.tflite'\nlabel_map_path = 'models/ssd-mobilenet_v1/labelmap.txt'\nconfidence_threshold = 0.5\ninput_path = 'data/object_detection/sheeps.mp4'\noutput_path = 'results/object_detection/test_results/sheeps_detections.mp4'\n\n## 1.3 Load Labels (Conditional)\nif os.path.exists(label_map_path):\n    with open(label_map_path, 'r') as f:\n        labels = [line.strip() for line in f.readlines()]\nelse:\n    labels = []\n\n## 1.4 Load Interpreter\ninterpreter = Interpreter(model_path=model_path)\ninterpreter.allocate_tensors()\n\n## 1.5 Get Model Details\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\n# Phase 2: Input Acquisition & Preprocessing Loop\n## 2.1 Acquire Input Data\ncap = cv2.VideoCapture(input_path)\nif not cap.isOpened():\n    raise IOError(\"Cannot open video file\")\n\nframe_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\nframe_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\nfps = cap.get(cv2.CAP_PROP_FPS)\n\n## 2.2 Preprocess Data\ninput_shape = input_details[0]['shape']\npreprocessed_frames = []\n\n# Phase 3: Inference (Run per preprocessed input)\ndef preprocess_frame(frame):\n    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n    frame_resized = cv2.resize(frame_rgb, (input_shape[1], input_shape[2]))\n    return np.expand_dims(frame_resized, axis=0).astype(input_details[0]['dtype'])\n\n# Phase 4: Output Interpretation & Handling Loop\n## 4.1 Get Output Tensor(s)\ndef interpret_output(output_data):\n    detection_boxes = output_data[0][0]\n    detection_classes = output_data[1][0].astype(int)\n    detection_scores = output_data[2][0]\n    \n    results = []\n    for i in range(len(detection_scores)):\n        if detection_scores[i] > confidence_threshold:\n            class_id = detection_classes[i]\n            box = detection_boxes[i]\n            label = labels[class_id - 1] if class_id <= len(labels) else f'unknown_{class_id}'\n            score = detection_scores[i]\n            results.append((box, label, score))\n    return results\n\n## 4.2 Interpret Results\n## 4.3 Handle Output\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\nout = cv2.VideoWriter(output_path, fourcc, fps, (frame_width, frame_height))\n\nwhile cap.isOpened():\n    ret, frame = cap.read()\n    if not ret:\n        break\n    \n    input_data = preprocess_frame(frame)\n    interpreter.set_tensor(input_details[0]['index'], input_data)\n    interpreter.invoke()\n    \n    output_data = [interpreter.get_tensor(output_details[i]['index']) for i in range(len(output_details))]\n    results = interpret_output(output_data)\n    \n    for box, label, score in results:\n        ymin, xmin, ymax, xmax = box\n        (left, right, top, bottom) = (xmin * frame_width, xmax * frame_width,\n                                      ymin * frame_height, ymax * frame_height)\n        cv2.rectangle(frame, (int(left), int(top)), (int(right), int(bottom)), (0, 255, 0), 2)\n        label_text = f'{label}: {score:.2f}'\n        cv2.putText(frame, label_text, (int(left), int(top) - 10),\n                    cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n    \n    out.write(frame)\n\n# Phase 5: Cleanup\ncap.release()\nout.release()\ncv2.destroyAllWindows()"
      },
      "session_id": "qwen2.5-coder:32b_6eca_psg_batch",
      "metadata": {
        "num_run": 30
      },
      "tags": [
        "benchmark",
        "py_sketch_generator",
        "qwen2.5-coder:32b"
      ],
      "latency": 26.351,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-03-01-31-144765_chatcmpl-f5ca3289-d4f3-45fe-bacc-3551f533abed",
          "traceId": "131fa856",
          "type": "GENERATION",
          "name": "131f_psg_gen_attempt#1",
          "startTime": "2025-05-20T00:01:31.144000+00:00",
          "endTime": "2025-05-20T00:01:57.495000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {
            "temperature": "0.1",
            "top_p": "0.3"
          },
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 26351.0,
          "costDetails": {},
          "totalTokens": 2503,
          "completionTokens": 923,
          "promptTokens": 1580,
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "createdAt": "2025-05-20T00:01:32.000Z",
      "updatedAt": "2025-05-20T00:02:56.942Z",
      "environment": "default"
    },
    {
      "id": "ef54f1a4",
      "timestamp": "2025-05-20T00:00:14.191000+00:00",
      "name": "ef54_py_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import numpy as np\nimport os\nimport time\nfrom ai_edge_litert.interpreter import Interpreter\nimport cv2\n\n# Phase 1: Setup\n## 1.1 Imports: Already done above\n\n## 1.2 Paths/Parameters\nmodel_path = 'models/ssd-mobilenet_v1/detect.tflite'\nlabel_map_path = 'models/ssd-mobilenet_v1/labelmap.txt'\nconfidence_threshold = 0.5\ninput_path = 'data/object_detection/sheeps.mp4'\noutput_path = 'results/object_detection/test_results/sheeps_detections.mp4'\n\n## 1.3 Load Labels (Conditional)\nif os.path.exists(label_map_path):\n    with open(label_map_path, 'r') as f:\n        labels = [line.strip() for line in f.readlines()]\nelse:\n    labels = []\n\n## 1.4 Load Interpreter\ninterpreter = Interpreter(model_path=model_path)\ninterpreter.allocate_tensors()\n\n## 1.5 Get Model Details\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\n# Phase 2: Input Acquisition & Preprocessing Loop\n## 2.1 Acquire Input Data\ncap = cv2.VideoCapture(input_path)\nif not cap.isOpened():\n    raise IOError(\"Cannot open video file\")\n\nframe_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\nframe_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\nfps = cap.get(cv2.CAP_PROP_FPS)\n\n## 2.2 Preprocess Data\ninput_shape = input_details[0]['shape']\npreprocessed_frames = []\n\n# Phase 3: Inference (Run per preprocessed input)\ndef preprocess_frame(frame):\n    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n    frame_resized = cv2.resize(frame_rgb, (input_shape[1], input_shape[2]))\n    return np.expand_dims(frame_resized, axis=0).astype(input_details[0]['dtype'])\n\n# Phase 4: Output Interpretation & Handling Loop\n## 4.1 Get Output Tensor(s)\ndef interpret_output(output_data):\n    detection_boxes = output_data[0][0]\n    detection_classes = output_data[1][0].astype(int)\n    detection_scores = output_data[2][0]\n    \n    results = []\n    for i in range(len(detection_scores)):\n        if detection_scores[i] > confidence_threshold:\n            class_id = detection_classes[i]\n            box = detection_boxes[i]\n            label = labels[class_id - 1] if class_id <= len(labels) else f'unknown_{class_id}'\n            score = detection_scores[i]\n            results.append((box, label, score))\n    return results\n\n## 4.2 Interpret Results\n## 4.3 Handle Output\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\nout = cv2.VideoWriter(output_path, fourcc, fps, (frame_width, frame_height))\n\nwhile cap.isOpened():\n    ret, frame = cap.read()\n    if not ret:\n        break\n    \n    input_data = preprocess_frame(frame)\n    interpreter.set_tensor(input_details[0]['index'], input_data)\n    interpreter.invoke()\n    \n    output_data = [interpreter.get_tensor(output_details[i]['index']) for i in range(len(output_details))]\n    results = interpret_output(output_data)\n    \n    for box, label, score in results:\n        ymin, xmin, ymax, xmax = box\n        (left, right, top, bottom) = (xmin * frame_width, xmax * frame_width,\n                                      ymin * frame_height, ymax * frame_height)\n        cv2.rectangle(frame, (int(left), int(top)), (int(right), int(bottom)), (0, 255, 0), 2)\n        label_text = f'{label}: {score:.2f}'\n        cv2.putText(frame, label_text, (int(left), int(top) - 10),\n                    cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n    \n    out.write(frame)\n\n# Phase 5: Cleanup\ncap.release()\nout.release()\ncv2.destroyAllWindows()"
      },
      "session_id": "qwen2.5-coder:32b_6eca_psg_batch",
      "metadata": {
        "num_run": 29
      },
      "tags": [
        "benchmark",
        "py_sketch_generator",
        "qwen2.5-coder:32b"
      ],
      "latency": 26.36,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-03-00-14-192891_chatcmpl-f618adcc-1037-4918-b924-98b895cf2624",
          "traceId": "ef54f1a4",
          "type": "GENERATION",
          "name": "ef54_psg_gen_attempt#1",
          "startTime": "2025-05-20T00:00:14.192000+00:00",
          "endTime": "2025-05-20T00:00:40.552000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {
            "temperature": "0.1",
            "top_p": "0.3"
          },
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 26360.0,
          "costDetails": {},
          "totalTokens": 2503,
          "completionTokens": 923,
          "promptTokens": 1580,
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "createdAt": "2025-05-20T00:00:15.000Z",
      "updatedAt": "2025-05-20T00:01:39.796Z",
      "environment": "default"
    },
    {
      "id": "0da4bd25",
      "timestamp": "2025-05-19T23:58:53.661000+00:00",
      "name": "0da4_py_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import numpy as np\nimport os\nimport time\nfrom ai_edge_litert.interpreter import Interpreter\nimport cv2\n\n# Phase 1: Setup\n## 1.1 Imports: Already done above\n\n## 1.2 Paths/Parameters\nmodel_path = 'models/ssd-mobilenet_v1/detect.tflite'\nlabel_map_path = 'models/ssd-mobilenet_v1/labelmap.txt'\nconfidence_threshold = 0.5\ninput_path = 'data/object_detection/sheeps.mp4'\noutput_path = 'results/object_detection/test_results/sheeps_detections.mp4'\n\n## 1.3 Load Labels (Conditional)\nif os.path.exists(label_map_path):\n    with open(label_map_path, 'r') as f:\n        labels = [line.strip() for line in f.readlines()]\nelse:\n    labels = []\n\n## 1.4 Load Interpreter\ninterpreter = Interpreter(model_path=model_path)\ninterpreter.allocate_tensors()\n\n## 1.5 Get Model Details\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\n# Phase 2: Input Acquisition & Preprocessing Loop\n## 2.1 Acquire Input Data\ncap = cv2.VideoCapture(input_path)\nif not cap.isOpened():\n    raise IOError(\"Cannot open video file\")\n\nframe_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\nframe_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\nfps = cap.get(cv2.CAP_PROP_FPS)\n\n## 2.2 Preprocess Data\ninput_shape = input_details[0]['shape']\npreprocessed_frames = []\n\n# Phase 3: Inference (Run per preprocessed input)\ndef preprocess_frame(frame):\n    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n    frame_resized = cv2.resize(frame_rgb, (input_shape[1], input_shape[2]))\n    return np.expand_dims(frame_resized, axis=0).astype(input_details[0]['dtype'])\n\n# Phase 4: Output Interpretation & Handling Loop\n## 4.1 Get Output Tensor(s)\ndef interpret_output(output_data):\n    detection_boxes = output_data[0][0]\n    detection_classes = output_data[1][0].astype(int)\n    detection_scores = output_data[2][0]\n    \n    results = []\n    for i in range(len(detection_scores)):\n        if detection_scores[i] > confidence_threshold:\n            class_id = detection_classes[i]\n            box = detection_boxes[i]\n            label = labels[class_id - 1] if class_id <= len(labels) else f'unknown_{class_id}'\n            score = detection_scores[i]\n            results.append((box, label, score))\n    return results\n\n## 4.2 Interpret Results\n## 4.3 Handle Output\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\nout = cv2.VideoWriter(output_path, fourcc, fps, (frame_width, frame_height))\n\nwhile cap.isOpened():\n    ret, frame = cap.read()\n    if not ret:\n        break\n    \n    input_data = preprocess_frame(frame)\n    interpreter.set_tensor(input_details[0]['index'], input_data)\n    interpreter.invoke()\n    \n    output_data = [interpreter.get_tensor(output_details[i]['index']) for i in range(len(output_details))]\n    results = interpret_output(output_data)\n    \n    for box, label, score in results:\n        ymin, xmin, ymax, xmax = box\n        (left, right, top, bottom) = (xmin * frame_width, xmax * frame_width,\n                                      ymin * frame_height, ymax * frame_height)\n        cv2.rectangle(frame, (int(left), int(top)), (int(right), int(bottom)), (0, 255, 0), 2)\n        label_text = f'{label}: {score:.2f}'\n        cv2.putText(frame, label_text, (int(left), int(top) - 10),\n                    cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n    \n    out.write(frame)\n\n# Phase 5: Cleanup\ncap.release()\nout.release()\ncv2.destroyAllWindows()"
      },
      "session_id": "qwen2.5-coder:32b_6eca_psg_batch",
      "metadata": {
        "num_run": 28
      },
      "tags": [
        "benchmark",
        "py_sketch_generator",
        "qwen2.5-coder:32b"
      ],
      "latency": 30.505,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-02-58-53-663481_chatcmpl-2993270c-3531-49d0-9b3f-a84089e0ae18",
          "traceId": "0da4bd25",
          "type": "GENERATION",
          "name": "0da4_psg_gen_attempt#1",
          "startTime": "2025-05-19T23:58:53.663000+00:00",
          "endTime": "2025-05-19T23:59:24.168000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {
            "temperature": "0.1",
            "top_p": "0.3"
          },
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 30505.0,
          "costDetails": {},
          "totalTokens": 2503,
          "completionTokens": 923,
          "promptTokens": 1580,
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "createdAt": "2025-05-19T23:58:55.000Z",
      "updatedAt": "2025-05-20T00:00:23.665Z",
      "environment": "default"
    },
    {
      "id": "a84ca79d",
      "timestamp": "2025-05-19T23:57:33.051000+00:00",
      "name": "a84c_py_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import numpy as np\nimport os\nimport time\nfrom ai_edge_litert.interpreter import Interpreter\nimport cv2\n\n# Phase 1: Setup\n## 1.1 Imports: Already done above\n\n## 1.2 Paths/Parameters\nmodel_path = 'models/ssd-mobilenet_v1/detect.tflite'\nlabel_map_path = 'models/ssd-mobilenet_v1/labelmap.txt'\nconfidence_threshold = 0.5\ninput_path = 'data/object_detection/sheeps.mp4'\noutput_path = 'results/object_detection/test_results/sheeps_detections.mp4'\n\n## 1.3 Load Labels (Conditional)\nif os.path.exists(label_map_path):\n    with open(label_map_path, 'r') as f:\n        labels = [line.strip() for line in f.readlines()]\nelse:\n    labels = []\n\n## 1.4 Load Interpreter\ninterpreter = Interpreter(model_path=model_path)\ninterpreter.allocate_tensors()\n\n## 1.5 Get Model Details\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\n# Phase 2: Input Acquisition & Preprocessing Loop\n## 2.1 Acquire Input Data\ncap = cv2.VideoCapture(input_path)\nif not cap.isOpened():\n    raise IOError(\"Cannot open video file\")\n\nframe_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\nframe_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\nfps = cap.get(cv2.CAP_PROP_FPS)\n\n## 2.2 Preprocess Data\ninput_shape = input_details[0]['shape']\npreprocessed_frames = []\n\n# Phase 3: Inference (Run per preprocessed input)\ndef preprocess_frame(frame):\n    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n    frame_resized = cv2.resize(frame_rgb, (input_shape[1], input_shape[2]))\n    return np.expand_dims(frame_resized, axis=0).astype(input_details[0]['dtype'])\n\n# Phase 4: Output Interpretation & Handling Loop\n## 4.1 Get Output Tensor(s)\ndef postprocess_output(output_data):\n    detection_boxes = output_data[0][0]\n    detection_classes = output_data[1][0].astype(int)\n    detection_scores = output_data[2][0]\n    \n    results = []\n    for i in range(len(detection_scores)):\n        if detection_scores[i] > confidence_threshold:\n            class_id = detection_classes[i]\n            box = detection_boxes[i]\n            label = labels[class_id] if class_id < len(labels) else f'unknown_{class_id}'\n            score = detection_scores[i]\n            results.append((box, label, score))\n    return results\n\n## 4.2 Interpret Results\ndef draw_results(frame, results):\n    for box, label, score in results:\n        ymin, xmin, ymax, xmax = box\n        (left, right, top, bottom) = (xmin * frame_width, xmax * frame_width,\n                                      ymin * frame_height, ymax * frame_height)\n        cv2.rectangle(frame, (int(left), int(top)), (int(right), int(bottom)), (0, 255, 0), 2)\n        label_text = f'{label}: {score:.2f}'\n        cv2.putText(frame, label_text, (int(left), int(top) - 10),\n                    cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n\n## 4.3 Handle Output\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\nout = cv2.VideoWriter(output_path, fourcc, fps, (frame_width, frame_height))\n\nwhile cap.isOpened():\n    ret, frame = cap.read()\n    if not ret:\n        break\n    \n    input_data = preprocess_frame(frame)\n    interpreter.set_tensor(input_details[0]['index'], input_data)\n    interpreter.invoke()\n    \n    output_data = [interpreter.get_tensor(output_details[i]['index']) for i in range(len(output_details))]\n    results = postprocess_output(output_data)\n    draw_results(frame, results)\n    \n    out.write(frame)\n\n# Phase 5: Cleanup\ncap.release()\nout.release()\ncv2.destroyAllWindows()"
      },
      "session_id": "qwen2.5-coder:32b_6eca_psg_batch",
      "metadata": {
        "num_run": 27
      },
      "tags": [
        "benchmark",
        "py_sketch_generator",
        "qwen2.5-coder:32b"
      ],
      "latency": 30.769,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-02-57-33-053159_chatcmpl-c3cef6f0-82a6-4186-b381-c73d0a488626",
          "traceId": "a84ca79d",
          "type": "GENERATION",
          "name": "a84c_psg_gen_attempt#1",
          "startTime": "2025-05-19T23:57:33.053000+00:00",
          "endTime": "2025-05-19T23:58:03.822000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {
            "temperature": "0.1",
            "top_p": "0.3"
          },
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 30769.0,
          "costDetails": {},
          "totalTokens": 2515,
          "completionTokens": 935,
          "promptTokens": 1580,
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "createdAt": "2025-05-19T23:57:34.000Z",
      "updatedAt": "2025-05-19T23:59:02.957Z",
      "environment": "default"
    },
    {
      "id": "0aa4f25b",
      "timestamp": "2025-05-19T23:55:04.461000+00:00",
      "name": "0aa4_py_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import numpy as np\nimport os\nimport time\nfrom ai_edge_litert.interpreter import Interpreter\nimport cv2\n\n# Phase 1: Setup\n## 1.1 Imports: Already done above\n\n## 1.2 Paths/Parameters\nmodel_path = 'models/ssd-mobilenet_v1/detect.tflite'\nlabel_map_path = 'models/ssd-mobilenet_v1/labelmap.txt'\nconfidence_threshold = 0.5\ninput_path = 'data/object_detection/sheeps.mp4'\noutput_path = 'results/object_detection/test_results/sheeps_detections.mp4'\n\n## 1.3 Load Labels (Conditional)\nif os.path.exists(label_map_path):\n    with open(label_map_path, 'r') as f:\n        labels = [line.strip() for line in f.readlines()]\nelse:\n    labels = []\n\n## 1.4 Load Interpreter\ninterpreter = Interpreter(model_path=model_path)\ninterpreter.allocate_tensors()\n\n## 1.5 Get Model Details\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\n# Phase 2: Input Acquisition & Preprocessing Loop\n## 2.1 Acquire Input Data\ncap = cv2.VideoCapture(input_path)\nif not cap.isOpened():\n    raise IOError(\"Cannot open video file\")\n\nframe_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\nframe_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\nfps = cap.get(cv2.CAP_PROP_FPS)\n\n## 2.2 Preprocess Data\ninput_shape = input_details[0]['shape']\npreprocessed_frames = []\n\n# Phase 3: Inference (Run per preprocessed input)\ndef preprocess_frame(frame):\n    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n    frame_resized = cv2.resize(frame_rgb, (input_shape[1], input_shape[2]))\n    return np.expand_dims(frame_resized, axis=0).astype(input_details[0]['dtype'])\n\n# Phase 4: Output Interpretation & Handling Loop\n## 4.1 Get Output Tensor(s)\ndef postprocess_output(output_data):\n    detection_boxes = output_data[0][0]\n    detection_classes = output_data[1][0].astype(int)\n    detection_scores = output_data[2][0]\n    \n    results = []\n    for i in range(len(detection_scores)):\n        if detection_scores[i] > confidence_threshold:\n            class_id = detection_classes[i]\n            box = detection_boxes[i]\n            label = labels[class_id] if class_id < len(labels) else f'unknown_{class_id}'\n            score = detection_scores[i]\n            results.append((box, label, score))\n    return results\n\n## 4.2 Interpret Results\ndef draw_results(frame, results):\n    for box, label, score in results:\n        ymin, xmin, ymax, xmax = box\n        (left, right, top, bottom) = (xmin * frame_width, xmax * frame_width,\n                                      ymin * frame_height, ymax * frame_height)\n        cv2.rectangle(frame, (int(left), int(top)), (int(right), int(bottom)), (0, 255, 0), 2)\n        label_text = f'{label}: {score:.2f}'\n        cv2.putText(frame, label_text, (int(left), int(top) - 10),\n                    cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n\n## 4.3 Handle Output\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\nout = cv2.VideoWriter(output_path, fourcc, fps, (frame_width, frame_height))\n\nwhile cap.isOpened():\n    ret, frame = cap.read()\n    if not ret:\n        break\n    \n    input_data = preprocess_frame(frame)\n    interpreter.set_tensor(input_details[0]['index'], input_data)\n    interpreter.invoke()\n    \n    output_data = [interpreter.get_tensor(output_details[i]['index']) for i in range(len(output_details))]\n    results = postprocess_output(output_data)\n    draw_results(frame, results)\n    \n    out.write(frame)\n\n# Phase 5: Cleanup\ncap.release()\nout.release()\ncv2.destroyAllWindows()"
      },
      "session_id": "qwen2.5-coder:32b_6eca_psg_batch",
      "metadata": {
        "num_run": 26
      },
      "tags": [
        "benchmark",
        "py_sketch_generator",
        "qwen2.5-coder:32b"
      ],
      "latency": 99.234,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-02-55-04-463009_chatcmpl-959890f9-95b9-4d69-a740-a34b5ed4b557",
          "traceId": "0aa4f25b",
          "type": "GENERATION",
          "name": "0aa4_psg_gen_attempt#1",
          "startTime": "2025-05-19T23:55:04.463000+00:00",
          "endTime": "2025-05-19T23:56:43.697000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {
            "temperature": "0.1",
            "top_p": "0.3"
          },
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 99234.0,
          "costDetails": {},
          "totalTokens": 2515,
          "completionTokens": 935,
          "promptTokens": 1580,
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "createdAt": "2025-05-19T23:55:06.000Z",
      "updatedAt": "2025-05-19T23:57:42.343Z",
      "environment": "default"
    },
    {
      "id": "68727757",
      "timestamp": "2025-05-19T23:53:27.928000+00:00",
      "name": "6872_py_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import numpy as np\nimport os\nimport time\nfrom ai_edge_litert.interpreter import Interpreter\nimport cv2\n\n# Phase 1: Setup\n## 1.1 Imports: Already done above\n\n## 1.2 Paths/Parameters\nmodel_path = 'models/ssd-mobilenet_v1/detect.tflite'\nlabel_map_path = 'models/ssd-mobilenet_v1/labelmap.txt'\nconfidence_threshold = 0.5\ninput_path = 'data/object_detection/sheeps.mp4'\noutput_path = 'results/object_detection/test_results/sheeps_detections.mp4'\n\n## 1.3 Load Labels (Conditional)\nif os.path.exists(label_map_path):\n    with open(label_map_path, 'r') as f:\n        labels = [line.strip() for line in f.readlines()]\nelse:\n    labels = []\n\n## 1.4 Load Interpreter\ninterpreter = Interpreter(model_path=model_path)\ninterpreter.allocate_tensors()\n\n## 1.5 Get Model Details\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\n# Phase 2: Input Acquisition & Preprocessing Loop\n## 2.1 Acquire Input Data\ncap = cv2.VideoCapture(input_path)\nif not cap.isOpened():\n    raise IOError(\"Cannot open video file\")\n\nframe_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\nframe_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\nfps = cap.get(cv2.CAP_PROP_FPS)\n\n## 2.2 Preprocess Data\ninput_shape = input_details[0]['shape']\npreprocessed_frames = []\n\n# Phase 3: Inference (Run per preprocessed input)\ndef preprocess_frame(frame):\n    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n    frame_resized = cv2.resize(frame_rgb, (input_shape[1], input_shape[2]))\n    return np.expand_dims(frame_resized, axis=0).astype(input_details[0]['dtype'])\n\n# Phase 4: Output Interpretation & Handling Loop\n## 4.1 Get Output Tensor(s)\ndef postprocess_output(output_data):\n    detection_boxes = output_data[0][0]\n    detection_classes = output_data[1][0].astype(int)\n    detection_scores = output_data[2][0]\n    \n    results = []\n    for i in range(len(detection_scores)):\n        if detection_scores[i] > confidence_threshold:\n            class_id = detection_classes[i]\n            box = detection_boxes[i]\n            label = labels[class_id] if class_id < len(labels) else f'unknown_{class_id}'\n            score = detection_scores[i]\n            results.append((box, label, score))\n    return results\n\n## 4.2 Interpret Results\ndef draw_results(frame, results):\n    for box, label, score in results:\n        ymin, xmin, ymax, xmax = box\n        (left, right, top, bottom) = (xmin * frame_width, xmax * frame_width,\n                                      ymin * frame_height, ymax * frame_height)\n        cv2.rectangle(frame, (int(left), int(top)), (int(right), int(bottom)), (0, 255, 0), 2)\n        label_text = f'{label}: {score:.2f}'\n        cv2.putText(frame, label_text, (int(left), int(top) - 10),\n                    cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n\n## 4.3 Handle Output\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\nout = cv2.VideoWriter(output_path, fourcc, fps, (frame_width, frame_height))\n\nwhile cap.isOpened():\n    ret, frame = cap.read()\n    if not ret:\n        break\n    \n    input_data = preprocess_frame(frame)\n    interpreter.set_tensor(input_details[0]['index'], input_data)\n    interpreter.invoke()\n    \n    output_data = [interpreter.get_tensor(output_details[i]['index']) for i in range(len(output_details))]\n    results = postprocess_output(output_data)\n    draw_results(frame, results)\n    \n    out.write(frame)\n\n# Phase 5: Cleanup\ncap.release()\nout.release()\ncv2.destroyAllWindows()"
      },
      "session_id": "qwen2.5-coder:32b_6eca_psg_batch",
      "metadata": {
        "num_run": 25
      },
      "tags": [
        "benchmark",
        "py_sketch_generator",
        "qwen2.5-coder:32b"
      ],
      "latency": 47.022,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-02-53-27-929591_chatcmpl-134b71cf-e0f7-4930-a987-df97e739897e",
          "traceId": "68727757",
          "type": "GENERATION",
          "name": "6872_psg_gen_attempt#1",
          "startTime": "2025-05-19T23:53:27.929000+00:00",
          "endTime": "2025-05-19T23:54:14.951000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {
            "temperature": "0.1",
            "top_p": "0.3"
          },
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 47022.0,
          "costDetails": {},
          "totalTokens": 2515,
          "completionTokens": 935,
          "promptTokens": 1580,
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "createdAt": "2025-05-19T23:53:29.000Z",
      "updatedAt": "2025-05-19T23:55:14.102Z",
      "environment": "default"
    },
    {
      "id": "0f0e5a49",
      "timestamp": "2025-05-19T23:51:30.394000+00:00",
      "name": "0f0e_py_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import numpy as np\nimport os\nimport time\nfrom ai_edge_litert.interpreter import Interpreter\nimport cv2\n\n# Phase 1: Setup\n## 1.1 Imports: Already done above\n\n## 1.2 Paths/Parameters\nmodel_path = 'models/ssd-mobilenet_v1/detect.tflite'\nlabel_map_path = 'models/ssd-mobilenet_v1/labelmap.txt'\nconfidence_threshold = 0.5\ninput_path = 'data/object_detection/sheeps.mp4'\noutput_path = 'results/object_detection/test_results/sheeps_detections.mp4'\n\n## 1.3 Load Labels (Conditional)\nif os.path.exists(label_map_path):\n    with open(label_map_path, 'r') as f:\n        labels = [line.strip() for line in f.readlines()]\nelse:\n    labels = []\n\n## 1.4 Load Interpreter\ninterpreter = Interpreter(model_path=model_path)\ninterpreter.allocate_tensors()\n\n## 1.5 Get Model Details\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\n# Phase 2: Input Acquisition & Preprocessing Loop\n## 2.1 Acquire Input Data\ncap = cv2.VideoCapture(input_path)\nif not cap.isOpened():\n    raise IOError(\"Cannot open video file\")\n\nframe_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\nframe_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\nfps = cap.get(cv2.CAP_PROP_FPS)\n\n## 2.2 Preprocess Data\ninput_shape = input_details[0]['shape']\npreprocessed_frames = []\n\n# Phase 3: Inference (Run per preprocessed input)\ndef preprocess_frame(frame):\n    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n    frame_resized = cv2.resize(frame_rgb, (input_shape[1], input_shape[2]))\n    return np.expand_dims(frame_resized, axis=0).astype(input_details[0]['dtype'])\n\n# Phase 4: Output Interpretation & Handling Loop\n## 4.1 Get Output Tensor(s)\ndef interpret_output(output_data):\n    detection_boxes = output_data[0][0]\n    detection_classes = output_data[1][0].astype(int)\n    detection_scores = output_data[2][0]\n    \n    results = []\n    for i in range(len(detection_scores)):\n        if detection_scores[i] > confidence_threshold:\n            class_id = detection_classes[i]\n            box = detection_boxes[i]\n            label = labels[class_id - 1] if class_id <= len(labels) else f'unknown_{class_id}'\n            score = detection_scores[i]\n            results.append((box, label, score))\n    return results\n\n## 4.2 Interpret Results\n## 4.3 Handle Output\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\nout = cv2.VideoWriter(output_path, fourcc, fps, (frame_width, frame_height))\n\nwhile cap.isOpened():\n    ret, frame = cap.read()\n    if not ret:\n        break\n    \n    input_data = preprocess_frame(frame)\n    interpreter.set_tensor(input_details[0]['index'], input_data)\n    interpreter.invoke()\n    \n    output_data = [interpreter.get_tensor(output_details[i]['index']) for i in range(len(output_details))]\n    results = interpret_output(output_data)\n    \n    for box, label, score in results:\n        ymin, xmin, ymax, xmax = box\n        (left, right, top, bottom) = (xmin * frame_width, xmax * frame_width,\n                                      ymin * frame_height, ymax * frame_height)\n        cv2.rectangle(frame, (int(left), int(top)), (int(right), int(bottom)), (0, 255, 0), 2)\n        label_text = f'{label}: {score:.2f}'\n        cv2.putText(frame, label_text, (int(left), int(top) - 10),\n                    cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n    \n    out.write(frame)\n\n# Phase 5: Cleanup\ncap.release()\nout.release()\ncv2.destroyAllWindows()"
      },
      "session_id": "qwen2.5-coder:32b_6eca_psg_batch",
      "metadata": {
        "num_run": 24
      },
      "tags": [
        "benchmark",
        "py_sketch_generator",
        "qwen2.5-coder:32b"
      ],
      "latency": 68.563,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-02-51-30-396243_chatcmpl-c644dd5d-02c7-490f-a770-9da733896e0e",
          "traceId": "0f0e5a49",
          "type": "GENERATION",
          "name": "0f0e_psg_gen_attempt#1",
          "startTime": "2025-05-19T23:51:30.396000+00:00",
          "endTime": "2025-05-19T23:52:38.959000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {
            "temperature": "0.1",
            "top_p": "0.3"
          },
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 68563.0,
          "costDetails": {},
          "totalTokens": 2503,
          "completionTokens": 923,
          "promptTokens": 1580,
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "createdAt": "2025-05-19T23:51:31.000Z",
      "updatedAt": "2025-05-19T23:53:37.813Z",
      "environment": "default"
    },
    {
      "id": "5d9a13e7",
      "timestamp": "2025-05-19T23:49:48.802000+00:00",
      "name": "5d9a_py_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import numpy as np\nimport os\nimport time\nfrom ai_edge_litert.interpreter import Interpreter\nimport cv2\n\n# Phase 1: Setup\n## 1.1 Imports: Already done above\n\n## 1.2 Paths/Parameters\nmodel_path = 'models/ssd-mobilenet_v1/detect.tflite'\nlabel_map_path = 'models/ssd-mobilenet_v1/labelmap.txt'\nconfidence_threshold = 0.5\ninput_path = 'data/object_detection/sheeps.mp4'\noutput_path = 'results/object_detection/test_results/sheeps_detections.mp4'\n\n## 1.3 Load Labels (Conditional)\nif os.path.exists(label_map_path):\n    with open(label_map_path, 'r') as f:\n        labels = [line.strip() for line in f.readlines()]\nelse:\n    labels = []\n\n## 1.4 Load Interpreter\ninterpreter = Interpreter(model_path=model_path)\ninterpreter.allocate_tensors()\n\n## 1.5 Get Model Details\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\n# Phase 2: Input Acquisition & Preprocessing Loop\n## 2.1 Acquire Input Data\ncap = cv2.VideoCapture(input_path)\nif not cap.isOpened():\n    raise IOError(\"Cannot open video file\")\n\nframe_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\nframe_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\nfps = cap.get(cv2.CAP_PROP_FPS)\n\n## 2.2 Preprocess Data\ninput_shape = input_details[0]['shape']\npreprocessed_frames = []\n\n# Phase 3: Inference (Run per preprocessed input)\ndef preprocess_frame(frame):\n    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n    frame_resized = cv2.resize(frame_rgb, (input_shape[1], input_shape[2]))\n    return np.expand_dims(frame_resized, axis=0).astype(input_details[0]['dtype'])\n\n# Phase 4: Output Interpretation & Handling Loop\n## 4.1 Get Output Tensor(s)\ndef interpret_output(output_data):\n    detection_boxes = output_data[0][0]\n    detection_classes = output_data[1][0].astype(int)\n    detection_scores = output_data[2][0]\n    \n    results = []\n    for i in range(len(detection_scores)):\n        if detection_scores[i] > confidence_threshold:\n            class_id = detection_classes[i]\n            box = detection_boxes[i]\n            label = labels[class_id - 1] if class_id <= len(labels) else f'unknown_{class_id}'\n            score = detection_scores[i]\n            results.append((box, label, score))\n    return results\n\n## 4.2 Interpret Results\n## 4.3 Handle Output\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\nout = cv2.VideoWriter(output_path, fourcc, fps, (frame_width, frame_height))\n\nwhile cap.isOpened():\n    ret, frame = cap.read()\n    if not ret:\n        break\n    \n    input_data = preprocess_frame(frame)\n    interpreter.set_tensor(input_details[0]['index'], input_data)\n    interpreter.invoke()\n    \n    output_data = [interpreter.get_tensor(output_details[i]['index']) for i in range(len(output_details))]\n    results = interpret_output(output_data)\n    \n    for box, label, score in results:\n        ymin, xmin, ymax, xmax = box\n        (left, right, top, bottom) = (xmin * frame_width, xmax * frame_width,\n                                      ymin * frame_height, ymax * frame_height)\n        cv2.rectangle(frame, (int(left), int(top)), (int(right), int(bottom)), (0, 255, 0), 2)\n        label_text = f'{label}: {score:.2f}'\n        cv2.putText(frame, label_text, (int(left), int(top) - 10),\n                    cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n    \n    out.write(frame)\n\n# Phase 5: Cleanup\ncap.release()\nout.release()\ncv2.destroyAllWindows()"
      },
      "session_id": "qwen2.5-coder:32b_6eca_psg_batch",
      "metadata": {
        "num_run": 23
      },
      "tags": [
        "benchmark",
        "py_sketch_generator",
        "qwen2.5-coder:32b"
      ],
      "latency": 51.934,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-02-49-48-803897_chatcmpl-1aac27ce-cf59-4ce6-a729-6964ef428465",
          "traceId": "5d9a13e7",
          "type": "GENERATION",
          "name": "5d9a_psg_gen_attempt#1",
          "startTime": "2025-05-19T23:49:48.803000+00:00",
          "endTime": "2025-05-19T23:50:40.737000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {
            "temperature": "0.1",
            "top_p": "0.3"
          },
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 51934.0,
          "costDetails": {},
          "totalTokens": 2503,
          "completionTokens": 923,
          "promptTokens": 1580,
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "createdAt": "2025-05-19T23:49:50.000Z",
      "updatedAt": "2025-05-19T23:51:39.081Z",
      "environment": "default"
    },
    {
      "id": "6cc8b3d6",
      "timestamp": "2025-05-19T23:48:32.309000+00:00",
      "name": "6cc8_py_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import numpy as np\nimport os\nimport time\nfrom ai_edge_litert.interpreter import Interpreter\nimport cv2\n\n# Phase 1: Setup\n## 1.1 Imports: Already done above\n\n## 1.2 Paths/Parameters\nmodel_path = 'models/ssd-mobilenet_v1/detect.tflite'\nlabel_map_path = 'models/ssd-mobilenet_v1/labelmap.txt'\nconfidence_threshold = 0.5\ninput_path = 'data/object_detection/sheeps.mp4'\noutput_path = 'results/object_detection/test_results/sheeps_detections.mp4'\n\n## 1.3 Load Labels (Conditional)\nif os.path.exists(label_map_path):\n    with open(label_map_path, 'r') as f:\n        labels = [line.strip() for line in f.readlines()]\nelse:\n    labels = []\n\n## 1.4 Load Interpreter\ninterpreter = Interpreter(model_path=model_path)\ninterpreter.allocate_tensors()\n\n## 1.5 Get Model Details\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\n# Phase 2: Input Acquisition & Preprocessing Loop\n## 2.1 Acquire Input Data\ncap = cv2.VideoCapture(input_path)\nif not cap.isOpened():\n    raise IOError(\"Cannot open video file\")\n\nframe_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\nframe_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\nfps = cap.get(cv2.CAP_PROP_FPS)\n\n## 2.2 Preprocess Data\ninput_shape = input_details[0]['shape']\npreprocessed_frames = []\n\n# Phase 3: Inference (Run per preprocessed input)\ndef preprocess_frame(frame):\n    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n    frame_resized = cv2.resize(frame_rgb, (input_shape[1], input_shape[2]))\n    return np.expand_dims(frame_resized, axis=0).astype(input_details[0]['dtype'])\n\n# Phase 4: Output Interpretation & Handling Loop\n## 4.1 Get Output Tensor(s)\ndef postprocess_output(output_data):\n    detection_boxes = output_data[0][0]\n    detection_classes = output_data[1][0].astype(int)\n    detection_scores = output_data[2][0]\n    \n    results = []\n    for i in range(len(detection_scores)):\n        if detection_scores[i] > confidence_threshold:\n            class_id = detection_classes[i]\n            box = detection_boxes[i]\n            label = labels[class_id] if class_id < len(labels) else f'unknown_{class_id}'\n            score = detection_scores[i]\n            results.append((box, label, score))\n    return results\n\n## 4.2 Interpret Results\ndef draw_results(frame, results):\n    for box, label, score in results:\n        ymin, xmin, ymax, xmax = box\n        (left, right, top, bottom) = (xmin * frame_width, xmax * frame_width,\n                                      ymin * frame_height, ymax * frame_height)\n        cv2.rectangle(frame, (int(left), int(top)), (int(right), int(bottom)), (0, 255, 0), 2)\n        label_text = f'{label}: {score:.2f}'\n        cv2.putText(frame, label_text, (int(left), int(top) - 10),\n                    cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n\n## 4.3 Handle Output\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\nout = cv2.VideoWriter(output_path, fourcc, fps, (frame_width, frame_height))\n\nwhile cap.isOpened():\n    ret, frame = cap.read()\n    if not ret:\n        break\n    \n    input_data = preprocess_frame(frame)\n    interpreter.set_tensor(input_details[0]['index'], input_data)\n    interpreter.invoke()\n    \n    output_data = [interpreter.get_tensor(output_details[i]['index']) for i in range(len(output_details))]\n    results = postprocess_output(output_data)\n    draw_results(frame, results)\n    \n    out.write(frame)\n\n# Phase 5: Cleanup\ncap.release()\nout.release()\ncv2.destroyAllWindows()"
      },
      "session_id": "qwen2.5-coder:32b_6eca_psg_batch",
      "metadata": {
        "num_run": 22
      },
      "tags": [
        "benchmark",
        "py_sketch_generator",
        "qwen2.5-coder:32b"
      ],
      "latency": 26.785,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-02-48-32-310448_chatcmpl-4a216eb1-4f18-4c24-b365-ed11d9f47027",
          "traceId": "6cc8b3d6",
          "type": "GENERATION",
          "name": "6cc8_psg_gen_attempt#1",
          "startTime": "2025-05-19T23:48:32.310000+00:00",
          "endTime": "2025-05-19T23:48:59.095000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {
            "temperature": "0.1",
            "top_p": "0.3"
          },
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 26785.0,
          "costDetails": {},
          "totalTokens": 2515,
          "completionTokens": 935,
          "promptTokens": 1580,
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "createdAt": "2025-05-19T23:48:33.000Z",
      "updatedAt": "2025-05-19T23:49:58.590Z",
      "environment": "default"
    },
    {
      "id": "4f1ab7c2",
      "timestamp": "2025-05-19T23:47:16.753000+00:00",
      "name": "4f1a_py_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import numpy as np\nimport os\nimport time\nfrom ai_edge_litert.interpreter import Interpreter\nimport cv2\n\n# Phase 1: Setup\n## 1.1 Imports: Already done above\n\n## 1.2 Paths/Parameters\nmodel_path = 'models/ssd-mobilenet_v1/detect.tflite'\nlabel_map_path = 'models/ssd-mobilenet_v1/labelmap.txt'\nconfidence_threshold = 0.5\ninput_path = 'data/object_detection/sheeps.mp4'\noutput_path = 'results/object_detection/test_results/sheeps_detections.mp4'\n\n## 1.3 Load Labels (Conditional)\nif os.path.exists(label_map_path):\n    with open(label_map_path, 'r') as f:\n        labels = [line.strip() for line in f.readlines()]\nelse:\n    labels = []\n\n## 1.4 Load Interpreter\ninterpreter = Interpreter(model_path=model_path)\ninterpreter.allocate_tensors()\n\n## 1.5 Get Model Details\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\n# Phase 2: Input Acquisition & Preprocessing Loop\n## 2.1 Acquire Input Data\ncap = cv2.VideoCapture(input_path)\nif not cap.isOpened():\n    raise IOError(\"Cannot open video file\")\n\nframe_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\nframe_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\nfps = cap.get(cv2.CAP_PROP_FPS)\n\n## 2.2 Preprocess Data\ninput_shape = input_details[0]['shape']\npreprocessed_frames = []\n\n# Phase 3: Inference (Run per preprocessed input)\ndef preprocess_frame(frame):\n    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n    frame_resized = cv2.resize(frame_rgb, (input_shape[1], input_shape[2]))\n    return np.expand_dims(frame_resized, axis=0).astype(input_details[0]['dtype'])\n\n# Phase 4: Output Interpretation & Handling Loop\n## 4.1 Get Output Tensor(s)\ndef interpret_output(output_data):\n    detection_boxes = output_data[0][0]\n    detection_classes = output_data[1][0].astype(int)\n    detection_scores = output_data[2][0]\n    \n    results = []\n    for i in range(len(detection_scores)):\n        if detection_scores[i] > confidence_threshold:\n            class_id = detection_classes[i]\n            box = detection_boxes[i]\n            label = labels[class_id - 1] if class_id <= len(labels) else f'unknown_{class_id}'\n            score = detection_scores[i]\n            results.append((box, label, score))\n    return results\n\n## 4.2 Interpret Results\n## 4.3 Handle Output\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\nout = cv2.VideoWriter(output_path, fourcc, fps, (frame_width, frame_height))\n\nwhile cap.isOpened():\n    ret, frame = cap.read()\n    if not ret:\n        break\n    \n    input_data = preprocess_frame(frame)\n    interpreter.set_tensor(input_details[0]['index'], input_data)\n    interpreter.invoke()\n    \n    output_data = [interpreter.get_tensor(output_details[i]['index']) for i in range(len(output_details))]\n    results = interpret_output(output_data)\n    \n    for box, label, score in results:\n        ymin, xmin, ymax, xmax = box\n        (left, right, top, bottom) = (xmin * frame_width, xmax * frame_width,\n                                      ymin * frame_height, ymax * frame_height)\n        cv2.rectangle(frame, (int(left), int(top)), (int(right), int(bottom)), (0, 255, 0), 2)\n        label_text = f'{label}: {score:.2f}'\n        cv2.putText(frame, label_text, (int(left), int(top) - 10),\n                    cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n    \n    out.write(frame)\n\n# Phase 5: Cleanup\ncap.release()\nout.release()\ncv2.destroyAllWindows()"
      },
      "session_id": "qwen2.5-coder:32b_6eca_psg_batch",
      "metadata": {
        "num_run": 21
      },
      "tags": [
        "benchmark",
        "py_sketch_generator",
        "qwen2.5-coder:32b"
      ],
      "latency": 26.357,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-02-47-16-755097_chatcmpl-689b18ed-f4c5-4d83-bc4e-12bdcced49bd",
          "traceId": "4f1ab7c2",
          "type": "GENERATION",
          "name": "4f1a_psg_gen_attempt#1",
          "startTime": "2025-05-19T23:47:16.755000+00:00",
          "endTime": "2025-05-19T23:47:43.112000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {
            "temperature": "0.1",
            "top_p": "0.3"
          },
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 26357.0,
          "costDetails": {},
          "totalTokens": 2503,
          "completionTokens": 923,
          "promptTokens": 1580,
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "createdAt": "2025-05-19T23:47:18.000Z",
      "updatedAt": "2025-05-19T23:48:41.685Z",
      "environment": "default"
    },
    {
      "id": "7f199541",
      "timestamp": "2025-05-19T23:46:00.231000+00:00",
      "name": "7f19_py_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import numpy as np\nimport os\nimport time\nfrom ai_edge_litert.interpreter import Interpreter\nimport cv2\n\n# Phase 1: Setup\n## 1.1 Imports: Already done above\n\n## 1.2 Paths/Parameters\nmodel_path = 'models/ssd-mobilenet_v1/detect.tflite'\nlabel_map_path = 'models/ssd-mobilenet_v1/labelmap.txt'\nconfidence_threshold = 0.5\ninput_path = 'data/object_detection/sheeps.mp4'\noutput_path = 'results/object_detection/test_results/sheeps_detections.mp4'\n\n## 1.3 Load Labels (Conditional)\nif os.path.exists(label_map_path):\n    with open(label_map_path, 'r') as f:\n        labels = [line.strip() for line in f.readlines()]\nelse:\n    labels = []\n\n## 1.4 Load Interpreter\ninterpreter = Interpreter(model_path=model_path)\ninterpreter.allocate_tensors()\n\n## 1.5 Get Model Details\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\n# Phase 2: Input Acquisition & Preprocessing Loop\n## 2.1 Acquire Input Data\ncap = cv2.VideoCapture(input_path)\nif not cap.isOpened():\n    raise IOError(\"Cannot open video file\")\n\nframe_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\nframe_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\nfps = cap.get(cv2.CAP_PROP_FPS)\n\n## 2.2 Preprocess Data\ninput_shape = input_details[0]['shape']\npreprocessed_frames = []\n\n# Phase 3: Inference (Run per preprocessed input)\ndef preprocess_frame(frame):\n    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n    frame_resized = cv2.resize(frame_rgb, (input_shape[1], input_shape[2]))\n    return np.expand_dims(frame_resized, axis=0).astype(input_details[0]['dtype'])\n\n# Phase 4: Output Interpretation & Handling Loop\n## 4.1 Get Output Tensor(s)\ndef postprocess_output(output_data):\n    detection_boxes = output_data[0][0]\n    detection_classes = output_data[1][0].astype(int)\n    detection_scores = output_data[2][0]\n    \n    results = []\n    for i in range(len(detection_scores)):\n        if detection_scores[i] > confidence_threshold:\n            class_id = detection_classes[i]\n            box = detection_boxes[i]\n            label = labels[class_id] if class_id < len(labels) else f'unknown_{class_id}'\n            score = detection_scores[i]\n            results.append((box, label, score))\n    return results\n\n## 4.2 Interpret Results\ndef draw_results(frame, results):\n    for box, label, score in results:\n        ymin, xmin, ymax, xmax = box\n        height, width, _ = frame.shape\n        (xminn, xmaxx, yminn, ymaxx) = (int(xmin * width), int(xmax * width),\n                                        int(ymin * height), int(ymax * height))\n        \n        cv2.rectangle(frame, (xminn, yminn), (xmaxx, ymaxx), (0, 255, 0), 2)\n        label_text = f'{label}: {score:.2f}'\n        cv2.putText(frame, label_text, (xminn, yminn - 10),\n                    cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n\n## 4.3 Handle Output\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\nout = cv2.VideoWriter(output_path, fourcc, fps, (frame_width, frame_height))\n\nwhile cap.isOpened():\n    ret, frame = cap.read()\n    if not ret:\n        break\n    \n    input_data = preprocess_frame(frame)\n    interpreter.set_tensor(input_details[0]['index'], input_data)\n    interpreter.invoke()\n    \n    output_data = [interpreter.get_tensor(output_details[i]['index']) for i in range(len(output_details))]\n    results = postprocess_output(output_data)\n    draw_results(frame, results)\n    \n    out.write(frame)\n\n# Phase 5: Cleanup\ncap.release()\nout.release()\ncv2.destroyAllWindows()"
      },
      "session_id": "qwen2.5-coder:32b_6eca_psg_batch",
      "metadata": {
        "num_run": 20
      },
      "tags": [
        "benchmark",
        "py_sketch_generator",
        "qwen2.5-coder:32b"
      ],
      "latency": 27.181,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-02-46-00-233042_chatcmpl-49de3135-b6d7-478a-8ad6-a92182187fbf",
          "traceId": "7f199541",
          "type": "GENERATION",
          "name": "7f19_psg_gen_attempt#1",
          "startTime": "2025-05-19T23:46:00.233000+00:00",
          "endTime": "2025-05-19T23:46:27.414000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {
            "temperature": "0.1",
            "top_p": "0.3"
          },
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 27181.0,
          "costDetails": {},
          "totalTokens": 2533,
          "completionTokens": 953,
          "promptTokens": 1580,
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "createdAt": "2025-05-19T23:46:01.000Z",
      "updatedAt": "2025-05-19T23:47:26.106Z",
      "environment": "default"
    },
    {
      "id": "7c8ce682",
      "timestamp": "2025-05-19T23:44:43.650000+00:00",
      "name": "7c8c_py_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import numpy as np\nimport os\nimport time\nfrom ai_edge_litert.interpreter import Interpreter\nimport cv2\n\n# Phase 1: Setup\n## 1.1 Imports: Already done above\n\n## 1.2 Paths/Parameters\nmodel_path = 'models/ssd-mobilenet_v1/detect.tflite'\nlabel_map_path = 'models/ssd-mobilenet_v1/labelmap.txt'\nconfidence_threshold = 0.5\ninput_path = 'data/object_detection/sheeps.mp4'\noutput_path = 'results/object_detection/test_results/sheeps_detections.mp4'\n\n## 1.3 Load Labels (Conditional)\nif os.path.exists(label_map_path):\n    with open(label_map_path, 'r') as f:\n        labels = [line.strip() for line in f.readlines()]\nelse:\n    labels = []\n\n## 1.4 Load Interpreter\ninterpreter = Interpreter(model_path=model_path)\ninterpreter.allocate_tensors()\n\n## 1.5 Get Model Details\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\n# Phase 2: Input Acquisition & Preprocessing Loop\n## 2.1 Acquire Input Data\ncap = cv2.VideoCapture(input_path)\nif not cap.isOpened():\n    raise IOError(\"Cannot open video file\")\n\nframe_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\nframe_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\nfps = cap.get(cv2.CAP_PROP_FPS)\n\n## 2.2 Preprocess Data\ninput_shape = input_details[0]['shape']\npreprocessed_frames = []\n\n# Phase 3: Inference (Run per preprocessed input)\ndef preprocess_frame(frame):\n    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n    frame_resized = cv2.resize(frame_rgb, (input_shape[1], input_shape[2]))\n    return np.expand_dims(frame_resized, axis=0).astype(input_details[0]['dtype'])\n\n# Phase 4: Output Interpretation & Handling Loop\n## 4.1 Get Output Tensor(s)\ndef postprocess_output(output_data):\n    detection_boxes = output_data[0][0]\n    detection_classes = output_data[1][0].astype(int)\n    detection_scores = output_data[2][0]\n    \n    results = []\n    for i in range(len(detection_scores)):\n        if detection_scores[i] > confidence_threshold:\n            class_id = detection_classes[i]\n            box = detection_boxes[i]\n            label = labels[class_id] if class_id < len(labels) else f'unknown_{class_id}'\n            score = detection_scores[i]\n            results.append((box, label, score))\n    return results\n\n## 4.2 Interpret Results\ndef draw_results(frame, results):\n    for box, label, score in results:\n        ymin, xmin, ymax, xmax = box\n        (left, right, top, bottom) = (xmin * frame_width, xmax * frame_width,\n                                      ymin * frame_height, ymax * frame_height)\n        cv2.rectangle(frame, (int(left), int(top)), (int(right), int(bottom)), (0, 255, 0), 2)\n        label_text = f'{label}: {score:.2f}'\n        cv2.putText(frame, label_text, (int(left), int(top) - 10),\n                    cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n\n## 4.3 Handle Output\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\nout = cv2.VideoWriter(output_path, fourcc, fps, (frame_width, frame_height))\n\nwhile cap.isOpened():\n    ret, frame = cap.read()\n    if not ret:\n        break\n    \n    input_data = preprocess_frame(frame)\n    interpreter.set_tensor(input_details[0]['index'], input_data)\n    interpreter.invoke()\n    \n    output_data = [interpreter.get_tensor(output_details[i]['index']) for i in range(len(output_details))]\n    results = postprocess_output(output_data)\n    draw_results(frame, results)\n    \n    out.write(frame)\n\n# Phase 5: Cleanup\ncap.release()\nout.release()\ncv2.destroyAllWindows()"
      },
      "session_id": "qwen2.5-coder:32b_6eca_psg_batch",
      "metadata": {
        "num_run": 19
      },
      "tags": [
        "benchmark",
        "py_sketch_generator",
        "qwen2.5-coder:32b"
      ],
      "latency": 26.8,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-02-44-43-652096_chatcmpl-c824d35f-f19b-4520-b81d-21fcc9dd3b45",
          "traceId": "7c8ce682",
          "type": "GENERATION",
          "name": "7c8c_psg_gen_attempt#1",
          "startTime": "2025-05-19T23:44:43.652000+00:00",
          "endTime": "2025-05-19T23:45:10.452000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {
            "temperature": "0.1",
            "top_p": "0.3"
          },
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 26800.0,
          "costDetails": {},
          "totalTokens": 2515,
          "completionTokens": 935,
          "promptTokens": 1580,
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "createdAt": "2025-05-19T23:44:45.000Z",
      "updatedAt": "2025-05-19T23:46:08.937Z",
      "environment": "default"
    },
    {
      "id": "cb6c242e",
      "timestamp": "2025-05-19T23:43:27.067000+00:00",
      "name": "cb6c_py_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import numpy as np\nimport os\nimport time\nfrom ai_edge_litert.interpreter import Interpreter\nimport cv2\n\n# Phase 1: Setup\n## 1.1 Imports: Already done above\n\n## 1.2 Paths/Parameters\nmodel_path = 'models/ssd-mobilenet_v1/detect.tflite'\nlabel_map_path = 'models/ssd-mobilenet_v1/labelmap.txt'\nconfidence_threshold = 0.5\ninput_path = 'data/object_detection/sheeps.mp4'\noutput_path = 'results/object_detection/test_results/sheeps_detections.mp4'\n\n## 1.3 Load Labels (Conditional)\nif os.path.exists(label_map_path):\n    with open(label_map_path, 'r') as f:\n        labels = [line.strip() for line in f.readlines()]\nelse:\n    labels = []\n\n## 1.4 Load Interpreter\ninterpreter = Interpreter(model_path=model_path)\ninterpreter.allocate_tensors()\n\n## 1.5 Get Model Details\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\n# Phase 2: Input Acquisition & Preprocessing Loop\n## 2.1 Acquire Input Data\ncap = cv2.VideoCapture(input_path)\nif not cap.isOpened():\n    raise IOError(\"Cannot open video file\")\n\nframe_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\nframe_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\nfps = cap.get(cv2.CAP_PROP_FPS)\n\n## 2.2 Preprocess Data\ninput_shape = input_details[0]['shape']\npreprocessed_frames = []\n\n# Phase 3: Inference (Run per preprocessed input)\ndef preprocess_frame(frame):\n    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n    frame_resized = cv2.resize(frame_rgb, (input_shape[1], input_shape[2]))\n    return np.expand_dims(frame_resized, axis=0).astype(input_details[0]['dtype'])\n\n# Phase 4: Output Interpretation & Handling Loop\n## 4.1 Get Output Tensor(s)\ndef postprocess_output(output_data):\n    detection_boxes = output_data[0][0]\n    detection_classes = output_data[1][0].astype(int)\n    detection_scores = output_data[2][0]\n    \n    results = []\n    for i in range(len(detection_scores)):\n        if detection_scores[i] > confidence_threshold:\n            class_id = detection_classes[i]\n            box = detection_boxes[i]\n            label = labels[class_id] if class_id < len(labels) else f'unknown_{class_id}'\n            score = detection_scores[i]\n            results.append((box, label, score))\n    return results\n\n## 4.2 Interpret Results\ndef draw_results(frame, results):\n    for box, label, score in results:\n        ymin, xmin, ymax, xmax = box\n        (left, right, top, bottom) = (xmin * frame_width, xmax * frame_width,\n                                      ymin * frame_height, ymax * frame_height)\n        cv2.rectangle(frame, (int(left), int(top)), (int(right), int(bottom)), (0, 255, 0), 2)\n        label_text = f'{label}: {score:.2f}'\n        cv2.putText(frame, label_text, (int(left), int(top) - 10),\n                    cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n\n## 4.3 Handle Output\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\nout = cv2.VideoWriter(output_path, fourcc, fps, (frame_width, frame_height))\n\nwhile cap.isOpened():\n    ret, frame = cap.read()\n    if not ret:\n        break\n    \n    input_data = preprocess_frame(frame)\n    interpreter.set_tensor(input_details[0]['index'], input_data)\n    interpreter.invoke()\n    \n    output_data = [interpreter.get_tensor(output_details[i]['index']) for i in range(len(output_details))]\n    results = postprocess_output(output_data)\n    draw_results(frame, results)\n    \n    out.write(frame)\n\n# Phase 5: Cleanup\ncap.release()\nout.release()\ncv2.destroyAllWindows()"
      },
      "session_id": "qwen2.5-coder:32b_6eca_psg_batch",
      "metadata": {
        "num_run": 18
      },
      "tags": [
        "benchmark",
        "py_sketch_generator",
        "qwen2.5-coder:32b"
      ],
      "latency": 26.691,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-02-43-27-069034_chatcmpl-bd550035-9629-43e7-a061-9e54b2d2e71f",
          "traceId": "cb6c242e",
          "type": "GENERATION",
          "name": "cb6c_psg_gen_attempt#1",
          "startTime": "2025-05-19T23:43:27.069000+00:00",
          "endTime": "2025-05-19T23:43:53.760000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {
            "temperature": "0.1",
            "top_p": "0.3"
          },
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 26691.0,
          "costDetails": {},
          "totalTokens": 2515,
          "completionTokens": 935,
          "promptTokens": 1580,
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "createdAt": "2025-05-19T23:43:28.000Z",
      "updatedAt": "2025-05-19T23:44:42.862Z",
      "environment": "default"
    },
    {
      "id": "60a1de7d",
      "timestamp": "2025-05-19T23:42:10.527000+00:00",
      "name": "60a1_py_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import numpy as np\nimport os\nimport time\nfrom ai_edge_litert.interpreter import Interpreter\nimport cv2\n\n# Phase 1: Setup\n## 1.1 Imports: Already done above\n\n## 1.2 Paths/Parameters\nmodel_path = 'models/ssd-mobilenet_v1/detect.tflite'\nlabel_map_path = 'models/ssd-mobilenet_v1/labelmap.txt'\nconfidence_threshold = 0.5\ninput_path = 'data/object_detection/sheeps.mp4'\noutput_path = 'results/object_detection/test_results/sheeps_detections.mp4'\n\n## 1.3 Load Labels (Conditional)\nif os.path.exists(label_map_path):\n    with open(label_map_path, 'r') as f:\n        labels = [line.strip() for line in f.readlines()]\nelse:\n    labels = []\n\n## 1.4 Load Interpreter\ninterpreter = Interpreter(model_path=model_path)\ninterpreter.allocate_tensors()\n\n## 1.5 Get Model Details\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\n# Phase 2: Input Acquisition & Preprocessing Loop\n## 2.1 Acquire Input Data\ncap = cv2.VideoCapture(input_path)\nif not cap.isOpened():\n    raise IOError(\"Cannot open video file\")\n\nframe_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\nframe_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\nfps = cap.get(cv2.CAP_PROP_FPS)\n\n## 2.2 Preprocess Data\ninput_shape = input_details[0]['shape']\npreprocessed_frames = []\n\n# Phase 3: Inference (Run per preprocessed input)\ndef preprocess_frame(frame):\n    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n    frame_resized = cv2.resize(frame_rgb, (input_shape[1], input_shape[2]))\n    return np.expand_dims(frame_resized, axis=0).astype(input_details[0]['dtype'])\n\n# Phase 4: Output Interpretation & Handling Loop\n## 4.1 Get Output Tensor(s)\ndef postprocess_output(output_data):\n    detection_boxes = output_data[0][0]\n    detection_classes = output_data[1][0].astype(int)\n    detection_scores = output_data[2][0]\n    \n    results = []\n    for i in range(len(detection_scores)):\n        if detection_scores[i] > confidence_threshold:\n            class_id = detection_classes[i]\n            box = detection_boxes[i]\n            label = labels[class_id] if class_id < len(labels) else f'unknown_{class_id}'\n            score = detection_scores[i]\n            results.append((box, label, score))\n    return results\n\n## 4.2 Interpret Results\ndef draw_results(frame, results):\n    for box, label, score in results:\n        ymin, xmin, ymax, xmax = box\n        (left, right, top, bottom) = (xmin * frame_width, xmax * frame_width,\n                                      ymin * frame_height, ymax * frame_height)\n        cv2.rectangle(frame, (int(left), int(top)), (int(right), int(bottom)), (0, 255, 0), 2)\n        label_text = f'{label}: {score:.2f}'\n        cv2.putText(frame, label_text, (int(left), int(top) - 10),\n                    cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n\n## 4.3 Handle Output\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\nout = cv2.VideoWriter(output_path, fourcc, fps, (frame_width, frame_height))\n\nwhile cap.isOpened():\n    ret, frame = cap.read()\n    if not ret:\n        break\n    \n    input_data = preprocess_frame(frame)\n    interpreter.set_tensor(input_details[0]['index'], input_data)\n    interpreter.invoke()\n    \n    output_data = [interpreter.get_tensor(output_details[i]['index']) for i in range(len(output_details))]\n    results = postprocess_output(output_data)\n    draw_results(frame, results)\n    \n    out.write(frame)\n\n# Phase 5: Cleanup\ncap.release()\nout.release()\ncv2.destroyAllWindows()"
      },
      "session_id": "qwen2.5-coder:32b_6eca_psg_batch",
      "metadata": {
        "num_run": 17
      },
      "tags": [
        "benchmark",
        "py_sketch_generator",
        "qwen2.5-coder:32b"
      ],
      "latency": 26.792,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-02-42-10-529165_chatcmpl-9c025688-9fe7-4b6b-9b38-335365cf9cf1",
          "traceId": "60a1de7d",
          "type": "GENERATION",
          "name": "60a1_psg_gen_attempt#1",
          "startTime": "2025-05-19T23:42:10.529000+00:00",
          "endTime": "2025-05-19T23:42:37.321000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {
            "temperature": "0.1",
            "top_p": "0.3"
          },
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 26792.0,
          "costDetails": {},
          "totalTokens": 2515,
          "completionTokens": 935,
          "promptTokens": 1580,
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "createdAt": "2025-05-19T23:42:12.000Z",
      "updatedAt": "2025-05-19T23:43:26.296Z",
      "environment": "default"
    },
    {
      "id": "c2eab9c7",
      "timestamp": "2025-05-19T23:40:54+00:00",
      "name": "c2ea_py_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import numpy as np\nimport os\nimport time\nfrom ai_edge_litert.interpreter import Interpreter\nimport cv2\n\n# Phase 1: Setup\n## 1.1 Imports: Already done above\n\n## 1.2 Paths/Parameters\nmodel_path = 'models/ssd-mobilenet_v1/detect.tflite'\nlabel_map_path = 'models/ssd-mobilenet_v1/labelmap.txt'\nconfidence_threshold = 0.5\ninput_path = 'data/object_detection/sheeps.mp4'\noutput_path = 'results/object_detection/test_results/sheeps_detections.mp4'\n\n## 1.3 Load Labels (Conditional)\nif os.path.exists(label_map_path):\n    with open(label_map_path, 'r') as f:\n        labels = [line.strip() for line in f.readlines()]\nelse:\n    labels = []\n\n## 1.4 Load Interpreter\ninterpreter = Interpreter(model_path=model_path)\ninterpreter.allocate_tensors()\n\n## 1.5 Get Model Details\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\n# Phase 2: Input Acquisition & Preprocessing Loop\n## 2.1 Acquire Input Data\ncap = cv2.VideoCapture(input_path)\nif not cap.isOpened():\n    raise IOError(\"Cannot open video file\")\n\nframe_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\nframe_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\nfps = cap.get(cv2.CAP_PROP_FPS)\n\n## 2.2 Preprocess Data\ninput_shape = input_details[0]['shape']\npreprocessed_frames = []\n\n# Phase 3: Inference (Run per preprocessed input)\ndef preprocess_frame(frame):\n    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n    frame_resized = cv2.resize(frame_rgb, (input_shape[1], input_shape[2]))\n    return np.expand_dims(frame_resized, axis=0).astype(input_details[0]['dtype'])\n\n# Phase 4: Output Interpretation & Handling Loop\n## 4.1 Get Output Tensor(s)\ndef postprocess_output(output_data):\n    detection_boxes = output_data[0][0]\n    detection_classes = output_data[1][0].astype(int)\n    detection_scores = output_data[2][0]\n    \n    results = []\n    for i in range(len(detection_scores)):\n        if detection_scores[i] > confidence_threshold:\n            class_id = detection_classes[i]\n            box = detection_boxes[i]\n            label = labels[class_id] if class_id < len(labels) else f'unknown_{class_id}'\n            score = detection_scores[i]\n            results.append((box, label, score))\n    return results\n\n## 4.2 Interpret Results\ndef draw_results(frame, results):\n    for box, label, score in results:\n        ymin, xmin, ymax, xmax = box\n        (left, right, top, bottom) = (xmin * frame_width, xmax * frame_width,\n                                      ymin * frame_height, ymax * frame_height)\n        cv2.rectangle(frame, (int(left), int(top)), (int(right), int(bottom)), (0, 255, 0), 2)\n        label_text = f'{label}: {score:.2f}'\n        cv2.putText(frame, label_text, (int(left), int(top) - 10),\n                    cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n\n## 4.3 Handle Output\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\nout = cv2.VideoWriter(output_path, fourcc, fps, (frame_width, frame_height))\n\nwhile cap.isOpened():\n    ret, frame = cap.read()\n    if not ret:\n        break\n    \n    input_data = preprocess_frame(frame)\n    interpreter.set_tensor(input_details[0]['index'], input_data)\n    interpreter.invoke()\n    \n    output_data = [interpreter.get_tensor(output_details[i]['index']) for i in range(len(output_details))]\n    results = postprocess_output(output_data)\n    draw_results(frame, results)\n    \n    out.write(frame)\n\n# Phase 5: Cleanup\ncap.release()\nout.release()\ncv2.destroyAllWindows()"
      },
      "session_id": "qwen2.5-coder:32b_6eca_psg_batch",
      "metadata": {
        "num_run": 16
      },
      "tags": [
        "benchmark",
        "py_sketch_generator",
        "qwen2.5-coder:32b"
      ],
      "latency": 26.812,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-02-40-54-001864_chatcmpl-5a0c9af9-4539-4338-99ed-a74b32408ee9",
          "traceId": "c2eab9c7",
          "type": "GENERATION",
          "name": "c2ea_psg_gen_attempt#1",
          "startTime": "2025-05-19T23:40:54.001000+00:00",
          "endTime": "2025-05-19T23:41:20.813000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {
            "temperature": "0.1",
            "top_p": "0.3"
          },
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 26812.0,
          "costDetails": {},
          "totalTokens": 2515,
          "completionTokens": 935,
          "promptTokens": 1580,
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "createdAt": "2025-05-19T23:40:55.000Z",
      "updatedAt": "2025-05-19T23:42:09.374Z",
      "environment": "default"
    },
    {
      "id": "d0624c41",
      "timestamp": "2025-05-19T23:39:35.434000+00:00",
      "name": "d062_py_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import numpy as np\nimport os\nimport time\nfrom ai_edge_litert.interpreter import Interpreter\nimport cv2\n\n# Phase 1: Setup\n## 1.1 Imports: Already done above\n\n## 1.2 Paths/Parameters\nmodel_path = 'models/ssd-mobilenet_v1/detect.tflite'\nlabel_map_path = 'models/ssd-mobilenet_v1/labelmap.txt'\nconfidence_threshold = 0.5\ninput_path = 'data/object_detection/sheeps.mp4'\noutput_path = 'results/object_detection/test_results/sheeps_detections.mp4'\n\n## 1.3 Load Labels (Conditional)\nif os.path.exists(label_map_path):\n    with open(label_map_path, 'r') as f:\n        labels = [line.strip() for line in f.readlines()]\nelse:\n    labels = []\n\n## 1.4 Load Interpreter\ninterpreter = Interpreter(model_path=model_path)\ninterpreter.allocate_tensors()\n\n## 1.5 Get Model Details\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\n# Phase 2: Input Acquisition & Preprocessing Loop\n## 2.1 Acquire Input Data\ncap = cv2.VideoCapture(input_path)\nif not cap.isOpened():\n    raise IOError(\"Cannot open video file\")\n\nframe_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\nframe_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\nfps = cap.get(cv2.CAP_PROP_FPS)\n\n## 2.2 Preprocess Data\ninput_shape = input_details[0]['shape']\npreprocessed_frames = []\n\n# Phase 3: Inference (Run per preprocessed input)\ndef preprocess_frame(frame):\n    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n    frame_resized = cv2.resize(frame_rgb, (input_shape[1], input_shape[2]))\n    return np.expand_dims(frame_resized, axis=0).astype(input_details[0]['dtype'])\n\n# Phase 4: Output Interpretation & Handling Loop\n## 4.1 Get Output Tensor(s)\ndef interpret_output(output_data):\n    detection_boxes = output_data[0][0]\n    detection_classes = output_data[1][0].astype(int)\n    detection_scores = output_data[2][0]\n    \n    results = []\n    for i in range(len(detection_scores)):\n        if detection_scores[i] > confidence_threshold:\n            class_id = detection_classes[i]\n            box = detection_boxes[i]\n            label = labels[class_id - 1] if class_id <= len(labels) else f'unknown_{class_id}'\n            score = detection_scores[i]\n            results.append((box, label, score))\n    return results\n\n## 4.2 Interpret Results\n## 4.3 Handle Output\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\nout = cv2.VideoWriter(output_path, fourcc, fps, (frame_width, frame_height))\n\nwhile cap.isOpened():\n    ret, frame = cap.read()\n    if not ret:\n        break\n    \n    input_data = preprocess_frame(frame)\n    interpreter.set_tensor(input_details[0]['index'], input_data)\n    interpreter.invoke()\n    \n    output_data = [interpreter.get_tensor(output_details[i]['index']) for i in range(len(output_details))]\n    results = interpret_output(output_data)\n    \n    for box, label, score in results:\n        ymin, xmin, ymax, xmax = box\n        (left, right, top, bottom) = (xmin * frame_width, xmax * frame_width,\n                                      ymin * frame_height, ymax * frame_height)\n        cv2.rectangle(frame, (int(left), int(top)), (int(right), int(bottom)), (0, 255, 0), 2)\n        label_text = f'{label}: {score:.2f}'\n        cv2.putText(frame, label_text, (int(left), int(top) - 10),\n                    cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n    \n    out.write(frame)\n\n# Phase 5: Cleanup\ncap.release()\nout.release()\ncv2.destroyAllWindows()"
      },
      "session_id": "qwen2.5-coder:32b_6eca_psg_batch",
      "metadata": {
        "num_run": 15
      },
      "tags": [
        "benchmark",
        "py_sketch_generator",
        "qwen2.5-coder:32b"
      ],
      "latency": 29.232,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-02-39-35-436529_chatcmpl-e6f95f1e-bc86-4fef-8065-2586627c147b",
          "traceId": "d0624c41",
          "type": "GENERATION",
          "name": "d062_psg_gen_attempt#1",
          "startTime": "2025-05-19T23:39:35.436000+00:00",
          "endTime": "2025-05-19T23:40:04.668000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {
            "temperature": "0.1",
            "top_p": "0.3"
          },
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 29232.0,
          "costDetails": {},
          "totalTokens": 2503,
          "completionTokens": 923,
          "promptTokens": 1580,
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "createdAt": "2025-05-19T23:39:36.000Z",
      "updatedAt": "2025-05-19T23:40:53.737Z",
      "environment": "default"
    },
    {
      "id": "64faa38f",
      "timestamp": "2025-05-19T23:38:17.898000+00:00",
      "name": "64fa_py_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import numpy as np\nimport os\nimport time\nfrom ai_edge_litert.interpreter import Interpreter\nimport cv2\n\n# Phase 1: Setup\n## 1.1 Imports: Already done above\n\n## 1.2 Paths/Parameters\nmodel_path = 'models/ssd-mobilenet_v1/detect.tflite'\nlabel_map_path = 'models/ssd-mobilenet_v1/labelmap.txt'\nconfidence_threshold = 0.5\ninput_path = 'data/object_detection/sheeps.mp4'\noutput_path = 'results/object_detection/test_results/sheeps_detections.mp4'\n\n## 1.3 Load Labels (Conditional)\nif os.path.exists(label_map_path):\n    with open(label_map_path, 'r') as f:\n        labels = [line.strip() for line in f.readlines()]\nelse:\n    labels = []\n\n## 1.4 Load Interpreter\ninterpreter = Interpreter(model_path=model_path)\ninterpreter.allocate_tensors()\n\n## 1.5 Get Model Details\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\n# Phase 2: Input Acquisition & Preprocessing Loop\n## 2.1 Acquire Input Data\ncap = cv2.VideoCapture(input_path)\nif not cap.isOpened():\n    raise IOError(\"Cannot open video file\")\n\nframe_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\nframe_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\nfps = cap.get(cv2.CAP_PROP_FPS)\n\n## 2.2 Preprocess Data\ninput_shape = input_details[0]['shape']\npreprocessed_frames = []\n\n# Phase 3: Inference (Run per preprocessed input)\ndef preprocess_frame(frame):\n    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n    frame_resized = cv2.resize(frame_rgb, (input_shape[1], input_shape[2]))\n    return np.expand_dims(frame_resized, axis=0).astype(input_details[0]['dtype'])\n\n# Phase 4: Output Interpretation & Handling Loop\n## 4.1 Get Output Tensor(s)\ndef postprocess_output(output_data):\n    detection_boxes = output_data[0][0]\n    detection_classes = output_data[1][0].astype(int)\n    detection_scores = output_data[2][0]\n    \n    results = []\n    for i in range(len(detection_scores)):\n        if detection_scores[i] > confidence_threshold:\n            class_id = detection_classes[i]\n            box = detection_boxes[i]\n            label = labels[class_id] if class_id < len(labels) else f'unknown_{class_id}'\n            score = detection_scores[i]\n            results.append((box, label, score))\n    return results\n\n## 4.2 Interpret Results\ndef draw_results(frame, results):\n    for box, label, score in results:\n        ymin, xmin, ymax, xmax = box\n        (left, right, top, bottom) = (xmin * frame_width, xmax * frame_width,\n                                      ymin * frame_height, ymax * frame_height)\n        cv2.rectangle(frame, (int(left), int(top)), (int(right), int(bottom)), (0, 255, 0), 2)\n        label_text = f'{label}: {score:.2f}'\n        cv2.putText(frame, label_text, (int(left), int(top) - 10),\n                    cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n\n## 4.3 Handle Output\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\nout = cv2.VideoWriter(output_path, fourcc, fps, (frame_width, frame_height))\n\nwhile cap.isOpened():\n    ret, frame = cap.read()\n    if not ret:\n        break\n    \n    input_data = preprocess_frame(frame)\n    interpreter.set_tensor(input_details[0]['index'], input_data)\n    interpreter.invoke()\n    \n    output_data = [interpreter.get_tensor(output_details[i]['index']) for i in range(len(output_details))]\n    results = postprocess_output(output_data)\n    draw_results(frame, results)\n    \n    out.write(frame)\n\n# Phase 5: Cleanup\ncap.release()\nout.release()\ncv2.destroyAllWindows()"
      },
      "session_id": "qwen2.5-coder:32b_6eca_psg_batch",
      "metadata": {
        "num_run": 14
      },
      "tags": [
        "benchmark",
        "py_sketch_generator",
        "qwen2.5-coder:32b"
      ],
      "latency": 27.04,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-02-38-17-900368_chatcmpl-0e15d505-62ab-4ca2-a17f-8a241bc6ee6b",
          "traceId": "64faa38f",
          "type": "GENERATION",
          "name": "64fa_psg_gen_attempt#1",
          "startTime": "2025-05-19T23:38:17.900000+00:00",
          "endTime": "2025-05-19T23:38:44.940000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {
            "temperature": "0.1",
            "top_p": "0.3"
          },
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 27040.0,
          "costDetails": {},
          "totalTokens": 2515,
          "completionTokens": 935,
          "promptTokens": 1580,
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "createdAt": "2025-05-19T23:38:19.000Z",
      "updatedAt": "2025-05-19T23:39:34.243Z",
      "environment": "default"
    },
    {
      "id": "af9a40c6",
      "timestamp": "2025-05-19T23:37:01.305000+00:00",
      "name": "af9a_py_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import numpy as np\nimport os\nimport time\nfrom ai_edge_litert.interpreter import Interpreter\nimport cv2\n\n# Phase 1: Setup\n## 1.1 Imports: Already done above\n\n## 1.2 Paths/Parameters\nmodel_path = 'models/ssd-mobilenet_v1/detect.tflite'\nlabel_map_path = 'models/ssd-mobilenet_v1/labelmap.txt'\nconfidence_threshold = 0.5\ninput_path = 'data/object_detection/sheeps.mp4'\noutput_path = 'results/object_detection/test_results/sheeps_detections.mp4'\n\n## 1.3 Load Labels (Conditional)\nif os.path.exists(label_map_path):\n    with open(label_map_path, 'r') as f:\n        labels = [line.strip() for line in f.readlines()]\nelse:\n    labels = []\n\n## 1.4 Load Interpreter\ninterpreter = Interpreter(model_path=model_path)\ninterpreter.allocate_tensors()\n\n## 1.5 Get Model Details\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\n# Phase 2: Input Acquisition & Preprocessing Loop\n## 2.1 Acquire Input Data\ncap = cv2.VideoCapture(input_path)\nif not cap.isOpened():\n    raise IOError(\"Cannot open video file\")\n\nframe_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\nframe_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\nfps = cap.get(cv2.CAP_PROP_FPS)\n\n## 2.2 Preprocess Data\ninput_shape = input_details[0]['shape']\npreprocessed_frames = []\n\n# Phase 3: Inference (Run per preprocessed input)\ndef preprocess_frame(frame):\n    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n    frame_resized = cv2.resize(frame_rgb, (input_shape[1], input_shape[2]))\n    return np.expand_dims(frame_resized, axis=0).astype(input_details[0]['dtype'])\n\n# Phase 4: Output Interpretation & Handling Loop\n## 4.1 Get Output Tensor(s)\ndef postprocess_output(output_data):\n    detection_boxes = output_data[0][0]\n    detection_classes = output_data[1][0].astype(int)\n    detection_scores = output_data[2][0]\n    \n    results = []\n    for i in range(len(detection_scores)):\n        if detection_scores[i] > confidence_threshold:\n            class_id = detection_classes[i]\n            box = detection_boxes[i]\n            label = labels[class_id] if class_id < len(labels) else f'unknown_{class_id}'\n            score = detection_scores[i]\n            results.append((box, label, score))\n    return results\n\n## 4.2 Interpret Results\ndef draw_results(frame, results):\n    for box, label, score in results:\n        ymin, xmin, ymax, xmax = box\n        (left, right, top, bottom) = (xmin * frame_width, xmax * frame_width,\n                                      ymin * frame_height, ymax * frame_height)\n        cv2.rectangle(frame, (int(left), int(top)), (int(right), int(bottom)), (0, 255, 0), 2)\n        label_text = f'{label}: {score:.2f}'\n        cv2.putText(frame, label_text, (int(left), int(top) - 10),\n                    cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n\n## 4.3 Handle Output\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\nout = cv2.VideoWriter(output_path, fourcc, fps, (frame_width, frame_height))\n\nwhile cap.isOpened():\n    ret, frame = cap.read()\n    if not ret:\n        break\n    \n    input_data = preprocess_frame(frame)\n    interpreter.set_tensor(input_details[0]['index'], input_data)\n    interpreter.invoke()\n    \n    output_data = [interpreter.get_tensor(output_details[i]['index']) for i in range(len(output_details))]\n    results = postprocess_output(output_data)\n    draw_results(frame, results)\n    \n    out.write(frame)\n\n# Phase 5: Cleanup\ncap.release()\nout.release()\ncv2.destroyAllWindows()"
      },
      "session_id": "qwen2.5-coder:32b_6eca_psg_batch",
      "metadata": {
        "num_run": 13
      },
      "tags": [
        "benchmark",
        "py_sketch_generator",
        "qwen2.5-coder:32b"
      ],
      "latency": 26.796,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-02-37-01-306763_chatcmpl-01f44d21-1236-444d-8c17-77d9bcf7fc0b",
          "traceId": "af9a40c6",
          "type": "GENERATION",
          "name": "af9a_psg_gen_attempt#1",
          "startTime": "2025-05-19T23:37:01.306000+00:00",
          "endTime": "2025-05-19T23:37:28.102000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {
            "temperature": "0.1",
            "top_p": "0.3"
          },
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 26796.0,
          "costDetails": {},
          "totalTokens": 2515,
          "completionTokens": 935,
          "promptTokens": 1580,
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "createdAt": "2025-05-19T23:37:02.000Z",
      "updatedAt": "2025-05-19T23:38:16.749Z",
      "environment": "default"
    },
    {
      "id": "b3dee6db",
      "timestamp": "2025-05-19T23:35:34.719000+00:00",
      "name": "b3de_py_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import numpy as np\nimport os\nimport time\nfrom ai_edge_litert.interpreter import Interpreter\nimport cv2\n\n# Phase 1: Setup\n## 1.1 Imports: Already done above\n\n## 1.2 Paths/Parameters\nmodel_path = 'models/ssd-mobilenet_v1/detect.tflite'\nlabel_map_path = 'models/ssd-mobilenet_v1/labelmap.txt'\nconfidence_threshold = 0.5\ninput_path = 'data/object_detection/sheeps.mp4'\noutput_path = 'results/object_detection/test_results/sheeps_detections.mp4'\n\n## 1.3 Load Labels (Conditional)\nif os.path.exists(label_map_path):\n    with open(label_map_path, 'r') as f:\n        labels = [line.strip() for line in f.readlines()]\nelse:\n    labels = []\n\n## 1.4 Load Interpreter\ninterpreter = Interpreter(model_path=model_path)\ninterpreter.allocate_tensors()\n\n## 1.5 Get Model Details\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\n# Phase 2: Input Acquisition & Preprocessing Loop\n## 2.1 Acquire Input Data\ncap = cv2.VideoCapture(input_path)\nif not cap.isOpened():\n    raise IOError(\"Cannot open video file\")\n\nframe_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\nframe_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\nfps = cap.get(cv2.CAP_PROP_FPS)\n\n## 2.2 Preprocess Data\ninput_shape = input_details[0]['shape']\npreprocessed_frames = []\n\n# Phase 3: Inference (Run per preprocessed input)\ndef preprocess_frame(frame):\n    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n    frame_resized = cv2.resize(frame_rgb, (input_shape[1], input_shape[2]))\n    return np.expand_dims(frame_resized, axis=0).astype(input_details[0]['dtype'])\n\n# Phase 4: Output Interpretation & Handling Loop\n## 4.1 Get Output Tensor(s)\ndef postprocess_output(output_data):\n    detection_boxes = output_data[0][0]\n    detection_classes = output_data[1][0].astype(int)\n    detection_scores = output_data[2][0]\n    \n    results = []\n    for i in range(len(detection_scores)):\n        if detection_scores[i] > confidence_threshold:\n            class_id = detection_classes[i]\n            box = detection_boxes[i]\n            label = labels[class_id] if class_id < len(labels) else f'unknown_{class_id}'\n            score = detection_scores[i]\n            results.append((box, label, score))\n    return results\n\n## 4.2 Interpret Results\ndef draw_results(frame, results):\n    for box, label, score in results:\n        ymin, xmin, ymax, xmax = box\n        height, width, _ = frame.shape\n        (xminn, xmaxx, yminn, ymaxx) = (int(xmin * width), int(xmax * width),\n                                        int(ymin * height), int(ymax * height))\n        \n        cv2.rectangle(frame, (xminn, yminn), (xmaxx, ymaxx), (0, 255, 0), 2)\n        label_text = f'{label}: {score:.2f}'\n        cv2.putText(frame, label_text, (xminn, yminn - 10),\n                    cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n\n## 4.3 Handle Output\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\nout = cv2.VideoWriter(output_path, fourcc, fps, (frame_width, frame_height))\n\nwhile cap.isOpened():\n    ret, frame = cap.read()\n    if not ret:\n        break\n    \n    input_data = preprocess_frame(frame)\n    interpreter.set_tensor(input_details[0]['index'], input_data)\n    interpreter.invoke()\n    \n    output_data = [interpreter.get_tensor(output_details[i]['index']) for i in range(len(output_details))]\n    results = postprocess_output(output_data)\n    draw_results(frame, results)\n    \n    out.write(frame)\n\n# Phase 5: Cleanup\ncap.release()\nout.release()\ncv2.destroyAllWindows()"
      },
      "session_id": "qwen2.5-coder:32b_6eca_psg_batch",
      "metadata": {
        "num_run": 12
      },
      "tags": [
        "benchmark",
        "py_sketch_generator",
        "qwen2.5-coder:32b"
      ],
      "latency": 36.839,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-02-35-34-721212_chatcmpl-69a58d0b-2b09-49ca-9690-b91afa311015",
          "traceId": "b3dee6db",
          "type": "GENERATION",
          "name": "b3de_psg_gen_attempt#1",
          "startTime": "2025-05-19T23:35:34.721000+00:00",
          "endTime": "2025-05-19T23:36:11.560000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {
            "temperature": "0.1",
            "top_p": "0.3"
          },
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 36839.0,
          "costDetails": {},
          "totalTokens": 2533,
          "completionTokens": 953,
          "promptTokens": 1580,
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "createdAt": "2025-05-19T23:35:36.000Z",
      "updatedAt": "2025-05-19T23:37:00.603Z",
      "environment": "default"
    },
    {
      "id": "1846299b",
      "timestamp": "2025-05-19T23:34:18.160000+00:00",
      "name": "1846_py_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import numpy as np\nimport os\nimport time\nfrom ai_edge_litert.interpreter import Interpreter\nimport cv2\n\n# Phase 1: Setup\n## 1.1 Imports: Already done above\n\n## 1.2 Paths/Parameters\nmodel_path = 'models/ssd-mobilenet_v1/detect.tflite'\nlabel_map_path = 'models/ssd-mobilenet_v1/labelmap.txt'\nconfidence_threshold = 0.5\ninput_path = 'data/object_detection/sheeps.mp4'\noutput_path = 'results/object_detection/test_results/sheeps_detections.mp4'\n\n## 1.3 Load Labels (Conditional)\nif os.path.exists(label_map_path):\n    with open(label_map_path, 'r') as f:\n        labels = [line.strip() for line in f.readlines()]\nelse:\n    labels = []\n\n## 1.4 Load Interpreter\ninterpreter = Interpreter(model_path=model_path)\ninterpreter.allocate_tensors()\n\n## 1.5 Get Model Details\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\n# Phase 2: Input Acquisition & Preprocessing Loop\n## 2.1 Acquire Input Data\ncap = cv2.VideoCapture(input_path)\nif not cap.isOpened():\n    raise IOError(\"Cannot open video file\")\n\nframe_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\nframe_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\nfps = cap.get(cv2.CAP_PROP_FPS)\n\n## 2.2 Preprocess Data\ninput_shape = input_details[0]['shape']\npreprocessed_frames = []\n\n# Phase 3: Inference (Run per preprocessed input)\ndef preprocess_frame(frame):\n    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n    frame_resized = cv2.resize(frame_rgb, (input_shape[1], input_shape[2]))\n    return np.expand_dims(frame_resized, axis=0).astype(input_details[0]['dtype'])\n\n# Phase 4: Output Interpretation & Handling Loop\n## 4.1 Get Output Tensor(s)\ndef postprocess_output(output_data):\n    detection_boxes = output_data[0][0]\n    detection_classes = output_data[1][0].astype(int)\n    detection_scores = output_data[2][0]\n    \n    results = []\n    for i in range(len(detection_scores)):\n        if detection_scores[i] > confidence_threshold:\n            class_id = detection_classes[i]\n            box = detection_boxes[i]\n            label = labels[class_id] if class_id < len(labels) else f'unknown_{class_id}'\n            score = detection_scores[i]\n            results.append((box, label, score))\n    return results\n\n## 4.2 Interpret Results\ndef draw_results(frame, results):\n    for box, label, score in results:\n        ymin, xmin, ymax, xmax = box\n        (left, right, top, bottom) = (xmin * frame_width, xmax * frame_width,\n                                      ymin * frame_height, ymax * frame_height)\n        cv2.rectangle(frame, (int(left), int(top)), (int(right), int(bottom)), (0, 255, 0), 2)\n        label_text = f'{label}: {score:.2f}'\n        cv2.putText(frame, label_text, (int(left), int(top) - 10),\n                    cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n\n## 4.3 Handle Output\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\nout = cv2.VideoWriter(output_path, fourcc, fps, (frame_width, frame_height))\n\nwhile cap.isOpened():\n    ret, frame = cap.read()\n    if not ret:\n        break\n    \n    input_data = preprocess_frame(frame)\n    interpreter.set_tensor(input_details[0]['index'], input_data)\n    interpreter.invoke()\n    \n    output_data = [interpreter.get_tensor(output_details[i]['index']) for i in range(len(output_details))]\n    results = postprocess_output(output_data)\n    draw_results(frame, results)\n    \n    out.write(frame)\n\n# Phase 5: Cleanup\ncap.release()\nout.release()\ncv2.destroyAllWindows()"
      },
      "session_id": "qwen2.5-coder:32b_6eca_psg_batch",
      "metadata": {
        "num_run": 11
      },
      "tags": [
        "benchmark",
        "py_sketch_generator",
        "qwen2.5-coder:32b"
      ],
      "latency": 26.793,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-02-34-18-161803_chatcmpl-80c13ce7-0e64-4f02-93d7-8648a1fb8902",
          "traceId": "1846299b",
          "type": "GENERATION",
          "name": "1846_psg_gen_attempt#1",
          "startTime": "2025-05-19T23:34:18.161000+00:00",
          "endTime": "2025-05-19T23:34:44.954000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {
            "temperature": "0.1",
            "top_p": "0.3"
          },
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 26793.0,
          "costDetails": {},
          "totalTokens": 2515,
          "completionTokens": 935,
          "promptTokens": 1580,
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "createdAt": "2025-05-19T23:34:19.000Z",
      "updatedAt": "2025-05-19T23:35:34.021Z",
      "environment": "default"
    },
    {
      "id": "54299392",
      "timestamp": "2025-05-19T23:33:01.627000+00:00",
      "name": "5429_py_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import numpy as np\nimport os\nimport time\nfrom ai_edge_litert.interpreter import Interpreter\nimport cv2\n\n# Phase 1: Setup\n## 1.1 Imports: Already done above\n\n## 1.2 Paths/Parameters\nmodel_path = 'models/ssd-mobilenet_v1/detect.tflite'\nlabel_map_path = 'models/ssd-mobilenet_v1/labelmap.txt'\nconfidence_threshold = 0.5\ninput_path = 'data/object_detection/sheeps.mp4'\noutput_path = 'results/object_detection/test_results/sheeps_detections.mp4'\n\n## 1.3 Load Labels (Conditional)\nif os.path.exists(label_map_path):\n    with open(label_map_path, 'r') as f:\n        labels = [line.strip() for line in f.readlines()]\nelse:\n    labels = []\n\n## 1.4 Load Interpreter\ninterpreter = Interpreter(model_path=model_path)\ninterpreter.allocate_tensors()\n\n## 1.5 Get Model Details\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\n# Phase 2: Input Acquisition & Preprocessing Loop\n## 2.1 Acquire Input Data\ncap = cv2.VideoCapture(input_path)\nif not cap.isOpened():\n    raise IOError(\"Cannot open video file\")\n\nframe_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\nframe_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\nfps = cap.get(cv2.CAP_PROP_FPS)\n\n## 2.2 Preprocess Data\ninput_shape = input_details[0]['shape']\npreprocessed_frames = []\n\n# Phase 3: Inference (Run per preprocessed input)\ndef preprocess_frame(frame):\n    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n    frame_resized = cv2.resize(frame_rgb, (input_shape[1], input_shape[2]))\n    return np.expand_dims(frame_resized, axis=0).astype(input_details[0]['dtype'])\n\n# Phase 4: Output Interpretation & Handling Loop\n## 4.1 Get Output Tensor(s)\ndef postprocess_output(output_data):\n    detection_boxes = output_data[0][0]\n    detection_classes = output_data[1][0].astype(int)\n    detection_scores = output_data[2][0]\n    \n    results = []\n    for i in range(len(detection_scores)):\n        if detection_scores[i] > confidence_threshold:\n            class_id = detection_classes[i]\n            box = detection_boxes[i]\n            label = labels[class_id] if class_id < len(labels) else f'unknown_{class_id}'\n            score = detection_scores[i]\n            results.append((box, label, score))\n    return results\n\n## 4.2 Interpret Results\ndef draw_results(frame, results):\n    for box, label, score in results:\n        ymin, xmin, ymax, xmax = box\n        (left, right, top, bottom) = (xmin * frame_width, xmax * frame_width,\n                                      ymin * frame_height, ymax * frame_height)\n        cv2.rectangle(frame, (int(left), int(top)), (int(right), int(bottom)), (0, 255, 0), 2)\n        label_text = f'{label}: {score:.2f}'\n        cv2.putText(frame, label_text, (int(left), int(top) - 10),\n                    cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n\n## 4.3 Handle Output\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\nout = cv2.VideoWriter(output_path, fourcc, fps, (frame_width, frame_height))\n\nwhile cap.isOpened():\n    ret, frame = cap.read()\n    if not ret:\n        break\n    \n    input_data = preprocess_frame(frame)\n    interpreter.set_tensor(input_details[0]['index'], input_data)\n    interpreter.invoke()\n    \n    output_data = [interpreter.get_tensor(output_details[i]['index']) for i in range(len(output_details))]\n    results = postprocess_output(output_data)\n    draw_results(frame, results)\n    \n    out.write(frame)\n\n# Phase 5: Cleanup\ncap.release()\nout.release()\ncv2.destroyAllWindows()"
      },
      "session_id": "qwen2.5-coder:32b_6eca_psg_batch",
      "metadata": {
        "num_run": 10
      },
      "tags": [
        "benchmark",
        "py_sketch_generator",
        "qwen2.5-coder:32b"
      ],
      "latency": 26.676,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-02-33-01-629093_chatcmpl-4a2f07ce-f51a-44fe-9a7d-6893eda927fa",
          "traceId": "54299392",
          "type": "GENERATION",
          "name": "5429_psg_gen_attempt#1",
          "startTime": "2025-05-19T23:33:01.629000+00:00",
          "endTime": "2025-05-19T23:33:28.305000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {
            "temperature": "0.1",
            "top_p": "0.3"
          },
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 26676.0,
          "costDetails": {},
          "totalTokens": 2515,
          "completionTokens": 935,
          "promptTokens": 1580,
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "createdAt": "2025-05-19T23:33:03.000Z",
      "updatedAt": "2025-05-19T23:34:16.902Z",
      "environment": "default"
    },
    {
      "id": "c283e110",
      "timestamp": "2025-05-19T23:31:46.126000+00:00",
      "name": "c283_py_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import numpy as np\nimport os\nimport time\nfrom ai_edge_litert.interpreter import Interpreter\nimport cv2\n\n# Phase 1: Setup\n## 1.1 Imports: Already done above\n\n## 1.2 Paths/Parameters\nmodel_path = 'models/ssd-mobilenet_v1/detect.tflite'\nlabel_map_path = 'models/ssd-mobilenet_v1/labelmap.txt'\nconfidence_threshold = 0.5\ninput_path = 'data/object_detection/sheeps.mp4'\noutput_path = 'results/object_detection/test_results/sheeps_detections.mp4'\n\n## 1.3 Load Labels (Conditional)\nif os.path.exists(label_map_path):\n    with open(label_map_path, 'r') as f:\n        labels = [line.strip() for line in f.readlines()]\nelse:\n    labels = []\n\n## 1.4 Load Interpreter\ninterpreter = Interpreter(model_path=model_path)\ninterpreter.allocate_tensors()\n\n## 1.5 Get Model Details\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\n# Phase 2: Input Acquisition & Preprocessing Loop\n## 2.1 Acquire Input Data\ncap = cv2.VideoCapture(input_path)\nif not cap.isOpened():\n    raise IOError(\"Cannot open video file\")\n\nframe_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\nframe_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\nfps = cap.get(cv2.CAP_PROP_FPS)\n\n## 2.2 Preprocess Data\ninput_shape = input_details[0]['shape']\npreprocessed_frames = []\n\n# Phase 3: Inference (Run per preprocessed input)\ndef preprocess_frame(frame):\n    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n    frame_resized = cv2.resize(frame_rgb, (input_shape[1], input_shape[2]))\n    return np.expand_dims(frame_resized, axis=0).astype(input_details[0]['dtype'])\n\n# Phase 4: Output Interpretation & Handling Loop\n## 4.1 Get Output Tensor(s)\ndef interpret_output(output_data):\n    detection_boxes = output_data[0][0]\n    detection_classes = output_data[1][0].astype(int)\n    detection_scores = output_data[2][0]\n    \n    results = []\n    for i in range(len(detection_scores)):\n        if detection_scores[i] > confidence_threshold:\n            class_id = detection_classes[i]\n            box = detection_boxes[i]\n            label = labels[class_id - 1] if class_id <= len(labels) else f'unknown_{class_id}'\n            score = detection_scores[i]\n            results.append((box, label, score))\n    return results\n\n## 4.2 Interpret Results\n## 4.3 Handle Output\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\nout = cv2.VideoWriter(output_path, fourcc, fps, (frame_width, frame_height))\n\nwhile cap.isOpened():\n    ret, frame = cap.read()\n    if not ret:\n        break\n    \n    input_data = preprocess_frame(frame)\n    interpreter.set_tensor(input_details[0]['index'], input_data)\n    interpreter.invoke()\n    \n    output_data = [interpreter.get_tensor(output_details[i]['index']) for i in range(len(output_details))]\n    results = interpret_output(output_data)\n    \n    for box, label, score in results:\n        ymin, xmin, ymax, xmax = box\n        (left, right, top, bottom) = (xmin * frame_width, xmax * frame_width,\n                                      ymin * frame_height, ymax * frame_height)\n        cv2.rectangle(frame, (int(left), int(top)), (int(right), int(bottom)), (0, 255, 0), 2)\n        label_text = f'{label}: {score:.2f}'\n        cv2.putText(frame, label_text, (int(left), int(top) - 10),\n                    cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n    \n    out.write(frame)\n\n# Phase 5: Cleanup\ncap.release()\nout.release()\ncv2.destroyAllWindows()"
      },
      "session_id": "qwen2.5-coder:32b_6eca_psg_batch",
      "metadata": {
        "num_run": 9
      },
      "tags": [
        "benchmark",
        "py_sketch_generator",
        "qwen2.5-coder:32b"
      ],
      "latency": 26.378,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-02-31-46-127404_chatcmpl-d866659d-3c5c-40b8-838d-42740dd2f8e7",
          "traceId": "c283e110",
          "type": "GENERATION",
          "name": "c283_psg_gen_attempt#1",
          "startTime": "2025-05-19T23:31:46.127000+00:00",
          "endTime": "2025-05-19T23:32:12.505000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {
            "temperature": "0.1",
            "top_p": "0.3"
          },
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 26378.0,
          "costDetails": {},
          "totalTokens": 2503,
          "completionTokens": 923,
          "promptTokens": 1580,
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "createdAt": "2025-05-19T23:31:47.000Z",
      "updatedAt": "2025-05-19T23:33:01.503Z",
      "environment": "default"
    },
    {
      "id": "09f8c9a2",
      "timestamp": "2025-05-19T23:30:30.477000+00:00",
      "name": "09f8_py_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import numpy as np\nimport os\nimport time\nfrom ai_edge_litert.interpreter import Interpreter\nimport cv2\n\n# Phase 1: Setup\n## 1.1 Imports: Already done above\n\n## 1.2 Paths/Parameters\nmodel_path = 'models/ssd-mobilenet_v1/detect.tflite'\nlabel_map_path = 'models/ssd-mobilenet_v1/labelmap.txt'\nconfidence_threshold = 0.5\ninput_path = 'data/object_detection/sheeps.mp4'\noutput_path = 'results/object_detection/test_results/sheeps_detections.mp4'\n\n## 1.3 Load Labels (Conditional)\nif os.path.exists(label_map_path):\n    with open(label_map_path, 'r') as f:\n        labels = [line.strip() for line in f.readlines()]\nelse:\n    labels = []\n\n## 1.4 Load Interpreter\ninterpreter = Interpreter(model_path=model_path)\ninterpreter.allocate_tensors()\n\n## 1.5 Get Model Details\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\n# Phase 2: Input Acquisition & Preprocessing Loop\n## 2.1 Acquire Input Data\ncap = cv2.VideoCapture(input_path)\nif not cap.isOpened():\n    raise IOError(\"Cannot open video file\")\n\nframe_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\nframe_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\nfps = cap.get(cv2.CAP_PROP_FPS)\n\n## 2.2 Preprocess Data\ninput_shape = input_details[0]['shape']\npreprocessed_frames = []\n\n# Phase 3: Inference (Run per preprocessed input)\ndef preprocess_frame(frame):\n    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n    frame_resized = cv2.resize(frame_rgb, (input_shape[1], input_shape[2]))\n    return np.expand_dims(frame_resized, axis=0).astype(input_details[0]['dtype'])\n\n# Phase 4: Output Interpretation & Handling Loop\n## 4.1 Get Output Tensor(s)\ndef interpret_output(output_data):\n    detection_boxes = output_data[0][0]\n    detection_classes = output_data[1][0].astype(int)\n    detection_scores = output_data[2][0]\n    \n    results = []\n    for i in range(len(detection_scores)):\n        if detection_scores[i] > confidence_threshold:\n            class_id = detection_classes[i]\n            box = detection_boxes[i]\n            label = labels[class_id - 1] if class_id <= len(labels) else f'unknown_{class_id}'\n            score = detection_scores[i]\n            results.append((box, label, score))\n    return results\n\n## 4.2 Interpret Results\n## 4.3 Handle Output\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\nout = cv2.VideoWriter(output_path, fourcc, fps, (frame_width, frame_height))\n\nwhile cap.isOpened():\n    ret, frame = cap.read()\n    if not ret:\n        break\n    \n    input_data = preprocess_frame(frame)\n    interpreter.set_tensor(input_details[0]['index'], input_data)\n    interpreter.invoke()\n    \n    output_data = [interpreter.get_tensor(output_details[i]['index']) for i in range(len(output_details))]\n    results = interpret_output(output_data)\n    \n    for box, label, score in results:\n        ymin, xmin, ymax, xmax = box\n        (left, right, top, bottom) = (xmin * frame_width, xmax * frame_width,\n                                      ymin * frame_height, ymax * frame_height)\n        cv2.rectangle(frame, (int(left), int(top)), (int(right), int(bottom)), (0, 255, 0), 2)\n        label_text = f'{label}: {score:.2f}'\n        cv2.putText(frame, label_text, (int(left), int(top) - 10),\n                    cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n    \n    out.write(frame)\n\n# Phase 5: Cleanup\ncap.release()\nout.release()\ncv2.destroyAllWindows()"
      },
      "session_id": "qwen2.5-coder:32b_6eca_psg_batch",
      "metadata": {
        "num_run": 8
      },
      "tags": [
        "benchmark",
        "py_sketch_generator",
        "qwen2.5-coder:32b"
      ],
      "latency": 26.4,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-02-30-30-479169_chatcmpl-3d6bbd6e-eede-46da-ae45-80b0b6e68ccf",
          "traceId": "09f8c9a2",
          "type": "GENERATION",
          "name": "09f8_psg_gen_attempt#1",
          "startTime": "2025-05-19T23:30:30.479000+00:00",
          "endTime": "2025-05-19T23:30:56.879000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {
            "temperature": "0.1",
            "top_p": "0.3"
          },
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 26400.0,
          "costDetails": {},
          "totalTokens": 2503,
          "completionTokens": 923,
          "promptTokens": 1580,
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "createdAt": "2025-05-19T23:30:32.000Z",
      "updatedAt": "2025-05-19T23:31:45.768Z",
      "environment": "default"
    },
    {
      "id": "b2cc4032",
      "timestamp": "2025-05-19T23:29:14.926000+00:00",
      "name": "b2cc_py_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import numpy as np\nimport os\nimport time\nfrom ai_edge_litert.interpreter import Interpreter\nimport cv2\n\n# Phase 1: Setup\n## 1.1 Imports: Already done above\n\n## 1.2 Paths/Parameters\nmodel_path = 'models/ssd-mobilenet_v1/detect.tflite'\nlabel_map_path = 'models/ssd-mobilenet_v1/labelmap.txt'\nconfidence_threshold = 0.5\ninput_path = 'data/object_detection/sheeps.mp4'\noutput_path = 'results/object_detection/test_results/sheeps_detections.mp4'\n\n## 1.3 Load Labels (Conditional)\nif os.path.exists(label_map_path):\n    with open(label_map_path, 'r') as f:\n        labels = [line.strip() for line in f.readlines()]\nelse:\n    labels = []\n\n## 1.4 Load Interpreter\ninterpreter = Interpreter(model_path=model_path)\ninterpreter.allocate_tensors()\n\n## 1.5 Get Model Details\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\n# Phase 2: Input Acquisition & Preprocessing Loop\n## 2.1 Acquire Input Data\ncap = cv2.VideoCapture(input_path)\nif not cap.isOpened():\n    raise IOError(\"Cannot open video file\")\n\nframe_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\nframe_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\nfps = cap.get(cv2.CAP_PROP_FPS)\n\n## 2.2 Preprocess Data\ninput_shape = input_details[0]['shape']\npreprocessed_frames = []\n\n# Phase 3: Inference (Run per preprocessed input)\ndef preprocess_frame(frame):\n    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n    frame_resized = cv2.resize(frame_rgb, (input_shape[1], input_shape[2]))\n    return np.expand_dims(frame_resized, axis=0).astype(input_details[0]['dtype'])\n\n# Phase 4: Output Interpretation & Handling Loop\n## 4.1 Get Output Tensor(s)\ndef postprocess_output(output_data):\n    detection_boxes = output_data[0][0]\n    detection_classes = output_data[1][0].astype(int)\n    detection_scores = output_data[2][0]\n    \n    results = []\n    for i in range(len(detection_scores)):\n        if detection_scores[i] > confidence_threshold:\n            class_id = detection_classes[i]\n            box = detection_boxes[i]\n            label = labels[class_id] if class_id < len(labels) else f'unknown_{class_id}'\n            score = detection_scores[i]\n            results.append((box, label, score))\n    return results\n\n## 4.2 Interpret Results\ndef draw_results(frame, results):\n    for box, label, score in results:\n        ymin, xmin, ymax, xmax = box\n        (left, right, top, bottom) = (xmin * frame_width, xmax * frame_width,\n                                      ymin * frame_height, ymax * frame_height)\n        cv2.rectangle(frame, (int(left), int(top)), (int(right), int(bottom)), (0, 255, 0), 2)\n        label_text = f'{label}: {score:.2f}'\n        cv2.putText(frame, label_text, (int(left), int(top) - 10),\n                    cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n\n## 4.3 Handle Output\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\nout = cv2.VideoWriter(output_path, fourcc, fps, (frame_width, frame_height))\n\nwhile cap.isOpened():\n    ret, frame = cap.read()\n    if not ret:\n        break\n    \n    input_data = preprocess_frame(frame)\n    interpreter.set_tensor(input_details[0]['index'], input_data)\n    interpreter.invoke()\n    \n    output_data = [interpreter.get_tensor(output_details[i]['index']) for i in range(len(output_details))]\n    results = postprocess_output(output_data)\n    draw_results(frame, results)\n    \n    out.write(frame)\n\n# Phase 5: Cleanup\ncap.release()\nout.release()\ncv2.destroyAllWindows()"
      },
      "session_id": "qwen2.5-coder:32b_6eca_psg_batch",
      "metadata": {
        "num_run": 7
      },
      "tags": [
        "benchmark",
        "py_sketch_generator",
        "qwen2.5-coder:32b"
      ],
      "latency": 26.656,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-02-29-14-928364_chatcmpl-515bf25c-28db-47b6-a89b-78728d72fe78",
          "traceId": "b2cc4032",
          "type": "GENERATION",
          "name": "b2cc_psg_gen_attempt#1",
          "startTime": "2025-05-19T23:29:14.928000+00:00",
          "endTime": "2025-05-19T23:29:41.584000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {
            "temperature": "0.1",
            "top_p": "0.3"
          },
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 26656.0,
          "costDetails": {},
          "totalTokens": 2515,
          "completionTokens": 935,
          "promptTokens": 1580,
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "createdAt": "2025-05-19T23:29:16.000Z",
      "updatedAt": "2025-05-19T23:30:30.300Z",
      "environment": "default"
    },
    {
      "id": "6c8474cb",
      "timestamp": "2025-05-19T23:27:59.399000+00:00",
      "name": "6c84_py_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import numpy as np\nimport os\nimport time\nfrom ai_edge_litert.interpreter import Interpreter\nimport cv2\n\n# Phase 1: Setup\n## 1.1 Imports: Already done above\n\n## 1.2 Paths/Parameters\nmodel_path = 'models/ssd-mobilenet_v1/detect.tflite'\nlabel_map_path = 'models/ssd-mobilenet_v1/labelmap.txt'\nconfidence_threshold = 0.5\ninput_path = 'data/object_detection/sheeps.mp4'\noutput_path = 'results/object_detection/test_results/sheeps_detections.mp4'\n\n## 1.3 Load Labels (Conditional)\nif os.path.exists(label_map_path):\n    with open(label_map_path, 'r') as f:\n        labels = [line.strip() for line in f.readlines()]\nelse:\n    labels = []\n\n## 1.4 Load Interpreter\ninterpreter = Interpreter(model_path=model_path)\ninterpreter.allocate_tensors()\n\n## 1.5 Get Model Details\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\n# Phase 2: Input Acquisition & Preprocessing Loop\n## 2.1 Acquire Input Data\ncap = cv2.VideoCapture(input_path)\nif not cap.isOpened():\n    raise IOError(\"Cannot open video file\")\n\nframe_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\nframe_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\nfps = cap.get(cv2.CAP_PROP_FPS)\n\n## 2.2 Preprocess Data\ninput_shape = input_details[0]['shape']\npreprocessed_frames = []\n\n# Phase 3: Inference (Run per preprocessed input)\ndef preprocess_frame(frame):\n    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n    frame_resized = cv2.resize(frame_rgb, (input_shape[1], input_shape[2]))\n    return np.expand_dims(frame_resized, axis=0).astype(input_details[0]['dtype'])\n\n# Phase 4: Output Interpretation & Handling Loop\n## 4.1 Get Output Tensor(s)\ndef interpret_output(output_data):\n    detection_boxes = output_data[0][0]\n    detection_classes = output_data[1][0].astype(int)\n    detection_scores = output_data[2][0]\n    \n    results = []\n    for i in range(len(detection_scores)):\n        if detection_scores[i] > confidence_threshold:\n            class_id = detection_classes[i]\n            box = detection_boxes[i]\n            label = labels[class_id - 1] if class_id <= len(labels) else f'unknown_{class_id}'\n            score = detection_scores[i]\n            results.append((box, label, score))\n    return results\n\n## 4.2 Interpret Results\n## 4.3 Handle Output\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\nout = cv2.VideoWriter(output_path, fourcc, fps, (frame_width, frame_height))\n\nwhile cap.isOpened():\n    ret, frame = cap.read()\n    if not ret:\n        break\n    \n    input_data = preprocess_frame(frame)\n    interpreter.set_tensor(input_details[0]['index'], input_data)\n    interpreter.invoke()\n    \n    output_data = [interpreter.get_tensor(output_details[i]['index']) for i in range(len(output_details))]\n    results = interpret_output(output_data)\n    \n    for box, label, score in results:\n        ymin, xmin, ymax, xmax = box\n        (left, right, top, bottom) = (xmin * frame_width, xmax * frame_width,\n                                      ymin * frame_height, ymax * frame_height)\n        cv2.rectangle(frame, (int(left), int(top)), (int(right), int(bottom)), (0, 255, 0), 2)\n        label_text = f'{label}: {score:.2f}'\n        cv2.putText(frame, label_text, (int(left), int(top) - 10),\n                    cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n    \n    out.write(frame)\n\n# Phase 5: Cleanup\ncap.release()\nout.release()\ncv2.destroyAllWindows()"
      },
      "session_id": "qwen2.5-coder:32b_6eca_psg_batch",
      "metadata": {
        "num_run": 6
      },
      "tags": [
        "benchmark",
        "py_sketch_generator",
        "qwen2.5-coder:32b"
      ],
      "latency": 26.359,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-02-27-59-401402_chatcmpl-300d1d29-fbbc-46cf-aeec-aa17b69c06b8",
          "traceId": "6c8474cb",
          "type": "GENERATION",
          "name": "6c84_psg_gen_attempt#1",
          "startTime": "2025-05-19T23:27:59.401000+00:00",
          "endTime": "2025-05-19T23:28:25.760000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {
            "temperature": "0.1",
            "top_p": "0.3"
          },
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 26359.0,
          "costDetails": {},
          "totalTokens": 2503,
          "completionTokens": 923,
          "promptTokens": 1580,
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "createdAt": "2025-05-19T23:28:00.000Z",
      "updatedAt": "2025-05-19T23:29:14.343Z",
      "environment": "default"
    },
    {
      "id": "86311f26",
      "timestamp": "2025-05-19T23:26:42.907000+00:00",
      "name": "8631_py_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import numpy as np\nimport os\nimport time\nfrom ai_edge_litert.interpreter import Interpreter\nimport cv2\n\n# Phase 1: Setup\n## 1.1 Imports: Already done above\n\n## 1.2 Paths/Parameters\nmodel_path = 'models/ssd-mobilenet_v1/detect.tflite'\nlabel_map_path = 'models/ssd-mobilenet_v1/labelmap.txt'\nconfidence_threshold = 0.5\ninput_path = 'data/object_detection/sheeps.mp4'\noutput_path = 'results/object_detection/test_results/sheeps_detections.mp4'\n\n## 1.3 Load Labels (Conditional)\nif os.path.exists(label_map_path):\n    with open(label_map_path, 'r') as f:\n        labels = [line.strip() for line in f.readlines()]\nelse:\n    labels = []\n\n## 1.4 Load Interpreter\ninterpreter = Interpreter(model_path=model_path)\ninterpreter.allocate_tensors()\n\n## 1.5 Get Model Details\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\n# Phase 2: Input Acquisition & Preprocessing Loop\n## 2.1 Acquire Input Data\ncap = cv2.VideoCapture(input_path)\nif not cap.isOpened():\n    raise IOError(\"Cannot open video file\")\n\nframe_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\nframe_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\nfps = cap.get(cv2.CAP_PROP_FPS)\n\n## 2.2 Preprocess Data\ninput_shape = input_details[0]['shape']\npreprocessed_frames = []\n\n# Phase 3: Inference (Run per preprocessed input)\ndef preprocess_frame(frame):\n    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n    frame_resized = cv2.resize(frame_rgb, (input_shape[1], input_shape[2]))\n    return np.expand_dims(frame_resized, axis=0).astype(input_details[0]['dtype'])\n\n# Phase 4: Output Interpretation & Handling Loop\n## 4.1 Get Output Tensor(s)\ndef postprocess_output(output_data):\n    detection_boxes = output_data[0][0]\n    detection_classes = output_data[1][0].astype(int)\n    detection_scores = output_data[2][0]\n    \n    results = []\n    for i in range(len(detection_scores)):\n        if detection_scores[i] > confidence_threshold:\n            class_id = detection_classes[i]\n            box = detection_boxes[i]\n            label = labels[class_id] if class_id < len(labels) else f'unknown_{class_id}'\n            score = detection_scores[i]\n            results.append((box, label, score))\n    return results\n\n## 4.2 Interpret Results\ndef draw_results(frame, results):\n    for box, label, score in results:\n        ymin, xmin, ymax, xmax = box\n        (left, right, top, bottom) = (xmin * frame_width, xmax * frame_width,\n                                      ymin * frame_height, ymax * frame_height)\n        cv2.rectangle(frame, (int(left), int(top)), (int(right), int(bottom)), (0, 255, 0), 2)\n        label_text = f'{label}: {score:.2f}'\n        cv2.putText(frame, label_text, (int(left), int(top) - 10),\n                    cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n\n## 4.3 Handle Output\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\nout = cv2.VideoWriter(output_path, fourcc, fps, (frame_width, frame_height))\n\nwhile cap.isOpened():\n    ret, frame = cap.read()\n    if not ret:\n        break\n    \n    input_data = preprocess_frame(frame)\n    interpreter.set_tensor(input_details[0]['index'], input_data)\n    interpreter.invoke()\n    \n    output_data = [interpreter.get_tensor(output_details[i]['index']) for i in range(len(output_details))]\n    results = postprocess_output(output_data)\n    draw_results(frame, results)\n    \n    out.write(frame)\n\n# Phase 5: Cleanup\ncap.release()\nout.release()\ncv2.destroyAllWindows()"
      },
      "session_id": "qwen2.5-coder:32b_6eca_psg_batch",
      "metadata": {
        "num_run": 5
      },
      "tags": [
        "benchmark",
        "py_sketch_generator",
        "qwen2.5-coder:32b"
      ],
      "latency": 26.817,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-02-26-42-909036_chatcmpl-d2cf0808-d9eb-4d2e-9405-b598a79115e4",
          "traceId": "86311f26",
          "type": "GENERATION",
          "name": "8631_psg_gen_attempt#1",
          "startTime": "2025-05-19T23:26:42.909000+00:00",
          "endTime": "2025-05-19T23:27:09.726000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {
            "temperature": "0.1",
            "top_p": "0.3"
          },
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 26817.0,
          "costDetails": {},
          "totalTokens": 2515,
          "completionTokens": 935,
          "promptTokens": 1580,
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "createdAt": "2025-05-19T23:26:44.000Z",
      "updatedAt": "2025-05-19T23:27:58.797Z",
      "environment": "default"
    },
    {
      "id": "224f543b",
      "timestamp": "2025-05-19T23:25:23.346000+00:00",
      "name": "224f_py_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import numpy as np\nimport os\nimport time\nfrom ai_edge_litert.interpreter import Interpreter\nimport cv2\n\n# Phase 1: Setup\n## 1.1 Imports: Already done above\n\n## 1.2 Paths/Parameters\nmodel_path = 'models/ssd-mobilenet_v1/detect.tflite'\nlabel_map_path = 'models/ssd-mobilenet_v1/labelmap.txt'\nconfidence_threshold = 0.5\ninput_path = 'data/object_detection/sheeps.mp4'\noutput_path = 'results/object_detection/test_results/sheeps_detections.mp4'\n\n## 1.3 Load Labels (Conditional)\nif os.path.exists(label_map_path):\n    with open(label_map_path, 'r') as f:\n        labels = [line.strip() for line in f.readlines()]\nelse:\n    labels = []\n\n## 1.4 Load Interpreter\ninterpreter = Interpreter(model_path=model_path)\ninterpreter.allocate_tensors()\n\n## 1.5 Get Model Details\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\n# Phase 2: Input Acquisition & Preprocessing Loop\n## 2.1 Acquire Input Data\ncap = cv2.VideoCapture(input_path)\nif not cap.isOpened():\n    raise IOError(\"Cannot open video file\")\n\nframe_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\nframe_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\nfps = cap.get(cv2.CAP_PROP_FPS)\n\n## 2.2 Preprocess Data\ninput_shape = input_details[0]['shape']\npreprocessed_frames = []\n\n# Phase 3: Inference (Run per preprocessed input)\ndef preprocess_frame(frame):\n    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n    frame_resized = cv2.resize(frame_rgb, (input_shape[1], input_shape[2]))\n    return np.expand_dims(frame_resized, axis=0).astype(input_details[0]['dtype'])\n\n# Phase 4: Output Interpretation & Handling Loop\n## 4.1 Get Output Tensor(s)\ndef interpret_output(output_data):\n    detection_boxes = output_data[0][0]\n    detection_classes = output_data[1][0].astype(int)\n    detection_scores = output_data[2][0]\n    \n    results = []\n    for i in range(len(detection_scores)):\n        if detection_scores[i] > confidence_threshold:\n            class_id = detection_classes[i]\n            box = detection_boxes[i]\n            label = labels[class_id - 1] if class_id <= len(labels) else f'unknown_{class_id}'\n            score = detection_scores[i]\n            results.append((box, label, score))\n    return results\n\n## 4.2 Interpret Results\n## 4.3 Handle Output\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\nout = cv2.VideoWriter(output_path, fourcc, fps, (frame_width, frame_height))\n\nwhile cap.isOpened():\n    ret, frame = cap.read()\n    if not ret:\n        break\n    \n    input_data = preprocess_frame(frame)\n    interpreter.set_tensor(input_details[0]['index'], input_data)\n    interpreter.invoke()\n    \n    output_data = [interpreter.get_tensor(output_details[i]['index']) for i in range(len(output_details))]\n    results = interpret_output(output_data)\n    \n    for box, label, score in results:\n        ymin, xmin, ymax, xmax = box\n        (left, right, top, bottom) = (xmin * frame_width, xmax * frame_width,\n                                      ymin * frame_height, ymax * frame_height)\n        cv2.rectangle(frame, (int(left), int(top)), (int(right), int(bottom)), (0, 255, 0), 2)\n        label_text = f'{label}: {score:.2f}'\n        cv2.putText(frame, label_text, (int(left), int(top) - 10),\n                    cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n    \n    out.write(frame)\n\n# Phase 5: Cleanup\ncap.release()\nout.release()\ncv2.destroyAllWindows()"
      },
      "session_id": "qwen2.5-coder:32b_6eca_psg_batch",
      "metadata": {
        "num_run": 4
      },
      "tags": [
        "benchmark",
        "py_sketch_generator",
        "qwen2.5-coder:32b"
      ],
      "latency": 30.41,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-02-25-23-347554_chatcmpl-d48d51eb-91a7-4063-b19f-867bf695a190",
          "traceId": "224f543b",
          "type": "GENERATION",
          "name": "224f_psg_gen_attempt#1",
          "startTime": "2025-05-19T23:25:23.347000+00:00",
          "endTime": "2025-05-19T23:25:53.757000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {
            "temperature": "0.1",
            "top_p": "0.3"
          },
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 30410.0,
          "costDetails": {},
          "totalTokens": 2512,
          "completionTokens": 932,
          "promptTokens": 1580,
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "createdAt": "2025-05-19T23:25:24.000Z",
      "updatedAt": "2025-05-19T23:26:42.722Z",
      "environment": "default"
    },
    {
      "id": "7ba86c30",
      "timestamp": "2025-05-19T23:24:03.763000+00:00",
      "name": "7ba8_py_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import numpy as np\nimport os\nimport time\nfrom ai_edge_litert.interpreter import Interpreter\nimport cv2\n\n# Phase 1: Setup\n## 1.1 Imports: Already done above\n\n## 1.2 Paths/Parameters\nmodel_path = 'models/ssd-mobilenet_v1/detect.tflite'\nlabel_map_path = 'models/ssd-mobilenet_v1/labelmap.txt'\nconfidence_threshold = 0.5\ninput_path = 'data/object_detection/sheeps.mp4'\noutput_path = 'results/object_detection/test_results/sheeps_detections.mp4'\n\n## 1.3 Load Labels (Conditional)\nif os.path.exists(label_map_path):\n    with open(label_map_path, 'r') as f:\n        labels = [line.strip() for line in f.readlines()]\nelse:\n    labels = []\n\n## 1.4 Load Interpreter\ninterpreter = Interpreter(model_path=model_path)\ninterpreter.allocate_tensors()\n\n## 1.5 Get Model Details\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\n# Phase 2: Input Acquisition & Preprocessing Loop\n## 2.1 Acquire Input Data\ncap = cv2.VideoCapture(input_path)\nif not cap.isOpened():\n    raise IOError(\"Cannot open video file\")\n\nframe_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\nframe_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\nfps = cap.get(cv2.CAP_PROP_FPS)\n\n## 2.2 Preprocess Data\ninput_shape = input_details[0]['shape']\npreprocessed_frames = []\n\n# Phase 3: Inference (Run per preprocessed input)\ndef preprocess_frame(frame):\n    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n    frame_resized = cv2.resize(frame_rgb, (input_shape[1], input_shape[2]))\n    return np.expand_dims(frame_resized, axis=0).astype(input_details[0]['dtype'])\n\n# Phase 4: Output Interpretation & Handling Loop\n## 4.1 Get Output Tensor(s)\ndef postprocess_output(output_data):\n    detection_boxes = output_data[0][0]\n    detection_classes = output_data[1][0].astype(int)\n    detection_scores = output_data[2][0]\n    \n    results = []\n    for i in range(len(detection_scores)):\n        if detection_scores[i] > confidence_threshold:\n            class_id = detection_classes[i]\n            box = detection_boxes[i]\n            label = labels[class_id] if class_id < len(labels) else f'unknown_{class_id}'\n            score = detection_scores[i]\n            results.append((box, label, score))\n    return results\n\n## 4.2 Interpret Results\ndef draw_results(frame, results):\n    for box, label, score in results:\n        ymin, xmin, ymax, xmax = box\n        (left, right, top, bottom) = (xmin * frame_width, xmax * frame_width,\n                                      ymin * frame_height, ymax * frame_height)\n        cv2.rectangle(frame, (int(left), int(top)), (int(right), int(bottom)), (0, 255, 0), 2)\n        label_text = f'{label}: {score:.2f}'\n        cv2.putText(frame, label_text, (int(left), int(top) - 10),\n                    cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n\n## 4.3 Handle Output\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\nout = cv2.VideoWriter(output_path, fourcc, fps, (frame_width, frame_height))\n\nwhile cap.isOpened():\n    ret, frame = cap.read()\n    if not ret:\n        break\n    \n    input_data = preprocess_frame(frame)\n    interpreter.set_tensor(input_details[0]['index'], input_data)\n    interpreter.invoke()\n    \n    output_data = [interpreter.get_tensor(output_details[i]['index']) for i in range(len(output_details))]\n    results = postprocess_output(output_data)\n    draw_results(frame, results)\n    \n    out.write(frame)\n\n# Phase 5: Cleanup\ncap.release()\nout.release()\ncv2.destroyAllWindows()"
      },
      "session_id": "qwen2.5-coder:32b_6eca_psg_batch",
      "metadata": {
        "num_run": 3
      },
      "tags": [
        "benchmark",
        "py_sketch_generator",
        "qwen2.5-coder:32b"
      ],
      "latency": 30.531,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-02-24-03-764643_chatcmpl-5bcaa3c4-d9e3-44ea-befa-1805d8c1079e",
          "traceId": "7ba86c30",
          "type": "GENERATION",
          "name": "7ba8_psg_gen_attempt#1",
          "startTime": "2025-05-19T23:24:03.764000+00:00",
          "endTime": "2025-05-19T23:24:34.295000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {
            "temperature": "0.1",
            "top_p": "0.3"
          },
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 30531.0,
          "costDetails": {},
          "totalTokens": 2515,
          "completionTokens": 935,
          "promptTokens": 1580,
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "createdAt": "2025-05-19T23:24:05.000Z",
      "updatedAt": "2025-05-19T23:25:23.035Z",
      "environment": "default"
    },
    {
      "id": "9ca08c4a",
      "timestamp": "2025-05-19T23:22:37.238000+00:00",
      "name": "9ca0_py_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import numpy as np\nimport os\nimport time\nfrom ai_edge_litert.interpreter import Interpreter\nimport cv2\n\n# Phase 1: Setup\n## 1.1 Imports: Already done above\n\n## 1.2 Paths/Parameters\nmodel_path = 'models/ssd-mobilenet_v1/detect.tflite'\nlabel_map_path = 'models/ssd-mobilenet_v1/labelmap.txt'\nconfidence_threshold = 0.5\ninput_path = 'data/object_detection/sheeps.mp4'\noutput_path = 'results/object_detection/test_results/sheeps_detections.mp4'\n\n## 1.3 Load Labels (Conditional)\nif os.path.exists(label_map_path):\n    with open(label_map_path, 'r') as f:\n        labels = [line.strip() for line in f.readlines()]\nelse:\n    labels = []\n\n## 1.4 Load Interpreter\ninterpreter = Interpreter(model_path=model_path)\ninterpreter.allocate_tensors()\n\n## 1.5 Get Model Details\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\n# Phase 2: Input Acquisition & Preprocessing Loop\n## 2.1 Acquire Input Data\ncap = cv2.VideoCapture(input_path)\nif not cap.isOpened():\n    raise IOError(\"Cannot open video file\")\n\nframe_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\nframe_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\nfps = cap.get(cv2.CAP_PROP_FPS)\n\n## 2.2 Preprocess Data\ninput_shape = input_details[0]['shape']\npreprocessed_frames = []\n\n# Phase 3: Inference (Run per preprocessed input)\ndef preprocess_frame(frame):\n    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n    frame_resized = cv2.resize(frame_rgb, (input_shape[1], input_shape[2]))\n    return np.expand_dims(frame_resized, axis=0).astype(input_details[0]['dtype'])\n\n# Phase 4: Output Interpretation & Handling Loop\n## 4.1 Get Output Tensor(s)\ndef postprocess_output(output_data):\n    detection_boxes = output_data[0][0]\n    detection_classes = output_data[1][0].astype(int)\n    detection_scores = output_data[2][0]\n    \n    results = []\n    for i in range(len(detection_scores)):\n        if detection_scores[i] > confidence_threshold:\n            class_id = detection_classes[i]\n            box = detection_boxes[i]\n            label = labels[class_id] if class_id < len(labels) else f'unknown_{class_id}'\n            score = detection_scores[i]\n            results.append((box, label, score))\n    return results\n\n## 4.2 Interpret Results\ndef draw_results(frame, results):\n    for box, label, score in results:\n        ymin, xmin, ymax, xmax = box\n        height, width, _ = frame.shape\n        (xminn, xmaxx, yminn, ymaxx) = (int(xmin * width), int(xmax * width),\n                                        int(ymin * height), int(ymax * height))\n        \n        cv2.rectangle(frame, (xminn, yminn), (xmaxx, ymaxx), (0, 255, 0), 2)\n        label_text = f'{label}: {score:.2f}'\n        cv2.putText(frame, label_text, (xminn, yminn - 10),\n                    cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n\n## 4.3 Handle Output\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\nout = cv2.VideoWriter(output_path, fourcc, fps, (frame_width, frame_height))\n\nwhile cap.isOpened():\n    ret, frame = cap.read()\n    if not ret:\n        break\n    \n    input_data = preprocess_frame(frame)\n    interpreter.set_tensor(input_details[0]['index'], input_data)\n    interpreter.invoke()\n    \n    output_data = [interpreter.get_tensor(output_details[i]['index']) for i in range(len(output_details))]\n    results = postprocess_output(output_data)\n    draw_results(frame, results)\n    \n    out.write(frame)\n\n# Phase 5: Cleanup\ncap.release()\nout.release()\ncv2.destroyAllWindows()"
      },
      "session_id": "qwen2.5-coder:32b_6eca_psg_batch",
      "metadata": {
        "num_run": 2
      },
      "tags": [
        "benchmark",
        "py_sketch_generator",
        "qwen2.5-coder:32b"
      ],
      "latency": 37.131,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-02-22-37-239589_chatcmpl-1712faa7-e5b0-40d4-b215-47c398c58d3f",
          "traceId": "9ca08c4a",
          "type": "GENERATION",
          "name": "9ca0_psg_gen_attempt#1",
          "startTime": "2025-05-19T23:22:37.239000+00:00",
          "endTime": "2025-05-19T23:23:14.370000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {
            "temperature": "0.1",
            "top_p": "0.3"
          },
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 37131.0,
          "costDetails": {},
          "totalTokens": 2533,
          "completionTokens": 953,
          "promptTokens": 1580,
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "createdAt": "2025-05-19T23:22:38.000Z",
      "updatedAt": "2025-05-19T23:24:03.126Z",
      "environment": "default"
    },
    {
      "id": "25149142",
      "timestamp": "2025-05-19T23:14:47.604000+00:00",
      "name": "2514_py_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import numpy as np\nimport os\nimport time\nfrom ai_edge_litert.interpreter import Interpreter\nimport cv2\n\n# Phase 1: Setup\n## 1.1 Imports: Already done above\n\n## 1.2 Paths/Parameters\nmodel_path = 'models/ssd-mobilenet_v1/detect.tflite'\nlabel_map_path = 'models/ssd-mobilenet_v1/labelmap.txt'\nconfidence_threshold = 0.5\ninput_path = 'data/object_detection/sheeps.mp4'\noutput_path = 'results/object_detection/test_results/sheeps_detections.mp4'\n\n## 1.3 Load Labels (Conditional)\nif os.path.exists(label_map_path):\n    with open(label_map_path, 'r') as f:\n        labels = [line.strip() for line in f.readlines()]\nelse:\n    labels = []\n\n## 1.4 Load Interpreter\ninterpreter = Interpreter(model_path=model_path)\ninterpreter.allocate_tensors()\n\n## 1.5 Get Model Details\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\n# Phase 2: Input Acquisition & Preprocessing Loop\n## 2.1 Acquire Input Data\ncap = cv2.VideoCapture(input_path)\nif not cap.isOpened():\n    raise IOError(\"Cannot open video file\")\n\nframe_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\nframe_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\nfps = cap.get(cv2.CAP_PROP_FPS)\n\n## 2.2 Preprocess Data\ninput_shape = input_details[0]['shape']\npreprocessed_frames = []\n\n# Phase 3: Inference (Run per preprocessed input)\ndef preprocess_frame(frame):\n    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n    frame_resized = cv2.resize(frame_rgb, (input_shape[1], input_shape[2]))\n    return np.expand_dims(frame_resized, axis=0).astype(input_details[0]['dtype'])\n\n# Phase 4: Output Interpretation & Handling Loop\n## 4.1 Get Output Tensor(s)\ndef postprocess_output(output_data):\n    detection_boxes = output_data[0][0]\n    detection_classes = output_data[1][0].astype(int)\n    detection_scores = output_data[2][0]\n    \n    results = []\n    for i in range(len(detection_scores)):\n        if detection_scores[i] > confidence_threshold:\n            class_id = detection_classes[i]\n            box = detection_boxes[i]\n            label = labels[class_id] if class_id < len(labels) else f'unknown_{class_id}'\n            score = detection_scores[i]\n            results.append((box, label, score))\n    return results\n\n## 4.2 Interpret Results\ndef draw_results(frame, results):\n    for box, label, score in results:\n        ymin, xmin, ymax, xmax = box\n        height, width, _ = frame.shape\n        (xminn, xmaxx, yminn, ymaxx) = (int(xmin * width), int(xmax * width),\n                                        int(ymin * height), int(ymax * height))\n        \n        cv2.rectangle(frame, (xminn, yminn), (xmaxx, ymaxx), (0, 255, 0), 2)\n        label_text = f'{label}: {score:.2f}'\n        cv2.putText(frame, label_text, (xminn, yminn - 10),\n                    cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n\n## 4.3 Handle Output\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\nout = cv2.VideoWriter(output_path, fourcc, fps, (frame_width, frame_height))\n\nwhile cap.isOpened():\n    ret, frame = cap.read()\n    if not ret:\n        break\n    \n    input_data = preprocess_frame(frame)\n    interpreter.set_tensor(input_details[0]['index'], input_data)\n    interpreter.invoke()\n    \n    output_data = [interpreter.get_tensor(output_details[i]['index']) for i in range(len(output_details))]\n    results = postprocess_output(output_data)\n    draw_results(frame, results)\n    \n    out.write(frame)\n\n# Phase 5: Cleanup\ncap.release()\nout.release()\ncv2.destroyAllWindows()"
      },
      "session_id": "qwen2.5-coder:32b_6eca_psg_batch",
      "metadata": {
        "num_run": 1
      },
      "tags": [
        "benchmark",
        "py_sketch_generator",
        "qwen2.5-coder:32b"
      ],
      "latency": 418.851,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-02-14-47-605464_chatcmpl-4eb2561f-1912-4691-a623-eb7148f7a480",
          "traceId": "25149142",
          "type": "GENERATION",
          "name": "2514_psg_gen_attempt#1",
          "startTime": "2025-05-19T23:14:47.605000+00:00",
          "endTime": "2025-05-19T23:21:46.456000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {
            "temperature": "0.1",
            "top_p": "0.3"
          },
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 418851.0,
          "costDetails": {},
          "totalTokens": 2533,
          "completionTokens": 953,
          "promptTokens": 1580,
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "createdAt": "2025-05-19T23:14:49.000Z",
      "updatedAt": "2025-05-19T23:22:37.286Z",
      "environment": "default"
    }
  ],
  "meta": {
    "total_items": 30
  }
}
{
  "data": [
    {
      "id": "afad0c8e",
      "timestamp": "2025-07-29T11:40:03.731000+00:00",
      "name": "afad_tpu_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import numpy as np\nfrom tflite_runtime.interpreter import Interpreter, load_delegate\nimport cv2\n\n# Phase 1: Setup\nmodel_path  = \"/home/mendel/tinyml_autopilot/models/edgetpu_detect.tflite\"\nlabel_path  = \"/home/mendel/tinyml_autopilot/models/labelmap.txt\"\ninput_path  = \"/home/mendel/tinyml_autopilot/data//sheeps.mp4\"\noutput_path  = \"/home/mendel/tinyml_autopilot/results/sheeps_detections.mp4\"\nconfidence_threshold  = 0.5\n\n# Load Labels\nwith open(label_path, 'r') as f:\n    labels = [line.strip() for line in f.readlines()]\n\n# Load Interpreter with EdgeTPU\ntry:\n    interpreter = Interpreter(model_path=model_path,\n                              experimental_delegates=[load_delegate('libedgetpu.so.1.0')])\nexcept Exception as e:\n    print(f\"Failed to load delegate with libedgetpu.so.1.0: {e}\")\n    try:\n        interpreter = Interpreter(model_path=model_path,\n                                  experimental_delegates=[load_delegate('/usr/lib/aarch64-linux-gnu/libedgetpu.so.1.0')])\n    except Exception as e:\n        print(f\"Failed to load delegate with /usr/lib/aarch64-linux-gnu/libedgetpu.so.1.0: {e}\")\n        raise RuntimeError(\"Could not load EdgeTPU delegate\")\n\ninterpreter.allocate_tensors()\n\n# Get Model Details\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\n# Extract height and width from the input shape, assuming the last two dimensions are height and width\ninput_shape = input_details[0]['shape']\nheight, width = input_shape[1], input_shape[2]  # Adjusted to account for batch dimension\nfloating_model = (input_details[0]['dtype'] == np.float32)\n\n# Phase 2: Input Acquisition & Preprocessing Loop\ncap = cv2.VideoCapture(input_path)\nif not cap.isOpened():\n    raise RuntimeError(\"Could not open video file\")\n\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\nout = cv2.VideoWriter(output_path, fourcc, 30.0, (int(cap.get(3)), int(cap.get(4))))\n\nwhile cap.isOpened():\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    # Preprocess Data\n    input_data = cv2.resize(frame, (width, height))\n    input_data = np.expand_dims(input_data, axis=0)  # Add batch dimension\n    \n    if floating_model:\n        input_data = (np.float32(input_data) - 127.5) / 127.5\n\n    # Phase 3: Inference\n    interpreter.set_tensor(input_details[0]['index'], input_data)\n    interpreter.invoke()\n\n    # Phase 4: Output Interpretation & Handling Loop\n    boxes = interpreter.get_tensor(output_details[0]['index'])[0]      # Bounding boxes\n    class_ids = interpreter.get_tensor(output_details[1]['index'])[0]   # Class IDs\n    scores = interpreter.get_tensor(output_details[2]['index'])[0]     # Scores\n    num_detections = int(interpreter.get_tensor(output_details[3]['index']))\n\n    for i in range(num_detections):\n        score = float(scores[i])\n        if score > confidence_threshold:\n            bbox = boxes[i] * np.array([height, width, height, width])\n            ymin, xmin, ymax, xmax = bbox.astype(int)\n            class_id = int(class_ids[i])\n\n            label = f\"{labels[class_id]}: {score:.2f}\"\n            cv2.rectangle(frame, (xmin, ymin), (xmax, ymax), (0, 255, 0), 2)\n            cv2.putText(frame, label, (xmin, ymin - 10),\n                        cv2.FONT_HERSHEY_SIMPLEX, 0.9, (36, 255, 12), 2)\n\n    out.write(frame)\n\n# Phase 5: Cleanup\ncap.release()\nout.release()\ncv2.destroyAllWindows()"
      },
      "session_id": "qwen2.5-coder:32b_ae24_tpsg_batch",
      "metadata": {
        "num_run": 19
      },
      "tags": [
        "experiment",
        "qwen2.5-coder:32b",
        "tpu_sketch_generator"
      ],
      "latency": 133.115,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-14-40-03-732864_chatcmpl-3fd206e5-8d2a-4a7f-8770-750d82aa831f",
          "traceId": "afad0c8e",
          "type": "GENERATION",
          "name": "afad_tpusg_gen_attempt#1",
          "startTime": "2025-07-29T11:40:03.732000+00:00",
          "endTime": "2025-07-29T11:40:27.031000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 23299.0,
          "costDetails": {},
          "completionTokens": 790,
          "totalTokens": 2609,
          "promptTokens": 1819,
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-14-40-31-689168_chatcmpl-5f107ee4-fc6f-41f5-9625-6078e037d1e1",
          "traceId": "afad0c8e",
          "type": "GENERATION",
          "name": "afad_tpusg_gen_attempt#2",
          "startTime": "2025-07-29T11:40:31.689000+00:00",
          "endTime": "2025-07-29T11:40:59.759000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 28070.0,
          "costDetails": {},
          "completionTokens": 960,
          "totalTokens": 3008,
          "promptTokens": 2048,
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-14-41-07-763870_chatcmpl-fc9e55e3-6c06-4489-99a2-6512da9ba356",
          "traceId": "afad0c8e",
          "type": "GENERATION",
          "name": "afad_tpusg_gen_attempt#3",
          "startTime": "2025-07-29T11:41:07.763000+00:00",
          "endTime": "2025-07-29T11:41:37.252000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 29489.0,
          "costDetails": {},
          "completionTokens": 1009,
          "totalTokens": 3057,
          "promptTokens": 2048,
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-14-41-45-670922_chatcmpl-76962b31-394b-48d7-9b13-e77f96b41638",
          "traceId": "afad0c8e",
          "type": "GENERATION",
          "name": "afad_tpusg_gen_attempt#4",
          "startTime": "2025-07-29T11:41:45.670000+00:00",
          "endTime": "2025-07-29T11:42:16.847000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 31177.0,
          "costDetails": {},
          "completionTokens": 1076,
          "totalTokens": 3124,
          "promptTokens": 2048,
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "createdAt": "2025-07-29T11:40:04.250Z",
      "updatedAt": "2025-07-29T11:43:14.644Z",
      "environment": "default"
    },
    {
      "id": "b9b901f7",
      "timestamp": "2025-07-29T11:37:38.969000+00:00",
      "name": "b9b9_tpu_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import os\nimport time\nimport numpy as np  # Corrected import for numpy\nfrom tflite_runtime.interpreter import Interpreter, load_delegate\nimport cv2  # Import OpenCV\n\n# Phase 1: Setup\n# Paths/Parameters\nmodel_path   = \"/home/mendel/tinyml_autopilot/models/edgetpu_detect.tflite\"\nlabel_path   = \"/home/mendel/tinyml_autopilot/models/labelmap.txt\"\ninput_path   = \"/home/mendel/tinyml_autopilot/data//sheeps.mp4\"\noutput_path   = \"/home/mendel/tinyml_autopilot/results/sheeps_detections.mp4\"\nconfidence_threshold   = 0.5\n\n# Load Labels\nwith open(label_path, 'r') as f:\n    labels = [line.strip() for line in f.readlines()]\n\n# Load Interpreter with EdgeTPU\ntry:\n    interpreter = Interpreter(model_path=model_path,\n                              experimental_delegates=[load_delegate('libedgetpu.so.1.0')])\nexcept Exception as e:\n    print(f\"Failed to load EdgeTPU delegate: {e}\")\n    interpreter = Interpreter(model_path=model_path,\n                              experimental_delegates=[load_delegate('/usr/lib/aarch64-linux-gnu/libedgetpu.so.1.0')])\n\ninterpreter.allocate_tensors()\n\n# Get Model Details\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\nfloating_model = (input_details[0]['dtype'] == np.float32)\nheight, width = input_details[0]['shape'][1:3]\n\n# Phase 2: Input Acquisition & Preprocessing Loop\ncap = cv2.VideoCapture(input_path)\nfourcc = cv2.VideoWriter_fourcc(*'XVID')\nout = cv2.VideoWriter(output_path, fourcc, 20.0, (int(cap.get(3)), int(cap.get(4))))\n\nwhile cap.isOpened():\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    # Preprocess Data\n    input_data = cv2.resize(frame, (width, height))\n    input_data = np.expand_dims(input_data, axis=0)\n\n    if floating_model:\n        input_data = (np.float32(input_data) - 127.5) / 127.5\n\n    # Phase 3: Inference\n    interpreter.set_tensor(input_details[0]['index'], input_data)\n    interpreter.invoke()\n\n    # Phase 4: Output Interpretation & Handling Loop\n    output_data = [interpreter.get_tensor(output_details[i]['index']) for i in range(len(output_details))]\n\n    boxes = output_data[0][0]\n    classes = output_data[1][0]\n    scores = output_data[2][0]\n    num_detections = int(output_data[3][0])\n\n    # Interpret Results and Apply Confidence Threshold\n    for i in range(num_detections):\n        if scores[i] > confidence_threshold:\n            ymin, xmin, ymax, xmax = boxes[i]\n            (left, right, top, bottom) = (xmin * cap.get(3), xmax * cap.get(3),\n                                          ymin * cap.get(4), ymax * cap.get(4))\n            \n            # Draw bounding box\n            cv2.rectangle(frame, (int(left), int(top)), (int(right), int(bottom)), (0, 255, 0), 2)\n            label = f'{labels[int(classes[i])]}: {scores[i]:.2f}'\n            cv2.putText(frame, label, (int(left), int(top) - 10),\n                        cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n\n    # Write the frame with detection to output video\n    out.write(frame)\n\n# Phase 5: Cleanup\ncap.release()\nout.release()\ncv2.destroyAllWindows()"
      },
      "session_id": "qwen2.5-coder:32b_ae24_tpsg_batch",
      "metadata": {
        "num_run": 18
      },
      "tags": [
        "experiment",
        "qwen2.5-coder:32b",
        "tpu_sketch_generator"
      ],
      "latency": 85.978,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-14-37-38-971438_chatcmpl-a8feb0be-6667-4202-944d-16b9d1394cf6",
          "traceId": "b9b901f7",
          "type": "GENERATION",
          "name": "b9b9_tpusg_gen_attempt#1",
          "startTime": "2025-07-29T11:37:38.971000+00:00",
          "endTime": "2025-07-29T11:38:02.280000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 23309.0,
          "costDetails": {},
          "completionTokens": 790,
          "totalTokens": 2609,
          "promptTokens": 1819,
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-14-38-07-142755_chatcmpl-2d17eef3-f2b2-4c14-aa49-93cc073f9109",
          "traceId": "b9b901f7",
          "type": "GENERATION",
          "name": "b9b9_tpusg_gen_attempt#2",
          "startTime": "2025-07-29T11:38:07.142000+00:00",
          "endTime": "2025-07-29T11:38:33.844000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 26702.0,
          "costDetails": {},
          "completionTokens": 908,
          "totalTokens": 2956,
          "promptTokens": 2048,
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-14-38-38-526625_chatcmpl-7f0ec69e-7af8-426b-9bb5-59796e280443",
          "traceId": "b9b901f7",
          "type": "GENERATION",
          "name": "b9b9_tpusg_gen_attempt#3",
          "startTime": "2025-07-29T11:38:38.526000+00:00",
          "endTime": "2025-07-29T11:39:04.949000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 26423.0,
          "costDetails": {},
          "completionTokens": 900,
          "totalTokens": 2948,
          "promptTokens": 2048,
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "createdAt": "2025-07-29T11:37:39.488Z",
      "updatedAt": "2025-07-29T11:40:02.679Z",
      "environment": "default"
    },
    {
      "id": "c0e0adb7",
      "timestamp": "2025-07-29T11:36:14.912000+00:00",
      "name": "c0e0_tpu_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import numpy as np\nfrom tflite_runtime.interpreter import Interpreter, load_delegate\nimport cv2\n\n# Phase 1: Setup\n# Paths/Parameters\nmodel_path  = \"/home/mendel/tinyml_autopilot/models/edgetpu_detect.tflite\"\nlabel_path  = \"/home/mendel/tinyml_autopilot/models/labelmap.txt\"\ninput_path  = \"/home/mendel/tinyml_autopilot/data//sheeps.mp4\"\noutput_path  = \"/home/mendel/tinyml_autopilot/results/sheeps_detections.mp4\"\nconfidence_threshold  = 0.5\n\n# Load Labels\nwith open(label_path, 'r') as f:\n    labels = [line.strip() for line in f.readlines()]\n\n# Load Interpreter with EdgeTPU\ntry:\n    interpreter = Interpreter(model_path=model_path, experimental_delegates=[load_delegate('libedgetpu.so.1.0')])\nexcept:\n    try:\n        interpreter = Interpreter(model_path=model_path, experimental_delegates=[load_delegate('/usr/lib/aarch64-linux-gnu/libedgetpu.so.1.0')])\n    except Exception as e:\n        print(f\"Failed to load EdgeTPU delegate: {e}\")\n        raise\n\ninterpreter.allocate_tensors()\n\n# Get Model Details\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\n# Phase 2: Input Acquisition & Preprocessing Loop\ncap = cv2.VideoCapture(input_path)\nif not cap.isOpened():\n    print(\"Error: Could not open video.\")\n    exit()\n\nframe_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\nframe_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\nfps = cap.get(cv2.CAP_PROP_FPS)\n\nout = cv2.VideoWriter(output_path, cv2.VideoWriter_fourcc(*'mp4v'), fps, (frame_width, frame_height))\n\nwhile True:\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    # Preprocess Data\n    input_data = cv2.resize(frame, (300, 300))  # Assuming the model expects 300x300 input\n    input_data = np.expand_dims(input_data, axis=0)\n\n    # Quantization Handling\n    floating_model = (input_details[0]['dtype'] == np.float32)\n    if floating_model:\n        input_data = (np.float32(input_data) - 127.5) / 127.5\n\n    # Phase 3: Inference\n    interpreter.set_tensor(input_details[0]['index'], input_data)\n    interpreter.invoke()\n\n    # Phase 4: Output Interpretation & Handling Loop\n    boxes = interpreter.get_tensor(output_details[0]['index'])\n    classes = interpreter.get_tensor(output_details[1]['index'])\n    scores = interpreter.get_tensor(output_details[2]['index'])\n    num_detections = int(interpreter.get_tensor(output_details[3]['index']))\n\n    # Interpret Results and Apply Confidence Threshold\n    for i in range(num_detections):\n        if scores[0][i] >= confidence_threshold:\n            class_id = int(classes[0][i])\n            label = labels[class_id]\n            box = boxes[0][i]\n\n            ymin, xmin, ymax, xmax = box\n\n            # Scale coordinates to frame dimensions\n            (left, right, top, bottom) = (xmin * frame_width, xmax * frame_width,\n                                          ymin * frame_height, ymax * frame_height)\n\n            # Draw bounding box and label on the frame\n            cv2.rectangle(frame, (int(left), int(top)), (int(right), int(bottom)), (0, 255, 0), 2)\n            cv2.putText(frame, f'{label}: {scores[0][i]:.2f}', (int(left), int(top) - 10),\n                        cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n\n    # Handle Output\n    out.write(frame)\n\n# Phase 5: Cleanup\ncap.release()\nout.release()\ncv2.destroyAllWindows()"
      },
      "session_id": "qwen2.5-coder:32b_ae24_tpsg_batch",
      "metadata": {
        "num_run": 17
      },
      "tags": [
        "experiment",
        "qwen2.5-coder:32b",
        "tpu_sketch_generator"
      ],
      "latency": 25.373,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-14-36-14-913417_chatcmpl-bab7a8f2-618a-4d4b-8cbe-5cc26bd9fe90",
          "traceId": "c0e0adb7",
          "type": "GENERATION",
          "name": "c0e0_tpusg_gen_attempt#1",
          "startTime": "2025-07-29T11:36:14.913000+00:00",
          "endTime": "2025-07-29T11:36:40.286000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 25373.0,
          "costDetails": {},
          "completionTokens": 858,
          "totalTokens": 2677,
          "promptTokens": 1819,
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "createdAt": "2025-07-29T11:36:15.431Z",
      "updatedAt": "2025-07-29T11:37:38.078Z",
      "environment": "default"
    },
    {
      "id": "8955aa03",
      "timestamp": "2025-07-29T11:34:51.309000+00:00",
      "name": "8955_tpu_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import numpy as np\nimport time\nimport os\nimport cv2\n\n# Phase 1: Setup\ntry:\n    from tflite_runtime.interpreter import Interpreter, load_delegate\nexcept ImportError:\n    from tensorflow.lite import Interpreter\n    from tensorflow.lite.experimental import load_delegate\n\n# Paths/Parameters\nmodel_path  = \"/home/mendel/tinyml_autopilot/models/edgetpu_detect.tflite\"\nlabel_path  = \"/home/mendel/tinyml_autopilot/models/labelmap.txt\"\ninput_path  = \"/home/mendel/tinyml_autopilot/data//sheeps.mp4\"\noutput_path  = \"/home/mendel/tinyml_autopilot/results/sheeps_detections.mp4\"\nconfidence_threshold  = 0.5\n\n# Load Labels\nwith open(label_path, 'r') as f:\n    labels = [line.strip() for line in f.readlines()]\n\n# Load Interpreter with EdgeTPU\ntry:\n    interpreter = Interpreter(model_path=model_path,\n                              experimental_delegates=[load_delegate('libedgetpu.so.1.0')])\nexcept Exception as e:\n    print(f\"Failed to load libedgetpu.so.1.0: {e}\")\n    try:\n        interpreter = Interpreter(model_path=model_path,\n                                  experimental_delegates=[load_delegate('/usr/lib/aarch64-linux-gnu/libedgetpu.so.1.0')])\n    except Exception as e:\n        print(f\"Failed to load /usr/lib/aarch64-linux-gnu/libedgetpu.so.1.0: {e}\")\n        raise RuntimeError(\"EdgeTPU delegate not found\")\n\ninterpreter.allocate_tensors()\n\n# Get Model Details\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\nheight = input_details[0]['shape'][1]\nwidth = input_details[0]['shape'][2]\nfloating_model = (input_details[0]['dtype'] == np.float32)\n\n# Phase 2: Input Acquisition & Preprocessing Loop\ncap = cv2.VideoCapture(input_path)\nif not cap.isOpened():\n    raise IOError(\"Cannot open video file\")\n\nframe_width = int(cap.get(3))\nframe_height = int(cap.get(4))\n\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\nout = cv2.VideoWriter(output_path, fourcc, 20.0, (frame_width, frame_height))\n\nwhile cap.isOpened():\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    # Preprocess Data\n    input_data = cv2.resize(frame, (width, height))\n    input_data = np.expand_dims(input_data, axis=0)\n    \n    if floating_model:\n        input_data = (np.float32(input_data) - 127.5) / 127.5\n\n    # Phase 3: Inference\n    interpreter.set_tensor(input_details[0]['index'], input_data)\n    interpreter.invoke()\n\n    # Phase 4: Output Interpretation & Handling Loop\n    boxes = interpreter.get_tensor(output_details[0]['index'])[0]\n    classes = interpreter.get_tensor(output_details[1]['index'])[0]\n    scores = interpreter.get_tensor(output_details[2]['index'])[0]\n\n    for i in range(len(scores)):\n        if scores[i] > confidence_threshold:\n            ymin, xmin, ymax, xmax = boxes[i]\n            (left, right, top, bottom) = (xmin * frame_width, xmax * frame_width,\n                                          ymin * frame_height, ymax * frame_height)\n            \n            label = labels[int(classes[i])]\n            cv2.rectangle(frame, (int(left), int(top)), (int(right), int(bottom)), (0, 255, 0), 2)\n            cv2.putText(frame, f'{label}: {scores[i]:.2f}', (int(left), int(top) - 10),\n                        cv2.FONT_HERSHEY_SIMPLEX, 0.9, (36, 255, 12), 2)\n\n    out.write(frame)\n\n# Phase 5: Cleanup\ncap.release()\nout.release()\ncv2.destroyAllWindows()"
      },
      "session_id": "qwen2.5-coder:32b_ae24_tpsg_batch",
      "metadata": {
        "num_run": 16
      },
      "tags": [
        "experiment",
        "qwen2.5-coder:32b",
        "tpu_sketch_generator"
      ],
      "latency": 25.58,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-14-34-51-310266_chatcmpl-b96b625f-8ea7-433e-b513-2b476835fca9",
          "traceId": "8955aa03",
          "type": "GENERATION",
          "name": "8955_tpusg_gen_attempt#1",
          "startTime": "2025-07-29T11:34:51.310000+00:00",
          "endTime": "2025-07-29T11:35:16.890000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 25580.0,
          "costDetails": {},
          "completionTokens": 850,
          "totalTokens": 2669,
          "promptTokens": 1819,
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "createdAt": "2025-07-29T11:34:51.830Z",
      "updatedAt": "2025-07-29T11:36:14.498Z",
      "environment": "default"
    },
    {
      "id": "a8c38329",
      "timestamp": "2025-07-29T11:32:48.747000+00:00",
      "name": "a8c3_tpu_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import numpy as np\nfrom tflite_runtime.interpreter import Interpreter, load_delegate\nimport cv2\n\n# Phase 1: Setup\n\n# Paths/Parameters\nmodel_path  = \"/home/mendel/tinyml_autopilot/models/edgetpu_detect.tflite\"\nlabel_path  = \"/home/mendel/tinyml_autopilot/models/labelmap.txt\"\ninput_path  = \"/home/mendel/tinyml_autopilot/data//sheeps.mp4\"\noutput_path  = \"/home/mendel/tinyml_autopilot/results/sheeps_detections.mp4\"\nconfidence_threshold  = 0.5\n\n# Load Labels\nwith open(label_path, 'r') as f:\n    labels = [line.strip() for line in f.readlines()]\n\n# Load Interpreter with EdgeTPU\ntry:\n    interpreter = Interpreter(model_path=model_path, experimental_delegates=[load_delegate('libedgetpu.so.1.0')])\nexcept:\n    try:\n        interpreter = Interpreter(model_path=model_path, experimental_delegates=[load_delegate('/usr/lib/aarch64-linux-gnu/libedgetpu.so.1.0')])\n    except Exception as e:\n        raise RuntimeError(\"Failed to load EdgeTPU delegate:\", str(e))\n\ninterpreter.allocate_tensors()\n\n# Get Model Details\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\nfloating_model = (input_details[0]['dtype'] == np.float32)\n\n# Extract height, width from the shape\ninput_shape = input_details[0]['shape']\nheight = input_shape[1]\nwidth = input_shape[2]\n\n# Phase 2: Input Acquisition & Preprocessing Loop\n\n# Acquire Input Data\ncap = cv2.VideoCapture(input_path)\nif not cap.isOpened():\n    raise RuntimeError(\"Error opening video stream or file\")\n\nframe_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\nframe_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\nfps = cap.get(cv2.CAP_PROP_FPS)\n\n# Initialize Video Writer\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\nout = cv2.VideoWriter(output_path, fourcc, fps, (frame_width, frame_height))\n\nwhile cap.isOpened():\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    # Preprocess Data\n    input_data = cv2.resize(frame, (width, height))\n    input_data = np.expand_dims(input_data, axis=0)\n    \n    if floating_model:\n        input_data = (np.float32(input_data) - 127.5) / 127.5\n\n    # Phase 3: Inference\n    interpreter.set_tensor(input_details[0]['index'], input_data)\n    interpreter.invoke()\n\n    # Phase 4: Output Interpretation & Handling Loop\n    output_data = []\n    for i in range(len(output_details)):\n        output_data.append(interpreter.get_tensor(output_details[i]['index']))\n\n    # Assume the model outputs: boxes, class_ids, scores, count\n    boxes = output_data[0][0]\n    class_ids = output_data[1][0]\n    scores = output_data[2][0]\n\n    for box, class_id, score in zip(boxes, class_ids, scores):\n        if score > confidence_threshold:\n            # Scale bounding box coordinates\n            ymin, xmin, ymax, xmax = box\n            (left, right, top, bottom) = (xmin * frame_width, xmax * frame_width,\n                                          ymin * frame_height, ymax * frame_height)\n            \n            left, right, top, bottom = int(left), int(right), int(top), int(bottom)\n\n            # Draw bounding box and label\n            cv2.rectangle(frame, (left, top), (right, bottom), (0, 255, 0), 2)\n            label = f\"{labels[int(class_id)]}: {score:.2f}\"\n            cv2.putText(frame, label, (left, top - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (36, 255, 12), 2)\n\n    # Handle Output\n    out.write(frame)\n\n# Phase 5: Cleanup\ncap.release()\nout.release()\ncv2.destroyAllWindows()"
      },
      "session_id": "qwen2.5-coder:32b_ae24_tpsg_batch",
      "metadata": {
        "num_run": 15
      },
      "tags": [
        "experiment",
        "qwen2.5-coder:32b",
        "tpu_sketch_generator"
      ],
      "latency": 64.602,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-14-32-48-749164_chatcmpl-7a632ae2-9883-4dbb-b610-a012fe214ec6",
          "traceId": "a8c38329",
          "type": "GENERATION",
          "name": "a8c3_tpusg_gen_attempt#1",
          "startTime": "2025-07-29T11:32:48.749000+00:00",
          "endTime": "2025-07-29T11:33:14.689000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 25940.0,
          "costDetails": {},
          "completionTokens": 861,
          "totalTokens": 2680,
          "promptTokens": 1819,
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-14-33-22-344456_chatcmpl-500f9e40-2ddf-43d5-aa52-13e642e6454b",
          "traceId": "a8c38329",
          "type": "GENERATION",
          "name": "a8c3_tpusg_gen_attempt#2",
          "startTime": "2025-07-29T11:33:22.344000+00:00",
          "endTime": "2025-07-29T11:33:53.351000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 31007.0,
          "costDetails": {},
          "completionTokens": 1027,
          "totalTokens": 3075,
          "promptTokens": 2048,
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "createdAt": "2025-07-29T11:32:49.267Z",
      "updatedAt": "2025-07-29T11:34:50.911Z",
      "environment": "default"
    },
    {
      "id": "10b03f33",
      "timestamp": "2025-07-29T11:31:10.977000+00:00",
      "name": "10b0_tpu_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import numpy as np\nfrom tflite_runtime.interpreter import Interpreter, load_delegate\nimport os\nimport cv2\n\n# Phase 1: Setup\n## Paths/Parameters\nmodel_path  = \"/home/mendel/tinyml_autopilot/models/edgetpu_detect.tflite\"\nlabel_path  = \"/home/mendel/tinyml_autopilot/models/labelmap.txt\"\ninput_path  = \"/home/mendel/tinyml_autopilot/data//sheeps.mp4\"\noutput_path  = \"/home/mendel/tinyml_autopilot/results/sheeps_detections.mp4\"\nconfidence_threshold  = 0.5\n\n## Load Labels\nwith open(label_path, 'r') as f:\n    labels = [line.strip() for line in f.readlines()]\n\n## Load Interpreter with EdgeTPU\ntry:\n    interpreter = Interpreter(model_path=model_path, \n                              experimental_delegates=[load_delegate('libedgetpu.so.1.0')])\nexcept Exception as e:\n    try:\n        interpreter = Interpreter(model_path=model_path, \n                                  experimental_delegates=[load_delegate('/usr/lib/aarch64-linux-gnu/libedgetpu.so.1.0')])\n    except Exception as e2:\n        raise RuntimeError(\"Failed to load EdgeTPU delegate\") from e2\n\ninterpreter.allocate_tensors()\n\n## Get Model Details\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\nfloating_model = (input_details[0]['dtype'] == np.float32)\n\n# Phase 2: Input Acquisition & Preprocessing Loop\ncap = cv2.VideoCapture(input_path)\nif not cap.isOpened():\n    raise RuntimeError(\"Could not open video file\")\n\nframe_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\nframe_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\nfps = cap.get(cv2.CAP_PROP_FPS)\n\nout = cv2.VideoWriter(output_path, cv2.VideoWriter_fourcc(*'mp4v'), fps, (frame_width, frame_height))\n\nwhile cap.isOpened():\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    # Preprocess Data\n    input_data = cv2.resize(frame, (input_details[0]['shape'][1], input_details[0]['shape'][2]))\n    input_data = np.expand_dims(input_data, axis=0)\n    \n    if floating_model:\n        input_data = (np.float32(input_data) - 127.5) / 127.5\n\n    # Phase 3: Inference\n    interpreter.set_tensor(input_details[0]['index'], input_data)\n    interpreter.invoke()\n\n    # Phase 4: Output Interpretation & Handling Loop\n    ## Get Output Tensor(s)\n    boxes = interpreter.get_tensor(output_details[0]['index'])[0]\n    classes = interpreter.get_tensor(output_details[1]['index'])[0]\n    scores = interpreter.get_tensor(output_details[2]['index'])[0]\n    num_detections = int(interpreter.get_tensor(output_details[3]['index']))\n\n    ## Interpret Results\n    for i in range(num_detections):\n        if scores[i] > confidence_threshold:\n            ymin, xmin, ymax, xmax = boxes[i]\n            class_id = int(classes[i])\n            label = labels[class_id]\n\n            # Post-processing: Scale coordinates to original frame dimensions\n            ymin, ymax = int(ymin * frame_height), int(ymax * frame_height)\n            xmin, xmax = int(xmin * frame_width), int(xmax * frame_width)\n\n            # Draw bounding box and label on the frame\n            cv2.rectangle(frame, (xmin, ymin), (xmax, ymax), (0, 255, 0), 2)\n            cv2.putText(frame, f'{label}: {scores[i]:.2f}', (xmin, ymin - 10),\n                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n\n    ## Handle Output\n    out.write(frame)\n\n# Phase 5: Cleanup\ncap.release()\nout.release()\ncv2.destroyAllWindows()"
      },
      "session_id": "qwen2.5-coder:32b_ae24_tpsg_batch",
      "metadata": {
        "num_run": 14
      },
      "tags": [
        "experiment",
        "qwen2.5-coder:32b",
        "tpu_sketch_generator"
      ],
      "latency": 39.396,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-14-31-10-978540_chatcmpl-9c6b18f4-2b80-4c56-8786-22994d148356",
          "traceId": "10b03f33",
          "type": "GENERATION",
          "name": "10b0_tpusg_gen_attempt#1",
          "startTime": "2025-07-29T11:31:10.978000+00:00",
          "endTime": "2025-07-29T11:31:50.374000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 39396.0,
          "costDetails": {},
          "completionTokens": 847,
          "totalTokens": 2666,
          "promptTokens": 1819,
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "createdAt": "2025-07-29T11:31:11.498Z",
      "updatedAt": "2025-07-29T11:32:48.392Z",
      "environment": "default"
    },
    {
      "id": "99429558",
      "timestamp": "2025-07-29T11:28:58.544000+00:00",
      "name": "9942_tpu_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "from tflite_runtime.interpreter import Interpreter, load_delegate\nimport numpy as np\nimport os\nimport cv2\n\n# Phase 1: Setup\n# Paths/Parameters\nmodel_path  = \"/home/mendel/tinyml_autopilot/models/edgetpu_detect.tflite\"\nlabel_path  = \"/home/mendel/tinyml_autopilot/models/labelmap.txt\"\ninput_path  = \"/home/mendel/tinyml_autopilot/data//sheeps.mp4\"\noutput_path  = \"/home/mendel/tinyml_autopilot/results/sheeps_detections.mp4\"\nconfidence_threshold  = 0.5\n\n# Load Labels\nwith open(label_path, 'r') as f:\n    labels = [line.strip() for line in f.readlines()]\n\n# Load Interpreter with EdgeTPU\ntry:\n    interpreter = Interpreter(model_path=model_path, experimental_delegates=[load_delegate('libedgetpu.so.1.0')])\nexcept Exception as e:\n    try:\n        interpreter = Interpreter(model_path=model_path, experimental_delegates=[load_delegate('/usr/lib/aarch64-linux-gnu/libedgetpu.so.1.0')])\n    except Exception as e:\n        raise RuntimeError(\"Could not load EdgeTPU delegate\") from e\n\ninterpreter.allocate_tensors()\n\n# Get Model Details\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\nfloating_model = (input_details[0]['dtype'] == np.float32)\nheight, width, _ = input_details[0]['shape'][1:4]\n\n# Phase 2: Input Acquisition & Preprocessing Loop\ncap = cv2.VideoCapture(input_path)\n\nif not cap.isOpened():\n    raise RuntimeError(\"Could not open video file\")\n\nframe_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\nframe_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\nfps = cap.get(cv2.CAP_PROP_FPS)\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\nout = cv2.VideoWriter(output_path, fourcc, fps, (frame_width, frame_height))\n\nwhile True:\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    # Preprocess Data\n    input_data = cv2.resize(frame, (width, height))\n    input_data = np.expand_dims(input_data, axis=0)\n\n    # Quantization Handling\n    if floating_model:\n        input_data = (np.float32(input_data) - 127.5) / 127.5\n\n    # Phase 3: Inference\n    interpreter.set_tensor(input_details[0]['index'], input_data)\n    interpreter.invoke()\n\n    # Phase 4: Output Interpretation & Handling Loop\n    boxes = interpreter.get_tensor(output_details[0]['index'])[0]\n    classes = interpreter.get_tensor(output_details[1]['index'])[0].astype(np.int64)\n    scores = interpreter.get_tensor(output_details[2]['index'])[0]\n    num_detections = int(interpreter.get_tensor(output_details[3]['index']))\n\n    for i in range(num_detections):\n        if scores[i] > confidence_threshold:\n            ymin, xmin, ymax, xmax = boxes[i]\n            (left, right, top, bottom) = (xmin * frame_width, xmax * frame_width,\n                                          ymin * frame_height, ymax * frame_height)\n            \n            label = labels[int(classes[i])]\n            cv2.rectangle(frame, (int(left), int(top)), (int(right), int(bottom)), (0, 255, 0), 2)\n            cv2.putText(frame, f'{label}: {scores[i]:.2f}', (int(left), int(top) - 10),\n                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n\n    out.write(frame)\n\n# Phase 5: Cleanup\ncap.release()\nout.release()\ncv2.destroyAllWindows()"
      },
      "session_id": "qwen2.5-coder:32b_ae24_tpsg_batch",
      "metadata": {
        "num_run": 13
      },
      "tags": [
        "experiment",
        "qwen2.5-coder:32b",
        "tpu_sketch_generator"
      ],
      "latency": 73.007,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-14-28-58-545751_chatcmpl-9ebe5f10-b63d-4cd3-ad87-5dbe14cf45a8",
          "traceId": "99429558",
          "type": "GENERATION",
          "name": "9942_tpusg_gen_attempt#1",
          "startTime": "2025-07-29T11:28:58.545000+00:00",
          "endTime": "2025-07-29T11:29:28.940000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 30395.0,
          "costDetails": {},
          "completionTokens": 803,
          "totalTokens": 2622,
          "promptTokens": 1819,
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-14-29-37-205656_chatcmpl-d34d90a3-9f66-41ea-abb2-857f1a46e6e2",
          "traceId": "99429558",
          "type": "GENERATION",
          "name": "9942_tpusg_gen_attempt#2",
          "startTime": "2025-07-29T11:29:37.205000+00:00",
          "endTime": "2025-07-29T11:30:11.552000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 34347.0,
          "costDetails": {},
          "completionTokens": 1038,
          "totalTokens": 3086,
          "promptTokens": 2048,
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "createdAt": "2025-07-29T11:28:59.065Z",
      "updatedAt": "2025-07-29T11:31:09.257Z",
      "environment": "default"
    },
    {
      "id": "1b62b3d4",
      "timestamp": "2025-07-29T11:27:35.717000+00:00",
      "name": "1b62_tpu_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "from tflite_runtime.interpreter import Interpreter, load_delegate\nimport numpy as np\nimport cv2\n\n# Phase 1: Setup\n\n# Paths/Parameters\nmodel_path  = \"/home/mendel/tinyml_autopilot/models/edgetpu_detect.tflite\"\nlabel_path  = \"/home/mendel/tinyml_autopilot/models/labelmap.txt\"\ninput_path  = \"/home/mendel/tinyml_autopilot/data//sheeps.mp4\"\noutput_path  = \"/home/mendel/tinyml_autopilot/results/sheeps_detections.mp4\"\nconfidence_threshold  = 0.5\n\n# Load Labels\nwith open(label_path, 'r') as f:\n    labels = [line.strip() for line in f.readlines()]\n\n# Load Interpreter with EdgeTPU\ntry:\n    interpreter = Interpreter(model_path=model_path,\n                              experimental_delegates=[load_delegate('libedgetpu.so.1.0')])\nexcept Exception as e:\n    try:\n        interpreter = Interpreter(model_path=model_path,\n                                  experimental_delegates=[load_delegate('/usr/lib/aarch64-linux-gnu/libedgetpu.so.1.0')])\n    except Exception as e:\n        raise RuntimeError(\"Failed to load EdgeTPU delegate\") from e\n\ninterpreter.allocate_tensors()\n\n# Get Model Details\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\nfloating_model = (input_details[0]['dtype'] == np.float32)\n\n# Phase 2: Input Acquisition & Preprocessing Loop\n\n# Acquire Input Data\ncap = cv2.VideoCapture(input_path)\nif not cap.isOpened():\n    raise RuntimeError(\"Error opening video stream or file\")\n\nframe_width = int(cap.get(3))\nframe_height = int(cap.get(4))\nout = cv2.VideoWriter(output_path, cv2.VideoWriter_fourcc(*'mp4v'), 30, (frame_width, frame_height))\n\n# Phase 3: Inference\n\nwhile cap.isOpened():\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    # Preprocess Data\n    input_data = cv2.resize(frame, (input_details[0]['shape'][1], input_details[0]['shape'][2]))\n    input_data = np.expand_dims(input_data, axis=0)\n\n    # Quantization Handling\n    if floating_model:\n        input_data = (np.float32(input_data) - 127.5) / 127.5\n\n    # Set Input Tensor(s)\n    interpreter.set_tensor(input_details[0]['index'], input_data)\n\n    # Run Inference\n    interpreter.invoke()\n\n    # Phase 4: Output Interpretation & Handling Loop\n\n    # Get Output Tensor(s)\n    boxes = interpreter.get_tensor(output_details[0]['index'])[0]\n    classes = interpreter.get_tensor(output_details[1]['index'])[0]\n    scores = interpreter.get_tensor(output_details[2]['index'])[0]\n\n    # Interpret Results\n    for i in range(len(scores)):\n        if scores[i] > confidence_threshold:\n            ymin, xmin, ymax, xmax = boxes[i]\n            (left, right, top, bottom) = (xmin * frame_width, xmax * frame_width,\n                                          ymin * frame_height, ymax * frame_height)\n            cv2.rectangle(frame, (int(left), int(top)), (int(right), int(bottom)), (0, 255, 0), 2)\n            label = f'{labels[int(classes[i])]}: {scores[i]:.2f}'\n            cv2.putText(frame, label, (int(left), int(top) - 10),\n                        cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n\n    # Handle Output\n    out.write(frame)\n\n# Phase 5: Cleanup\n\ncap.release()\nout.release()\ncv2.destroyAllWindows()"
      },
      "session_id": "qwen2.5-coder:32b_ae24_tpsg_batch",
      "metadata": {
        "num_run": 12
      },
      "tags": [
        "experiment",
        "qwen2.5-coder:32b",
        "tpu_sketch_generator"
      ],
      "latency": 24.023,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-14-27-35-718711_chatcmpl-a2469144-6b93-41f2-bdaa-a569bd47afdc",
          "traceId": "1b62b3d4",
          "type": "GENERATION",
          "name": "1b62_tpusg_gen_attempt#1",
          "startTime": "2025-07-29T11:27:35.718000+00:00",
          "endTime": "2025-07-29T11:27:59.741000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 24023.0,
          "costDetails": {},
          "completionTokens": 800,
          "totalTokens": 2619,
          "promptTokens": 1819,
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "createdAt": "2025-07-29T11:27:36.239Z",
      "updatedAt": "2025-07-29T11:28:57.411Z",
      "environment": "default"
    },
    {
      "id": "55476dad",
      "timestamp": "2025-07-29T11:24:04.705000+00:00",
      "name": "5547_tpu_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import cv2\nimport numpy as np\nfrom tflite_runtime.interpreter import Interpreter, load_delegate\n\n# Phase 1: Setup\n## 1.1 Imports\n## 1.2 Paths/Parameters\nmodel_path   = \"/home/mendel/tinyml_autopilot/models/edgetpu_detect.tflite\"\nlabel_path   = \"/home/mendel/tinyml_autopilot/models/labelmap.txt\"\ninput_path   = \"/home/mendel/tinyml_autopilot/data//sheeps.mp4\"\noutput_path  = \"/home/mendel/tinyml_autopilot/results/sheeps_detections.mp4\"\nconfidence_threshold  = 0.5\n\n## 1.3 Load Labels\nwith open(label_path, 'r') as f:\n    labels = [line.strip() for line in f.readlines()]\n\n## 1.4 Load Interpreter with EdgeTPU\ntry:\n    interpreter = Interpreter(model_path=model_path,\n                              experimental_delegates=[load_delegate('libedgetpu.so.1.0')])\nexcept Exception as e:\n    print(f\"Failed to load delegate from libedgetpu.so.1.0: {e}\")\n    try:\n        interpreter = Interpreter(model_path=model_path,\n                                  experimental_delegates=[load_delegate('/usr/lib/aarch64-linux-gnu/libedgetpu.so.1.0')])\n    except Exception as e:\n        print(f\"Failed to load delegate from /usr/lib/aarch64-linux-gnu/libedgetpu.so.1.0: {e}\")\n        interpreter = Interpreter(model_path=model_path)\n\ninterpreter.allocate_tensors()\n\n## 1.5 Get Model Details\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\n# Assuming input is a single image of shape (height, width, channels)\nheight = input_details[0]['shape'][1]\nwidth = input_details[0]['shape'][2]\nfloating_model = (input_details[0]['dtype'] == np.float32)\n\n# Phase 2: Input Acquisition & Preprocessing Loop\n## 2.1 Acquire Input Data\ncap = cv2.VideoCapture(input_path)\nif not cap.isOpened():\n    raise IOError(\"Cannot open video file\")\n\n# Get video properties to write output video\nfps = int(cap.get(cv2.CAP_PROP_FPS))\nframe_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\nframe_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\nout = cv2.VideoWriter(output_path, fourcc, fps, (frame_width, frame_height))\n\n## 2.2 Preprocess Data\nwhile cap.isOpened():\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    # Resize and normalize image\n    input_data = cv2.resize(frame, (width, height))\n    input_data = np.expand_dims(input_data, axis=0)\n\n    ## 2.3 Quantization Handling\n    if floating_model:\n        input_data = (np.float32(input_data) - 127.5) / 127.5\n\n    # Phase 3: Inference\n    interpreter.set_tensor(input_details[0]['index'], input_data)\n    interpreter.invoke()\n\n    # Phase 4: Output Interpretation & Handling Loop\n    ## 4.1 Get Output Tensor(s)\n    boxes = interpreter.get_tensor(output_details[0]['index'])\n    classes = interpreter.get_tensor(output_details[1]['index'])\n    scores = interpreter.get_tensor(output_details[2]['index'])\n    num_detections = int(interpreter.get_tensor(output_details[3]['index']))\n\n    ## 4.2 Interpret Results\n    for i in range(num_detections):\n        if scores[0][i] > confidence_threshold:  # Correctly access the score\n            # Get bounding box coordinates and scale them to the original frame size\n            ymin, xmin, ymax, xmax = boxes[0][i]\n            (left, right, top, bottom) = (xmin * frame_width, xmax * frame_width,\n                                          ymin * frame_height, ymax * frame_height)\n\n            # Draw bounding box and label\n            cv2.rectangle(frame, (int(left), int(top)), (int(right), int(bottom)), (0, 255, 0), 2)\n            label = f\"{labels[int(classes[0][i])]}: {scores[0][i]:.2f}\"\n            cv2.putText(frame, label, (int(left), int(top) - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (36, 255, 12), 2)\n\n    # Write the frame to the output video\n    out.write(frame)\n\n# Release resources\ncap.release()\nout.release()\ncv2.destroyAllWindows()"
      },
      "session_id": "qwen2.5-coder:32b_ae24_tpsg_batch",
      "metadata": {
        "num_run": 11
      },
      "tags": [
        "experiment",
        "qwen2.5-coder:32b",
        "tpu_sketch_generator"
      ],
      "latency": 152.24,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-14-24-04-706777_chatcmpl-12444b63-38f9-4090-8fc8-8e85b97a80b1",
          "traceId": "55476dad",
          "type": "GENERATION",
          "name": "5547_tpusg_gen_attempt#1",
          "startTime": "2025-07-29T11:24:04.706000+00:00",
          "endTime": "2025-07-29T11:24:35.014000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 30308.0,
          "costDetails": {},
          "completionTokens": 1015,
          "totalTokens": 2834,
          "promptTokens": 1819,
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-14-24-43-202813_chatcmpl-d4689458-79a2-4030-938c-3af9b4b77aa5",
          "traceId": "55476dad",
          "type": "GENERATION",
          "name": "5547_tpusg_gen_attempt#2",
          "startTime": "2025-07-29T11:24:43.202000+00:00",
          "endTime": "2025-07-29T11:25:17.751000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 34549.0,
          "costDetails": {},
          "completionTokens": 1191,
          "totalTokens": 3239,
          "promptTokens": 2048,
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-14-25-21-833108_chatcmpl-f0b179d6-48db-4992-bbf8-0509bcff19ff",
          "traceId": "55476dad",
          "type": "GENERATION",
          "name": "5547_tpusg_gen_attempt#3",
          "startTime": "2025-07-29T11:25:21.833000+00:00",
          "endTime": "2025-07-29T11:25:55.244000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 33411.0,
          "costDetails": {},
          "completionTokens": 1140,
          "totalTokens": 3188,
          "promptTokens": 2048,
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-14-26-03-578778_chatcmpl-16003678-66d1-432a-8032-9513351a87c7",
          "traceId": "55476dad",
          "type": "GENERATION",
          "name": "5547_tpusg_gen_attempt#4",
          "startTime": "2025-07-29T11:26:03.578000+00:00",
          "endTime": "2025-07-29T11:26:36.946000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 33368.0,
          "costDetails": {},
          "completionTokens": 1160,
          "totalTokens": 3208,
          "promptTokens": 2048,
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "createdAt": "2025-07-29T11:24:05.225Z",
      "updatedAt": "2025-07-29T11:27:35.133Z",
      "environment": "default"
    },
    {
      "id": "9ea6e58c",
      "timestamp": "2025-07-29T11:22:04.024000+00:00",
      "name": "9ea6_tpu_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import numpy as np\nimport cv2\nfrom tflite_runtime.interpreter import Interpreter, load_delegate\n\n# Phase 1: Setup\nmodel_path  = \"/home/mendel/tinyml_autopilot/models/edgetpu_detect.tflite\"\nlabel_path  = \"/home/mendel/tinyml_autopilot/models/labelmap.txt\"\ninput_path  = \"/home/mendel/tinyml_autopilot/data//sheeps.mp4\"\noutput_path  = \"/home/mendel/tinyml_autopilot/results/sheeps_detections.mp4\"\nconfidence_threshold  = 0.5\n\n# Load Labels\nwith open(label_path, 'r') as f:\n    labels = [line.strip() for line in f.readlines()]\n\n# Load Interpreter with EdgeTPU\ntry:\n    interpreter = Interpreter(model_path=model_path, experimental_delegates=[load_delegate('libedgetpu.so.1.0')])\nexcept Exception as e:\n    try:\n        interpreter = Interpreter(model_path=model_path, experimental_delegates=[load_delegate('/usr/lib/aarch64-linux-gnu/libedgetpu.so.1.0')])\n    except Exception as e:\n        raise RuntimeError(\"Failed to load EdgeTPU delegate\") from e\n\ninterpreter.allocate_tensors()\n\n# Get Model Details\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\nbatch_size, height, width, channels = input_details[0]['shape']\nfloating_model = (input_details[0]['dtype'] == np.float32)\n\n# Phase 2: Input Acquisition & Preprocessing Loop\ncap = cv2.VideoCapture(input_path)\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\nout = cv2.VideoWriter(output_path, fourcc, cap.get(cv2.CAP_PROP_FPS), (int(cap.get(3)), int(cap.get(4))))\n\nwhile cap.isOpened():\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    # Preprocess Data\n    input_data = cv2.resize(frame, (width, height))\n    input_data = np.expand_dims(input_data, axis=0)\n\n    if floating_model:\n        input_data = (np.float32(input_data) - 127.5) / 127.5\n\n    # Phase 3: Inference\n    interpreter.set_tensor(input_details[0]['index'], input_data)\n    interpreter.invoke()\n\n    # Phase 4: Output Interpretation & Handling Loop\n    output_data = interpreter.get_tensor(output_details[0]['index'])\n    num_detections = int(interpreter.get_tensor(output_details[3]['index']))\n\n    for i in range(num_detections):\n        confidence = float(output_data[0, i, 2])\n        if confidence > confidence_threshold:\n            ymin = int(max(1, (output_data[0, i, 0] * height)))\n            xmin = int(max(1, (output_data[0, i, 1] * width)))\n            ymax = int(min(height, (output_data[0, i, 2] * height)))\n            xmax = int(min(width, (output_data[0, i, 3] * width)))\n            label_index = int(output_data[0, i, 1])\n            if label_index < len(labels):  # Ensure the label index is within bounds\n                label = labels[label_index]\n            else:\n                label = \"Unknown\"\n\n            cv2.rectangle(frame, (xmin, ymin), (xmax, ymax), (0, 255, 0), 2)\n            cv2.putText(frame, f'{label}: {confidence:.2f}', (xmin, ymin - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (36, 255, 12), 2)\n\n    out.write(frame)\n\n# Phase 5: Cleanup\ncap.release()\nout.release()\ncv2.destroyAllWindows()"
      },
      "session_id": "qwen2.5-coder:32b_ae24_tpsg_batch",
      "metadata": {
        "num_run": 10
      },
      "tags": [
        "experiment",
        "qwen2.5-coder:32b",
        "tpu_sketch_generator"
      ],
      "latency": 61.471,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-14-22-04-026456_chatcmpl-670044b7-6585-49da-a0cd-db5d20273739",
          "traceId": "9ea6e58c",
          "type": "GENERATION",
          "name": "9ea6_tpusg_gen_attempt#1",
          "startTime": "2025-07-29T11:22:04.026000+00:00",
          "endTime": "2025-07-29T11:22:27.941000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 23915.0,
          "costDetails": {},
          "completionTokens": 772,
          "totalTokens": 2591,
          "promptTokens": 1819,
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-14-22-35-555064_chatcmpl-0dfe87a3-bf13-422b-9b7f-2348104a9741",
          "traceId": "9ea6e58c",
          "type": "GENERATION",
          "name": "9ea6_tpusg_gen_attempt#2",
          "startTime": "2025-07-29T11:22:35.555000+00:00",
          "endTime": "2025-07-29T11:23:05.497000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 29942.0,
          "costDetails": {},
          "completionTokens": 998,
          "totalTokens": 3046,
          "promptTokens": 2048,
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "createdAt": "2025-07-29T11:22:04.545Z",
      "updatedAt": "2025-07-29T11:24:04.207Z",
      "environment": "default"
    },
    {
      "id": "295b7bd5",
      "timestamp": "2025-07-29T11:18:40.142000+00:00",
      "name": "295b_tpu_sketch_generator",
      "input": null,
      "output": {
        "status": "failed",
        "last_code": "import numpy as np\nimport cv2\nfrom tflite_runtime.interpreter import Interpreter, load_delegate\n\n# Phase 1: Setup\nmodel_path  = \"/home/mendel/tinyml_autopilot/models/edgetpu_detect.tflite\"\nlabel_path  = \"/home/mendel/tinyml_autopilot/models/labelmap.txt\"\ninput_path  = \"/home/mendel/tinyml_autopilot/data//sheeps.mp4\"\noutput_path  = \"/home/mendel/tinyml_autopilot/results/sheeps_detections.mp4\"\nconfidence_threshold  = 0.5\n\n# Load labels\nwith open(label_path, 'r') as f:\n    labels = [line.strip() for line in f.readlines()]\n\n# Load interpreter with EdgeTPU acceleration\ntry:\n    interpreter = Interpreter(model_path=model_path,\n                              experimental_delegates=[load_delegate('libedgetpu.so.1.0')])\nexcept Exception as e:\n    print(f\"Failed to load libedgetpu.so.1.0: {e}\")\n    try:\n        interpreter = Interpreter(model_path=model_path,\n                                  experimental_delegates=[load_delegate('/usr/lib/aarch64-linux-gnu/libedgetpu.so.1.0')])\n    except Exception as e:\n        print(f\"Failed to load /usr/lib/aarch64-linux-gnu/libedgetpu.so.1.0: {e}\")\n        raise RuntimeError(\"EdgeTPU delegate could not be loaded.\")\n\ninterpreter.allocate_tensors()\n\n# Get model details\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\nfloating_model = (input_details[0]['dtype'] == np.float32)\n\n# Phase 2: Input Acquisition & Preprocessing Loop\ncap = cv2.VideoCapture(input_path)\nif not cap.isOpened():\n    raise RuntimeError(\"Could not open video file.\")\n\nframe_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\nframe_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\nfps = cap.get(cv2.CAP_PROP_FPS)\n\nout = cv2.VideoWriter(output_path, cv2.VideoWriter_fourcc(*'mp4v'), fps, (frame_width, frame_height))\n\nwhile True:\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    input_data = cv2.resize(frame, (input_details[0]['shape'][1], input_details[0]['shape'][2]))\n    input_data = np.expand_dims(input_data, axis=0)\n    \n    # Quantization handling\n    if floating_model:\n        input_data = (np.float32(input_data) - 127.5) / 127.5\n\n    # Phase 3: Inference\n    interpreter.set_tensor(input_details[0]['index'], input_data)\n    interpreter.invoke()\n\n    # Phase 4: Output Interpretation & Handling Loop\n    boxes = interpreter.get_tensor(output_details[0]['index'])\n    classes = interpreter.get_tensor(output_details[1]['index'])\n    scores = interpreter.get_tensor(output_details[2]['index'])\n    num_detections = int(interpreter.get_tensor(output_details[3]['index']))\n\n    for i in range(num_detections):\n        if scores[i][0] > confidence_threshold:\n            class_id = int(classes[i][0])\n            label = labels[class_id]\n            box = boxes[i][0]  # Ensure we are accessing the correct dimension\n\n            # Scale the bounding box coordinates\n            ymin, xmin, ymax, xmax = box * np.array([frame_height, frame_width, frame_height, frame_width])\n\n            cv2.rectangle(frame, (int(xmin), int(ymin)), (int(xmax), int(ymax)), (0, 255, 0), 2)\n            label_text = f'{label}: {scores[i][0]:.2f}'\n            cv2.putText(frame, label_text, (int(xmin), int(ymin) - 10),\n                        cv2.FONT_HERSHEY_SIMPLEX, 0.9, (36, 255, 12), 2)\n\n    out.write(frame)\n\n# Phase 5: Clean up\ncap.release()\nout.release()\n\nprint(\"Video processing complete.\")",
        "last_error": "Traceback (most recent call last):\n  File \"script_5f5301c2_1753788109.py\", line 71, in <module>\n    if scores[i][0] > confidence_threshold:\nIndexError: index 1 is out of bounds for axis 0 with size 1"
      },
      "session_id": "qwen2.5-coder:32b_ae24_tpsg_batch",
      "metadata": {
        "num_run": 9
      },
      "tags": [
        "experiment",
        "qwen2.5-coder:32b",
        "tpu_sketch_generator"
      ],
      "latency": 197.457,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-14-18-40-144443_chatcmpl-40306b77-2fbe-4031-801f-fe7b9ee78250",
          "traceId": "295b7bd5",
          "type": "GENERATION",
          "name": "295b_tpusg_gen_attempt#1",
          "startTime": "2025-07-29T11:18:40.144000+00:00",
          "endTime": "2025-07-29T11:19:05.803000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 25659.0,
          "costDetails": {},
          "completionTokens": 859,
          "totalTokens": 2678,
          "promptTokens": 1819,
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-14-19-14-084603_chatcmpl-f2ab6155-2d9d-4d6d-a7dd-92ca5a873564",
          "traceId": "295b7bd5",
          "type": "GENERATION",
          "name": "295b_tpusg_gen_attempt#2",
          "startTime": "2025-07-29T11:19:14.084000+00:00",
          "endTime": "2025-07-29T11:19:45.786000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 31702.0,
          "costDetails": {},
          "completionTokens": 1042,
          "totalTokens": 3090,
          "promptTokens": 2048,
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-14-19-53-968684_chatcmpl-3cc94f02-5791-4b5e-a672-f08ddeaa745e",
          "traceId": "295b7bd5",
          "type": "GENERATION",
          "name": "295b_tpusg_gen_attempt#3",
          "startTime": "2025-07-29T11:19:53.968000+00:00",
          "endTime": "2025-07-29T11:20:28.048000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 34080.0,
          "costDetails": {},
          "completionTokens": 1175,
          "totalTokens": 3223,
          "promptTokens": 2048,
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-14-20-36-241504_chatcmpl-711a9e83-a81d-4777-a33a-cf35d0180f7c",
          "traceId": "295b7bd5",
          "type": "GENERATION",
          "name": "295b_tpusg_gen_attempt#4",
          "startTime": "2025-07-29T11:20:36.241000+00:00",
          "endTime": "2025-07-29T11:21:09.449000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 33208.0,
          "costDetails": {},
          "completionTokens": 1084,
          "totalTokens": 3132,
          "promptTokens": 2048,
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-14-21-17-715576_chatcmpl-1e58a6e1-f0aa-4715-b563-8fbd994f8b7b",
          "traceId": "295b7bd5",
          "type": "GENERATION",
          "name": "295b_tpusg_gen_attempt#5",
          "startTime": "2025-07-29T11:21:17.715000+00:00",
          "endTime": "2025-07-29T11:21:49.378000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 31663.0,
          "costDetails": {},
          "completionTokens": 1060,
          "totalTokens": 3108,
          "promptTokens": 2048,
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "7309589e-8651-4637-a430-5056759b5dbc",
          "traceId": "295b7bd5",
          "type": "SPAN",
          "name": "error_29_tpusg_failure_signal_tpu_sketch_generator",
          "startTime": "2025-07-29T11:21:57.601000+00:00",
          "level": "ERROR",
          "statusMessage": "Failed. Last error: Traceback (most recent call last):\n  File \"script_5f5301c2_1753788109.py\", line 71, in <module>\n    if scores[i][0] > confidence_threshold:\nIndexError: index 1 is out of bounds for axis 0 with size 1",
          "calculatedTotalCost": 0.0,
          "costDetails": {},
          "completionTokens": 0,
          "totalTokens": 0,
          "promptTokens": 0,
          "environment": "default",
          "endTime": null,
          "model": null,
          "modelParameters": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null,
          "latency": null
        }
      ],
      "createdAt": "2025-07-29T11:18:40.663Z",
      "updatedAt": "2025-07-29T11:22:02.902Z",
      "environment": "default"
    },
    {
      "id": "2e49d667",
      "timestamp": "2025-07-29T11:17:17.116000+00:00",
      "name": "2e49_tpu_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import numpy as np\nfrom tflite_runtime.interpreter import Interpreter, load_delegate\nimport cv2\n\n# Phase 1: Setup\nmodel_path  = \"/home/mendel/tinyml_autopilot/models/edgetpu_detect.tflite\"\nlabel_path  = \"/home/mendel/tinyml_autopilot/models/labelmap.txt\"\ninput_path  = \"/home/mendel/tinyml_autopilot/data//sheeps.mp4\"\noutput_path  = \"/home/mendel/tinyml_autopilot/results/sheeps_detections.mp4\"\nconfidence_threshold  = 0.5\n\n# Load labels\nwith open(label_path, 'r') as f:\n    labels = [line.strip() for line in f.readlines()]\n\n# Load Interpreter with EdgeTPU\ntry:\n    interpreter = Interpreter(model_path=model_path,\n                              experimental_delegates=[load_delegate('libedgetpu.so.1.0')])\nexcept Exception as e:\n    try:\n        interpreter = Interpreter(model_path=model_path,\n                                  experimental_delegates=[load_delegate('/usr/lib/aarch64-linux-gnu/libedgetpu.so.1.0')])\n    except Exception as e:\n        print(f\"Failed to load EdgeTPU delegate: {e}\")\n        raise\n\ninterpreter.allocate_tensors()\n\n# Get model details\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\nfloating_model = (input_details[0]['dtype'] == np.float32)\n\nheight, width = input_details[0]['shape'][1], input_details[0]['shape'][2]\n\n# Phase 2: Input Acquisition & Preprocessing Loop\ncap = cv2.VideoCapture(input_path)\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\nout = cv2.VideoWriter(output_path, fourcc, cap.get(cv2.CAP_PROP_FPS), (int(cap.get(3)), int(cap.get(4))))\n\nwhile cap.isOpened():\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    # Preprocess Data\n    input_data = cv2.resize(frame, (width, height))\n    input_data = np.expand_dims(input_data, axis=0)\n\n    if floating_model:\n        input_data = (np.float32(input_data) - 127.5) / 127.5\n\n    # Phase 3: Inference\n    interpreter.set_tensor(input_details[0]['index'], input_data)\n    interpreter.invoke()\n\n    # Phase 4: Output Interpretation & Handling Loop\n    output_data = []\n    for i in range(len(output_details)):\n        output_data.append(interpreter.get_tensor(output_details[i]['index']))\n\n    boxes = output_data[0][0]\n    class_ids = output_data[1][0]\n    scores = output_data[2][0]\n\n    # Interpret Results and Post-processing\n    for box, class_id, score in zip(boxes, class_ids, scores):\n        if score > confidence_threshold:\n            y_min, x_min, y_max, x_max = box\n            im_height, im_width, _ = frame.shape\n            left, right, top, bottom = int(x_min * im_width), int(x_max * im_width), int(y_min * im_height), int(y_max * im_height)\n            label = labels[int(class_id)]\n            cv2.rectangle(frame, (left, top), (right, bottom), (0, 255, 0), 2)\n            cv2.putText(frame, f'{label}: {score:.2f}', (left, top - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n\n    # Handle Output\n    out.write(frame)\n\n# Phase 5: Cleanup\ncap.release()\nout.release()\ncv2.destroyAllWindows()"
      },
      "session_id": "qwen2.5-coder:32b_ae24_tpsg_batch",
      "metadata": {
        "num_run": 8
      },
      "tags": [
        "experiment",
        "qwen2.5-coder:32b",
        "tpu_sketch_generator"
      ],
      "latency": 24.341,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-14-17-17-118192_chatcmpl-e52680ea-1951-41e3-b6e9-c00d753e272d",
          "traceId": "2e49d667",
          "type": "GENERATION",
          "name": "2e49_tpusg_gen_attempt#1",
          "startTime": "2025-07-29T11:17:17.118000+00:00",
          "endTime": "2025-07-29T11:17:41.459000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 24341.0,
          "costDetails": {},
          "completionTokens": 790,
          "totalTokens": 2609,
          "promptTokens": 1819,
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "createdAt": "2025-07-29T11:17:17.637Z",
      "updatedAt": "2025-07-29T11:18:39.304Z",
      "environment": "default"
    },
    {
      "id": "820bde22",
      "timestamp": "2025-07-29T11:15:06.539000+00:00",
      "name": "820b_tpu_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import os\nimport numpy as np\nfrom tflite_runtime.interpreter import Interpreter, load_delegate\n\n# Phase 1: Setup\nmodel_path  = \"/home/mendel/tinyml_autopilot/models/edgetpu_detect.tflite\"\nlabel_path  = \"/home/mendel/tinyml_autopilot/models/labelmap.txt\"\ninput_path  = \"/home/mendel/tinyml_autopilot/data//sheeps.mp4\"\noutput_path  = \"/home/mendel/tinyml_autopilot/results/sheeps_detections.mp4\"\nconfidence_threshold  = 0.5\n\n# Load Labels\nwith open(label_path, 'r') as f:\n    labels = [line.strip() for line in f.readlines()]\n\n# Load Interpreter with EdgeTPU\ntry:\n    interpreter = Interpreter(model_path=model_path,\n                              experimental_delegates=[load_delegate('libedgetpu.so.1.0')])\nexcept Exception as e:\n    print(f\"Error loading libedgetpu.so.1.0: {e}\")\n    try:\n        interpreter = Interpreter(model_path=model_path,\n                                  experimental_delegates=[load_delegate('/usr/lib/aarch64-linux-gnu/libedgetpu.so.1.0')])\n    except Exception as e:\n        print(f\"Error loading /usr/lib/aarch64-linux-gnu/libedgetpu.so.1.0: {e}\")\n        raise RuntimeError(\"Could not load EdgeTPU delegate\")\n\ninterpreter.allocate_tensors()\n\n# Get Model Details\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\nfloating_model = (input_details[0]['dtype'] == np.float32)\n\n# Phase 2: Input Acquisition & Preprocessing Loop\nimport cv2\n\ncap = cv2.VideoCapture(input_path)\nif not cap.isOpened():\n    raise RuntimeError(\"Could not open video file\")\n\nwidth = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\nheight = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\nfps = cap.get(cv2.CAP_PROP_FPS)\n\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\nout = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n\nwhile True:\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    # Preprocess Data\n    input_data = cv2.resize(frame, (input_details[0]['shape'][1], input_details[0]['shape'][2]))\n    input_data = np.expand_dims(input_data, axis=0)\n\n    if floating_model:\n        input_data = (np.float32(input_data) - 127.5) / 127.5\n\n    # Phase 3: Inference\n    interpreter.set_tensor(input_details[0]['index'], input_data)\n    interpreter.invoke()\n\n    # Phase 4: Output Interpretation & Handling Loop\n    output_data = []\n    for i in range(len(output_details)):\n        output_data.append(interpreter.get_tensor(output_details[i]['index']))\n\n    boxes = output_data[0][0]\n    classes = output_data[1][0]\n    scores = output_data[2][0]\n    num_detections = int(output_data[3][0])\n\n    for i in range(num_detections):\n        if scores[i] > confidence_threshold:\n            ymin, xmin, ymax, xmax = boxes[i]\n            (left, right, top, bottom) = (xmin * width, xmax * width,\n                                          ymin * height, ymax * height)\n\n            cv2.rectangle(frame, (int(left), int(top)), (int(right), int(bottom)), (0, 255, 0), 2)\n            label = labels[int(classes[i])]\n            label_text = f\"{label}: {scores[i]:.2f}\"\n            cv2.putText(frame, label_text, (int(left), int(top) - 10),\n                        cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n\n    # Handle Output\n    out.write(frame)\n\n# Phase 5: Cleanup\ncap.release()\nout.release()\ncv2.destroyAllWindows()"
      },
      "session_id": "qwen2.5-coder:32b_ae24_tpsg_batch",
      "metadata": {
        "num_run": 7
      },
      "tags": [
        "experiment",
        "qwen2.5-coder:32b",
        "tpu_sketch_generator"
      ],
      "latency": 72.25,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-14-15-06-541020_chatcmpl-42b83882-aba4-44c0-825a-3d5ba4300aeb",
          "traceId": "820bde22",
          "type": "GENERATION",
          "name": "820b_tpusg_gen_attempt#1",
          "startTime": "2025-07-29T11:15:06.541000+00:00",
          "endTime": "2025-07-29T11:15:39.715000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 33174.0,
          "costDetails": {},
          "completionTokens": 852,
          "totalTokens": 2671,
          "promptTokens": 1819,
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-14-15-47-489697_chatcmpl-632f9555-dea7-472e-8fc5-210242e26b4d",
          "traceId": "820bde22",
          "type": "GENERATION",
          "name": "820b_tpusg_gen_attempt#2",
          "startTime": "2025-07-29T11:15:47.489000+00:00",
          "endTime": "2025-07-29T11:16:18.791000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 31302.0,
          "costDetails": {},
          "completionTokens": 1037,
          "totalTokens": 3085,
          "promptTokens": 2048,
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "createdAt": "2025-07-29T11:15:07.058Z",
      "updatedAt": "2025-07-29T11:17:16.926Z",
      "environment": "default"
    },
    {
      "id": "edc64c30",
      "timestamp": "2025-07-29T11:13:44.983000+00:00",
      "name": "edc6_tpu_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import numpy as np\nfrom tflite_runtime.interpreter import Interpreter, load_delegate\nimport cv2\n\n# Phase 1: Setup\nmodel_path  = \"/home/mendel/tinyml_autopilot/models/edgetpu_detect.tflite\"\nlabel_path  = \"/home/mendel/tinyml_autopilot/models/labelmap.txt\"\ninput_path  = \"/home/mendel/tinyml_autopilot/data//sheeps.mp4\"\noutput_path  = \"/home/mendel/tinyml_autopilot/results/sheeps_detections.mp4\"\nconfidence_threshold  = 0.5\n\n# Load Labels\nwith open(label_path, 'r') as f:\n    labels = [line.strip() for line in f.readlines()]\n\n# Load Interpreter with EdgeTPU\ntry:\n    interpreter = Interpreter(model_path=model_path, experimental_delegates=[load_delegate('libedgetpu.so.1.0')])\nexcept Exception as e:\n    try:\n        interpreter = Interpreter(model_path=model_path, experimental_delegates=[load_delegate('/usr/lib/aarch64-linux-gnu/libedgetpu.so.1.0')])\n    except Exception as e:\n        raise RuntimeError(f\"Failed to load EdgeTPU delegate: {e}\")\n\ninterpreter.allocate_tensors()\n\n# Get Model Details\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\nfloating_model = (input_details[0]['dtype'] == np.float32)\n\n# Phase 2: Input Acquisition & Preprocessing Loop\ncap = cv2.VideoCapture(input_path)\nwidth = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\nheight = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\nfps = cap.get(cv2.CAP_PROP_FPS)\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\nout = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n\nwhile cap.isOpened():\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    input_data = cv2.resize(frame, (input_details[0]['shape'][1], input_details[0]['shape'][2]))\n    input_data = np.expand_dims(input_data, axis=0)\n\n    # Quantization Handling\n    if floating_model:\n        input_data = (np.float32(input_data) - 127.5) / 127.5\n\n    # Phase 3: Inference\n    interpreter.set_tensor(input_details[0]['index'], input_data)\n    interpreter.invoke()\n\n    # Phase 4: Output Interpretation & Handling Loop\n    boxes = interpreter.get_tensor(output_details[0]['index'])\n    classes = interpreter.get_tensor(output_details[1]['index'])\n    scores = interpreter.get_tensor(output_details[2]['index'])\n\n    for i in range(len(scores[0])):\n        if scores[0][i] >= confidence_threshold:\n            ymin, xmin, ymax, xmax = boxes[0][i]\n            (left, right, top, bottom) = (xmin * width, xmax * width,\n                                          ymin * height, ymax * height)\n\n            cv2.rectangle(frame, (int(left), int(top)), (int(right), int(bottom)), (0, 255, 0), 2)\n            label = f'{labels[int(classes[0][i])]}: {scores[0][i]:.2f}'\n            cv2.putText(frame, label, (int(left), int(top) - 10),\n                        cv2.FONT_HERSHEY_SIMPLEX, 0.9, (36, 255, 12), 2)\n\n    # Handle Output\n    out.write(frame)\n\n# Phase 5: Cleanup\ncap.release()\nout.release()\ncv2.destroyAllWindows()"
      },
      "session_id": "qwen2.5-coder:32b_ae24_tpsg_batch",
      "metadata": {
        "num_run": 6
      },
      "tags": [
        "experiment",
        "qwen2.5-coder:32b",
        "tpu_sketch_generator"
      ],
      "latency": 22.694,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-14-13-44-984530_chatcmpl-83f89312-098f-4636-bff5-0b7e8b33fe7f",
          "traceId": "edc64c30",
          "type": "GENERATION",
          "name": "edc6_tpusg_gen_attempt#1",
          "startTime": "2025-07-29T11:13:44.984000+00:00",
          "endTime": "2025-07-29T11:14:07.678000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 22694.0,
          "costDetails": {},
          "completionTokens": 768,
          "totalTokens": 2587,
          "promptTokens": 1819,
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "createdAt": "2025-07-29T11:13:45.502Z",
      "updatedAt": "2025-07-29T11:15:05.681Z",
      "environment": "default"
    },
    {
      "id": "aebfebdb",
      "timestamp": "2025-07-29T11:11:10.431000+00:00",
      "name": "aebf_tpu_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import numpy as np\nimport time\nimport os\nfrom tflite_runtime.interpreter import Interpreter, load_delegate\n\n# Paths and parameters\nmodel_path    = \"/home/mendel/tinyml_autopilot/models/edgetpu_detect.tflite\"\nlabel_path    = \"/home/mendel/tinyml_autopilot/models/labelmap.txt\"\ninput_path    = \"/home/mendel/tinyml_autopilot/data//sheeps.mp4\"\noutput_path   = \"/home/mendel/tinyml_autopilot/results/sheeps_detections.mp4\"\nconfidence_threshold  = 0.5\n\n# Load labels\nwith open(label_path, 'r') as f:\n    labels = [line.strip() for line in f.readlines()]\n\n# Load interpreter with EdgeTPU\ntry:\n    interpreter = Interpreter(model_path=model_path, experimental_delegates=[load_delegate('libedgetpu.so.1.0')])\nexcept Exception as e:\n    print(f\"Failed to load delegate: {e}. Falling back to alternative path.\")\n    try:\n        interpreter = Interpreter(model_path=model_path, experimental_delegates=[load_delegate('/usr/lib/aarch64-linux-gnu/libedgetpu.so.1.0')])\n    except Exception as e:\n        raise RuntimeError(\"Could not initialize Coral TPU delegate.\") from e\n\ninterpreter.allocate_tensors()\n\n# Get model details\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\nfloating_model = (input_details[0]['dtype'] == np.float32)\nbatch_size, height, width, channels = input_details[0]['shape']\n\n# Phase 2: Input Acquisition & Preprocessing Loop\nimport cv2\n\ncap = cv2.VideoCapture(input_path)\nif not cap.isOpened():\n    raise IOError(\"Cannot open video file\")\n\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\nout = cv2.VideoWriter(output_path, fourcc, cap.get(cv2.CAP_PROP_FPS), (int(cap.get(3)), int(cap.get(4))))\n\nwhile cap.isOpened():\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    # Preprocess data\n    resized_frame = cv2.resize(frame, (width, height))\n    input_data = np.expand_dims(resized_frame, axis=0)\n    \n    if floating_model:\n        input_data = (np.float32(input_data) - 127.5) / 127.5\n\n    # Phase 3: Inference\n    interpreter.set_tensor(input_details[0]['index'], input_data)\n    interpreter.invoke()\n\n    # Phase 4: Output Interpretation & Handling Loop\n    boxes = interpreter.get_tensor(output_details[0]['index'])\n    classes = interpreter.get_tensor(output_details[1]['index'])\n    scores = interpreter.get_tensor(output_details[2]['index'])\n    num_detections = int(interpreter.get_tensor(output_details[3]['index']))\n\n    for i in range(num_detections):\n        # Ensure scores[i] is a scalar value\n        if scores[0][i] > confidence_threshold:\n            ymin, xmin, ymax, xmax = boxes[0][i]\n            class_id = int(classes[0][i])\n            label = labels[class_id]\n            \n            # Scale coordinates\n            im_height, im_width, _ = frame.shape\n            (xminn, xmaxx, yminn, ymaxx) = (xmin * im_width, xmax * im_width,\n                                              ymin * im_height, ymax * im_height)\n            \n            cv2.rectangle(frame, (int(xminn), int(yminn)), (int(xmaxx), int(ymaxx)), (0, 255, 0), 2)\n            cv2.putText(frame, f'{label}: {scores[0][i]:.2f}', (int(xminn), int(yminn) - 10),\n                        cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n\n    # Write the frame with detection to output video\n    out.write(frame)\n\n# Phase 5: Cleanup\ncap.release()\nout.release()\ncv2.destroyAllWindows()"
      },
      "session_id": "qwen2.5-coder:32b_ae24_tpsg_batch",
      "metadata": {
        "num_run": 5
      },
      "tags": [
        "experiment",
        "qwen2.5-coder:32b",
        "tpu_sketch_generator"
      ],
      "latency": 96.153,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-14-11-10-432863_chatcmpl-c88de91d-6c00-4881-a2ae-9c765e2a124d",
          "traceId": "aebfebdb",
          "type": "GENERATION",
          "name": "aebf_tpusg_gen_attempt#1",
          "startTime": "2025-07-29T11:11:10.432000+00:00",
          "endTime": "2025-07-29T11:11:34.988000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 24556.0,
          "costDetails": {},
          "completionTokens": 836,
          "totalTokens": 2655,
          "promptTokens": 1819,
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-14-11-39-828135_chatcmpl-4b27d0bd-8c07-4efb-a8c6-90798782533e",
          "traceId": "aebfebdb",
          "type": "GENERATION",
          "name": "aebf_tpusg_gen_attempt#2",
          "startTime": "2025-07-29T11:11:39.828000+00:00",
          "endTime": "2025-07-29T11:12:09.568000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 29740.0,
          "costDetails": {},
          "completionTokens": 1022,
          "totalTokens": 3070,
          "promptTokens": 2048,
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-14-12-17-759622_chatcmpl-96a117e4-101c-4a60-a986-c2d5a2bd2d12",
          "traceId": "aebfebdb",
          "type": "GENERATION",
          "name": "aebf_tpusg_gen_attempt#3",
          "startTime": "2025-07-29T11:12:17.759000+00:00",
          "endTime": "2025-07-29T11:12:46.585000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 28826.0,
          "costDetails": {},
          "completionTokens": 988,
          "totalTokens": 3036,
          "promptTokens": 2048,
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "createdAt": "2025-07-29T11:11:10.951Z",
      "updatedAt": "2025-07-29T11:13:44.100Z",
      "environment": "default"
    },
    {
      "id": "725dcebe",
      "timestamp": "2025-07-29T11:07:50.887000+00:00",
      "name": "725d_tpu_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import numpy as np\nimport cv2\nfrom tflite_runtime.interpreter import Interpreter, load_delegate\n\n# Phase 1: Setup\nmodel_path   = \"/home/mendel/tinyml_autopilot/models/edgetpu_detect.tflite\"\nlabel_path   = \"/home/mendel/tinyml_autopilot/models/labelmap.txt\"\ninput_path   = \"/home/mendel/tinyml_autopilot/data//sheeps.mp4\"\noutput_path  = \"/home/mendel/tinyml_autopilot/results/sheeps_detections.mp4\"\nconfidence_threshold  = 0.5\n\n# Load labels\nwith open(label_path, 'r') as f:\n    labels = [line.strip() for line in f.readlines()]\n\n# Load Interpreter with EdgeTPU\ntry:\n    interpreter = Interpreter(model_path=model_path, experimental_delegates=[load_delegate('libedgetpu.so.1.0')])\nexcept Exception as e:\n    try:\n        interpreter = Interpreter(model_path=model_path, experimental_delegates=[load_delegate('/usr/lib/aarch64-linux-gnu/libedgetpu.so.1.0')])\n    except Exception as e:\n        raise RuntimeError(\"Failed to load EdgeTPU delegate: \" + str(e))\n\ninterpreter.allocate_tensors()\n\n# Get Model Details\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\nfloating_model = (input_details[0]['dtype'] == np.float32)\n\n# Print output tensor shapes for debugging\nfor i, detail in enumerate(output_details):\n    print(f\"Output {i} shape: {detail['shape']}\")\n\n# Phase 2: Input Acquisition & Preprocessing Loop\ncap = cv2.VideoCapture(input_path)\nif not cap.isOpened():\n    raise RuntimeError(\"Error opening video stream or file\")\n\nframe_width = int(cap.get(3))\nframe_height = int(cap.get(4))\nout = cv2.VideoWriter(output_path, cv2.VideoWriter_fourcc(*'mp4v'), 30, (frame_width, frame_height))\n\nwhile cap.isOpened():\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    # Preprocess Data\n    input_data = cv2.resize(frame, (input_details[0]['shape'][1], input_details[0]['shape'][2]))\n    input_data = np.expand_dims(input_data, axis=0)\n\n    if floating_model:\n        input_data = (np.float32(input_data) - 127.5) / 127.5\n\n    # Phase 3: Inference\n    interpreter.set_tensor(input_details[0]['index'], input_data)\n    interpreter.invoke()\n\n    # Phase 4: Output Interpretation & Handling Loop\n    boxes = interpreter.get_tensor(output_details[0]['index'])[0]  # [10, 4]\n    class_ids = interpreter.get_tensor(output_details[1]['index'])[0]  # [10]\n    scores = interpreter.get_tensor(output_details[2]['index'])[0]  # [10]\n    num_detections = int(interpreter.get_tensor(output_details[3]['index']))  # Corrected index based on printed shapes\n\n    for i in range(num_detections):\n        score = float(scores[i])\n        if score > confidence_threshold:\n            class_id = int(class_ids[i])\n            box = boxes[i]\n            ymin, xmin, ymax, xmax = box\n\n            # Scale coordinates\n            ymin = int(max(1, ymin * frame_height))\n            xmin = int(max(1, xmin * frame_width))\n            ymax = int(min(frame_height, ymax * frame_height))\n            xmax = int(min(frame_width, xmax * frame_width))\n\n            label = labels[class_id]\n            cv2.rectangle(frame, (xmin, ymin), (xmax, ymax), (0, 255, 0), 2)\n            cv2.putText(frame, f'{label}: {score:.2f}', (xmin, ymin - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n\n    # Handle Output\n    out.write(frame)\n\n# Phase 5: Cleanup\ncap.release()\nout.release()\ncv2.destroyAllWindows()"
      },
      "session_id": "qwen2.5-coder:32b_ae24_tpsg_batch",
      "metadata": {
        "num_run": 4
      },
      "tags": [
        "experiment",
        "qwen2.5-coder:32b",
        "tpu_sketch_generator"
      ],
      "latency": 141.307,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-14-07-50-889186_chatcmpl-2b8249f5-4fb4-49fb-ab28-20624a4e8230",
          "traceId": "725dcebe",
          "type": "GENERATION",
          "name": "725d_tpusg_gen_attempt#1",
          "startTime": "2025-07-29T11:07:50.889000+00:00",
          "endTime": "2025-07-29T11:08:14.411000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 23522.0,
          "costDetails": {},
          "completionTokens": 798,
          "totalTokens": 2617,
          "promptTokens": 1819,
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-14-08-22-662901_chatcmpl-1388717d-12fd-4020-8c47-6ceb958bde6c",
          "traceId": "725dcebe",
          "type": "GENERATION",
          "name": "725d_tpusg_gen_attempt#2",
          "startTime": "2025-07-29T11:08:22.662000+00:00",
          "endTime": "2025-07-29T11:08:54.709000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 32047.0,
          "costDetails": {},
          "completionTokens": 1109,
          "totalTokens": 3157,
          "promptTokens": 2048,
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-14-09-03-524739_chatcmpl-b6c9e68c-2f9a-4b95-9c1f-5e7ed1714fc5",
          "traceId": "725dcebe",
          "type": "GENERATION",
          "name": "725d_tpusg_gen_attempt#3",
          "startTime": "2025-07-29T11:09:03.524000+00:00",
          "endTime": "2025-07-29T11:09:31.907000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 28383.0,
          "costDetails": {},
          "completionTokens": 972,
          "totalTokens": 3020,
          "promptTokens": 2048,
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-14-09-40-103402_chatcmpl-32251249-d046-4576-9836-96a734f66f28",
          "traceId": "725dcebe",
          "type": "GENERATION",
          "name": "725d_tpusg_gen_attempt#4",
          "startTime": "2025-07-29T11:09:40.103000+00:00",
          "endTime": "2025-07-29T11:10:12.196000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 32093.0,
          "costDetails": {},
          "completionTokens": 1111,
          "totalTokens": 3159,
          "promptTokens": 2048,
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "createdAt": "2025-07-29T11:07:51.407Z",
      "updatedAt": "2025-07-29T11:11:10.173Z",
      "environment": "default"
    },
    {
      "id": "70a91c51",
      "timestamp": "2025-07-29T11:06:25.915000+00:00",
      "name": "70a9_tpu_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import numpy as np\nfrom tflite_runtime.interpreter import Interpreter, load_delegate\nimport cv2\n\n# Phase 1: Setup\n\n# Paths/Parameters\nmodel_path  = \"/home/mendel/tinyml_autopilot/models/edgetpu_detect.tflite\"\nlabel_path  = \"/home/mendel/tinyml_autopilot/models/labelmap.txt\"\ninput_path  = \"/home/mendel/tinyml_autopilot/data//sheeps.mp4\"\noutput_path  = \"/home/mendel/tinyml_autopilot/results/sheeps_detections.mp4\"\nconfidence_threshold  = 0.5\n\n# Load Labels\nwith open(label_path, 'r') as f:\n    labels = [line.strip() for line in f.readlines()]\n\n# Load Interpreter with EdgeTPU\ntry:\n    interpreter = Interpreter(model_path=model_path,\n                              experimental_delegates=[load_delegate('libedgetpu.so.1.0')])\nexcept Exception as e:\n    print(f\"Failed to load delegate: {e}. Trying alternative path.\")\n    interpreter = Interpreter(model_path=model_path,\n                              experimental_delegates=[load_delegate('/usr/lib/aarch64-linux-gnu/libedgetpu.so.1.0')])\n\ninterpreter.allocate_tensors()\n\n# Get Model Details\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\nfloating_model = (input_details[0]['dtype'] == np.float32)\nheight, width = input_details[0]['shape'][1:3]\n\n# Phase 2: Input Acquisition & Preprocessing Loop\n\n# Acquire Input Data\ncap = cv2.VideoCapture(input_path)\n\n# Get video properties to save output video\nfps = cap.get(cv2.CAP_PROP_FPS)\nframe_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\nframe_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n\n# Define codec and create VideoWriter object\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\nout = cv2.VideoWriter(output_path, fourcc, fps, (frame_width, frame_height))\n\nwhile cap.isOpened():\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    # Preprocess Data\n    input_data = cv2.resize(frame, (width, height))\n    input_data = np.expand_dims(input_data, axis=0)\n\n    if floating_model:\n        input_data = (np.float32(input_data) - 127.5) / 127.5\n\n    # Phase 3: Inference\n    interpreter.set_tensor(input_details[0]['index'], input_data)\n    interpreter.invoke()\n\n    # Phase 4: Output Interpretation & Handling Loop\n\n    # Get Output Tensor(s)\n    boxes = interpreter.get_tensor(output_details[0]['index'])\n    classes = interpreter.get_tensor(output_details[1]['index'])\n    scores = interpreter.get_tensor(output_details[2]['index'])\n    num_detections = int(interpreter.get_tensor(output_details[3]['index']))\n\n    # Interpret Results\n    for i in range(num_detections):\n        if scores[0][i] > confidence_threshold:\n            ymin, xmin, ymax, xmax = boxes[0][i]\n            class_id = int(classes[0][i])\n            label = labels[class_id]\n\n            # Post-processing: Scale and clip bounding box coordinates\n            (left, right, top, bottom) = (xmin * frame_width, xmax * frame_width,\n                                          ymin * frame_height, ymax * frame_height)\n\n            # Draw bounding box and label on the frame\n            cv2.rectangle(frame, (int(left), int(top)), (int(right), int(bottom)), (0, 255, 0), 2)\n            cv2.putText(frame, f'{label}: {scores[0][i]:.2f}', (int(left), int(top) - 8),\n                        cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n\n    # Handle Output\n    out.write(frame)\n\n# Phase 5: Cleanup\ncap.release()\nout.release()\ncv2.destroyAllWindows()"
      },
      "session_id": "qwen2.5-coder:32b_ae24_tpsg_batch",
      "metadata": {
        "num_run": 3
      },
      "tags": [
        "experiment",
        "qwen2.5-coder:32b",
        "tpu_sketch_generator"
      ],
      "latency": 26.225,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-14-06-25-917350_chatcmpl-90f3940c-81e2-4672-a6bd-b401a2df27fc",
          "traceId": "70a91c51",
          "type": "GENERATION",
          "name": "70a9_tpusg_gen_attempt#1",
          "startTime": "2025-07-29T11:06:25.917000+00:00",
          "endTime": "2025-07-29T11:06:52.142000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 26225.0,
          "costDetails": {},
          "completionTokens": 897,
          "totalTokens": 2716,
          "promptTokens": 1819,
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "createdAt": "2025-07-29T11:06:26.436Z",
      "updatedAt": "2025-07-29T11:07:50.095Z",
      "environment": "default"
    },
    {
      "id": "27eeb6be",
      "timestamp": "2025-07-29T11:04:59.953000+00:00",
      "name": "27ee_tpu_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import os\nimport numpy as np\nimport time\nfrom tflite_runtime.interpreter import Interpreter, load_delegate\nimport cv2\n\n# Phase 1: Setup\n\n# Paths/Parameters\nmodel_path  = \"/home/mendel/tinyml_autopilot/models/edgetpu_detect.tflite\"\nlabel_path  = \"/home/mendel/tinyml_autopilot/models/labelmap.txt\"\ninput_path  = \"/home/mendel/tinyml_autopilot/data//sheeps.mp4\"\noutput_path  = \"/home/mendel/tinyml_autopilot/results/sheeps_detections.mp4\"\nconfidence_threshold  = 0.5\n\n# Load Labels\nwith open(label_path, 'r') as f:\n    labels = [line.strip() for line in f.readlines()]\n\n# Load Interpreter with EdgeTPU\ntry:\n    interpreter = Interpreter(model_path=model_path,\n                              experimental_delegates=[load_delegate('libedgetpu.so.1.0')])\nexcept Exception as e:\n    print(f\"Error loading delegate: {e}\")\n    try:\n        interpreter = Interpreter(model_path=model_path,\n                                  experimental_delegates=[load_delegate('/usr/lib/aarch64-linux-gnu/libedgetpu.so.1.0')])\n    except Exception as e:\n        print(f\"Fallback error loading delegate: {e}\")\n        raise\n\ninterpreter.allocate_tensors()\n\n# Get Model Details\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\n# Phase 2: Input Acquisition & Preprocessing Loop\n\n# Acquire Input Data\ncap = cv2.VideoCapture(input_path)\nif not cap.isOpened():\n    print(\"Error opening video stream or file\")\n    exit()\n\nframe_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\nframe_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\nfps = cap.get(cv2.CAP_PROP_FPS)\n\n# Define the codec and create VideoWriter object\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\nout = cv2.VideoWriter(output_path, fourcc, fps, (frame_width, frame_height))\n\nwhile cap.isOpened():\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    # Preprocess Data\n    input_data = cv2.resize(frame, (input_details[0]['shape'][1], input_details[0]['shape'][2]))\n    input_data = np.expand_dims(input_data, axis=0)\n\n    # Quantization Handling\n    floating_model = (input_details[0]['dtype'] == np.float32)\n    if floating_model:\n        input_data = (np.float32(input_data) - 127.5) / 127.5\n\n    # Phase 3: Inference\n\n    # Set Input Tensor(s)\n    interpreter.set_tensor(input_details[0]['index'], input_data)\n\n    # Run Inference\n    interpreter.invoke()\n\n    # Phase 4: Output Interpretation & Handling Loop\n\n    # Get Output Tensor(s)\n    boxes = interpreter.get_tensor(output_details[0]['index'])[0]\n    classes = interpreter.get_tensor(output_details[1]['index'])[0]\n    scores = interpreter.get_tensor(output_details[2]['index'])[0]\n    num_detections = int(interpreter.get_tensor(output_details[3]['index']))\n\n    # Interpret Results and Post-processing\n    for i in range(num_detections):\n        if scores[i] > confidence_threshold:\n            ymin, xmin, ymax, xmax = boxes[i]\n            (left, right, top, bottom) = (xmin * frame_width, xmax * frame_width,\n                                          ymin * frame_height, ymax * frame_height)\n            p1 = (int(left), int(top))\n            p2 = (int(right), int(bottom))\n\n            # Draw bounding box and label\n            cv2.rectangle(frame, p1, p2, (0, 255, 0), 2)\n            class_label = labels[int(classes[i])]\n            label = f'{class_label}: {scores[i]:.2f}'\n            cv2.putText(frame, label, (int(left), int(top) - 10),\n                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n\n    # Handle Output\n    out.write(frame)\n\n# Phase 5: Cleanup\n\ncap.release()\nout.release()\ncv2.destroyAllWindows()"
      },
      "session_id": "qwen2.5-coder:32b_ae24_tpsg_batch",
      "metadata": {
        "num_run": 2
      },
      "tags": [
        "experiment",
        "qwen2.5-coder:32b",
        "tpu_sketch_generator"
      ],
      "latency": 26.582,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-14-04-59-955086_chatcmpl-75b30013-0ec3-4325-b092-4ca3acada2da",
          "traceId": "27eeb6be",
          "type": "GENERATION",
          "name": "27ee_tpusg_gen_attempt#1",
          "startTime": "2025-07-29T11:04:59.955000+00:00",
          "endTime": "2025-07-29T11:05:26.537000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 26582.0,
          "costDetails": {},
          "completionTokens": 905,
          "totalTokens": 2724,
          "promptTokens": 1819,
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "createdAt": "2025-07-29T11:05:00.471Z",
      "updatedAt": "2025-07-29T11:06:24.902Z",
      "environment": "default"
    },
    {
      "id": "ae61b594",
      "timestamp": "2025-07-29T11:00:38.384000+00:00",
      "name": "ae61_tpu_sketch_generator",
      "input": null,
      "output": {
        "status": "failed",
        "last_code": "import cv2\nimport numpy as np\n\n# Phase 1: Setup & Initialization\n# Load TFLite model and allocate tensors.\ninterpreter = tf.lite.Interpreter(model_path=\"/path/to/model.tflite\")  # Update with actual path\ninterpreter.allocate_tensors()\n\n# Get input and output details\ninput_details  = \"/home/mendel/tinyml_autopilot/data//sheeps.mp4\"\noutput_details  = \"/home/mendel/tinyml_autopilot/results/sheeps_detections.mp4\"\n\n# Read labels\nwith open(\"/home/mendel/tinyml_autopilot/labels.txt\", 'r') as f:  # Update with actual path\n    labels = [line.strip() for line in f.readlines()]\n\n# Phase 2: Input Acquisition & Preprocessing Loop\ncap = cv2.VideoCapture(\"/home/mendel/tinyml_autopilot/sheeps.mp4\")  # Update with actual path\n\nwhile cap.isOpened():\n    ret, frame = cap.read()\n    if not ret:\n        break\n    \n    frame_height, frame_width, _ = frame.shape\n    \n    # Preprocess frame\n    input_data = cv2.resize(frame, (input_details[0]['shape'][1], input_details[0]['shape'][2]))\n    input_data = np.expand_dims(input_data, axis=0)\n    \n    # Handle quantization if necessary\n    floating_model = (input_details[0]['dtype'] == np.float32)\n    if floating_model:\n        input_data = (np.float32(input_data) - 127.5) / 127.5\n    \n    # Phase 3: Inference\n    interpreter.set_tensor(input_details[0]['index'], input_data)\n    interpreter.invoke()\n    \n    # Phase 4: Output Interpretation & Handling Loop\n    detection_boxes = interpreter.get_tensor(output_details[1]['index'])\n    num_detections = int(interpreter.get_tensor(output_details[2]['index']))  # Adjust index if needed\n\n    for i in range(num_detections):\n        confidence = float(detection_boxes[i][5])\n        if confidence > float(confidence_threshold):\n            # Get bounding box coordinates\n            ymin, xmin, ymax, xmax = detection_boxes[i][1:5]\n            ymin = int(max(1, ymin * frame_height))\n            xmin = int(max(1, xmin * frame_width))\n            ymax = int(min(frame_height, ymax * frame_height))\n            xmax = int(min(frame_width, xmax * frame_width))\n            \n            # Draw bounding box and label\n            cv2.rectangle(frame, (xmin, ymin), (xmax, ymax), (0, 255, 0), 2)\n            class_id = int(detection_boxes[i][0])\n            label = labels[class_id]\n            label_text = f'{label}: {confidence:.2f}'\n            cv2.putText(frame, label_text, (xmin, ymin - 10), \n                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n    \n    # Display the frame\n    cv2.imshow('Object Detection', frame)\n    \n    if cv2.waitKey(1) & 0xFF == ord('q'):\n        break\n\n# Phase 5: Cleanup\ncap.release()\ncv2.destroyAllWindows()",
        "last_error": "Traceback (most recent call last):\n  File \"script_7c56f049_1753787086.py\", line 6, in <module>\n    interpreter = tf.lite.Interpreter(model_path=\"/path/to/model.tflite\")  # Update with actual path\nNameError: name 'tf' is not defined"
      },
      "session_id": "qwen2.5-coder:32b_ae24_tpsg_batch",
      "metadata": {
        "num_run": 1
      },
      "tags": [
        "experiment",
        "qwen2.5-coder:32b",
        "tpu_sketch_generator"
      ],
      "latency": 255.342,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-14-00-38-388811_chatcmpl-9c26c242-d2ac-487d-b0c5-0d4265dab874",
          "traceId": "ae61b594",
          "type": "GENERATION",
          "name": "ae61_tpusg_gen_attempt#1",
          "startTime": "2025-07-29T11:00:38.388000+00:00",
          "endTime": "2025-07-29T11:02:12.309000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 93921.0,
          "costDetails": {},
          "completionTokens": 814,
          "totalTokens": 2633,
          "promptTokens": 1819,
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-14-02-20-863868_chatcmpl-e08df2a5-5eb1-43dd-9d5c-9b22d9e2cc2c",
          "traceId": "ae61b594",
          "type": "GENERATION",
          "name": "ae61_tpusg_gen_attempt#2",
          "startTime": "2025-07-29T11:02:20.863000+00:00",
          "endTime": "2025-07-29T11:02:50.957000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 30094.0,
          "costDetails": {},
          "completionTokens": 1034,
          "totalTokens": 3082,
          "promptTokens": 2048,
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-14-02-59-332659_chatcmpl-c45b8136-4ff9-41e5-81ba-949fd4699560",
          "traceId": "ae61b594",
          "type": "GENERATION",
          "name": "ae61_tpusg_gen_attempt#3",
          "startTime": "2025-07-29T11:02:59.332000+00:00",
          "endTime": "2025-07-29T11:03:29.041000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 29709.0,
          "costDetails": {},
          "completionTokens": 1020,
          "totalTokens": 3068,
          "promptTokens": 2048,
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-14-03-37-614092_chatcmpl-9a7962d9-1da8-41c5-a507-28a1b2ab398a",
          "traceId": "ae61b594",
          "type": "GENERATION",
          "name": "ae61_tpusg_gen_attempt#4",
          "startTime": "2025-07-29T11:03:37.614000+00:00",
          "endTime": "2025-07-29T11:04:16.294000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 38680.0,
          "costDetails": {},
          "completionTokens": 1360,
          "totalTokens": 3408,
          "promptTokens": 2048,
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-14-04-20-485490_chatcmpl-428757cf-9db6-42ad-9dfb-370c8ddd7411",
          "traceId": "ae61b594",
          "type": "GENERATION",
          "name": "ae61_tpusg_gen_attempt#5",
          "startTime": "2025-07-29T11:04:20.485000+00:00",
          "endTime": "2025-07-29T11:04:46.114000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 25629.0,
          "costDetails": {},
          "completionTokens": 871,
          "totalTokens": 2919,
          "promptTokens": 2048,
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "5ebd6407-fec3-4627-8735-f47e88d11659",
          "traceId": "ae61b594",
          "type": "SPAN",
          "name": "error_ae_tpusg_failure_signal_tpu_sketch_generator",
          "startTime": "2025-07-29T11:04:53.730000+00:00",
          "level": "ERROR",
          "statusMessage": "Failed. Last error: Traceback (most recent call last):\n  File \"script_7c56f049_1753787086.py\", line 6, in <module>\n    interpreter = tf.lite.Interpreter(model_path=\"/path/to/model.tflite\")  # Update with actual path\nNameError: name 'tf' is not defined",
          "calculatedTotalCost": 0.0,
          "costDetails": {},
          "completionTokens": 0,
          "totalTokens": 0,
          "promptTokens": 0,
          "environment": "default",
          "endTime": null,
          "model": null,
          "modelParameters": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null,
          "latency": null
        }
      ],
      "createdAt": "2025-07-29T11:00:38.905Z",
      "updatedAt": "2025-07-29T11:04:59.221Z",
      "environment": "default"
    }
  ],
  "meta": {
    "total_items": 19
  }
}
{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "c4ae8806-be35-44e1-93c7-878eed7f14f3",
      "metadata": {
        "id": "c4ae8806-be35-44e1-93c7-878eed7f14f3"
      },
      "source": [
        "# Exporting LLM Runs and Feedback\n",
        "[![Open In Collab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/langchain-ai/langsmith-cookbook/blob/main/exploratory-data-analysis/exporting-llm-runs-and-feedback/llm_run_etl.ipynb)\n",
        "\n",
        "Understanding how your LLM app interacts with users is crucial. LangSmith offers a number of useful ways to interact with and annotate trace data directly in the app. You can also easily query that trace data so you can process it with your tool of choice.\n",
        "\n",
        "This tutorial guides you through exporting LLM traces and associated feedback from LangSmith for further analysis. By the end, you'll be able to export a flat table of LLM run information that you can analyze, enrich, or use for model training.\n",
        "\n",
        "Before we start, ensure you have a LangChain project with some logged traces. You can generate some using almost any of the other recipes in this cookbook. The overall steps will be:\n",
        "\n",
        "1. Query runs, filtering by time, tags, or other attributes.\n",
        "2. Add in associated feedback metrics (if captured)\n",
        "3. Export to analysis tool.\n",
        "\n",
        "To make things easy, we will be loading the data into a pandas dataframe. We will be doing the ETL on LLM runs logged from LangChain, but you can modify the code below to handle whatever schema is used by your deployed model. Now let's set up our environment!\n",
        "\n",
        "#### Setup\n",
        "\n",
        "First, install langsmith and pandas and set your langsmith API key to connect to your project.\n",
        "We will also install LangChain to use one of its formatting utilities."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "5a6b4974-1f59-4517-bce3-b4836783b3dc",
      "metadata": {
        "id": "5a6b4974-1f59-4517-bce3-b4836783b3dc"
      },
      "outputs": [],
      "source": [
        "# %pip install --upgrade --force-reinstall langchain langsmith pandas seaborn --quiet\n",
        "import os  # Add this line at the beginning of your code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "4c37e579-d044-4d95-8dc5-1763f956f481",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4c37e579-d044-4d95-8dc5-1763f956f481",
        "outputId": "dc591518-8a70-4610-b09d-009e00409459"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "env: LANGCHAIN_API_KEY='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'\n"
          ]
        }
      ],
      "source": [
        "%env LANGCHAIN_API_KEY='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'\n",
        "# %pip install --upgrade pandas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "428695ac-6ca5-4967-8e17-7b0fc6a4df51",
      "metadata": {
        "id": "428695ac-6ca5-4967-8e17-7b0fc6a4df51"
      },
      "outputs": [],
      "source": [
        "from langsmith import Client\n",
        "import pandas as pd\n",
        "\n",
        "client = Client()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d3143554-6a2a-4f42-b6a3-0df9cb521314",
      "metadata": {
        "id": "d3143554-6a2a-4f42-b6a3-0df9cb521314"
      },
      "source": [
        "## 1. Query Runs\n",
        " query and save to json files\n",
        " \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "d85e089d-d7c1-4f61-bb4f-d6728da98b4e",
      "metadata": {
        "id": "d85e089d-d7c1-4f61-bb4f-d6728da98b4e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2172_batch\n"
          ]
        }
      ],
      "source": [
        "from datetime import datetime, timedelta\n",
        "from datetime import timezone\n",
        "import json\n",
        "\n",
        "# batch_tests 10.28:\n",
        "#   data_processor: batch_id: 2117_batch\n",
        "#   model_converter: batch_id: 7121_batch\n",
        "#   sketch_generator: batch_id: 1730_batch\n",
        "\n",
        "#   sketch_generato2: batch_id: 2172_batch\n",
        "\n",
        "dp_batch = {\"name\": \"data_processor\", \"batch_id\": \"2117_batch\"}\n",
        "mc_batch = {\"name\": \"model_converter\", \"batch_id\": \"7121_batch\"}\n",
        "# sg_batch = {\"name\": \"sketch_generator\", \"batch_id\": \"1730_batch\"}\n",
        "sg_batch = {\"name\": \"sketch_generator\", \"batch_id\": \"2172_batch\"}\n",
        "\n",
        "\n",
        "def run_to_dict(run):\n",
        "    return {\n",
        "        \"id\": str(run.id),\n",
        "        \"name\": run.name,\n",
        "        \"timing\": {\n",
        "            \"start_time\": run.start_time.strftime(\"%Y-%m-%d %H:%M:%S.%f\"),\n",
        "            \"end_time\": run.end_time.strftime(\"%Y-%m-%d %H:%M:%S.%f\"),\n",
        "        },\n",
        "        \"run_type\": run.run_type,\n",
        "        \"metadata\": {\n",
        "            \"trace_id\": run.extra.get(\"metadata\", {}).get(\"trace_id\"),\n",
        "            \"num_run\": run.extra.get(\"metadata\", {}).get(\"num_run\"),\n",
        "            \"batch_id\": run.extra.get(\"metadata\", {}).get(\"batch_id\"),\n",
        "            \"network_latency\": run.extra.get(\"metadata\", {}).get(\"network_latency\"),\n",
        "            \"ls_method\": run.extra.get(\"metadata\", {}).get(\"ls_method\"),\n",
        "            \"revision_id\": run.extra.get(\"metadata\", {}).get(\"revision_id\"),\n",
        "        },\n",
        "        \"runtime\": run.extra.get(\"runtime\", {}),\n",
        "        \"tokens\": {\n",
        "            \"prompt_tokens\": run.prompt_tokens,\n",
        "            \"completion_tokens\": run.completion_tokens,\n",
        "            \"total_tokens\": run.total_tokens,\n",
        "        },\n",
        "        \"cost\": {\n",
        "            \"prompt_cost\": float(run.prompt_cost) if run.prompt_cost else None,\n",
        "            \"completion_cost\": (\n",
        "                float(run.completion_cost) if run.completion_cost else None\n",
        "            ),\n",
        "            \"total_cost\": float(run.total_cost) if run.total_cost else None,\n",
        "        },\n",
        "        \"status\": run.status,\n",
        "        \"session_id\": str(run.session_id) if run.session_id else None,\n",
        "        \"child_run_ids\": (\n",
        "            [str(run_id) for run_id in run.child_run_ids] if run.child_run_ids else []\n",
        "        ),\n",
        "        \"tags\": run.tags,\n",
        "    }\n",
        "\n",
        "\n",
        "def convert_runs_to_json(runs):\n",
        "    # Convert list of Run objects to list of dicts\n",
        "    runs_data = [run_to_dict(run) for run in runs]\n",
        "    return runs_data\n",
        "\n",
        "\n",
        "def save_runs(current_batch):\n",
        "    runs = list(\n",
        "        client.list_runs(\n",
        "            project_name=\"default\",\n",
        "            start_time=datetime.now(timezone.utc) - timedelta(days=2),\n",
        "            run_type=\"llm\",\n",
        "            # filter=f'and(has(tags, \\'gpt-4o-mini\\'),has(tags, {task_type}))',\n",
        "            filter=f\"and(eq(metadata_key, 'batch_id'), eq(metadata_value, '{current_batch['batch_id']}'))\",\n",
        "        )\n",
        "    )\n",
        "\n",
        "    output_file = (\n",
        "        f\"raw_export/js_{current_batch['name']}_{current_batch['batch_id']}.json\"\n",
        "    )\n",
        "    runs_data = convert_runs_to_json(runs)\n",
        "    # Write to JSON file\n",
        "\n",
        "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(runs_data, f, indent=2)\n",
        "\n",
        "\n",
        "current_batch = sg_batch\n",
        "print(current_batch[\"batch_id\"])\n",
        "save_runs(current_batch)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0b9bf6f2",
      "metadata": {},
      "source": [
        "## Start from reading data from saved json files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "1cfbed3f",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Latency statistics (in seconds):\n",
            "count    186.000000\n",
            "mean       9.824087\n",
            "std        4.917885\n",
            "min        0.000463\n",
            "25%        6.893415\n",
            "50%       10.484762\n",
            "75%       12.634783\n",
            "max       25.399238\n",
            "Name: latency, dtype: float64\n",
            "\n",
            "Sample rows with latency:\n",
            "                                    name                 start_time  \\\n",
            "0                  84_sg_error_handling2 2024-10-29 18:16:42.115096   \n",
            "1                      84_sg_sketch_gen1 2024-10-29 18:16:05.495374   \n",
            "2                     84_sg_spec_filling 2024-10-29 18:15:57.931333   \n",
            "3  61_sg_failure_signal_sketch_generator 2024-10-29 18:15:36.448609   \n",
            "4                  61_sg_error_handling5 2024-10-29 18:15:27.374036   \n",
            "\n",
            "                    end_time    latency  \n",
            "0 2024-10-29 18:16:53.611612  11.496516  \n",
            "1 2024-10-29 18:16:16.336921  10.841547  \n",
            "2 2024-10-29 18:16:05.473788   7.542455  \n",
            "3 2024-10-29 18:15:36.449276   0.000667  \n",
            "4 2024-10-29 18:15:34.667773   7.293737  \n",
            "                                     id  \\\n",
            "0  5867ac02-020f-48e2-9262-c96494d34a4e   \n",
            "1  77d5b9b3-8df2-4652-a6c5-f21ad913516e   \n",
            "2  32d6ae7a-73d7-4613-967f-af8420b348d9   \n",
            "3  96419355-be45-4ed7-a3f7-9bbae6ce26f9   \n",
            "4  e353978d-bb12-4ba0-be0b-589de7db0070   \n",
            "\n",
            "                                    name                 start_time  \\\n",
            "0                  84_sg_error_handling2 2024-10-29 18:16:42.115096   \n",
            "1                      84_sg_sketch_gen1 2024-10-29 18:16:05.495374   \n",
            "2                     84_sg_spec_filling 2024-10-29 18:15:57.931333   \n",
            "3  61_sg_failure_signal_sketch_generator 2024-10-29 18:15:36.448609   \n",
            "4                  61_sg_error_handling5 2024-10-29 18:15:27.374036   \n",
            "\n",
            "                    end_time run_type  trace_id  num_run    batch_id  \\\n",
            "0 2024-10-29 18:16:53.611612      llm  84c640b6       29  2172_batch   \n",
            "1 2024-10-29 18:16:16.336921      llm  84c640b6       29  2172_batch   \n",
            "2 2024-10-29 18:16:05.473788      llm  84c640b6       29  2172_batch   \n",
            "3 2024-10-29 18:15:36.449276      llm  61fb3a8a       28  2172_batch   \n",
            "4 2024-10-29 18:15:34.667773      llm  61fb3a8a       28  2172_batch   \n",
            "\n",
            "   network_latency sdk_version  ... completion_tokens total_tokens  \\\n",
            "0         0.031784     0.1.137  ...               765         3331   \n",
            "1         0.020736     0.1.137  ...               743         2325   \n",
            "2         0.025522     0.1.137  ...               371         1598   \n",
            "3              NaN     0.1.137  ...                 0            0   \n",
            "4         0.025065     0.1.137  ...               639         3938   \n",
            "\n",
            "  prompt_cost  completion_cost  total_cost   status  \\\n",
            "0    0.012830         0.011475    0.024305  success   \n",
            "1    0.007910         0.011145    0.019055  success   \n",
            "2    0.006135         0.005565    0.011700  success   \n",
            "3         NaN              NaN         NaN    error   \n",
            "4    0.016495         0.009585    0.026080  success   \n",
            "\n",
            "                             session_id                         child_run_ids  \\\n",
            "0  601dc250-34d5-4229-a99b-74074fd8c221  e46ad239-2118-41e8-b048-61bd86352b01   \n",
            "1  601dc250-34d5-4229-a99b-74074fd8c221  961d15d3-d5a8-4328-82fe-749daaab8802   \n",
            "2  601dc250-34d5-4229-a99b-74074fd8c221  7585f23a-abc5-4ebe-9f5f-6447948a19ef   \n",
            "3  601dc250-34d5-4229-a99b-74074fd8c221                                         \n",
            "4  601dc250-34d5-4229-a99b-74074fd8c221  e08c4803-20e0-4e70-8b52-114f2e0ddb2d   \n",
            "\n",
            "                                tags    latency  \n",
            "0  benchmark,gpt-4o,sketch_generator  11.496516  \n",
            "1  benchmark,gpt-4o,sketch_generator  10.841547  \n",
            "2  benchmark,gpt-4o,sketch_generator   7.542455  \n",
            "3  benchmark,gpt-4o,sketch_generator   0.000667  \n",
            "4  benchmark,gpt-4o,sketch_generator   7.293737  \n",
            "\n",
            "[5 rows x 24 columns]\n",
            "\n",
            "DataFrame Info:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 186 entries, 0 to 185\n",
            "Data columns (total 24 columns):\n",
            " #   Column             Non-Null Count  Dtype         \n",
            "---  ------             --------------  -----         \n",
            " 0   id                 186 non-null    object        \n",
            " 1   name               186 non-null    object        \n",
            " 2   start_time         186 non-null    datetime64[ns]\n",
            " 3   end_time           186 non-null    datetime64[ns]\n",
            " 4   run_type           186 non-null    object        \n",
            " 5   trace_id           186 non-null    object        \n",
            " 6   num_run            186 non-null    int64         \n",
            " 7   batch_id           186 non-null    object        \n",
            " 8   network_latency    167 non-null    float64       \n",
            " 9   sdk_version        186 non-null    object        \n",
            " 10  platform           186 non-null    object        \n",
            " 11  python_version     186 non-null    object        \n",
            " 12  langchain_version  186 non-null    object        \n",
            " 13  prompt_tokens      186 non-null    int64         \n",
            " 14  completion_tokens  186 non-null    int64         \n",
            " 15  total_tokens       186 non-null    int64         \n",
            " 16  prompt_cost        167 non-null    float64       \n",
            " 17  completion_cost    167 non-null    float64       \n",
            " 18  total_cost         167 non-null    float64       \n",
            " 19  status             186 non-null    object        \n",
            " 20  session_id         186 non-null    object        \n",
            " 21  child_run_ids      186 non-null    object        \n",
            " 22  tags               186 non-null    object        \n",
            " 23  latency            186 non-null    float64       \n",
            "dtypes: datetime64[ns](2), float64(5), int64(4), object(13)\n",
            "memory usage: 35.0+ KB\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import json\n",
        "\n",
        "current_batch = sg_batch\n",
        "\n",
        "\n",
        "# Open json file\n",
        "def load_runs_df(json_path):\n",
        "    # Read JSON file\n",
        "    with open(json_path, \"r\") as f:\n",
        "        data = json.load(f)\n",
        "\n",
        "    # Flatten nested structures\n",
        "    flattened_data = []\n",
        "    for run in data:\n",
        "        flat_run = {\n",
        "            \"id\": run[\"id\"],\n",
        "            \"name\": run[\"name\"],\n",
        "            \"start_time\": run[\"timing\"][\"start_time\"],\n",
        "            \"end_time\": run[\"timing\"][\"end_time\"],\n",
        "            \"run_type\": run[\"run_type\"],\n",
        "            # Metadata\n",
        "            \"trace_id\": run[\"metadata\"][\"trace_id\"],\n",
        "            \"num_run\": run[\"metadata\"][\"num_run\"],\n",
        "            \"batch_id\": run[\"metadata\"][\"batch_id\"],\n",
        "            \"network_latency\": run[\"metadata\"][\"network_latency\"],\n",
        "            # Runtime\n",
        "            \"sdk_version\": run[\"runtime\"][\"sdk_version\"],\n",
        "            \"platform\": run[\"runtime\"][\"platform\"],\n",
        "            \"python_version\": run[\"runtime\"][\"runtime_version\"],\n",
        "            \"langchain_version\": run[\"runtime\"][\"langchain_version\"],\n",
        "            # Tokens\n",
        "            \"prompt_tokens\": run[\"tokens\"][\"prompt_tokens\"],\n",
        "            \"completion_tokens\": run[\"tokens\"][\"completion_tokens\"],\n",
        "            \"total_tokens\": run[\"tokens\"][\"total_tokens\"],\n",
        "            # Cost\n",
        "            \"prompt_cost\": run[\"cost\"][\"prompt_cost\"],\n",
        "            \"completion_cost\": run[\"cost\"][\"completion_cost\"],\n",
        "            \"total_cost\": run[\"cost\"][\"total_cost\"],\n",
        "            \"status\": run[\"status\"],\n",
        "            \"session_id\": run[\"session_id\"],\n",
        "            \"child_run_ids\": \",\".join(run[\"child_run_ids\"]),  # Convert list to string\n",
        "            \"tags\": \",\".join(run[\"tags\"]),  # Convert list to string\n",
        "        }\n",
        "        flattened_data.append(flat_run)\n",
        "\n",
        "    # Create DataFrame\n",
        "    df = pd.DataFrame(flattened_data)\n",
        "\n",
        "    # Convert timestamp strings to datetime\n",
        "    df[\"start_time\"] = pd.to_datetime(df[\"start_time\"])\n",
        "    df[\"end_time\"] = pd.to_datetime(df[\"end_time\"])\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "# Usage\n",
        "json_path = f\"raw_export/js_{current_batch['name']}_{current_batch['batch_id']}.json\"\n",
        "df = load_runs_df(json_path)\n",
        "\n",
        "# In-place deletion of the duplicateed rows 'ChatOpenAI'\n",
        "df.drop(df[df[\"name\"] == \"ChatOpenAI\"].index, inplace=True)\n",
        "df.reset_index(drop=True, inplace=True)\n",
        "\n",
        "# Add latency column in seconds\n",
        "df[\"latency\"] = (df[\"end_time\"] - df[\"start_time\"]).dt.total_seconds()\n",
        "\n",
        "# Show the new column\n",
        "print(\"\\nLatency statistics (in seconds):\")\n",
        "print(df[\"latency\"].describe())\n",
        "\n",
        "# Show sample rows with timestamps and latency\n",
        "print(\"\\nSample rows with latency:\")\n",
        "print(df[[\"name\", \"start_time\", \"end_time\", \"latency\"]].head())\n",
        "\n",
        "# Now you can do analysis\n",
        "print(df.head())\n",
        "print(\"\\nDataFrame Info:\")\n",
        "print(df.info())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b2f2889f",
      "metadata": {},
      "source": [
        "## Drop the columns that are not needed\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "b9898ff5",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Remaining columns:\n",
            "['id', 'name', 'trace_id', 'num_run', 'batch_id', 'network_latency', 'prompt_tokens', 'completion_tokens', 'total_tokens', 'prompt_cost', 'completion_cost', 'total_cost', 'status', 'tags', 'latency']\n"
          ]
        }
      ],
      "source": [
        "# List of columns to drop\n",
        "columns_to_drop = [\n",
        "    \"session_id\",\n",
        "    \"child_run_ids\",\n",
        "    \"sdk_version\",\n",
        "    \"start_time\",\n",
        "    \"end_time\",\n",
        "    \"platform\",\n",
        "    \"python_version\",\n",
        "    \"langchain_version\",\n",
        "    \"run_type\",\n",
        "]\n",
        "\n",
        "# Drop the columns\n",
        "for col in columns_to_drop:\n",
        "    if col in df.columns:\n",
        "        df = df.drop(columns=[col])\n",
        "# Show remaining columns\n",
        "print(\"Remaining columns:\")\n",
        "print(df.columns.tolist())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3e0f6d65",
      "metadata": {},
      "source": [
        "## Sort the dataframe by num_run\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "5667c32e",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sorted DataFrame by num_run:\n",
            "   num_run                   name  trace_id\n",
            "0        0     38_sg_spec_filling  3856dbb2\n",
            "1        0  38_sg_error_handling5  3856dbb2\n",
            "2        0  38_sg_error_handling4  3856dbb2\n",
            "3        0      38_sg_sketch_gen1  3856dbb2\n",
            "4        0  38_sg_error_handling2  3856dbb2\n",
            "\n",
            "Range of num_run values:\n",
            "Min: 0, Max: 29\n"
          ]
        }
      ],
      "source": [
        "# Sort DataFrame by num_run\n",
        "df = df.sort_values(\"num_run\")\n",
        "\n",
        "# Reset index after sorting\n",
        "df = df.reset_index(drop=True)\n",
        "\n",
        "# Display first few rows to verify sorting\n",
        "print(\"Sorted DataFrame by num_run:\")\n",
        "print(df[[\"num_run\", \"name\", \"trace_id\"]].head())\n",
        "\n",
        "# Optional: Display full range of num_run\n",
        "print(\"\\nRange of num_run values:\")\n",
        "print(f\"Min: {df['num_run'].min()}, Max: {df['num_run'].max()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "91a8e806",
      "metadata": {},
      "source": [
        "## Combine the subtasks into one task (based on the shared trace_id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "743c0020",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Processed DataFrame Info:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Index: 30 entries, 6 to 16\n",
            "Data columns (total 14 columns):\n",
            " #   Column             Non-Null Count  Dtype  \n",
            "---  ------             --------------  -----  \n",
            " 0   num_run            30 non-null     int64  \n",
            " 1   name               30 non-null     object \n",
            " 2   trace_id           30 non-null     object \n",
            " 3   latency            30 non-null     float64\n",
            " 4   network_latency    30 non-null     float64\n",
            " 5   total_tokens       30 non-null     int64  \n",
            " 6   total_cost         30 non-null     float64\n",
            " 7   status             30 non-null     object \n",
            " 8   batch_id           30 non-null     object \n",
            " 9   tags               30 non-null     object \n",
            " 10  prompt_tokens      30 non-null     int64  \n",
            " 11  completion_tokens  30 non-null     int64  \n",
            " 12  prompt_cost        30 non-null     float64\n",
            " 13  completion_cost    30 non-null     float64\n",
            "dtypes: float64(5), int64(4), object(5)\n",
            "memory usage: 3.5+ KB\n",
            "None\n",
            "\n",
            "First few rows:\n",
            "    num_run   name  trace_id    latency  network_latency  total_tokens  \\\n",
            "6         0  38_sg  3856dbb2  68.183984         0.236032         13374   \n",
            "17        1  8a_sg  8a37c78d  66.365198         0.165282         15406   \n",
            "20        2  ad_sg  ad3ad388  74.591318         0.131317         15297   \n",
            "15        3  76_sg  7616819c  85.886011         0.137040         13837   \n",
            "14        4  6a_sg  6a883845  88.175405         0.126731         17181   \n",
            "\n",
            "    total_cost   status    batch_id                               tags  \\\n",
            "6     0.109630  success  2172_batch  benchmark,gpt-4o,sketch_generator   \n",
            "17    0.120120  success  2172_batch  benchmark,gpt-4o,sketch_generator   \n",
            "20    0.120235    error  2172_batch  benchmark,gpt-4o,sketch_generator   \n",
            "15    0.106565    error  2172_batch  benchmark,gpt-4o,sketch_generator   \n",
            "14    0.127505    error  2172_batch  benchmark,gpt-4o,sketch_generator   \n",
            "\n",
            "    prompt_tokens  completion_tokens  prompt_cost  completion_cost  \n",
            "6            9098               4276     0.045490         0.064140  \n",
            "17          11097               4309     0.055485         0.064635  \n",
            "20          10922               4375     0.054610         0.065625  \n",
            "15          10099               3738     0.050495         0.056070  \n",
            "14          13021               4160     0.065105         0.062400  \n",
            "\n",
            "Unique names per trace_id after processing:\n",
            "name\n",
            "1    30\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def process_df(df):\n",
        "    # 1. Process names - take first 2 sections\n",
        "    df[\"name\"] = df[\"name\"].apply(lambda x: \"_\".join(x.split(\"_\")[:2]))\n",
        "\n",
        "    # Check name consistency within trace_id groups\n",
        "    name_consistency = df.groupby(\"trace_id\")[\"name\"].nunique()\n",
        "    if not all(name_consistency == 1):\n",
        "        print(\"Warning: Inconsistent names found within trace_id groups:\")\n",
        "        print(name_consistency[name_consistency > 1])\n",
        "\n",
        "    # 2. Remove id column\n",
        "    df = df.drop(columns=[\"id\"])\n",
        "\n",
        "    # Check consistency of num_run, batch_id, tags within trace_id groups\n",
        "    for col in [\"num_run\", \"batch_id\", \"tags\"]:\n",
        "        consistency = df.groupby(\"trace_id\")[col].nunique()\n",
        "        if not all(consistency == 1):\n",
        "            print(f\"Warning: Inconsistent {col} found within trace_id groups:\")\n",
        "            print(consistency[consistency > 1])\n",
        "\n",
        "    # Columns to aggregate\n",
        "    sum_columns = [\n",
        "        \"latency\",\n",
        "        \"network_latency\",\n",
        "        \"prompt_tokens\",\n",
        "        \"completion_tokens\",\n",
        "        \"total_tokens\",\n",
        "        \"prompt_cost\",\n",
        "        \"completion_cost\",\n",
        "        \"total_cost\",\n",
        "    ]\n",
        "\n",
        "    def aggregate_status(x):\n",
        "        if \"error\" in x.values:\n",
        "            return \"error\"\n",
        "        return \"success\" if all(x == \"success\") else \"error\"\n",
        "\n",
        "    # Aggregate by trace_id\n",
        "    agg_dict = {\n",
        "        \"name\": \"first\",\n",
        "        \"num_run\": \"first\",\n",
        "        \"batch_id\": \"first\",\n",
        "        \"tags\": \"first\",\n",
        "        \"status\": aggregate_status,\n",
        "    }\n",
        "\n",
        "    # Add sum aggregation for numeric columns\n",
        "    for col in sum_columns:\n",
        "        agg_dict[col] = lambda x: x[x.notna()].sum()\n",
        "\n",
        "    # Perform groupby and aggregation\n",
        "    df_combined = df.groupby(\"trace_id\").agg(agg_dict).reset_index()\n",
        "\n",
        "    # 7. Reorder columns\n",
        "    column_order = [\n",
        "        \"num_run\",\n",
        "        \"name\",\n",
        "        \"trace_id\",\n",
        "        \"latency\",\n",
        "        \"network_latency\",\n",
        "        \"total_tokens\",\n",
        "        \"total_cost\",\n",
        "        \"status\",\n",
        "        \"batch_id\",\n",
        "        \"tags\",\n",
        "        \"prompt_tokens\",\n",
        "        \"completion_tokens\",\n",
        "        \"prompt_cost\",\n",
        "        \"completion_cost\",\n",
        "    ]\n",
        "    df_combined = df_combined[column_order]\n",
        "\n",
        "    # 8. Sort by num_run\n",
        "    df_combined = df_combined.sort_values(\"num_run\")\n",
        "\n",
        "    return df_combined\n",
        "\n",
        "\n",
        "# Apply the processing\n",
        "df_processed = process_df(df)\n",
        "\n",
        "# Display results\n",
        "print(\"\\nProcessed DataFrame Info:\")\n",
        "print(df_processed.info())\n",
        "print(\"\\nFirst few rows:\")\n",
        "print(df_processed.head())\n",
        "\n",
        "# Additional verification\n",
        "print(\"\\nUnique names per trace_id after processing:\")\n",
        "print(df_processed.groupby(\"trace_id\")[\"name\"].nunique().value_counts())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "09739400",
      "metadata": {},
      "source": [
        "## Calculate clean latency\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "4be66738",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "After calculation:\n",
            "      latency  network_latency\n",
            "6   67.711920         0.236032\n",
            "17  66.034635         0.165282\n",
            "20  74.328685         0.131317\n",
            "15  85.611930         0.137040\n",
            "14  87.921943         0.126731\n"
          ]
        }
      ],
      "source": [
        "# Calculate new latency\n",
        "df_processed[\"latency\"] = df_processed[\"latency\"] - (\n",
        "    2 * df_processed[\"network_latency\"]\n",
        ")\n",
        "\n",
        "# Show results\n",
        "print(\"\\nAfter calculation:\")\n",
        "print(df_processed[[\"latency\", \"network_latency\"]].head())\n",
        "\n",
        "# Check for any negative values (which might indicate issues)\n",
        "negative_latency = df_processed[df_processed[\"latency\"] < 0]\n",
        "if len(negative_latency) > 0:\n",
        "    print(\"\\nWarning: Found negative latency values:\")\n",
        "    print(negative_latency[[\"trace_id\", \"latency\", \"network_latency\"]])\n",
        "\n",
        "# Remove network_latency column\n",
        "df_processed = df_processed.drop(columns=[\"network_latency\"])\n",
        "\n",
        "# Reorder columns\n",
        "column_order = [\n",
        "    \"num_run\",\n",
        "    \"name\",\n",
        "    \"trace_id\",\n",
        "    \"latency\",\n",
        "    \"total_tokens\",\n",
        "    \"total_cost\",\n",
        "    \"status\",\n",
        "    \"batch_id\",\n",
        "    \"tags\",\n",
        "    \"prompt_tokens\",\n",
        "    \"completion_tokens\",\n",
        "    \"prompt_cost\",\n",
        "    \"completion_cost\",\n",
        "]\n",
        "df_processed = df_processed[column_order]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f54feca7",
      "metadata": {},
      "source": [
        "## Save to csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "c0f44bbe",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DataFrame saved to: processed_data/clean_sketch_generator_2172_batch.csv\n",
            "File size: 4326 bytes\n",
            "\n",
            "First few lines of saved file:\n",
            "num_run,name,trace_id,latency,total_tokens,total_cost,status,batch_id,tags,prompt_tokens,completion_tokens,prompt_cost,completion_cost\n",
            "0,38_sg,3856dbb2,67.71191998175048,13374,0.10962999999999999,success,2172_batch,\"benchmark,gpt-4o,sketch_generator\",9098,4276,0.04549,0.06414\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "\n",
        "# Create the filename\n",
        "filename = (\n",
        "    f\"processed_data/clean_{current_batch['name']}_{current_batch['batch_id']}.csv\"\n",
        ")\n",
        "\n",
        "# Save to CSV\n",
        "df_processed.to_csv(filename, index=False)\n",
        "\n",
        "print(f\"DataFrame saved to: {filename}\")\n",
        "\n",
        "# Verify the file was created\n",
        "if os.path.exists(filename):\n",
        "    print(f\"File size: {os.path.getsize(filename)} bytes\")\n",
        "    # Show first few lines of saved file\n",
        "    print(\"\\nFirst few lines of saved file:\")\n",
        "    with open(filename, \"r\") as f:\n",
        "        print(f.readline().strip())  # Headers\n",
        "        print(f.readline().strip())  # First data row"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0967437a",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "766b584e",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "==================================================\n",
            "Validation for Data Processor\n",
            "==================================================\n",
            "\n",
            "=== Cost Validation ===\n",
            "\n",
            "=== Token Validation ===\n",
            "\n",
            "Cost Analysis:\n",
            "Total rows: 30\n",
            "Rows with cost mismatches: 0\n",
            "\n",
            "Token Analysis:\n",
            "Total rows: 30\n",
            "Rows with token mismatches: 0\n",
            "\n",
            "Statistics:\n",
            "Costs:\n",
            "Mean difference: 0.000000\n",
            "Max difference: 0.000000\n",
            "\n",
            "Tokens:\n",
            "Mean difference: 0.0\n",
            "Max difference: 0.0\n",
            "\n",
            "==================================================\n",
            "Validation for Model Converter\n",
            "==================================================\n",
            "\n",
            "=== Cost Validation ===\n",
            "\n",
            "=== Token Validation ===\n",
            "\n",
            "Cost Analysis:\n",
            "Total rows: 30\n",
            "Rows with cost mismatches: 0\n",
            "\n",
            "Token Analysis:\n",
            "Total rows: 30\n",
            "Rows with token mismatches: 0\n",
            "\n",
            "Statistics:\n",
            "Costs:\n",
            "Mean difference: 0.000000\n",
            "Max difference: 0.000000\n",
            "\n",
            "Tokens:\n",
            "Mean difference: 0.0\n",
            "Max difference: 0.0\n",
            "\n",
            "==================================================\n",
            "Validation for Sketch Generator\n",
            "==================================================\n",
            "\n",
            "=== Cost Validation ===\n",
            "\n",
            "=== Token Validation ===\n",
            "\n",
            "Cost Analysis:\n",
            "Total rows: 30\n",
            "Rows with cost mismatches: 0\n",
            "\n",
            "Token Analysis:\n",
            "Total rows: 30\n",
            "Rows with token mismatches: 0\n",
            "\n",
            "Statistics:\n",
            "Costs:\n",
            "Mean difference: 0.000000\n",
            "Max difference: 0.000000\n",
            "\n",
            "Tokens:\n",
            "Mean difference: 0.0\n",
            "Max difference: 0.0\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def validate_data(df, task_name):\n",
        "    print(f\"\\n{'='*50}\")\n",
        "    print(f\"Validation for {task_name}\")\n",
        "    print(f\"{'='*50}\")\n",
        "\n",
        "    # Cost Validation\n",
        "    print(\"\\n=== Cost Validation ===\")\n",
        "    df[\"calculated_total_cost\"] = df[\"prompt_cost\"] + df[\"completion_cost\"]\n",
        "    df[\"cost_difference\"] = (df[\"total_cost\"] - df[\"calculated_total_cost\"]).abs()\n",
        "\n",
        "    # Token Validation\n",
        "    print(\"\\n=== Token Validation ===\")\n",
        "    df[\"calculated_total_tokens\"] = df[\"prompt_tokens\"] + df[\"completion_tokens\"]\n",
        "    df[\"token_difference\"] = (df[\"total_tokens\"] - df[\"calculated_total_tokens\"]).abs()\n",
        "\n",
        "    # Check for mismatches\n",
        "    cost_threshold = 0.0001\n",
        "    cost_mismatches = df[df[\"cost_difference\"] > cost_threshold]\n",
        "    token_mismatches = df[df[\"token_difference\"] > 0]  # Tokens should match exactly\n",
        "\n",
        "    # Print results\n",
        "    print(\"\\nCost Analysis:\")\n",
        "    print(f\"Total rows: {len(df)}\")\n",
        "    print(f\"Rows with cost mismatches: {len(cost_mismatches)}\")\n",
        "    if len(cost_mismatches) > 0:\n",
        "        print(\"\\nSample of cost mismatches:\")\n",
        "        print(\n",
        "            cost_mismatches[\n",
        "                [\n",
        "                    \"prompt_cost\",\n",
        "                    \"completion_cost\",\n",
        "                    \"calculated_total_cost\",\n",
        "                    \"total_cost\",\n",
        "                    \"cost_difference\",\n",
        "                ]\n",
        "            ]\n",
        "            .head()\n",
        "            .to_string()\n",
        "        )\n",
        "\n",
        "    print(\"\\nToken Analysis:\")\n",
        "    print(f\"Total rows: {len(df)}\")\n",
        "    print(f\"Rows with token mismatches: {len(token_mismatches)}\")\n",
        "    if len(token_mismatches) > 0:\n",
        "        print(\"\\nSample of token mismatches:\")\n",
        "        print(\n",
        "            token_mismatches[\n",
        "                [\n",
        "                    \"prompt_tokens\",\n",
        "                    \"completion_tokens\",\n",
        "                    \"calculated_total_tokens\",\n",
        "                    \"total_tokens\",\n",
        "                    \"token_difference\",\n",
        "                ]\n",
        "            ]\n",
        "            .head()\n",
        "            .to_string()\n",
        "        )\n",
        "\n",
        "    # Statistics\n",
        "    print(\"\\nStatistics:\")\n",
        "    print(\"Costs:\")\n",
        "    print(f\"Mean difference: {df['cost_difference'].mean():.6f}\")\n",
        "    print(f\"Max difference: {df['cost_difference'].max():.6f}\")\n",
        "    print(\"\\nTokens:\")\n",
        "    print(f\"Mean difference: {df['token_difference'].mean():.1f}\")\n",
        "    print(f\"Max difference: {df['token_difference'].max():.1f}\")\n",
        "\n",
        "\n",
        "# Define batches\n",
        "batches = {\n",
        "    \"Data Processor\": {\"name\": \"data_processor\", \"batch_id\": \"2117_batch\"},\n",
        "    \"Model Converter\": {\"name\": \"model_converter\", \"batch_id\": \"7121_batch\"},\n",
        "    \"Sketch Generator\": {\"name\": \"sketch_generator\", \"batch_id\": \"1730_batch\"},\n",
        "}\n",
        "\n",
        "# Load and validate each dataset\n",
        "data_dir = \"processed_data\"\n",
        "for task_name, batch_info in batches.items():\n",
        "    try:\n",
        "        input_csv = f\"clean_{batch_info['name']}_{batch_info['batch_id']}.csv\"\n",
        "        df = pd.read_csv(f\"{data_dir}/{input_csv}\")\n",
        "        validate_data(df, task_name)\n",
        "    except Exception as e:\n",
        "        print(f\"\\nError processing {task_name}: {str(e)}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.15"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}

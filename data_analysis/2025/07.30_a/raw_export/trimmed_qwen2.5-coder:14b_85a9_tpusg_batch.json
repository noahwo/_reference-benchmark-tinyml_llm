{
  "data": [
    {
      "id": "9719fa0e",
      "timestamp": "2025-07-30T15:46:29.690000+00:00",
      "name": "9719_tpu_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import os\nimport time\nfrom tflite_runtime.interpreter import Interpreter, load_delegate\nimport numpy as np\n\n# Configuration parameters\nmodel_path  = \"/home/mendel/tinyml_autopilot/models/edgetpu_detect.tflite\"\nlabel_path  = \"/home/mendel/tinyml_autopilot/models/labelmap.txt\"\ninput_path  = \"/home/mendel/tinyml_autopilot/data//sheeps.mp4\"\noutput_path  = \"/home/mendel/tinyml_autopilot/results/sheeps_detections.mp4\"\nconfidence_threshold  = 0.5\n\n# Load labels\nwith open(label_path, 'r') as f:\n    labels = [line.strip() for line in f.readlines()]\n\n# Load Interpreter with EdgeTPU acceleration\ntry:\n    interpreter = Interpreter(model_path=model_path, \n                              experimental_delegates=[load_delegate('libedgetpu.so.1.0')])\nexcept Exception as e:\n    try:\n        interpreter = Interpreter(model_path=model_path, \n                                  experimental_delegates=[load_delegate('/usr/lib/aarch64-linux-gnu/libedgetpu.so.1.0')])\n    except Exception as e:\n        raise RuntimeError(\"Failed to load EdgeTPU delegate: \" + str(e))\n\ninterpreter.allocate_tensors()\n\n# Get model details\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\n# Get input tensor details\ninput_shape = input_details[0]['shape']\ninput_dtype = input_details[0]['dtype']\nfloating_model = (input_dtype == np.float32)\n\n# Phase 2: Input Acquisition & Preprocessing Loop\nimport cv2\n\ncap = cv2.VideoCapture(input_path)\nif not cap.isOpened():\n    raise IOError(\"Cannot open video file\")\n\nfourcc = cv2.VideoWriter_fourcc(*'XVID')\nout = cv2.VideoWriter(output_path, fourcc, 20.0, (int(cap.get(3)), int(cap.get(4))))\n\nwhile True:\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    # Preprocess the frame\n    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n    frame_resized = cv2.resize(frame_rgb, (input_shape[1], input_shape[2]))\n    input_data = np.expand_dims(frame_resized, axis=0)\n\n    if floating_model:\n        input_data = (np.float32(input_data) - 127.5) / 127.5\n\n    # Phase 3: Inference\n    interpreter.set_tensor(input_details[0]['index'], input_data)\n    interpreter.invoke()\n\n    # Phase 4: Output Interpretation & Handling Loop\n    # Get output tensors\n    boxes = interpreter.get_tensor(output_details[0]['index'])[0]  # Bounding box coordinates\n    classes = interpreter.get_tensor(output_details[1]['index'])[0]  # Class indices\n    scores = interpreter.get_tensor(output_details[2]['index'])[0]   # Confidence scores\n\n    for i in range(len(scores)):\n        if scores[i] > confidence_threshold:\n            class_name = labels[int(classes[i])]\n            box = boxes[i]\n            x_min, y_min, x_max, y_max = int(box[1] * frame.shape[1]), int(box[0] * frame.shape[0]), \\\n                                         int(box[3] * frame.shape[1]), int(box[2] * frame.shape[0])\n            cv2.rectangle(frame, (x_min, y_min), (x_max, y_max), (0, 255, 0), 2)\n            label = f'{class_name}: {scores[i]:.2f}'\n            cv2.putText(frame, label, (x_min, y_min - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n\n    # Write the frame with detected objects to the output video file\n    out.write(frame)\n\n# Phase 5: Cleanup\ncap.release()\nout.release()\ncv2.destroyAllWindows()"
      },
      "session_id": "qwen2.5-coder:14b_85a9_tpusg_batch",
      "metadata": {
        "num_run": 30
      },
      "tags": [
        "benchmark",
        "qwen2.5-coder:14b",
        "tpu_sketch_generator"
      ],
      "latency": 21.761,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-18-46-29-691709_chatcmpl-45838e77-f685-4705-95d7-a8de40a2b290",
          "traceId": "9719fa0e",
          "type": "GENERATION",
          "name": "9719_tpusg_gen_attempt#1",
          "startTime": "2025-07-30T15:46:29.691000+00:00",
          "endTime": "2025-07-30T15:46:51.452000+00:00",
          "model": "qwen2.5-coder:14b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 21761.0,
          "costDetails": {},
          "environment": "default",
          "totalTokens": 2740,
          "promptTokens": 1833,
          "completionTokens": 907,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "environment": "default",
      "createdAt": "2025-07-30T15:46:30.210Z",
      "updatedAt": "2025-07-30T15:47:49.340Z"
    },
    {
      "id": "1c2479b5",
      "timestamp": "2025-07-30T15:44:37.154000+00:00",
      "name": "1c24_tpu_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import numpy as np\nfrom tflite_runtime.interpreter import Interpreter, load_delegate\nimport cv2\n\n# Phase 1: Setup\nmodel_path  = \"/home/mendel/tinyml_autopilot/models/edgetpu_detect.tflite\"\nlabel_path  = \"/home/mendel/tinyml_autopilot/models/labelmap.txt\"\ninput_path  = \"/home/mendel/tinyml_autopilot/data//sheeps.mp4\"\noutput_path  = \"/home/mendel/tinyml_autopilot/results/sheeps_detections.mp4\"\nconfidence_threshold  = 0.5\n\n# Load labels\nwith open(label_path, 'r') as f:\n    labels = [line.strip() for line in f.readlines()]\n\n# Load interpreter with EdgeTPU\ntry:\n    interpreter = Interpreter(model_path=model_path,\n                              experimental_delegates=[load_delegate('libedgetpu.so.1.0')])\nexcept Exception as e:\n    print(f\"Failed to load delegate: {e}\")\n    try:\n        interpreter = Interpreter(model_path=model_path,\n                                  experimental_delegates=[load_delegate('/usr/lib/aarch64-linux-gnu/libedgetpu.so.1.0')])\n    except Exception as e:\n        raise RuntimeError(\"Could not find EdgeTPU library\") from e\n\ninterpreter.allocate_tensors()\n\n# Get model details\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\ninput_shape = input_details[0]['shape']\ninput_dtype = input_details[0]['dtype']\n\n# Phase 2: Input Acquisition & Preprocessing Loop\ncap = cv2.VideoCapture(input_path)\nif not cap.isOpened():\n    raise IOError(\"Could not open video file\")\n\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\nout = cv2.VideoWriter(output_path, fourcc, 30.0, (int(cap.get(3)), int(cap.get(4))))\n\nwhile True:\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    # Preprocess data\n    input_data = cv2.resize(frame, (input_shape[1], input_shape[2]))\n    input_data = input_data.reshape((1,) + input_data.shape)\n    input_data = input_data.astype(input_dtype)\n\n    floating_model = (input_details[0]['dtype'] == np.float32)\n    if floating_model:\n        input_data = (np.float32(input_data) - 127.5) / 127.5\n\n    # Phase 3: Inference\n    interpreter.set_tensor(input_details[0]['index'], input_data)\n    interpreter.invoke()\n\n    # Phase 4: Output Interpretation & Handling Loop\n    boxes = None\n    classes = None\n    scores = None\n\n    for detail in output_details:\n        if 'detection_boxes' in detail['name']:\n            boxes = interpreter.get_tensor(detail['index'])\n        elif 'detection_classes' in detail['name']:\n            classes = interpreter.get_tensor(detail['index'])\n        elif 'detection_scores' in detail['name']:\n            scores = interpreter.get_tensor(detail['index'])\n\n    if boxes is not None and classes is not None and scores is not None:\n        for i in range(len(scores[0])):\n            if scores[0][i] > confidence_threshold:\n                y1, x1, y2, x2 = boxes[0][i]\n                label = labels[int(classes[0][i]) - 1]  # Assuming classes are 1-indexed\n                cv2.rectangle(frame, (int(x1 * frame.shape[1]), int(y1 * frame.shape[0])),\n                              (int(x2 * frame.shape[1]), int(y2 * frame.shape[0])), (0, 255, 0), 2)\n                cv2.putText(frame, f\"{label}: {scores[0][i]:.2f}\", \n                            (int(x1 * frame.shape[1]), int(y1 * frame.shape[0]) - 10),\n                            cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n\n    out.write(frame)\n\ncap.release()\nout.release()\ncv2.destroyAllWindows()\n\n# Phase 5: Cleanup\ndel interpreter"
      },
      "session_id": "qwen2.5-coder:14b_85a9_tpusg_batch",
      "metadata": {
        "num_run": 29
      },
      "tags": [
        "benchmark",
        "qwen2.5-coder:14b",
        "tpu_sketch_generator"
      ],
      "latency": 55.59,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-18-44-37-156139_chatcmpl-1ef5e061-01a1-432f-8080-f276a900da74",
          "traceId": "1c2479b5",
          "type": "GENERATION",
          "name": "1c24_tpusg_gen_attempt#1",
          "startTime": "2025-07-30T15:44:37.156000+00:00",
          "endTime": "2025-07-30T15:44:57.272000+00:00",
          "model": "qwen2.5-coder:14b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 20116.0,
          "costDetails": {},
          "environment": "default",
          "totalTokens": 2633,
          "promptTokens": 1833,
          "completionTokens": 800,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-18-45-05-484561_chatcmpl-78837052-3ce5-46a9-89aa-2334e4f3b763",
          "traceId": "1c2479b5",
          "type": "GENERATION",
          "name": "1c24_tpusg_gen_attempt#2",
          "startTime": "2025-07-30T15:45:05.484000+00:00",
          "endTime": "2025-07-30T15:45:32.746000+00:00",
          "model": "qwen2.5-coder:14b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 27262.0,
          "costDetails": {},
          "environment": "default",
          "totalTokens": 3174,
          "promptTokens": 2048,
          "completionTokens": 1126,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "environment": "default",
      "createdAt": "2025-07-30T15:44:37.678Z",
      "updatedAt": "2025-07-30T15:46:29.552Z"
    },
    {
      "id": "d27e6a9e",
      "timestamp": "2025-07-30T15:43:19.617000+00:00",
      "name": "d27e_tpu_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import numpy as np\nfrom tflite_runtime.interpreter import Interpreter, load_delegate\n\n# Phase 1: Setup\n\n# 1.1. Imports\ntry:\n    interpreter = Interpreter(model_path='/home/mendel/tinyml_autopilot/models/edgetpu_detect.tflite', \n                            experimental_delegates=[load_delegate('libedgetpu.so.1.0')])\nexcept Exception as e:\n    try:\n        interpreter = Interpreter(model_path='/home/mendel/tinyml_autopilot/models/edgetpu_detect.tflite', \n                                experimental_delegates=[load_delegate('/usr/lib/aarch64-linux-gnu/libedgetpu.so.1.0')])\n    except Exception as e:\n        print(f\"Failed to load EdgeTPU delegate: {e}\")\n        exit(1)\n\n# 1.2. Paths/Parameters\nmodel_path  = \"/home/mendel/tinyml_autopilot/models/edgetpu_detect.tflite\"\nlabel_path  = \"/home/mendel/tinyml_autopilot/models/labelmap.txt\"\ninput_path  = \"/home/mendel/tinyml_autopilot/data//sheeps.mp4\"\noutput_path  = \"/home/mendel/tinyml_autopilot/results/sheeps_detections.mp4\"\n\n# 1.3. Load Labels (Conditional)\nwith open(label_path, 'r') as f:\n    labels = [line.strip() for line in f.readlines()]\n\n# 1.4. Load Interpreter with EdgeTPU\ninterpreter.allocate_tensors()\n\n# 1.5. Get Model Details\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\nfloating_model = (input_details[0]['dtype'] == np.float32)\nheight = input_details[0]['shape'][1]\nwidth = input_details[0]['shape'][2]\n\n# Phase 2: Input Acquisition & Preprocessing Loop\nimport cv2\n\ncap = cv2.VideoCapture(input_path)\n\nif not cap.isOpened():\n    print(\"Error opening video file\")\n    exit(1)\n\nfourcc = cv2.VideoWriter_fourcc(*'XVID')\nout = cv2.VideoWriter(output_path, fourcc, 20.0, (width, height))\n\nwhile cap.isOpened():\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    # Resize frame to match input size\n    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n    frame_resized = cv2.resize(frame_rgb, (width, height))\n    input_data = np.expand_dims(frame_resized, axis=0)\n\n    # Quantization Handling\n    if floating_model:\n        input_data = (np.float32(input_data) - 127.5) / 127.5\n\n    # Phase 3: Inference\n    interpreter.set_tensor(input_details[0]['index'], input_data)\n    interpreter.invoke()\n\n    # Phase 4: Output Interpretation & Handling Loop\n    # 4.1. Get Output Tensor(s)\n    boxes = interpreter.get_tensor(output_details[0]['index'])[0]\n    classes = interpreter.get_tensor(output_details[1]['index'])[0]\n    scores = interpreter.get_tensor(output_details[2]['index'])[0]\n\n    # 4.2. Interpret Results\n    detection_count = int(interpreter.get_tensor(output_details[3]['index'])[0])\n    for i in range(detection_count):\n        if scores[i] > 0.5:  # Confidence thresholding\n            class_name = labels[int(classes[i])]\n            box = boxes[i]\n            ymin, xmin, ymax, xmax = box\n\n            # Scale coordinates to frame size\n            xmin *= width\n            xmax *= width\n            ymin *= height\n            ymax *= height\n\n            # Draw bounding box and label\n            cv2.rectangle(frame, (int(xmin), int(ymin)), (int(xmax), int(ymax)), (0, 255, 0), 2)\n            cv2.putText(frame, f\"{class_name}: {scores[i]:.2f}\", (int(xmin), int(ymin) - 10),\n                        cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n\n    # Write the frame with bounding boxes to output video\n    out.write(frame)\n\n# Phase 5: Cleanup\ncap.release()\nout.release()\ncv2.destroyAllWindows()"
      },
      "session_id": "qwen2.5-coder:14b_85a9_tpusg_batch",
      "metadata": {
        "num_run": 28
      },
      "tags": [
        "benchmark",
        "qwen2.5-coder:14b",
        "tpu_sketch_generator"
      ],
      "latency": 22.962,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-18-43-19-619447_chatcmpl-4cba1fe1-ee8b-42c8-9bb0-3b266e7cda7b",
          "traceId": "d27e6a9e",
          "type": "GENERATION",
          "name": "d27e_tpusg_gen_attempt#1",
          "startTime": "2025-07-30T15:43:19.619000+00:00",
          "endTime": "2025-07-30T15:43:42.581000+00:00",
          "model": "qwen2.5-coder:14b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 22962.0,
          "costDetails": {},
          "environment": "default",
          "totalTokens": 2749,
          "promptTokens": 1833,
          "completionTokens": 916,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "environment": "default",
      "createdAt": "2025-07-30T15:43:20.138Z",
      "updatedAt": "2025-07-30T15:44:36.253Z"
    },
    {
      "id": "4c6ae20f",
      "timestamp": "2025-07-30T15:41:01.053000+00:00",
      "name": "4c6a_tpu_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import numpy as np\nfrom tflite_runtime.interpreter import Interpreter, load_delegate\nimport cv2\n\n# Phase 1: Setup\n# 1.1. Imports\nmodel_path   = \"/home/mendel/tinyml_autopilot/models/edgetpu_detect.tflite\"\nlabel_path   = \"/home/mendel/tinyml_autopilot/models/labelmap.txt\"\ninput_path   = \"/home/mendel/tinyml_autopilot/data//sheeps.mp4\"\noutput_path   = \"/home/mendel/tinyml_autopilot/results/sheeps_detections.mp4\"\nconfidence_threshold   = 0.5\n\ntry:\n    interpreter = Interpreter(model_path=model_path,\n                                experimental_delegates=[load_delegate('libedgetpu.so.1.0')])\nexcept Exception as e:\n    print(f\"Failed to load Edge TPU delegate: {e}\")\n    try:\n        interpreter = Interpreter(model_path=model_path,\n                                    experimental_delegates=[load_delegate('/usr/lib/aarch64-linux-gnu/libedgetpu.so.1.0')])\n    except Exception as e:\n        print(f\"Failed to load alternative Edge TPU delegate: {e}\")\n        exit()\n\ninterpreter.allocate_tensors()\n\n# 1.3. Load Labels\nwith open(label_path, 'r') as f:\n    labels = [line.strip() for line in f.readlines()]\n\n# 1.5. Get Model Details\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\nfloating_model = (input_details[0]['dtype'] == np.float32)\n\n# Phase 2: Input Acquisition & Preprocessing Loop\ncap = cv2.VideoCapture(input_path)\nif not cap.isOpened():\n    print(\"Error opening video file\")\n    exit()\n\nframe_width = int(cap.get(3))\nframe_height = int(cap.get(4))\n\nfourcc = cv2.VideoWriter_fourcc(*'XVID')\nout = cv2.VideoWriter(output_path, fourcc, 20.0, (frame_width, frame_height))\n\nwhile cap.isOpened():\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    # Preprocess Data\n    input_data = cv2.resize(frame, (input_details[0]['shape'][1], input_details[0]['shape'][2]))\n    input_data = np.expand_dims(input_data, axis=0)\n    \n    if floating_model:\n        input_data = (np.float32(input_data) - 127.5) / 127.5\n\n    # Phase 3: Inference\n    interpreter.set_tensor(input_details[0]['index'], input_data)\n    interpreter.invoke()\n\n    # Phase 4: Output Interpretation & Handling Loop\n    # Get Output Tensor(s)\n    boxes = interpreter.get_tensor(output_details[0]['index'])\n    classes = interpreter.get_tensor(output_details[1]['index'])\n    scores = interpreter.get_tensor(output_details[2]['index'])\n\n    # Ensure the tensors are 1D arrays\n    boxes = np.squeeze(boxes)\n    classes = np.squeeze(classes)\n    scores = np.squeeze(scores)\n\n    for i in range(len(scores)):\n        if scores[i] > confidence_threshold:\n            ymin, xmin, ymax, xmax = boxes[i]\n            label = labels[int(classes[i])]\n            cv2.rectangle(frame, (int(xmin * frame_width), int(ymin * frame_height)),\n                            (int(xmax * frame_width), int(ymax * frame_height)), (0, 255, 0), 2)\n            cv2.putText(frame, f'{label}: {scores[i]:.2f}', \n                        (int(xmin * frame_width), int(ymin * frame_height) - 10),\n                        cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n\n    out.write(frame)\n\ncap.release()\nout.release()\ncv2.destroyAllWindows()"
      },
      "session_id": "qwen2.5-coder:14b_85a9_tpusg_batch",
      "metadata": {
        "num_run": 27
      },
      "tags": [
        "benchmark",
        "qwen2.5-coder:14b",
        "tpu_sketch_generator"
      ],
      "latency": 79.69,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-18-41-01-055237_chatcmpl-9ad6e67a-f2ac-4f65-bafd-76fc6221eaee",
          "traceId": "4c6ae20f",
          "type": "GENERATION",
          "name": "4c6a_tpusg_gen_attempt#1",
          "startTime": "2025-07-30T15:41:01.055000+00:00",
          "endTime": "2025-07-30T15:41:20.485000+00:00",
          "model": "qwen2.5-coder:14b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 19430.0,
          "costDetails": {},
          "environment": "default",
          "totalTokens": 2597,
          "promptTokens": 1833,
          "completionTokens": 764,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-18-41-28-109778_chatcmpl-63db544b-705c-4d73-b4f4-9dcc06b80f5a",
          "traceId": "4c6ae20f",
          "type": "GENERATION",
          "name": "4c6a_tpusg_gen_attempt#2",
          "startTime": "2025-07-30T15:41:28.109000+00:00",
          "endTime": "2025-07-30T15:41:50.404000+00:00",
          "model": "qwen2.5-coder:14b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 22295.0,
          "costDetails": {},
          "environment": "default",
          "totalTokens": 2941,
          "promptTokens": 2048,
          "completionTokens": 893,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-18-41-58-710098_chatcmpl-166e5d70-a778-4c04-b997-d27e20ba157e",
          "traceId": "4c6ae20f",
          "type": "GENERATION",
          "name": "4c6a_tpusg_gen_attempt#3",
          "startTime": "2025-07-30T15:41:58.710000+00:00",
          "endTime": "2025-07-30T15:42:20.745000+00:00",
          "model": "qwen2.5-coder:14b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 22035.0,
          "costDetails": {},
          "environment": "default",
          "totalTokens": 2930,
          "promptTokens": 2048,
          "completionTokens": 882,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "environment": "default",
      "createdAt": "2025-07-30T15:41:01.575Z",
      "updatedAt": "2025-07-30T15:43:18.777Z"
    },
    {
      "id": "a3826e77",
      "timestamp": "2025-07-30T15:39:11.540000+00:00",
      "name": "a382_tpu_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import numpy as np\nimport time\nfrom tflite_runtime.interpreter import Interpreter, load_delegate\n\n# Configuration parameters\nmodel_path   = \"/home/mendel/tinyml_autopilot/models/edgetpu_detect.tflite\"\nlabel_path   = \"/home/mendel/tinyml_autopilot/models/labelmap.txt\"\ninput_path   = \"/home/mendel/tinyml_autopilot/data//sheeps.mp4\"\noutput_path   = \"/home/mendel/tinyml_autopilot/results/sheeps_detections.mp4\"\nconfidence_threshold   = 0.5\n\n# Load labels\nwith open(label_path, 'r') as f:\n    labels = [line.strip() for line in f.readlines()]\n\n# Load interpreter with EdgeTPU\ntry:\n    interpreter = Interpreter(model_path=model_path, experimental_delegates=[load_delegate('libedgetpu.so.1.0')])\nexcept Exception as e:\n    print(f\"Error loading EdgeTPU delegate: {e}\")\n    try:\n        interpreter = Interpreter(model_path=model_path, experimental_delegates=[load_delegate('/usr/lib/aarch64-linux-gnu/libedgetpu.so.1.0')])\n    except Exception as e:\n        raise RuntimeError(\"Failed to load EdgeTPU delegate\") from e\n\ninterpreter.allocate_tensors()\n\n# Get model details\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\nfloating_model = (input_details[0]['dtype'] == np.float32)\n\n# Open video file\nimport cv2\ncap = cv2.VideoCapture(input_path)\nif not cap.isOpened():\n    raise IOError(\"Could not open input video\")\n\n# Prepare video writer for output\nfps = int(cap.get(cv2.CAP_PROP_FPS))\nframe_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\nframe_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\nout = cv2.VideoWriter(output_path, cv2.VideoWriter_fourcc(*'XVID'), fps, (frame_width, frame_height))\n\nwhile True:\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    # Preprocess input\n    resized_frame = cv2.resize(frame, (input_details[0]['shape'][1], input_details[0]['shape'][2]))\n    input_data = np.expand_dims(resized_frame, axis=0)\n    if floating_model:\n        input_data = (np.float32(input_data) - 127.5) / 127.5\n\n    # Set input tensor\n    interpreter.set_tensor(input_details[0]['index'], input_data)\n\n    # Run inference\n    start_time = time.time()\n    interpreter.invoke()\n    end_time = time.time()\n    print(f\"Inference time: {end_time - start_time:.2f} seconds\")\n\n    # Get output tensors\n    boxes = interpreter.get_tensor(output_details[0]['index'])[0]\n    classes = interpreter.get_tensor(output_details[1]['index'])[0]  # Removed label_offset\n    scores = interpreter.get_tensor(output_details[2]['index'])[0]\n\n    # Post-process results\n    for i in range(len(scores)):\n        if scores[i] >= confidence_threshold:\n            class_name = labels[int(classes[i])]\n            box = boxes[i]\n            ymin, xmin, ymax, xmax = map(int, box)\n            cv2.rectangle(frame, (xmin, ymin), (xmax, ymax), (0, 255, 0), 2)\n            cv2.putText(frame, f\"{class_name}: {scores[i]:.2f}\", (xmin, ymin - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n\n    # Write frame to output video\n    out.write(frame)\n\n# Release resources\ncap.release()\nout.release()\ncv2.destroyAllWindows()"
      },
      "session_id": "qwen2.5-coder:14b_85a9_tpusg_batch",
      "metadata": {
        "num_run": 26
      },
      "tags": [
        "benchmark",
        "qwen2.5-coder:14b",
        "tpu_sketch_generator"
      ],
      "latency": 51.033,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-18-39-11-541985_chatcmpl-8bf6a755-24d1-4cce-81b5-fee518e48c73",
          "traceId": "a3826e77",
          "type": "GENERATION",
          "name": "a382_tpusg_gen_attempt#1",
          "startTime": "2025-07-30T15:39:11.541000+00:00",
          "endTime": "2025-07-30T15:39:31.882000+00:00",
          "model": "qwen2.5-coder:14b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 20341.0,
          "costDetails": {},
          "environment": "default",
          "totalTokens": 2627,
          "promptTokens": 1833,
          "completionTokens": 794,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-18-39-40-178007_chatcmpl-7aa4e3cc-168c-479f-b263-581595958251",
          "traceId": "a3826e77",
          "type": "GENERATION",
          "name": "a382_tpusg_gen_attempt#2",
          "startTime": "2025-07-30T15:39:40.178000+00:00",
          "endTime": "2025-07-30T15:40:02.574000+00:00",
          "model": "qwen2.5-coder:14b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 22396.0,
          "costDetails": {},
          "environment": "default",
          "totalTokens": 2942,
          "promptTokens": 2048,
          "completionTokens": 894,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "environment": "default",
      "createdAt": "2025-07-30T15:39:12.061Z",
      "updatedAt": "2025-07-30T15:41:00.293Z"
    },
    {
      "id": "6c96296d",
      "timestamp": "2025-07-30T15:37:52.487000+00:00",
      "name": "6c96_tpu_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import os\nimport time\nimport numpy as np\nfrom tflite_runtime.interpreter import Interpreter, load_delegate\n\n# Phase 1: Setup\nmodel_path  = \"/home/mendel/tinyml_autopilot/models/edgetpu_detect.tflite\"\nlabel_path  = \"/home/mendel/tinyml_autopilot/models/labelmap.txt\"\ninput_path  = \"/home/mendel/tinyml_autopilot/data//sheeps.mp4\"\noutput_path  = \"/home/mendel/tinyml_autopilot/results/sheeps_detections.mp4\"\nconfidence_threshold  = 0.5\n\n# Load labels\nwith open(label_path, 'r') as f:\n    labels = [line.strip() for line in f.readlines()]\n\n# Load Interpreter with EdgeTPU acceleration\ntry:\n    interpreter = Interpreter(model_path=model_path, \n                              experimental_delegates=[load_delegate('libedgetpu.so.1')])\nexcept Exception as e:\n    try:\n        interpreter = Interpreter(model_path=model_path, \n                                  experimental_delegates=[load_delegate('/usr/lib/aarch64-linux-gnu/libedgetpu.so.1')])\n    except Exception as e:\n        print(\"Failed to load EdgeTPU delegate:\", e)\n        exit()\n\ninterpreter.allocate_tensors()\n\n# Get Model Details\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\nfloating_model = (input_details[0]['dtype'] == np.float32)\n\n# Phase 2: Input Acquisition & Preprocessing Loop\nimport cv2\n\ncap = cv2.VideoCapture(input_path)\nif not cap.isOpened():\n    print(\"Error opening video file\")\n    exit()\n\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\nout = cv2.VideoWriter(output_path, fourcc, 30.0, (int(cap.get(3)), int(cap.get(4))))\n\nwhile cap.isOpened():\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    # Preprocess Data\n    input_data = cv2.resize(frame, (input_details[0]['shape'][1], input_details[0]['shape'][2]))\n    input_data = np.expand_dims(input_data, axis=0)\n    input_data = input_data[:, :, :, ::-1].astype(np.uint8)\n\n    if floating_model:\n        input_data = (np.float32(input_data) - 127.5) / 127.5\n\n    # Phase 3: Inference\n    interpreter.set_tensor(input_details[0]['index'], input_data)\n    interpreter.invoke()\n\n    # Phase 4: Output Interpretation & Handling Loop\n    output_data = []\n    for detail in output_details:\n        output_data.append(interpreter.get_tensor(detail['index']))\n\n    boxes, scores, classes, num_detections = output_data\n\n    h, w, _ = frame.shape\n\n    for i in range(int(num_detections[0])):\n        if scores[0][i] > confidence_threshold:\n            y1, x1, y2, x2 = map(int, boxes[0][i] * [h, w, h, w])\n            label = labels[int(classes[0][i])]\n            cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n            cv2.putText(frame, f'{label}: {scores[0][i]:.2f}', (x1, y1 - 10),\n                        cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n\n    # Handle Output\n    out.write(frame)\n\n# Phase 5: Cleanup\ncap.release()\nout.release()\ncv2.destroyAllWindows()"
      },
      "session_id": "qwen2.5-coder:14b_85a9_tpusg_batch",
      "metadata": {
        "num_run": 25
      },
      "tags": [
        "benchmark",
        "qwen2.5-coder:14b",
        "tpu_sketch_generator"
      ],
      "latency": 19.646,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-18-37-52-489176_chatcmpl-595456d8-d3f2-45cb-8b63-a90a077eae64",
          "traceId": "6c96296d",
          "type": "GENERATION",
          "name": "6c96_tpusg_gen_attempt#1",
          "startTime": "2025-07-30T15:37:52.489000+00:00",
          "endTime": "2025-07-30T15:38:12.135000+00:00",
          "model": "qwen2.5-coder:14b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 19646.0,
          "costDetails": {},
          "environment": "default",
          "totalTokens": 2612,
          "promptTokens": 1833,
          "completionTokens": 779,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "environment": "default",
      "createdAt": "2025-07-30T15:37:53.008Z",
      "updatedAt": "2025-07-30T15:39:10.705Z"
    },
    {
      "id": "ec769b33",
      "timestamp": "2025-07-30T15:36:04.985000+00:00",
      "name": "ec76_tpu_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import os\nimport numpy as np\nfrom tflite_runtime.interpreter import Interpreter, load_delegate\n\n# Configuration Parameters\nmodel_path   = \"/home/mendel/tinyml_autopilot/models/edgetpu_detect.tflite\"\nlabel_path   = \"/home/mendel/tinyml_autopilot/models/labelmap.txt\"\ninput_path   = \"/home/mendel/tinyml_autopilot/data//sheeps.mp4\"\noutput_path   = \"/home/mendel/tinyml_autopilot/results/sheeps_detections.mp4\"\nconfidence_threshold   = 0.5\n\n# Phase 1: Setup\ntry:\n    interpreter = Interpreter(model_path=model_path, \n                                experimental_delegates=[load_delegate('libedgetpu.so.1.0')])\nexcept Exception as e:\n    print(f\"Failed to load EdgeTPU delegate with libedgetpu.so.1.0: {e}\")\n    try:\n        interpreter = Interpreter(model_path=model_path, \n                                  experimental_delegates=[load_delegate('/usr/lib/aarch64-linux-gnu/libedgetpu.so.1.0')])\n    except Exception as e:\n        print(f\"Failed to load EdgeTPU delegate with /usr/lib/aarch64-linux-gnu/libedgetpu.so.1.0: {e}\")\n        raise\n\ninterpreter.allocate_tensors()\n\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\n# Load Labels\nwith open(label_path, 'r') as f:\n    labels = [line.strip() for line in f.readlines()]\n\n# Phase 2: Input Acquisition & Preprocessing Loop\nimport cv2\n\ncap = cv2.VideoCapture(input_path)\nfourcc = cv2.VideoWriter_fourcc(*'XVID')\nout = cv2.VideoWriter(output_path, fourcc, 20.0, (int(cap.get(3)), int(cap.get(4))))\n\nwhile cap.isOpened():\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    # Preprocess Data\n    input_data = np.expand_dims(frame, axis=0)\n    input_data = cv2.resize(input_data[0], (input_details[0]['shape'][1], input_details[0]['shape'][2]))\n    input_data = input_data.reshape((1, *input_details[0]['shape'][1:]))\n    \n    # Convert to the correct data type expected by the model\n    if input_details[0]['dtype'] == np.float32:\n        input_data = (np.float32(input_data) - 127.5) / 127.5\n    elif input_details[0]['dtype'] == np.uint8:\n        input_data = np.uint8(input_data)\n\n    # Phase 3: Inference\n    interpreter.set_tensor(input_details[0]['index'], input_data)\n    interpreter.invoke()\n\n    # Phase 4: Output Interpretation & Handling Loop\n    output_boxes = interpreter.get_tensor(output_details[0]['index'])[0]\n    output_classes = interpreter.get_tensor(output_details[1]['index'])[0]\n    output_scores = interpreter.get_tensor(output_details[2]['index'])[0]\n\n    for i in range(len(output_boxes)):\n        if output_scores[i] > confidence_threshold:\n            ymin, xmin, ymax, xmax = map(int, output_boxes[i])\n            class_id = int(output_classes[i])\n            label = labels[class_id]\n            cv2.rectangle(frame, (xmin, ymin), (xmax, ymax), (0, 255, 0), 2)\n            cv2.putText(frame, f'{label}: {output_scores[i]:.2f}', (xmin, ymin - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n\n    out.write(frame)\n\n# Phase 5: Cleanup\ncap.release()\nout.release()\ncv2.destroyAllWindows()"
      },
      "session_id": "qwen2.5-coder:14b_85a9_tpusg_batch",
      "metadata": {
        "num_run": 24
      },
      "tags": [
        "benchmark",
        "qwen2.5-coder:14b",
        "tpu_sketch_generator"
      ],
      "latency": 50.145,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-18-36-04-987520_chatcmpl-7aa81455-77d7-4d42-8210-a3ad3b53a6ed",
          "traceId": "ec769b33",
          "type": "GENERATION",
          "name": "ec76_tpusg_gen_attempt#1",
          "startTime": "2025-07-30T15:36:04.987000+00:00",
          "endTime": "2025-07-30T15:36:24.843000+00:00",
          "model": "qwen2.5-coder:14b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 19856.0,
          "costDetails": {},
          "environment": "default",
          "totalTokens": 2619,
          "promptTokens": 1833,
          "completionTokens": 786,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-18-36-32-884449_chatcmpl-cd23986f-9d6a-4634-b840-1099956b3b08",
          "traceId": "ec769b33",
          "type": "GENERATION",
          "name": "ec76_tpusg_gen_attempt#2",
          "startTime": "2025-07-30T15:36:32.884000+00:00",
          "endTime": "2025-07-30T15:36:55.132000+00:00",
          "model": "qwen2.5-coder:14b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 22248.0,
          "costDetails": {},
          "environment": "default",
          "totalTokens": 2940,
          "promptTokens": 2048,
          "completionTokens": 892,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "environment": "default",
      "createdAt": "2025-07-30T15:36:05.506Z",
      "updatedAt": "2025-07-30T15:37:52.134Z"
    },
    {
      "id": "190153e1",
      "timestamp": "2025-07-30T15:34:14.378000+00:00",
      "name": "1901_tpu_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import os\nimport numpy as np\nimport cv2\nfrom tflite_runtime.interpreter import Interpreter, load_delegate\n\n# Define paths and parameters\nmodel_path   = \"/home/mendel/tinyml_autopilot/models/edgetpu_detect.tflite\"\nlabel_path   = \"/home/mendel/tinyml_autopilot/models/labelmap.txt\"\ninput_path   = \"/home/mendel/tinyml_autopilot/data//sheeps.mp4\"\noutput_path   = \"/home/mendel/tinyml_autopilot/results/sheeps_detections.mp4\"\nconfidence_threshold   = 0.5\n\n# Load labels\nwith open(label_path, 'r') as f:\n    labels = [line.strip() for line in f.readlines()]\n\n# Load interpreter with EdgeTPU\ntry:\n    interpreter = Interpreter(model_path=model_path,\n                              experimental_delegates=[load_delegate('libedgetpu.so.1.0')])\nexcept Exception as e:\n    try:\n        interpreter = Interpreter(model_path=model_path,\n                                  experimental_delegates=[load_delegate('/usr/lib/aarch64-linux-gnu/libedgetpu.so.1.0')])\n    except Exception as e:\n        raise Exception(\"Failed to load EdgeTPU delegate\") from e\n\ninterpreter.allocate_tensors()\n\n# Get model details\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\n# Input shape and dtype\ninput_shape = input_details[0]['shape']\ninput_dtype = input_details[0]['dtype']\n\n# Load video\ncap = cv2.VideoCapture(input_path)\nif not cap.isOpened():\n    raise Exception(\"Error opening video file\")\n\n# Define the codec and create VideoWriter object\nfourcc = cv2.VideoWriter_fourcc(*'XVID')\nout = cv2.VideoWriter(output_path, fourcc, 20.0, (int(cap.get(3)), int(cap.get(4))))\n\nwhile cap.isOpened():\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    # Preprocess data\n    input_data = cv2.resize(frame, (input_shape[1], input_shape[2]))\n    input_data = input_data[:, :, ::-1]  # Convert BGR to RGB\n    input_data = np.expand_dims(input_data, axis=0)\n    \n    if input_dtype == np.float32:\n        input_data = np.float32(input_data) / 127.5 - 1.0\n    else:\n        input_data = np.uint8(input_data)\n\n    # Set input tensor and run inference\n    interpreter.set_tensor(input_details[0]['index'], input_data)\n    interpreter.invoke()\n\n    # Get output tensor(s)\n    boxes = interpreter.get_tensor(output_details[0]['index'])[0]\n    classes = interpreter.get_tensor(output_details[1]['index'])[0]\n    scores = interpreter.get_tensor(output_details[2]['index'])[0]\n\n    # Interpret results\n    for i in range(len(scores)):\n        if scores[i] > confidence_threshold:\n            label = labels[int(classes[i])]\n            box = boxes[i]\n            x_min, y_min, x_max, y_max = int(box[1] * frame.shape[1]), int(box[0] * frame.shape[0]), \\\n                                         int(box[3] * frame.shape[1]), int(box[2] * frame.shape[0])\n            cv2.rectangle(frame, (x_min, y_min), (x_max, y_max), (0, 255, 0), 2)\n            cv2.putText(frame, f'{label}: {scores[i]:.2f}', (x_min, y_min - 10),\n                        cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n\n    # Write the frame into the file 'output.avi'\n    out.write(frame)\n\n# Release everything when job is finished\ncap.release()\nout.release()\ncv2.destroyAllWindows()"
      },
      "session_id": "qwen2.5-coder:14b_85a9_tpusg_batch",
      "metadata": {
        "num_run": 23
      },
      "tags": [
        "benchmark",
        "qwen2.5-coder:14b",
        "tpu_sketch_generator"
      ],
      "latency": 52.33,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-18-34-14-380181_chatcmpl-633361cc-e7cd-49be-921c-8f88807b681d",
          "traceId": "190153e1",
          "type": "GENERATION",
          "name": "1901_tpusg_gen_attempt#1",
          "startTime": "2025-07-30T15:34:14.380000+00:00",
          "endTime": "2025-07-30T15:34:34.737000+00:00",
          "model": "qwen2.5-coder:14b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 20357.0,
          "costDetails": {},
          "environment": "default",
          "totalTokens": 2644,
          "promptTokens": 1833,
          "completionTokens": 811,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-18-34-42-821264_chatcmpl-0521a7eb-7c99-4bca-956a-b19db40ae602",
          "traceId": "190153e1",
          "type": "GENERATION",
          "name": "1901_tpusg_gen_attempt#2",
          "startTime": "2025-07-30T15:34:42.821000+00:00",
          "endTime": "2025-07-30T15:35:06.710000+00:00",
          "model": "qwen2.5-coder:14b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 23889.0,
          "costDetails": {},
          "environment": "default",
          "totalTokens": 3010,
          "promptTokens": 2048,
          "completionTokens": 962,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "environment": "default",
      "createdAt": "2025-07-30T15:34:14.899Z",
      "updatedAt": "2025-07-30T15:36:04.655Z"
    },
    {
      "id": "513d6159",
      "timestamp": "2025-07-30T15:31:39.837000+00:00",
      "name": "513d_tpu_sketch_generator",
      "input": null,
      "output": {
        "status": "failed",
        "last_code": "import cv2\nimport numpy as np\nfrom tflite_runtime.interpreter import Interpreter, load_delegate\n\n# Phase 1: Paths/Parameters\nmodel_path  = \"/home/mendel/tinyml_autopilot/models/edgetpu_detect.tflite\"\ninput_path  = \"/home/mendel/tinyml_autopilot/data//sheeps.mp4\"\nlabel_path  = \"/home/mendel/tinyml_autopilot/models/labelmap.txt\"\noutput_path  = \"/home/mendel/tinyml_autopilot/results/sheeps_detections.mp4\"\n\n# Load Labels (Conditional)\nlabels = None\nif label_path:\n    with open(label_path, 'r') as f:\n        labels = [line.strip() for line in f.readlines()]\n\n# Load Interpreter with EdgeTPU\ntry:\n    interpreter = Interpreter(model_path=model_path, \n                            experimental_delegates=[load_delegate('libedgetpu.so.1.0')])\nexcept:\n    interpreter = Interpreter(model_path=model_path, \n                            experimental_delegates=[load_delegate('/usr/lib/aarch64-linux-gnu/libedgetpu.so.1.0')])\n\ninterpreter.allocate_tensors()\n\n# Get Model Details\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\n# Phase 2: Input Acquisition & Preprocessing Loop\ncap = cv2.VideoCapture(input_path)\nfourcc = cv2.VideoWriter_fourcc(*'XVID')\nout = cv2.VideoWriter(output_path, fourcc, 20.0, (640, 480))\n\nwhile cap.isOpened():\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    # Preprocess data\n    input_data = cv2.resize(frame, (input_details[0]['shape'][1], input_details[0]['shape'][2]))\n    input_data = np.expand_dims(input_data, axis=0)\n    \n    floating_model = (input_details[0]['dtype'] == np.float32)\n    if floating_model:\n        input_data = (np.float32(input_data) - 127.5) / 127.5\n\n    # Phase 3: Inference\n    interpreter.set_tensor(input_details[0]['index'], input_data)\n    interpreter.invoke()\n\n    # Phase 4: Output Interpretation & Handling Loop\n    output_data = []\n    for output_detail in output_details:\n        output_data.append(interpreter.get_tensor(output_detail['index']))\n\n    # Assuming the model outputs detection results (boxes, classes, scores)\n    boxes = output_data[0]\n    classes = output_data[1]\n    scores = output_data[2]\n\n    # Interpret Results\n    num_detections = int(boxes.shape[0])\n    for i in range(num_detections):\n        if scores[i] > 0.5:  # Confidence threshold\n            class_name = labels[int(classes[i])] if labels else str(int(classes[i]))\n            box = boxes[i]\n            ymin, xmin, ymax, xmax = map(int, box * [480, 640, 480, 640])  # Scale to image dimensions\n            cv2.rectangle(frame, (xmin, ymin), (xmax, ymax), (0, 255, 0), 2)\n            cv2.putText(frame, f'{class_name}: {scores[i]:.2f}', (xmin, ymin - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n\n    # Handle Output\n    out.write(frame)\n\n# Phase 5: Cleanup\ncap.release()\nout.release()\ncv2.destroyAllWindows()",
        "last_error": "OpenCV: FFMPEG: tag 0x44495658/'XVID' is not supported with codec id 12 and format 'mp4 / MP4 (MPEG-4 Part 14)'\nOpenCV: FFMPEG: fallback to use tag 0x7634706d/'mp4v'\nTraceback (most recent call last):\n  File \"script_13ea7ccf_1753889639.py\", line 66, in <module>\n    if scores[i] > 0.5:  # Confidence threshold\nValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()"
      },
      "session_id": "qwen2.5-coder:14b_85a9_tpusg_batch",
      "metadata": {
        "num_run": 22
      },
      "tags": [
        "benchmark",
        "qwen2.5-coder:14b",
        "tpu_sketch_generator"
      ],
      "latency": 148.233,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-18-31-39-840040_chatcmpl-068ea04c-7c02-4470-9370-2a975eae5c10",
          "traceId": "513d6159",
          "type": "GENERATION",
          "name": "513d_tpusg_gen_attempt#1",
          "startTime": "2025-07-30T15:31:39.840000+00:00",
          "endTime": "2025-07-30T15:31:59.699000+00:00",
          "model": "qwen2.5-coder:14b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 19859.0,
          "costDetails": {},
          "environment": "default",
          "totalTokens": 2620,
          "promptTokens": 1833,
          "completionTokens": 787,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-18-32-04-361572_chatcmpl-7ae7d916-91fc-4a3d-b6fc-c9e48e2c6947",
          "traceId": "513d6159",
          "type": "GENERATION",
          "name": "513d_tpusg_gen_attempt#2",
          "startTime": "2025-07-30T15:32:04.361000+00:00",
          "endTime": "2025-07-30T15:32:32.987000+00:00",
          "model": "qwen2.5-coder:14b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 28626.0,
          "costDetails": {},
          "environment": "default",
          "totalTokens": 3207,
          "promptTokens": 2048,
          "completionTokens": 1159,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-18-32-41-114288_chatcmpl-823404e2-1b23-4d0c-a6a4-81eee0ab29f2",
          "traceId": "513d6159",
          "type": "GENERATION",
          "name": "513d_tpusg_gen_attempt#3",
          "startTime": "2025-07-30T15:32:41.114000+00:00",
          "endTime": "2025-07-30T15:33:08.197000+00:00",
          "model": "qwen2.5-coder:14b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 27083.0,
          "costDetails": {},
          "environment": "default",
          "totalTokens": 3144,
          "promptTokens": 2048,
          "completionTokens": 1096,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-18-33-08-201671_chatcmpl-12305fa5-602e-49e8-a44f-267b5e9084dd",
          "traceId": "513d6159",
          "type": "GENERATION",
          "name": "513d_tpusg_gen_attempt#4",
          "startTime": "2025-07-30T15:33:08.201000+00:00",
          "endTime": "2025-07-30T15:33:30.056000+00:00",
          "model": "qwen2.5-coder:14b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 21855.0,
          "costDetails": {},
          "environment": "default",
          "totalTokens": 2917,
          "promptTokens": 2048,
          "completionTokens": 869,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-18-33-37-812541_chatcmpl-70e346aa-4049-4545-b90f-0e64221ca289",
          "traceId": "513d6159",
          "type": "GENERATION",
          "name": "513d_tpusg_gen_attempt#5",
          "startTime": "2025-07-30T15:33:37.812000+00:00",
          "endTime": "2025-07-30T15:33:59.636000+00:00",
          "model": "qwen2.5-coder:14b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 21824.0,
          "costDetails": {},
          "environment": "default",
          "totalTokens": 2920,
          "promptTokens": 2048,
          "completionTokens": 872,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "f49198a1-d3b3-4e6b-a7e6-481a3c2d78c3",
          "traceId": "513d6159",
          "type": "SPAN",
          "name": "error_51_tpusg_failure_signal_tpu_sketch_generator",
          "startTime": "2025-07-30T15:34:08.073000+00:00",
          "level": "ERROR",
          "statusMessage": "Failed. Last error: OpenCV: FFMPEG: tag 0x44495658/'XVID' is not supported with codec id 12 and format 'mp4 / MP4 (MPEG-4 Part 14)'\nOpenCV: FFMPEG: fallback to use tag 0x7634706d/'mp4v'\nTraceback (most recent call last):\n  File \"script_13ea7ccf_1753889639.py\", line 66, in <module>\n    if scores[i] > 0.5:  # Confidence threshold\nValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()",
          "calculatedTotalCost": 0.0,
          "costDetails": {},
          "environment": "default",
          "totalTokens": 0,
          "promptTokens": 0,
          "completionTokens": 0,
          "endTime": null,
          "model": null,
          "modelParameters": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null,
          "latency": null
        }
      ],
      "environment": "default",
      "createdAt": "2025-07-30T15:31:40.359Z",
      "updatedAt": "2025-07-30T15:34:13.569Z"
    },
    {
      "id": "a688ebca",
      "timestamp": "2025-07-30T15:30:24.858000+00:00",
      "name": "a688_tpu_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import os\nimport numpy as np\nfrom tflite_runtime.interpreter import Interpreter, load_delegate\n\n# Phase 1: Setup\nmodel_path  = \"/home/mendel/tinyml_autopilot/models/edgetpu_detect.tflite\"\nlabel_path  = \"/home/mendel/tinyml_autopilot/models/labelmap.txt\"\ninput_path  = \"/home/mendel/tinyml_autopilot/data//sheeps.mp4\"\noutput_path  = \"/home/mendel/tinyml_autopilot/results/sheeps_detections.mp4\"\nconfidence_threshold  = 0.5\n\n# Load labels\nwith open(label_path, 'r') as f:\n    labels = [line.strip() for line in f.readlines()]\n\n# Load Interpreter with EdgeTPU\ntry:\n    interpreter = Interpreter(model_path=model_path,\n                              experimental_delegates=[load_delegate('libedgetpu.so.1.0')])\nexcept Exception as e:\n    try:\n        interpreter = Interpreter(model_path=model_path,\n                                  experimental_delegates=[load_delegate('/usr/lib/aarch64-linux-gnu/libedgetpu.so.1.0')])\n    except Exception as e:\n        raise RuntimeError(\"Failed to load EdgeTPU delegate\") from e\n\ninterpreter.allocate_tensors()\n\n# Get Model Details\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\ninput_shape = input_details[0]['shape']\ninput_dtype = input_details[0]['dtype']\n\n# Phase 2: Input Acquisition & Preprocessing Loop\nimport cv2\n\ncap = cv2.VideoCapture(input_path)\nif not cap.isOpened():\n    raise IOError(\"Cannot open video\")\n\nwhile True:\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    # Resize and preprocess frame\n    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n    frame_resized = cv2.resize(frame_rgb, (input_shape[1], input_shape[2]))\n    input_data = np.expand_dims(frame_resized, axis=0)\n\n    # Quantization Handling\n    if input_dtype == np.float32:\n        input_data = (np.float32(input_data) - 127.5) / 127.5\n\n    # Phase 3: Inference\n    interpreter.set_tensor(input_details[0]['index'], input_data)\n    interpreter.invoke()\n\n    # Phase 4: Output Interpretation & Handling Loop\n    boxes = interpreter.get_tensor(output_details[0]['index'])[0]  # Bounding box coordinates\n    classes = interpreter.get_tensor(output_details[1]['index'])[0]  # Class IDs\n    scores = interpreter.get_tensor(output_details[2]['index'])[0]  # Confidence scores\n\n    for i in range(len(scores)):\n        if scores[i] > confidence_threshold:\n            class_name = labels[int(classes[i])]\n            box = boxes[i]\n            y1, x1, y2, x2 = box\n            cv2.rectangle(frame, (int(x1 * frame.shape[1]), int(y1 * frame.shape[0])),\n                           (int(x2 * frame.shape[1]), int(y2 * frame.shape[0])), (0, 255, 0), 2)\n            cv2.putText(frame, f\"{class_name}: {scores[i]:.2f}\", (int(x1 * frame.shape[1]), int(y1 * frame.shape[0]) - 10),\n                        cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n\n    # Handle Output\n    cv2.imshow('Object Detection', frame)\n    if cv2.waitKey(1) & 0xFF == ord('q'):\n        break\n\n# Phase 5: Cleanup\ncap.release()\ncv2.destroyAllWindows()"
      },
      "session_id": "qwen2.5-coder:14b_85a9_tpusg_batch",
      "metadata": {
        "num_run": 21
      },
      "tags": [
        "benchmark",
        "qwen2.5-coder:14b",
        "tpu_sketch_generator"
      ],
      "latency": 20.004,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-18-30-24-860481_chatcmpl-978738ba-b911-4587-9a80-e3859cef0e13",
          "traceId": "a688ebca",
          "type": "GENERATION",
          "name": "a688_tpusg_gen_attempt#1",
          "startTime": "2025-07-30T15:30:24.860000+00:00",
          "endTime": "2025-07-30T15:30:44.864000+00:00",
          "model": "qwen2.5-coder:14b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 20004.0,
          "costDetails": {},
          "environment": "default",
          "totalTokens": 2618,
          "promptTokens": 1833,
          "completionTokens": 785,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "environment": "default",
      "createdAt": "2025-07-30T15:30:25.378Z",
      "updatedAt": "2025-07-30T15:31:39.123Z"
    },
    {
      "id": "2a07c914",
      "timestamp": "2025-07-30T15:29:09.284000+00:00",
      "name": "2a07_tpu_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import os\nfrom tflite_runtime.interpreter import Interpreter, load_delegate\nimport numpy as np\nimport cv2\n\n# Configuration parameters\nmodel_path  = \"/home/mendel/tinyml_autopilot/models/edgetpu_detect.tflite\"\nlabel_path  = \"/home/mendel/tinyml_autopilot/models/labelmap.txt\"\ninput_path  = \"/home/mendel/tinyml_autopilot/data//sheeps.mp4\"\noutput_path  = \"/home/mendel/tinyml_autopilot/results/sheeps_detections.mp4\"\nconfidence_threshold  = 0.5\n\n# Load labels\nwith open(label_path, 'r') as f:\n    labels = [line.strip() for line in f.readlines()]\n\n# Load interpreter with EdgeTPU acceleration\ntry:\n    interpreter = Interpreter(model_path=model_path, \n                              experimental_delegates=[load_delegate('libedgetpu.so.1.0')])\nexcept Exception as e:\n    try:\n        interpreter = Interpreter(model_path=model_path, \n                                  experimental_delegates=[load_delegate('/usr/lib/aarch64-linux-gnu/libedgetpu.so.1.0')])\n    except Exception as e:\n        print(f\"Failed to load EdgeTPU delegate: {e}\")\n        exit(1)\n\ninterpreter.allocate_tensors()\n\n# Get model details\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\nheight, width = input_details[0]['shape'][1], input_details[0]['shape'][2]\nfloating_model = (input_details[0]['dtype'] == np.float32)\ninput_mean = 127.5\ninput_std = 127.5\n\n# Open video file\ncap = cv2.VideoCapture(input_path)\nif not cap.isOpened():\n    print(f\"Error: Could not open video file {input_path}\")\n    exit(1)\n\n# Define the codec and create VideoWriter object\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\nout = cv2.VideoWriter(output_path, fourcc, 30.0, (width, height))\n\nwhile cap.isOpened():\n    ret, frame = cap.read()\n    if not ret:\n        print(\"Can't receive frame (stream end?). Exiting ...\")\n        break\n\n    # Preprocess the frame\n    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n    frame_resized = cv2.resize(frame_rgb, (width, height))\n    input_data = np.expand_dims(frame_resized, axis=0)\n\n    if floating_model:\n        input_data = (np.float32(input_data) - input_mean) / input_std\n\n    # Set the tensor\n    interpreter.set_tensor(input_details[0]['index'], input_data)\n\n    # Run inference\n    interpreter.invoke()\n\n    # Get results\n    boxes = interpreter.get_tensor(output_details[0]['index'])[0]  # Bounding box coordinates\n    classes = interpreter.get_tensor(output_details[1]['index'])[0]  # Class indices\n    scores = interpreter.get_tensor(output_details[2]['index'])[0]   # Confidence scores\n\n    # Post-process the results\n    for i in range(len(scores)):\n        if scores[i] > confidence_threshold:\n            label = labels[int(classes[i])]\n            ymin, xmin, ymax, xmax = boxes[i]\n            xmin = int(xmin * width)\n            xmax = int(xmax * width)\n            ymin = int(ymin * height)\n            ymax = int(ymax * height)\n\n            # Draw the bounding box and label\n            cv2.rectangle(frame, (xmin, ymin), (xmax, ymax), (10, 255, 0), 4)\n            cv2.putText(frame, f'{label}: {scores[i]:.2f}', (xmin, ymin - 10),\n                        cv2.FONT_HERSHEY_SIMPLEX, 0.9, (10, 255, 0), 2)\n\n    # Write the frame to output\n    out.write(frame)\n\n# Release everything when done\ncap.release()\nout.release()\ncv2.destroyAllWindows()"
      },
      "session_id": "qwen2.5-coder:14b_85a9_tpusg_batch",
      "metadata": {
        "num_run": 20
      },
      "tags": [
        "benchmark",
        "qwen2.5-coder:14b",
        "tpu_sketch_generator"
      ],
      "latency": 21.455,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-18-29-09-286195_chatcmpl-de5e7974-6ecc-40e8-aaa0-0f96d714e513",
          "traceId": "2a07c914",
          "type": "GENERATION",
          "name": "2a07_tpusg_gen_attempt#1",
          "startTime": "2025-07-30T15:29:09.286000+00:00",
          "endTime": "2025-07-30T15:29:30.741000+00:00",
          "model": "qwen2.5-coder:14b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 21455.0,
          "costDetails": {},
          "environment": "default",
          "totalTokens": 2679,
          "promptTokens": 1833,
          "completionTokens": 846,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "environment": "default",
      "createdAt": "2025-07-30T15:29:09.804Z",
      "updatedAt": "2025-07-30T15:30:24.571Z"
    },
    {
      "id": "e73de696",
      "timestamp": "2025-07-30T15:27:47.775000+00:00",
      "name": "e73d_tpu_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import os\nfrom tflite_runtime.interpreter import Interpreter, load_delegate\nimport numpy as np\nimport cv2\n\n# Configuration parameters\nmodel_path  = \"/home/mendel/tinyml_autopilot/models/edgetpu_detect.tflite\"\nlabel_path  = \"/home/mendel/tinyml_autopilot/models/labelmap.txt\"\ninput_path  = \"/home/mendel/tinyml_autopilot/data//sheeps.mp4\"\noutput_path  = \"/home/mendel/tinyml_autopilot/results/sheeps_detections.mp4\"\nconfidence_threshold  = 0.5\n\n# Load labels\nlabels = []\nwith open(label_path, 'r') as f:\n    for line in f:\n        labels.append(line.strip())\n\n# Load interpreter with EdgeTPU acceleration\ntry:\n    interpreter = Interpreter(model_path=model_path, experimental_delegates=[load_delegate('libedgetpu.so.1.0')])\nexcept Exception as e:\n    try:\n        interpreter = Interpreter(model_path=model_path, experimental_delegates=[load_delegate('/usr/lib/aarch64-linux-gnu/libedgetpu.so.1.0')])\n    except Exception as e:\n        print(f\"Failed to load EdgeTPU delegate: {e}\")\n        exit(1)\ninterpreter.allocate_tensors()\n\n# Get model details\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\n# Check if model uses floating point inputs\nfloating_model = (input_details[0]['dtype'] == np.float32)\n\n# Open video capture\ncap = cv2.VideoCapture(input_path)\nif not cap.isOpened():\n    print(\"Error opening video file\")\n    exit(1)\n\n# Define the codec and create VideoWriter object\nfourcc = cv2.VideoWriter_fourcc(*'XVID')\nout = cv2.VideoWriter(output_path, fourcc, 20.0, (int(cap.get(3)), int(cap.get(4))))\n\nwhile cap.isOpened():\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    # Preprocess data\n    input_data = cv2.resize(frame, (input_details[0]['shape'][1], input_details[0]['shape'][2]))\n    input_data = np.expand_dims(input_data, axis=0)\n    if floating_model:\n        input_data = (np.float32(input_data) - 127.5) / 127.5\n\n    # Set input tensor\n    interpreter.set_tensor(input_details[0]['index'], input_data)\n\n    # Run inference\n    interpreter.invoke()\n\n    # Get output tensor(s)\n    boxes = interpreter.get_tensor(output_details[0]['index'])[0]\n    classes = interpreter.get_tensor(output_details[1]['index'])[0]  # Class index\n    scores = interpreter.get_tensor(output_details[2]['index'])[0]   # Confidence of detected objects\n\n    # Interpret results and post-processing\n    for i in range(len(scores)):\n        if scores[i] > confidence_threshold:\n            ymin, xmin, ymax, xmax = boxes[i]\n            label = labels[int(classes[i])]\n\n            # Scale the bounding box coordinates back relative to the size of the original image.\n            height, width, _ = frame.shape\n            xmin *= width\n            xmax *= width\n            ymin *= height\n            ymax *= height\n\n            # Draw the bounding box and label on the image\n            cv2.rectangle(frame, (int(xmin), int(ymin)), (int(xmax), int(ymax)), (0, 255, 0), 4)\n            cv2.putText(frame, f'{label} {scores[i]:.2f}', (int(xmin), int(ymin) - 10),\n                        cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n\n    # Write the frame with detections\n    out.write(frame)\n\n# Release everything when done\ncap.release()\nout.release()\ncv2.destroyAllWindows()"
      },
      "session_id": "qwen2.5-coder:14b_85a9_tpusg_batch",
      "metadata": {
        "num_run": 19
      },
      "tags": [
        "benchmark",
        "qwen2.5-coder:14b",
        "tpu_sketch_generator"
      ],
      "latency": 23.057,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-18-27-47-777570_chatcmpl-ea6af001-d120-436e-a0f6-04cf51189c85",
          "traceId": "e73de696",
          "type": "GENERATION",
          "name": "e73d_tpusg_gen_attempt#1",
          "startTime": "2025-07-30T15:27:47.777000+00:00",
          "endTime": "2025-07-30T15:28:10.834000+00:00",
          "model": "qwen2.5-coder:14b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 23057.0,
          "costDetails": {},
          "environment": "default",
          "totalTokens": 2659,
          "promptTokens": 1833,
          "completionTokens": 826,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "environment": "default",
      "createdAt": "2025-07-30T15:27:48.295Z",
      "updatedAt": "2025-07-30T15:29:08.996Z"
    },
    {
      "id": "a3bd9f01",
      "timestamp": "2025-07-30T15:25:54.256000+00:00",
      "name": "a3bd_tpu_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import os\nimport time\nimport numpy as np\nfrom tflite_runtime.interpreter import Interpreter, load_delegate\n\n# Phase 1: Setup\n\n# Load labels from the label file\nlabel_path  = \"/home/mendel/tinyml_autopilot/models/labelmap.txt\"\nwith open(label_path, 'r') as f:\n    labels = [line.strip() for line in f.readlines()]\n\n# Model and video paths\nmodel_path  = \"/home/mendel/tinyml_autopilot/models/edgetpu_detect.tflite\"\ninput_path  = \"/home/mendel/tinyml_autopilot/data//sheeps.mp4\"\noutput_path  = \"/home/mendel/tinyml_autopilot/results/sheeps_detections.mp4\"\n\n# Load the model with EdgeTPU acceleration\ntry:\n    interpreter = Interpreter(model_path=model_path, \n                            experimental_delegates=[load_delegate('libedgetpu.so.1.0')])\nexcept Exception as e:\n    try:\n        interpreter = Interpreter(model_path=model_path, \n                                experimental_delegates=[load_delegate('/usr/lib/aarch64-linux-gnu/libedgetpu.so.1.0')])\n    except Exception as e:\n        raise RuntimeError(f\"Failed to load EdgeTPU delegate: {e}\")\n\ninterpreter.allocate_tensors()\n\n# Get input and output tensors\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\n# Phase 2: Input Acquisition & Preprocessing Loop\n\nimport cv2\n\n# Open the video file\ncap = cv2.VideoCapture(input_path)\nif not cap.isOpened():\n    raise IOError(f\"Could not open video {input_path}\")\n\nframe_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\nframe_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n\nfourcc = cv2.VideoWriter_fourcc(*'XVID')\nout = cv2.VideoWriter(output_path, fourcc, 30.0, (frame_width, frame_height))\n\n# Phase 3: Inference\n\nwhile cap.isOpened():\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    # Preprocess the frame\n    input_data = cv2.resize(frame, (input_details[0]['shape'][1], input_details[0]['shape'][2]))\n    input_data = np.expand_dims(input_data, axis=0)\n\n    # Convert to UINT8 as expected by the model\n    if input_details[0]['dtype'] == np.uint8:\n        input_data = input_data.astype(np.uint8)\n    else:\n        raise ValueError(\"Unexpected input data type\")\n\n    # Set the tensor and run inference\n    interpreter.set_tensor(input_details[0]['index'], input_data)\n    interpreter.invoke()\n\n    # Phase 4: Output Interpretation & Handling Loop\n\n    # Get output tensors\n    boxes = interpreter.get_tensor(output_details[0]['index'])[0]\n    classes = interpreter.get_tensor(output_details[1]['index'])[0]\n    scores = interpreter.get_tensor(output_details[2]['index'])[0]\n\n    # Apply confidence threshold\n    conf_threshold = 0.5\n    for i in range(len(scores)):\n        if scores[i] > conf_threshold:\n            ymin, xmin, ymax, xmax = boxes[i]\n            label = labels[int(classes[i])]\n\n            # Scale coordinates to frame dimensions\n            xmin = int(xmin * frame_width)\n            xmax = int(xmax * frame_width)\n            ymin = int(ymin * frame_height)\n            ymax = int(ymax * frame_height)\n\n            # Draw bounding box and label on the frame\n            cv2.rectangle(frame, (xmin, ymin), (xmax, ymax), (0, 255, 0), 2)\n            cv2.putText(frame, f'{label} {int(scores[i]*100)}%', (xmin, ymin-10), \n                        cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n\n    # Write the frame to output video\n    out.write(frame)\n\n# Release resources\ncap.release()\nout.release()\ncv2.destroyAllWindows()\n\nprint(\"Processing complete.\")"
      },
      "session_id": "qwen2.5-coder:14b_85a9_tpusg_batch",
      "metadata": {
        "num_run": 18
      },
      "tags": [
        "benchmark",
        "qwen2.5-coder:14b",
        "tpu_sketch_generator"
      ],
      "latency": 54.828,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-18-25-54-257502_chatcmpl-2e4b199d-c04e-4a0c-8e09-6378aa90c547",
          "traceId": "a3bd9f01",
          "type": "GENERATION",
          "name": "a3bd_tpusg_gen_attempt#1",
          "startTime": "2025-07-30T15:25:54.257000+00:00",
          "endTime": "2025-07-30T15:26:15.697000+00:00",
          "model": "qwen2.5-coder:14b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 21440.0,
          "costDetails": {},
          "environment": "default",
          "totalTokens": 2688,
          "promptTokens": 1833,
          "completionTokens": 855,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-18-26-24-108938_chatcmpl-275fd583-d915-4f33-9d36-22845a2132b0",
          "traceId": "a3bd9f01",
          "type": "GENERATION",
          "name": "a3bd_tpusg_gen_attempt#2",
          "startTime": "2025-07-30T15:26:24.108000+00:00",
          "endTime": "2025-07-30T15:26:49.085000+00:00",
          "model": "qwen2.5-coder:14b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 24977.0,
          "costDetails": {},
          "environment": "default",
          "totalTokens": 3056,
          "promptTokens": 2048,
          "completionTokens": 1008,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "environment": "default",
      "createdAt": "2025-07-30T15:25:54.774Z",
      "updatedAt": "2025-07-30T15:27:46.905Z"
    },
    {
      "id": "b5f19ff0",
      "timestamp": "2025-07-30T15:23:18.734000+00:00",
      "name": "b5f1_tpu_sketch_generator",
      "input": null,
      "output": {
        "status": "failed",
        "last_code": "import numpy as np\nfrom tflite_runtime.interpreter import Interpreter, load_delegate\n\n# Configuration Parameters\nmodel_path      = \"/home/mendel/tinyml_autopilot/models/edgetpu_detect.tflite\"\nlabel_path      = \"/home/mendel/tinyml_autopilot/models/labelmap.txt\"\ninput_path      = \"/home/mendel/tinyml_autopilot/data//sheeps.mp4\"\noutput_path     = \"/home/mendel/tinyml_autopilot/results/sheeps_detections.mp4\"\nconfidence_threshold   = 0.5\n\n# Load Labels\nwith open(label_path, 'r') as f:\n    labels = [line.strip() for line in f.readlines()]\n\n# Load Interpreter with EdgeTPU\ntry:\n    interpreter = Interpreter(model_path=model_path, experimental_delegates=[load_delegate('libedgetpu.so.1')])\nexcept Exception as e:\n    try:\n        interpreter = Interpreter(model_path=model_path, experimental_delegates=[load_delegate('/usr/lib/aarch64-linux-gnu/libedgetpu.so.1')])\n    except Exception as e:\n        print(f\"Failed to load EdgeTPU delegate: {e}\")\n        exit()\n\ninterpreter.allocate_tensors()\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\n# Get Model Details\ninput_shape = input_details[0]['shape']\ninput_dtype = input_details[0]['dtype']\n\n# Open Video File\nimport cv2\ncap = cv2.VideoCapture(input_path)\nif not cap.isOpened():\n    print(\"Error: Could not open video.\")\n    exit()\n\nfourcc = cv2.VideoWriter_fourcc(*'XVID')\nout = cv2.VideoWriter(output_path, fourcc, 20.0, (int(cap.get(3)), int(cap.get(4))))\n\nwhile cap.isOpened():\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    # Preprocess Data\n    input_data = cv2.resize(frame, (input_shape[1], input_shape[2]))\n    input_data = np.expand_dims(input_data, axis=0)\n    input_data = input_data.astype(input_dtype)\n\n    # Quantization Handling\n    floating_model = (input_dtype == np.float32)\n    if floating_model:\n        input_data = (np.float32(input_data) - 127.5) / 127.5\n\n    # Set Input Tensor(s)\n    interpreter.set_tensor(input_details[0]['index'], input_data)\n\n    # Run Inference\n    interpreter.invoke()\n\n    # Get Output Tensor(s)\n    boxes = interpreter.get_tensor(output_details[0]['index'])\n    classes = interpreter.get_tensor(output_details[1]['index']).astype(np.int32)\n    scores = interpreter.get_tensor(output_details[2]['index'])\n\n    # Interpret Results and Post-processing\n    num_detections = int(interpreter.get_tensor(output_details[3]['index']))\n    for i in range(num_detections):\n        if scores[i] > confidence_threshold:\n            class_id = int(classes[i])\n            box = boxes[i]\n            label = labels[class_id - 1]  # Adjusting index since classes are 1-indexed\n            score = scores[i]\n\n            ymin, xmin, ymax, xmax = map(int, box * [frame.shape[0], frame.shape[1], frame.shape[0], frame.shape[1]])\n            cv2.rectangle(frame, (xmin, ymin), (xmax, ymax), (255, 0, 0), 2)\n            cv2.putText(frame, f'{label}: {score:.2f}', (xmin, ymin - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (255, 0, 0), 2)\n\n    # Handle Output\n    out.write(frame)\n\ncap.release()\nout.release()\ncv2.destroyAllWindows()",
        "last_error": "OpenCV: FFMPEG: tag 0x44495658/'XVID' is not supported with codec id 12 and format 'mp4 / MP4 (MPEG-4 Part 14)'\nOpenCV: FFMPEG: fallback to use tag 0x7634706d/'mp4v'\nTraceback (most recent call last):\n  File \"script_6b23d452_1753889139.py\", line 72, in <module>\n    if scores[i] > confidence_threshold:\nValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()"
      },
      "session_id": "qwen2.5-coder:14b_85a9_tpusg_batch",
      "metadata": {
        "num_run": 17
      },
      "tags": [
        "benchmark",
        "qwen2.5-coder:14b",
        "tpu_sketch_generator"
      ],
      "latency": 149.31,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-18-23-18-736314_chatcmpl-428f400d-27f7-4f94-8b8f-a0f3b9608fa1",
          "traceId": "b5f19ff0",
          "type": "GENERATION",
          "name": "b5f1_tpusg_gen_attempt#1",
          "startTime": "2025-07-30T15:23:18.736000+00:00",
          "endTime": "2025-07-30T15:23:37.437000+00:00",
          "model": "qwen2.5-coder:14b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 18701.0,
          "costDetails": {},
          "environment": "default",
          "totalTokens": 2571,
          "promptTokens": 1833,
          "completionTokens": 738,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-18-23-45-684283_chatcmpl-ed865c50-e98c-4b76-9bec-cf11b7149118",
          "traceId": "b5f19ff0",
          "type": "GENERATION",
          "name": "b5f1_tpusg_gen_attempt#2",
          "startTime": "2025-07-30T15:23:45.684000+00:00",
          "endTime": "2025-07-30T15:24:06.680000+00:00",
          "model": "qwen2.5-coder:14b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 20996.0,
          "costDetails": {},
          "environment": "default",
          "totalTokens": 2886,
          "promptTokens": 2048,
          "completionTokens": 838,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-18-24-15-033886_chatcmpl-bf923a7c-122e-45a6-9f14-b22ffc7fafb9",
          "traceId": "b5f19ff0",
          "type": "GENERATION",
          "name": "b5f1_tpusg_gen_attempt#3",
          "startTime": "2025-07-30T15:24:15.033000+00:00",
          "endTime": "2025-07-30T15:24:37.027000+00:00",
          "model": "qwen2.5-coder:14b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 21994.0,
          "costDetails": {},
          "environment": "default",
          "totalTokens": 2929,
          "promptTokens": 2048,
          "completionTokens": 881,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-18-24-45-341850_chatcmpl-40df46d2-b864-4a69-9733-e9adb2c21f5d",
          "traceId": "b5f19ff0",
          "type": "GENERATION",
          "name": "b5f1_tpusg_gen_attempt#4",
          "startTime": "2025-07-30T15:24:45.341000+00:00",
          "endTime": "2025-07-30T15:25:08.640000+00:00",
          "model": "qwen2.5-coder:14b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 23299.0,
          "costDetails": {},
          "environment": "default",
          "totalTokens": 2983,
          "promptTokens": 2048,
          "completionTokens": 935,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-18-25-16-818223_chatcmpl-5dd58976-9ef4-4ecc-87d9-5e0d144d9e26",
          "traceId": "b5f19ff0",
          "type": "GENERATION",
          "name": "b5f1_tpusg_gen_attempt#5",
          "startTime": "2025-07-30T15:25:16.818000+00:00",
          "endTime": "2025-07-30T15:25:39.825000+00:00",
          "model": "qwen2.5-coder:14b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 23007.0,
          "costDetails": {},
          "environment": "default",
          "totalTokens": 2972,
          "promptTokens": 2048,
          "completionTokens": 924,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "9af30348-cb8d-45b6-a8df-606dca8c1f53",
          "traceId": "b5f19ff0",
          "type": "SPAN",
          "name": "error_b5_tpusg_failure_signal_tpu_sketch_generator",
          "startTime": "2025-07-30T15:25:48.046000+00:00",
          "level": "ERROR",
          "statusMessage": "Failed. Last error: OpenCV: FFMPEG: tag 0x44495658/'XVID' is not supported with codec id 12 and format 'mp4 / MP4 (MPEG-4 Part 14)'\nOpenCV: FFMPEG: fallback to use tag 0x7634706d/'mp4v'\nTraceback (most recent call last):\n  File \"script_6b23d452_1753889139.py\", line 72, in <module>\n    if scores[i] > confidence_threshold:\nValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()",
          "calculatedTotalCost": 0.0,
          "costDetails": {},
          "environment": "default",
          "totalTokens": 0,
          "promptTokens": 0,
          "completionTokens": 0,
          "endTime": null,
          "model": null,
          "modelParameters": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null,
          "latency": null
        }
      ],
      "environment": "default",
      "createdAt": "2025-07-30T15:23:19.254Z",
      "updatedAt": "2025-07-30T15:25:53.415Z"
    },
    {
      "id": "1735aee0",
      "timestamp": "2025-07-30T15:21:58.183000+00:00",
      "name": "1735_tpu_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import numpy as np\nimport cv2\nfrom tflite_runtime.interpreter import Interpreter, load_delegate\n\n# Phase 1: Setup\n\n# Define paths and parameters\nmodel_path  = \"/home/mendel/tinyml_autopilot/models/edgetpu_detect.tflite\"\nlabel_path  = \"/home/mendel/tinyml_autopilot/models/labelmap.txt\"\ninput_path  = \"/home/mendel/tinyml_autopilot/data//sheeps.mp4\"\noutput_path  = \"/home/mendel/tinyml_autopilot/results/sheeps_detections.mp4\"\nconfidence_threshold  = 0.5\n\n# Load labels\nwith open(label_path, 'r') as f:\n    labels = [line.strip() for line in f.readlines()]\n\n# Load interpreter with EdgeTPU acceleration\ntry:\n    interpreter = Interpreter(model_path=model_path, experimental_delegates=[load_delegate('libedgetpu.so.1.0')])\nexcept Exception as e:\n    print(f\"Failed to load EdgeTPU delegate with error: {e}\")\n    try:\n        interpreter = Interpreter(model_path=model_path, experimental_delegates=[load_delegate('/usr/lib/aarch64-linux-gnu/libedgetpu.so.1.0')])\n    except Exception as e:\n        print(f\"Failed to load alternative EdgeTPU delegate with error: {e}\")\n        raise\n\ninterpreter.allocate_tensors()\n\n# Get model details\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\n# Phase 2: Input Acquisition & Preprocessing Loop\n\n# Acquire input data\ncap = cv2.VideoCapture(input_path)\nif not cap.isOpened():\n    print(f\"Error opening video file {input_path}\")\n    exit()\n\nframe_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\nframe_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n\n# Define the codec and create VideoWriter object\nfourcc = cv2.VideoWriter_fourcc(*'XVID')\nout = cv2.VideoWriter(output_path, fourcc, 20.0, (frame_width, frame_height))\n\nwhile cap.isOpened():\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    # Preprocess data\n    input_data = cv2.resize(frame, (input_details[0]['shape'][1], input_details[0]['shape'][2]))\n    input_data = cv2.cvtColor(input_data, cv2.COLOR_BGR2RGB)\n    input_data = np.expand_dims(input_data, axis=0)\n\n    # Quantization handling\n    if input_details[0]['dtype'] == np.float32:\n        input_data = (np.float32(input_data) - 127.5) / 127.5\n\n    # Phase 3: Inference\n    interpreter.set_tensor(input_details[0]['index'], input_data)\n    interpreter.invoke()\n\n    # Phase 4: Output Interpretation & Handling Loop\n\n    # Get output tensors\n    boxes = interpreter.get_tensor(output_details[0]['index'])[0]  # Bounding boxes\n    classes = interpreter.get_tensor(output_details[1]['index'])[0]  # Class indices\n    scores = interpreter.get_tensor(output_details[2]['index'])[0]   # Confidence scores\n\n    # Interpret results and apply confidence thresholding\n    for i in range(len(scores)):\n        if scores[i] > confidence_threshold:\n            ymin, xmin, ymax, xmax = boxes[i]\n            label = labels[int(classes[i])]\n\n            # Scale coordinates to original frame size\n            ymin = int(ymin * frame_height)\n            xmin = int(xmin * frame_width)\n            ymax = int(ymax * frame_height)\n            xmax = int(xmax * frame_width)\n\n            # Draw bounding box and label on the frame\n            cv2.rectangle(frame, (xmin, ymin), (xmax, ymax), (0, 255, 0), 2)\n            cv2.putText(frame, f'{label}: {scores[i]:.2f}', (xmin, ymin - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n\n    # Handle output\n    out.write(frame)\n\n# Phase 5: Cleanup\n\ncap.release()\nout.release()\ncv2.destroyAllWindows()\nprint(\"Processing complete.\")"
      },
      "session_id": "qwen2.5-coder:14b_85a9_tpusg_batch",
      "metadata": {
        "num_run": 16
      },
      "tags": [
        "benchmark",
        "qwen2.5-coder:14b",
        "tpu_sketch_generator"
      ],
      "latency": 22.491,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-18-21-58-184701_chatcmpl-d4abcc4d-10e4-4063-8575-090fedf1fdbe",
          "traceId": "1735aee0",
          "type": "GENERATION",
          "name": "1735_tpusg_gen_attempt#1",
          "startTime": "2025-07-30T15:21:58.184000+00:00",
          "endTime": "2025-07-30T15:22:20.675000+00:00",
          "model": "qwen2.5-coder:14b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 22491.0,
          "costDetails": {},
          "environment": "default",
          "totalTokens": 2731,
          "promptTokens": 1833,
          "completionTokens": 898,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "environment": "default",
      "createdAt": "2025-07-30T15:21:58.703Z",
      "updatedAt": "2025-07-30T15:23:18.355Z"
    },
    {
      "id": "4ce16286",
      "timestamp": "2025-07-30T15:19:10.450000+00:00",
      "name": "4ce1_tpu_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import os\nimport numpy as np\nfrom tflite_runtime.interpreter import Interpreter, load_delegate\n\n# Define paths and parameters\nmodel_path    = \"/home/mendel/tinyml_autopilot/models/edgetpu_detect.tflite\"\nlabel_path    = \"/home/mendel/tinyml_autopilot/models/labelmap.txt\"\ninput_path    = \"/home/mendel/tinyml_autopilot/data//sheeps.mp4\"\noutput_path    = \"/home/mendel/tinyml_autopilot/results/sheeps_detections.mp4\"\nconfidence_threshold    = 0.5\n\n# Load labels\nwith open(label_path, 'r') as f:\n    labels = [line.strip() for line in f.readlines()]\n\n# Load Interpreter with EdgeTPU\ntry:\n    interpreter = Interpreter(model_path=model_path,\n                              experimental_delegates=[load_delegate('libedgetpu.so.1')])\nexcept Exception as e:\n    try:\n        interpreter = Interpreter(model_path=model_path,\n                                  experimental_delegates=[load_delegate('/usr/lib/aarch64-linux-gnu/libedgetpu.so.1')])\n    except Exception as e:\n        print(f\"Failed to load EdgeTPU delegate: {e}\")\n        exit(1)\n\ninterpreter.allocate_tensors()\n\n# Get model details\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\nfloating_model = (input_details[0]['dtype'] == np.float32)\nheight, width = input_details[0]['shape'][1], input_details[0]['shape'][2]\n\nimport cv2\n\n# Open video capture\ncap = cv2.VideoCapture(input_path)\n\nif not cap.isOpened():\n    print(f\"Error: Could not open video at {input_path}\")\n    exit(1)\n\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\nout = cv2.VideoWriter(output_path, fourcc, 30.0, (int(cap.get(cv2.CAP_PROP_FRAME_WIDTH)), int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))))\n\nwhile True:\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    # Preprocess data\n    input_data = cv2.resize(frame, (width, height))\n    input_data = np.expand_dims(input_data, axis=0)\n\n    if floating_model:\n        input_data = (np.float32(input_data) - 127.5) / 127.5\n\n    # Set input tensor\n    interpreter.set_tensor(input_details[0]['index'], input_data)\n\n    # Run inference\n    interpreter.invoke()\n\n    # Get output tensors\n    boxes = interpreter.get_tensor(output_details[0]['index'])\n    classes = interpreter.get_tensor(output_details[1]['index'])\n    scores = interpreter.get_tensor(output_details[2]['index'])\n    num_detections = int(interpreter.get_tensor(output_details[3]['index']))\n\n    # Process results\n    for i in range(num_detections):\n        if scores[0][i] > confidence_threshold:  # Access individual score\n            label = labels[int(classes[0][i])]\n            box = boxes[0][i]\n            x1, y1, x2, y2 = int(box[1] * frame.shape[1]), int(box[0] * frame.shape[0]), int(box[3] * frame.shape[1]), int(box[2] * frame.shape[0])\n            cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n            cv2.putText(frame, f\"{label}: {scores[0][i]:.2f}\", (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n\n    # Write frame to output\n    out.write(frame)\n\n# Release resources\ncap.release()\nout.release()\ncv2.destroyAllWindows()"
      },
      "session_id": "qwen2.5-coder:14b_85a9_tpusg_batch",
      "metadata": {
        "num_run": 15
      },
      "tags": [
        "benchmark",
        "qwen2.5-coder:14b",
        "tpu_sketch_generator"
      ],
      "latency": 109.293,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-18-19-10-451608_chatcmpl-56e73516-ee51-40b0-9e3d-b5175e59dcd1",
          "traceId": "4ce16286",
          "type": "GENERATION",
          "name": "4ce1_tpusg_gen_attempt#1",
          "startTime": "2025-07-30T15:19:10.451000+00:00",
          "endTime": "2025-07-30T15:19:53.360000+00:00",
          "model": "qwen2.5-coder:14b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 42909.0,
          "costDetails": {},
          "environment": "default",
          "totalTokens": 2658,
          "promptTokens": 1833,
          "completionTokens": 825,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-18-20-01-691886_chatcmpl-d6cc7bea-12bd-44fe-b2f0-677799b2e957",
          "traceId": "4ce16286",
          "type": "GENERATION",
          "name": "4ce1_tpusg_gen_attempt#2",
          "startTime": "2025-07-30T15:20:01.691000+00:00",
          "endTime": "2025-07-30T15:20:27.288000+00:00",
          "model": "qwen2.5-coder:14b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 25597.0,
          "costDetails": {},
          "environment": "default",
          "totalTokens": 3087,
          "promptTokens": 2048,
          "completionTokens": 1039,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-18-20-35-533645_chatcmpl-1c592ecf-583d-4977-8557-05a8f2074a76",
          "traceId": "4ce16286",
          "type": "GENERATION",
          "name": "4ce1_tpusg_gen_attempt#3",
          "startTime": "2025-07-30T15:20:35.533000+00:00",
          "endTime": "2025-07-30T15:20:59.744000+00:00",
          "model": "qwen2.5-coder:14b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 24211.0,
          "costDetails": {},
          "environment": "default",
          "totalTokens": 3024,
          "promptTokens": 2048,
          "completionTokens": 976,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "environment": "default",
      "createdAt": "2025-07-30T15:19:10.969Z",
      "updatedAt": "2025-07-30T15:21:57.679Z"
    },
    {
      "id": "91513dd5",
      "timestamp": "2025-07-30T15:18:38.947000+00:00",
      "name": "9151_tpu_sketch_generator",
      "input": null,
      "output": {
        "status": "failed",
        "last_code": "",
        "last_error": "Unexpected error during generation: litellm.APIConnectionError: OllamaException - {\"error\":\"llama runner process has terminated: CUDA error\"}"
      },
      "session_id": "qwen2.5-coder:14b_85a9_tpusg_batch",
      "metadata": {
        "num_run": 14
      },
      "tags": [
        "benchmark",
        "qwen2.5-coder:14b",
        "tpu_sketch_generator"
      ],
      "latency": 0.0,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "a19bb9aa-7e96-45f0-a03e-d6975f5de022",
          "traceId": "91513dd5",
          "type": "SPAN",
          "name": "error_91_tpusg_failure_signal_tpu_sketch_generator",
          "startTime": "2025-07-30T15:19:04.014000+00:00",
          "level": "ERROR",
          "statusMessage": "Failed. Last error: Unexpected error during generation: litellm.APIConnectionError: OllamaException - {\"error\":\"llama runner process has terminated: CUDA error\"}",
          "calculatedTotalCost": 0.0,
          "costDetails": {},
          "environment": "default",
          "totalTokens": 0,
          "promptTokens": 0,
          "completionTokens": 0,
          "endTime": null,
          "model": null,
          "modelParameters": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null,
          "latency": null
        }
      ],
      "environment": "default",
      "createdAt": "2025-07-30T15:18:39.465Z",
      "updatedAt": "2025-07-30T15:19:09.589Z"
    },
    {
      "id": "44abb92f",
      "timestamp": "2025-07-30T15:18:10.407000+00:00",
      "name": "44ab_tpu_sketch_generator",
      "input": null,
      "output": {
        "status": "failed",
        "last_code": "",
        "last_error": "Unexpected error during generation: litellm.APIConnectionError: OllamaException - {\"error\":\"llama runner process has terminated: CUDA error\"}"
      },
      "session_id": "qwen2.5-coder:14b_85a9_tpusg_batch",
      "metadata": {
        "num_run": 13
      },
      "tags": [
        "benchmark",
        "qwen2.5-coder:14b",
        "tpu_sketch_generator"
      ],
      "latency": 0.0,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "d3904792-6b1d-4c61-80f7-35aa38737ea4",
          "traceId": "44abb92f",
          "type": "SPAN",
          "name": "error_44_tpusg_failure_signal_tpu_sketch_generator",
          "startTime": "2025-07-30T15:18:32.951000+00:00",
          "level": "ERROR",
          "statusMessage": "Failed. Last error: Unexpected error during generation: litellm.APIConnectionError: OllamaException - {\"error\":\"llama runner process has terminated: CUDA error\"}",
          "calculatedTotalCost": 0.0,
          "costDetails": {},
          "environment": "default",
          "totalTokens": 0,
          "promptTokens": 0,
          "completionTokens": 0,
          "endTime": null,
          "model": null,
          "modelParameters": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null,
          "latency": null
        }
      ],
      "environment": "default",
      "createdAt": "2025-07-30T15:18:10.925Z",
      "updatedAt": "2025-07-30T15:18:38.575Z"
    },
    {
      "id": "cef87050",
      "timestamp": "2025-07-30T15:17:45.903000+00:00",
      "name": "cef8_tpu_sketch_generator",
      "input": null,
      "output": {
        "status": "failed",
        "last_code": "",
        "last_error": "Unexpected error during generation: litellm.APIConnectionError: OllamaException - {\"error\":\"llama runner process has terminated: CUDA error\"}"
      },
      "session_id": "qwen2.5-coder:14b_85a9_tpusg_batch",
      "metadata": {
        "num_run": 12
      },
      "tags": [
        "benchmark",
        "qwen2.5-coder:14b",
        "tpu_sketch_generator"
      ],
      "latency": 0.0,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "5b2466a6-7831-47d8-be1b-d0ca3ae9f22b",
          "traceId": "cef87050",
          "type": "SPAN",
          "name": "error_ce_tpusg_failure_signal_tpu_sketch_generator",
          "startTime": "2025-07-30T15:18:04.800000+00:00",
          "level": "ERROR",
          "statusMessage": "Failed. Last error: Unexpected error during generation: litellm.APIConnectionError: OllamaException - {\"error\":\"llama runner process has terminated: CUDA error\"}",
          "calculatedTotalCost": 0.0,
          "costDetails": {},
          "environment": "default",
          "totalTokens": 0,
          "promptTokens": 0,
          "completionTokens": 0,
          "endTime": null,
          "model": null,
          "modelParameters": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null,
          "latency": null
        }
      ],
      "environment": "default",
      "createdAt": "2025-07-30T15:17:46.422Z",
      "updatedAt": "2025-07-30T15:18:10.076Z"
    },
    {
      "id": "fb91c5ed",
      "timestamp": "2025-07-30T15:17:21.804000+00:00",
      "name": "fb91_tpu_sketch_generator",
      "input": null,
      "output": {
        "status": "failed",
        "last_code": "",
        "last_error": "Unexpected error during generation: litellm.APIConnectionError: OllamaException - {\"error\":\"llama runner process has terminated: CUDA error\"}"
      },
      "session_id": "qwen2.5-coder:14b_85a9_tpusg_batch",
      "metadata": {
        "num_run": 11
      },
      "tags": [
        "benchmark",
        "qwen2.5-coder:14b",
        "tpu_sketch_generator"
      ],
      "latency": 0.0,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "d135a782-5156-4b9d-ac1c-25a1a1fd03d9",
          "traceId": "fb91c5ed",
          "type": "SPAN",
          "name": "error_fb_tpusg_failure_signal_tpu_sketch_generator",
          "startTime": "2025-07-30T15:17:39.794000+00:00",
          "level": "ERROR",
          "statusMessage": "Failed. Last error: Unexpected error during generation: litellm.APIConnectionError: OllamaException - {\"error\":\"llama runner process has terminated: CUDA error\"}",
          "calculatedTotalCost": 0.0,
          "costDetails": {},
          "environment": "default",
          "totalTokens": 0,
          "promptTokens": 0,
          "completionTokens": 0,
          "endTime": null,
          "model": null,
          "modelParameters": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null,
          "latency": null
        }
      ],
      "environment": "default",
      "createdAt": "2025-07-30T15:17:22.323Z",
      "updatedAt": "2025-07-30T15:17:44.980Z"
    },
    {
      "id": "05d6df6e",
      "timestamp": "2025-07-30T15:16:55.292000+00:00",
      "name": "05d6_tpu_sketch_generator",
      "input": null,
      "output": {
        "status": "failed",
        "last_code": "",
        "last_error": "Unexpected error during generation: litellm.APIConnectionError: OllamaException - {\"error\":\"llama runner process has terminated: CUDA error\"}"
      },
      "session_id": "qwen2.5-coder:14b_85a9_tpusg_batch",
      "metadata": {
        "num_run": 10
      },
      "tags": [
        "benchmark",
        "qwen2.5-coder:14b",
        "tpu_sketch_generator"
      ],
      "latency": 0.0,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "b66e2b8e-acf6-4f66-b260-a4661eee2127",
          "traceId": "05d6df6e",
          "type": "SPAN",
          "name": "error_05_tpusg_failure_signal_tpu_sketch_generator",
          "startTime": "2025-07-30T15:17:14.796000+00:00",
          "level": "ERROR",
          "statusMessage": "Failed. Last error: Unexpected error during generation: litellm.APIConnectionError: OllamaException - {\"error\":\"llama runner process has terminated: CUDA error\"}",
          "calculatedTotalCost": 0.0,
          "costDetails": {},
          "environment": "default",
          "totalTokens": 0,
          "promptTokens": 0,
          "completionTokens": 0,
          "endTime": null,
          "model": null,
          "modelParameters": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null,
          "latency": null
        }
      ],
      "environment": "default",
      "createdAt": "2025-07-30T15:16:55.812Z",
      "updatedAt": "2025-07-30T15:17:19.892Z"
    },
    {
      "id": "b03ff574",
      "timestamp": "2025-07-30T15:16:31.793000+00:00",
      "name": "b03f_tpu_sketch_generator",
      "input": null,
      "output": {
        "status": "failed",
        "last_code": "",
        "last_error": "Unexpected error during generation: litellm.APIConnectionError: OllamaException - {\"error\":\"llama runner process has terminated: CUDA error\"}"
      },
      "session_id": "qwen2.5-coder:14b_85a9_tpusg_batch",
      "metadata": {
        "num_run": 9
      },
      "tags": [
        "benchmark",
        "qwen2.5-coder:14b",
        "tpu_sketch_generator"
      ],
      "latency": 0.0,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "a227dd3e-36ac-4928-aa7d-90d13b507f25",
          "traceId": "b03ff574",
          "type": "SPAN",
          "name": "error_b0_tpusg_failure_signal_tpu_sketch_generator",
          "startTime": "2025-07-30T15:16:49.781000+00:00",
          "level": "ERROR",
          "statusMessage": "Failed. Last error: Unexpected error during generation: litellm.APIConnectionError: OllamaException - {\"error\":\"llama runner process has terminated: CUDA error\"}",
          "calculatedTotalCost": 0.0,
          "costDetails": {},
          "environment": "default",
          "totalTokens": 0,
          "promptTokens": 0,
          "completionTokens": 0,
          "endTime": null,
          "model": null,
          "modelParameters": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null,
          "latency": null
        }
      ],
      "environment": "default",
      "createdAt": "2025-07-30T15:16:32.310Z",
      "updatedAt": "2025-07-30T15:16:54.904Z"
    },
    {
      "id": "36fcf199",
      "timestamp": "2025-07-30T15:16:05.294000+00:00",
      "name": "36fc_tpu_sketch_generator",
      "input": null,
      "output": {
        "status": "failed",
        "last_code": "",
        "last_error": "Unexpected error during generation: litellm.APIConnectionError: OllamaException - {\"error\":\"llama runner process has terminated: CUDA error\"}"
      },
      "session_id": "qwen2.5-coder:14b_85a9_tpusg_batch",
      "metadata": {
        "num_run": 8
      },
      "tags": [
        "benchmark",
        "qwen2.5-coder:14b",
        "tpu_sketch_generator"
      ],
      "latency": 0.0,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "a221f67f-a427-44ec-a674-7952e8ab6318",
          "traceId": "36fcf199",
          "type": "SPAN",
          "name": "error_36_tpusg_failure_signal_tpu_sketch_generator",
          "startTime": "2025-07-30T15:16:24.768000+00:00",
          "level": "ERROR",
          "statusMessage": "Failed. Last error: Unexpected error during generation: litellm.APIConnectionError: OllamaException - {\"error\":\"llama runner process has terminated: CUDA error\"}",
          "calculatedTotalCost": 0.0,
          "costDetails": {},
          "environment": "default",
          "totalTokens": 0,
          "promptTokens": 0,
          "completionTokens": 0,
          "endTime": null,
          "model": null,
          "modelParameters": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null,
          "latency": null
        }
      ],
      "environment": "default",
      "createdAt": "2025-07-30T15:16:05.812Z",
      "updatedAt": "2025-07-30T15:16:29.916Z"
    },
    {
      "id": "90c3e878",
      "timestamp": "2025-07-30T15:15:40.805000+00:00",
      "name": "90c3_tpu_sketch_generator",
      "input": null,
      "output": {
        "status": "failed",
        "last_code": "",
        "last_error": "Unexpected error during generation: litellm.APIConnectionError: OllamaException - {\"error\":\"llama runner process has terminated: CUDA error: all CUDA-capable devices are busy or unavailable\\n  current device: 0, in function ggml_backend_cuda_device_get_memory at //ml/backend/ggml/ggml/src/ggml-cuda/ggml-cuda.cu:2898\\n  cudaMemGetInfo(free, total)\\n//ml/backend/ggml/ggml/src/ggml-cuda/ggml-cuda.cu:73: CUDA error\"}"
      },
      "session_id": "qwen2.5-coder:14b_85a9_tpusg_batch",
      "metadata": {
        "num_run": 7
      },
      "tags": [
        "benchmark",
        "qwen2.5-coder:14b",
        "tpu_sketch_generator"
      ],
      "latency": 0.0,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "dc01f79c-a687-4e0f-a992-0a9151f01cfe",
          "traceId": "90c3e878",
          "type": "SPAN",
          "name": "error_90_tpusg_failure_signal_tpu_sketch_generator",
          "startTime": "2025-07-30T15:15:59.774000+00:00",
          "level": "ERROR",
          "statusMessage": "Failed. Last error: Unexpected error during generation: litellm.APIConnectionError: OllamaException - {\"error\":\"llama runner process has terminated: CUDA error: all CUDA-capable devices are busy or unavailable\\n  current device: 0, in function ggml_backend_cuda_device_get_memory at //ml/backend/ggml/ggml/src/ggml-cuda/ggml-cuda.cu:2898\\n  cudaMemGetInfo(free, total)\\n//ml/backend/ggml/ggml/src/ggml-cuda/ggml-cuda.cu:73: CUDA error\"}",
          "calculatedTotalCost": 0.0,
          "costDetails": {},
          "environment": "default",
          "totalTokens": 0,
          "promptTokens": 0,
          "completionTokens": 0,
          "endTime": null,
          "model": null,
          "modelParameters": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null,
          "latency": null
        }
      ],
      "environment": "default",
      "createdAt": "2025-07-30T15:15:41.322Z",
      "updatedAt": "2025-07-30T15:16:04.923Z"
    },
    {
      "id": "9078b118",
      "timestamp": "2025-07-30T15:15:15.279000+00:00",
      "name": "9078_tpu_sketch_generator",
      "input": null,
      "output": {
        "status": "failed",
        "last_code": "",
        "last_error": "Unexpected error during generation: litellm.APIConnectionError: OllamaException - {\"error\":\"llama runner process has terminated: CUDA error\"}"
      },
      "session_id": "qwen2.5-coder:14b_85a9_tpusg_batch",
      "metadata": {
        "num_run": 6
      },
      "tags": [
        "benchmark",
        "qwen2.5-coder:14b",
        "tpu_sketch_generator"
      ],
      "latency": 0.0,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "91c12b3e-3d58-4c46-a0c5-a451dbd7a27c",
          "traceId": "9078b118",
          "type": "SPAN",
          "name": "error_90_tpusg_failure_signal_tpu_sketch_generator",
          "startTime": "2025-07-30T15:15:34.755000+00:00",
          "level": "ERROR",
          "statusMessage": "Failed. Last error: Unexpected error during generation: litellm.APIConnectionError: OllamaException - {\"error\":\"llama runner process has terminated: CUDA error\"}",
          "calculatedTotalCost": 0.0,
          "costDetails": {},
          "environment": "default",
          "totalTokens": 0,
          "promptTokens": 0,
          "completionTokens": 0,
          "endTime": null,
          "model": null,
          "modelParameters": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null,
          "latency": null
        }
      ],
      "environment": "default",
      "createdAt": "2025-07-30T15:15:15.794Z",
      "updatedAt": "2025-07-30T15:15:39.932Z"
    },
    {
      "id": "a431af85",
      "timestamp": "2025-07-30T15:14:50.754000+00:00",
      "name": "a431_tpu_sketch_generator",
      "input": null,
      "output": {
        "status": "failed",
        "last_code": "",
        "last_error": "Unexpected error during generation: litellm.APIConnectionError: OllamaException - {\"error\":\"llama runner process has terminated: CUDA error\"}"
      },
      "session_id": "qwen2.5-coder:14b_85a9_tpusg_batch",
      "metadata": {
        "num_run": 5
      },
      "tags": [
        "benchmark",
        "qwen2.5-coder:14b",
        "tpu_sketch_generator"
      ],
      "latency": 0.0,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "476da73c-777e-4a78-b35e-be2bb6563eed",
          "traceId": "a431af85",
          "type": "SPAN",
          "name": "error_a4_tpusg_failure_signal_tpu_sketch_generator",
          "startTime": "2025-07-30T15:15:09.735000+00:00",
          "level": "ERROR",
          "statusMessage": "Failed. Last error: Unexpected error during generation: litellm.APIConnectionError: OllamaException - {\"error\":\"llama runner process has terminated: CUDA error\"}",
          "calculatedTotalCost": 0.0,
          "costDetails": {},
          "environment": "default",
          "totalTokens": 0,
          "promptTokens": 0,
          "completionTokens": 0,
          "endTime": null,
          "model": null,
          "modelParameters": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null,
          "latency": null
        }
      ],
      "environment": "default",
      "createdAt": "2025-07-30T15:14:51.271Z",
      "updatedAt": "2025-07-30T15:15:14.958Z"
    },
    {
      "id": "b5c1e985",
      "timestamp": "2025-07-30T15:14:25.238000+00:00",
      "name": "b5c1_tpu_sketch_generator",
      "input": null,
      "output": {
        "status": "failed",
        "last_code": "",
        "last_error": "Unexpected error during generation: litellm.APIConnectionError: OllamaException - {\"error\":\"llama runner process has terminated: CUDA error\"}"
      },
      "session_id": "qwen2.5-coder:14b_85a9_tpusg_batch",
      "metadata": {
        "num_run": 4
      },
      "tags": [
        "benchmark",
        "qwen2.5-coder:14b",
        "tpu_sketch_generator"
      ],
      "latency": 0.0,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "c8e42544-ba72-47de-8860-ff25dbe428b4",
          "traceId": "b5c1e985",
          "type": "SPAN",
          "name": "error_b5_tpusg_failure_signal_tpu_sketch_generator",
          "startTime": "2025-07-30T15:14:44.733000+00:00",
          "level": "ERROR",
          "statusMessage": "Failed. Last error: Unexpected error during generation: litellm.APIConnectionError: OllamaException - {\"error\":\"llama runner process has terminated: CUDA error\"}",
          "calculatedTotalCost": 0.0,
          "costDetails": {},
          "environment": "default",
          "totalTokens": 0,
          "promptTokens": 0,
          "completionTokens": 0,
          "endTime": null,
          "model": null,
          "modelParameters": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null,
          "latency": null
        }
      ],
      "environment": "default",
      "createdAt": "2025-07-30T15:14:25.756Z",
      "updatedAt": "2025-07-30T15:14:49.855Z"
    },
    {
      "id": "f204a5f7",
      "timestamp": "2025-07-30T15:14:01.732000+00:00",
      "name": "f204_tpu_sketch_generator",
      "input": null,
      "output": {
        "status": "failed",
        "last_code": "",
        "last_error": "Unexpected error during generation: litellm.APIConnectionError: OllamaException - {\"error\":\"llama runner process has terminated: CUDA error\"}"
      },
      "session_id": "qwen2.5-coder:14b_85a9_tpusg_batch",
      "metadata": {
        "num_run": 3
      },
      "tags": [
        "benchmark",
        "qwen2.5-coder:14b",
        "tpu_sketch_generator"
      ],
      "latency": 0.0,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "5c65bace-33b6-4a89-ab6c-1951b8a0026f",
          "traceId": "f204a5f7",
          "type": "SPAN",
          "name": "error_f2_tpusg_failure_signal_tpu_sketch_generator",
          "startTime": "2025-07-30T15:14:19.731000+00:00",
          "level": "ERROR",
          "statusMessage": "Failed. Last error: Unexpected error during generation: litellm.APIConnectionError: OllamaException - {\"error\":\"llama runner process has terminated: CUDA error\"}",
          "calculatedTotalCost": 0.0,
          "costDetails": {},
          "environment": "default",
          "totalTokens": 0,
          "promptTokens": 0,
          "completionTokens": 0,
          "endTime": null,
          "model": null,
          "modelParameters": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null,
          "latency": null
        }
      ],
      "environment": "default",
      "createdAt": "2025-07-30T15:14:02.248Z",
      "updatedAt": "2025-07-30T15:14:25.067Z"
    },
    {
      "id": "975fa700",
      "timestamp": "2025-07-30T15:13:35.152000+00:00",
      "name": "975f_tpu_sketch_generator",
      "input": null,
      "output": {
        "status": "failed",
        "last_code": "",
        "last_error": "Unexpected error during generation: litellm.APIConnectionError: OllamaException - {\"error\":\"llama runner process has terminated: CUDA error\"}"
      },
      "session_id": "qwen2.5-coder:14b_85a9_tpusg_batch",
      "metadata": {
        "num_run": 2
      },
      "tags": [
        "benchmark",
        "qwen2.5-coder:14b",
        "tpu_sketch_generator"
      ],
      "latency": 0.0,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "5acb6134-c7af-4725-86a9-bc5d9cb708f3",
          "traceId": "975fa700",
          "type": "SPAN",
          "name": "error_97_tpusg_failure_signal_tpu_sketch_generator",
          "startTime": "2025-07-30T15:13:54.709000+00:00",
          "level": "ERROR",
          "statusMessage": "Failed. Last error: Unexpected error during generation: litellm.APIConnectionError: OllamaException - {\"error\":\"llama runner process has terminated: CUDA error\"}",
          "calculatedTotalCost": 0.0,
          "costDetails": {},
          "environment": "default",
          "totalTokens": 0,
          "promptTokens": 0,
          "completionTokens": 0,
          "endTime": null,
          "model": null,
          "modelParameters": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null,
          "latency": null
        }
      ],
      "environment": "default",
      "createdAt": "2025-07-30T15:13:35.669Z",
      "updatedAt": "2025-07-30T15:14:00.277Z"
    },
    {
      "id": "c555c993",
      "timestamp": "2025-07-30T15:13:11.551000+00:00",
      "name": "c555_tpu_sketch_generator",
      "input": null,
      "output": {
        "status": "failed",
        "last_code": "",
        "last_error": "Unexpected error during generation: litellm.APIConnectionError: OllamaException - {\"error\":\"llama runner process has terminated: CUDA error\"}"
      },
      "session_id": "qwen2.5-coder:14b_85a9_tpusg_batch",
      "metadata": {
        "num_run": 1
      },
      "tags": [
        "benchmark",
        "qwen2.5-coder:14b",
        "tpu_sketch_generator"
      ],
      "latency": 0.0,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "ce260d12-a730-470a-8d43-5fc77ae544ea",
          "traceId": "c555c993",
          "type": "SPAN",
          "name": "error_c5_tpusg_failure_signal_tpu_sketch_generator",
          "startTime": "2025-07-30T15:13:29.701000+00:00",
          "level": "ERROR",
          "statusMessage": "Failed. Last error: Unexpected error during generation: litellm.APIConnectionError: OllamaException - {\"error\":\"llama runner process has terminated: CUDA error\"}",
          "calculatedTotalCost": 0.0,
          "costDetails": {},
          "environment": "default",
          "totalTokens": 0,
          "promptTokens": 0,
          "completionTokens": 0,
          "endTime": null,
          "model": null,
          "modelParameters": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null,
          "latency": null
        }
      ],
      "environment": "default",
      "createdAt": "2025-07-30T15:13:12.069Z",
      "updatedAt": "2025-07-30T15:13:35.284Z"
    }
  ],
  "meta": {
    "total_items": 30
  }
}
{
  "data": [
    {
      "id": "872906c0",
      "timestamp": "2025-08-18T17:15:01.725000+00:00",
      "name": "8729_tpu_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import cv2\nimport numpy as np\nfrom tflite_runtime.interpreter import Interpreter, load_delegate\n\n# Configuration parameters\nmodel_path  = \"/home/mendel/tinyml_autopilot/models/edgetpu_detect.tflite\"\nlabel_path  = \"/home/mendel/tinyml_autopilot/models/labelmap.txt\"\ninput_path  = \"/home/mendel/tinyml_autopilot/data//sheeps.mp4\"\noutput_path  = \"/home/mendel/tinyml_autopilot/results/sheeps_detections.mp4\"\nconfidence_threshold  = 0.5\n\n# Phase 1: Setup\ndef load_labels(path):\n    with open(path, 'r') as file:\n        return [line.strip() for line in file.readlines()]\n\nlabels = load_labels(label_path)\n\ntry:\n    interpreter = Interpreter(\n        model_path=model_path,\n        experimental_delegates=[load_delegate('libedgetpu.so.1.0')]\n    )\nexcept ValueError:\n    print(\"Failed to load Edge TPU delegate. Falling back.\")\n    interpreter = Interpreter(\n        model_path=model_path,\n        experimental_delegates=[load_delegate('/usr/lib/aarch64-linux-gnu/libedgetpu.so.1.0')]\n    )\n\ninterpreter.allocate_tensors()\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\n# Video capture and writer setup\ncap = cv2.VideoCapture(input_path)\nframe_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\nframe_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\nout = cv2.VideoWriter(output_path, cv2.VideoWriter_fourcc(*'mp4v'), 30, (frame_width, frame_height))\n\n# Phase 2: Input Acquisition & Preprocessing Loop\nwhile cap.isOpened():\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    # Resize and normalize the input image\n    img = cv2.resize(frame, (input_details[0]['shape'][2], input_details[0]['shape'][1]))\n    img = np.expand_dims(img, axis=0)\n\n    floating_model = (input_details[0]['dtype'] == np.float32)\n    if floating_model:\n        img = (np.float32(img) - 127.5) / 127.5\n\n    # Phase 3: Inference\n    interpreter.set_tensor(input_details[0]['index'], img)\n    interpreter.invoke()\n\n    # Phase 4: Output Interpretation & Handling Loop\n    boxes = interpreter.get_tensor(output_details[0]['index'])[0]  # Bounding box coordinates of detected objects\n    classes = interpreter.get_tensor(output_details[1]['index'])[0]  # Class index of detected objects\n    scores = interpreter.get_tensor(output_details[2]['index'])[0]  # Confidence of detected objects\n\n    for i in range(len(scores)):\n        if scores[i] > confidence_threshold:\n            ymin, xmin, ymax, xmax = boxes[i]\n            (left, right, top, bottom) = (xmin * frame_width, xmax * frame_width,\n                                          ymin * frame_height, ymax * frame_height)\n            \n            left, right, top, bottom = int(left), int(right), int(top), int(bottom)\n\n            # Draw the bounding box and label on the image\n            cv2.rectangle(frame, (left, top), (right, bottom), (0, 255, 0), 2)\n            object_name = labels[int(classes[i])]\n            label = f'{object_name}: {int(scores[i] * 100)}%'\n            label_size, base_line = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.7, 2)\n            top = max(top, label_size[1])\n            cv2.rectangle(frame, (left, top - round(1.5*label_size[1])), \n                          (left + round(1.5*label_size[0]), top + base_line), \n                          (255, 255, 255), cv2.FILLED)\n            cv2.putText(frame, label, (left, top), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 0), 2)\n\n    # Write the frame with detections to the output video\n    out.write(frame)\n\n# Release resources\ncap.release()\nout.release()\ncv2.destroyAllWindows()"
      },
      "session_id": "phi4_7854_tpusg_batch",
      "metadata": {
        "num_run": 30
      },
      "tags": [
        "benchmark",
        "phi4:latest",
        "tpu_sketch_generator"
      ],
      "latency": 19.737,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-20-15-01-727336_chatcmpl-b335b920-32d6-4450-9bff-3b19c91a6be4",
          "traceId": "872906c0",
          "type": "GENERATION",
          "name": "8729_tpusg_gen_attempt#1",
          "startTime": "2025-08-18T17:15:01.727000+00:00",
          "endTime": "2025-08-18T17:15:21.464000+00:00",
          "model": "phi4:latest",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 19737.0,
          "costDetails": {},
          "completionTokens": 958,
          "totalTokens": 2764,
          "environment": "default",
          "promptTokens": 1806,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "environment": "default",
      "createdAt": "2025-08-18T17:15:02.245Z",
      "updatedAt": "2025-08-18T17:16:19.446Z"
    },
    {
      "id": "3b284102",
      "timestamp": "2025-08-18T17:13:45.021000+00:00",
      "name": "3b28_tpu_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import cv2\nimport numpy as np\nfrom tflite_runtime.interpreter import Interpreter, load_delegate\n\n# Configuration Parameters\nmodel_path  = \"/home/mendel/tinyml_autopilot/models/edgetpu_detect.tflite\"\nlabel_path  = \"/home/mendel/tinyml_autopilot/models/labelmap.txt\"\ninput_path  = \"/home/mendel/tinyml_autopilot/data//sheeps.mp4\"\noutput_path  = \"/home/mendel/tinyml_autopilot/results/sheeps_detections.mp4\"\nconfidence_threshold  = 0.5\n\n# Phase 1: Setup\ntry:\n    interpreter = Interpreter(\n        model_path=model_path,\n        experimental_delegates=[load_delegate('libedgetpu.so.1.0')]\n    )\nexcept Exception as e:\n    try:\n        interpreter = Interpreter(\n            model_path=model_path,\n            experimental_delegates=[load_delegate('/usr/lib/aarch64-linux-gnu/libedgetpu.so.1.0')]\n        )\n    except Exception as inner_e:\n        raise RuntimeError(f\"Failed to load EdgeTPU delegate: {inner_e}\") from e\n\ninterpreter.allocate_tensors()\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\n# Load labels\nwith open(label_path, 'r') as f:\n    labels = [line.strip() for line in f.readlines()]\n\n# Phase 2: Input Acquisition & Preprocessing Loop\ncap = cv2.VideoCapture(input_path)\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\nout = cv2.VideoWriter(output_path, fourcc, 20.0, (int(cap.get(3)), int(cap.get(4))))\n\nwhile cap.isOpened():\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    input_frame = frame[:, :, [2, 1, 0]]  # Convert BGR to RGB\n    input_data = cv2.resize(input_frame, (input_details[0]['shape'][2], input_details[0]['shape'][1]))\n    \n    if input_details[0]['dtype'] == np.float32:\n        input_data = (np.float32(input_data) - 127.5) / 127.5\n\n    input_data = np.expand_dims(input_data, axis=0)\n\n    # Phase 3: Inference\n    interpreter.set_tensor(input_details[0]['index'], input_data)\n    interpreter.invoke()\n\n    # Phase 4: Output Interpretation & Handling Loop\n    boxes = interpreter.get_tensor(output_details[0]['index'])[0]  # Bounding box coordinates\n    classes = interpreter.get_tensor(output_details[1]['index'])[0]  # Class index\n    scores = interpreter.get_tensor(output_details[2]['index'])[0]  # Confidence\n\n    for i in range(len(scores)):\n        if scores[i] > confidence_threshold:\n            ymin, xmin, ymax, xmax = boxes[i]\n            (left, right, top, bottom) = (xmin * frame.shape[1], xmax * frame.shape[1],\n                                          ymin * frame.shape[0], ymax * frame.shape[0])\n            label = f'{labels[int(classes[i])]}: {int(scores[i] * 100)}%'\n            cv2.rectangle(frame, (int(left), int(top)), (int(right), int(bottom)), (0, 255, 0), 2)\n            cv2.putText(frame, label, (int(left), int(top) - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n\n    out.write(frame)\n\ncap.release()\nout.release()\n\n# Phase 5: Cleanup\ncv2.destroyAllWindows()"
      },
      "session_id": "phi4_7854_tpusg_batch",
      "metadata": {
        "num_run": 29
      },
      "tags": [
        "benchmark",
        "phi4:latest",
        "tpu_sketch_generator"
      ],
      "latency": 16.952,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-20-13-45-023613_chatcmpl-c98a0a81-4930-4fa1-a158-5aea2364ede4",
          "traceId": "3b284102",
          "type": "GENERATION",
          "name": "3b28_tpusg_gen_attempt#1",
          "startTime": "2025-08-18T17:13:45.023000+00:00",
          "endTime": "2025-08-18T17:14:01.975000+00:00",
          "model": "phi4:latest",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 16952.0,
          "costDetails": {},
          "completionTokens": 812,
          "totalTokens": 2618,
          "environment": "default",
          "promptTokens": 1806,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "environment": "default",
      "createdAt": "2025-08-18T17:13:45.543Z",
      "updatedAt": "2025-08-18T17:15:00.783Z"
    },
    {
      "id": "0cc89be8",
      "timestamp": "2025-08-18T17:12:31.017000+00:00",
      "name": "0cc8_tpu_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import os\nimport cv2\nimport numpy as np\nfrom tflite_runtime.interpreter import Interpreter, load_delegate\n\n# Configuration parameters\nmodel_path  = \"/home/mendel/tinyml_autopilot/models/edgetpu_detect.tflite\"\nlabel_path  = \"/home/mendel/tinyml_autopilot/models/labelmap.txt\"\ninput_path  = \"/home/mendel/tinyml_autopilot/data//sheeps.mp4\"\noutput_path  = \"/home/mendel/tinyml_autopilot/results/sheeps_detections.mp4\"\nconfidence_threshold  = 0.5\n\n# Phase 1: Setup\ndef load_labels(path):\n    with open(path, 'r') as file:\n        return [line.strip() for line in file.readlines()]\n\nlabels = load_labels(label_path)\n\ntry:\n    interpreter = Interpreter(\n        model_path=model_path,\n        experimental_delegates=[load_delegate('libedgetpu.so.1.0')]\n    )\nexcept ValueError:\n    try:\n        interpreter = Interpreter(\n            model_path=model_path,\n            experimental_delegates=[load_delegate('/usr/lib/aarch64-linux-gnu/libedgetpu.so.1.0')]\n        )\n    except Exception as e:\n        raise RuntimeError(\"Failed to load Edge TPU delegate: \" + str(e))\n\ninterpreter.allocate_tensors()\n\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\nfloating_model = (input_details[0]['dtype'] == np.float32)\n\n# Phase 2: Input Acquisition & Preprocessing Loop\ncap = cv2.VideoCapture(input_path)\nwidth, height = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH)), int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n\nout = cv2.VideoWriter(output_path, cv2.VideoWriter_fourcc(*'mp4v'), 30.0, (width, height))\n\nwhile cap.isOpened():\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    # Preprocess data\n    input_data = cv2.resize(frame, (input_details[0]['shape'][2], input_details[0]['shape'][1]))\n    input_data = np.expand_dims(input_data, axis=0)\n\n    if floating_model:\n        input_data = (np.float32(input_data) - 127.5) / 127.5\n\n    # Phase 3: Inference\n    interpreter.set_tensor(input_details[0]['index'], input_data)\n    interpreter.invoke()\n\n    # Phase 4: Output Interpretation & Handling Loop\n    boxes = np.squeeze(interpreter.get_tensor(output_details[0]['index']))\n    classes = np.squeeze(interpreter.get_tensor(output_details[1]['index']))\n    scores = np.squeeze(interpreter.get_tensor(output_details[2]['index']))\n\n    for i in range(len(scores)):\n        if scores[i] >= confidence_threshold:\n            ymin, xmin, ymax, xmax = boxes[i]\n            (left, right, top, bottom) = (xmin * width, xmax * width,\n                                          ymin * height, ymax * height)\n\n            cv2.rectangle(frame, (int(left), int(top)), (int(right), int(bottom)), (0, 255, 0), 2)\n            label = f'{labels[int(classes[i])]}: {scores[i]:.2f}'\n            cv2.putText(frame, label, (int(left), int(top) - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n\n    out.write(frame)\n\n# Phase 5: Cleanup\ncap.release()\nout.release()\ncv2.destroyAllWindows()"
      },
      "session_id": "phi4_7854_tpusg_batch",
      "metadata": {
        "num_run": 28
      },
      "tags": [
        "benchmark",
        "phi4:latest",
        "tpu_sketch_generator"
      ],
      "latency": 15.57,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-20-12-31-019060_chatcmpl-db2cc6c2-5b61-4711-8684-7c939b2fecd5",
          "traceId": "0cc89be8",
          "type": "GENERATION",
          "name": "0cc8_tpusg_gen_attempt#1",
          "startTime": "2025-08-18T17:12:31.019000+00:00",
          "endTime": "2025-08-18T17:12:46.589000+00:00",
          "model": "phi4:latest",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 15570.0,
          "costDetails": {},
          "completionTokens": 739,
          "totalTokens": 2545,
          "environment": "default",
          "promptTokens": 1806,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "environment": "default",
      "createdAt": "2025-08-18T17:12:31.537Z",
      "updatedAt": "2025-08-18T17:13:44.427Z"
    },
    {
      "id": "006d4f73",
      "timestamp": "2025-08-18T17:09:58.883000+00:00",
      "name": "006d_tpu_sketch_generator",
      "input": null,
      "output": {
        "status": "failed",
        "last_code": "import cv2\nfrom tflite_runtime.interpreter import Interpreter, load_delegate\nimport numpy as np\n\n# CONFIGURATION PARAMETERS\nmodel_path  = \"/home/mendel/tinyml_autopilot/models/edgetpu_detect.tflite\"\nlabel_path  = \"/home/mendel/tinyml_autopilot/models/labelmap.txt\"\ninput_path  = \"/home/mendel/tinyml_autopilot/data//sheeps.mp4\"\noutput_path  = \"/home/mendel/tinyml_autopilot/results/sheeps_detections.mp4\"\nconfidence_threshold  = 0.5\n\n# Phase 1: Setup\ndef load_labels(path):\n    with open(path, 'r') as f:\n        return [line.strip() for line in f.readlines()]\n\nlabels = load_labels(label_path)\n\ntry:\n    interpreter = Interpreter(\n        model_path=model_path,\n        experimental_delegates=[load_delegate('libedgetpu.so.1.0')]\n    )\nexcept ValueError:\n    interpreter = Interpreter(\n        model_path=model_path,\n        experimental_delegates=[load_delegate('/usr/lib/aarch64-linux-gnu/libedgetpu.so.1.0')]\n    )\n\ninterpreter.allocate_tensors()\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\n# Phase 2: Input Acquisition & Preprocessing Loop\ndef preprocess_frame(frame, input_shape):\n    frame_resized = cv2.resize(frame, (input_shape[2], input_shape[1]))\n    # Ensure the frame is in uint8 format before adding dimensions\n    frame_prepared = np.expand_dims(frame_resized, axis=0).astype(np.uint8)\n    return frame_prepared\n\n# Phase 4: Output Interpretation & Handling Loop\ndef postprocess(output_data, frame_height, frame_width):\n    # Extract detection results\n    num_detections = int(output_data[0][0])  # Correct access to the scalar value for number of detections\n    detections = []\n    for i in range(num_detections):\n        score = output_data[1][i]\n        if score >= confidence_threshold:\n            ymin, xmin, ymax, xmax = output_data[i+2][0]  # Correctly accessing bounding box coordinates\n            (left, right, top, bottom) = (\n                int(xmin * frame_width), \n                int(xmax * frame_width), \n                int(ymin * frame_height), \n                int(ymax * frame_height)\n            )\n            detections.append((labels[int(output_data[3][i])], score, left, top, right, bottom))\n    return detections\n\n# Phase 5: Cleanup\ndef main():\n    cap = cv2.VideoCapture(input_path)\n    fourcc = cv2.VideoWriter_fourcc(*'mp4v')  # Change codec to 'mp4v' for compatibility\n    out = cv2.VideoWriter(output_path, fourcc, 20.0, (int(cap.get(3)), int(cap.get(4))))\n\n    while cap.isOpened():\n        ret, frame = cap.read()\n        if not ret:\n            break\n        \n        input_data = preprocess_frame(frame, input_details[0]['shape'])\n        \n        interpreter.set_tensor(input_details[0]['index'], input_data)\n        interpreter.invoke()\n\n        output_data = [\n            interpreter.get_tensor(output_details[i]['index']) for i in range(len(output_details))\n        ]\n        \n        detections = postprocess(output_data, frame.shape[0], frame.shape[1])\n        \n        for label, score, left, top, right, bottom in detections:\n            cv2.rectangle(frame, (left, top), (right, bottom), (0, 255, 0), 2)\n            text = f'{label}: {score:.2f}'\n            cv2.putText(frame, text, (left, top - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 1)\n\n        out.write(frame)\n    \n    cap.release()\n    out.release()\n\nif __name__ == '__main__':\n    main()",
        "last_error": "Max retries reached with error. Last error: Traceback (most recent call last):\n  File \"script_1f5bbfca_1755537137.py\", line 92, in <module>\n    main()\n  File \"script_1f5bbfca_1755537137.py\", line 79, in main\n    detections = postprocess(output_data, frame.shape[0], frame.shape[1])\n  File \"script_1f5bbfca_1755537137.py\", line 44, in postprocess\n    num_detections = int(output_data[0][0])  # Correct access to the scalar value for number of detections\nTypeError: only size-1 arrays can be converted to Python scalars."
      },
      "session_id": "phi4_7854_tpusg_batch",
      "metadata": {
        "num_run": 27
      },
      "tags": [
        "benchmark",
        "phi4:latest",
        "tpu_sketch_generator"
      ],
      "latency": 146.621,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-20-09-58-885273_chatcmpl-797c5b3b-ccee-4e19-a4d3-73397f3ab046",
          "traceId": "006d4f73",
          "type": "GENERATION",
          "name": "006d_tpusg_gen_attempt#1",
          "startTime": "2025-08-18T17:09:58.885000+00:00",
          "endTime": "2025-08-18T17:10:15.276000+00:00",
          "model": "phi4:latest",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 16391.0,
          "costDetails": {},
          "completionTokens": 780,
          "totalTokens": 2586,
          "environment": "default",
          "promptTokens": 1806,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-20-10-23-343126_chatcmpl-11d8d6ab-2ffb-45fd-920a-ad5226e7ea1e",
          "traceId": "006d4f73",
          "type": "GENERATION",
          "name": "006d_tpusg_gen_attempt#2",
          "startTime": "2025-08-18T17:10:23.343000+00:00",
          "endTime": "2025-08-18T17:10:45.553000+00:00",
          "model": "phi4:latest",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 22210.0,
          "costDetails": {},
          "completionTokens": 1086,
          "totalTokens": 3134,
          "environment": "default",
          "promptTokens": 2048,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-20-10-53-840444_chatcmpl-95ae6f6b-ba6a-4d75-a61a-e14e3aea0517",
          "traceId": "006d4f73",
          "type": "GENERATION",
          "name": "006d_tpusg_gen_attempt#3",
          "startTime": "2025-08-18T17:10:53.840000+00:00",
          "endTime": "2025-08-18T17:11:15.053000+00:00",
          "model": "phi4:latest",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 21213.0,
          "costDetails": {},
          "completionTokens": 1035,
          "totalTokens": 3083,
          "environment": "default",
          "promptTokens": 2048,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-20-11-23-481999_chatcmpl-dbbbef3d-a8a7-45d4-abfb-aa44184d7d59",
          "traceId": "006d4f73",
          "type": "GENERATION",
          "name": "006d_tpusg_gen_attempt#4",
          "startTime": "2025-08-18T17:11:23.481000+00:00",
          "endTime": "2025-08-18T17:11:45.175000+00:00",
          "model": "phi4:latest",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 21694.0,
          "costDetails": {},
          "completionTokens": 1058,
          "totalTokens": 3106,
          "environment": "default",
          "promptTokens": 2048,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-20-11-53-476589_chatcmpl-656ffb86-16ea-49f6-8200-5933eceb470e",
          "traceId": "006d4f73",
          "type": "GENERATION",
          "name": "006d_tpusg_gen_attempt#5",
          "startTime": "2025-08-18T17:11:53.476000+00:00",
          "endTime": "2025-08-18T17:12:17.108000+00:00",
          "model": "phi4:latest",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 23632.0,
          "costDetails": {},
          "completionTokens": 1166,
          "totalTokens": 3214,
          "environment": "default",
          "promptTokens": 2048,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "ef222939-3ff6-4d40-86e3-c6d33f97d21c",
          "traceId": "006d4f73",
          "type": "SPAN",
          "name": "error_00_tpusg_failure_signal_tpu_sketch_generator",
          "startTime": "2025-08-18T17:12:25.506000+00:00",
          "level": "ERROR",
          "statusMessage": "Failed: Max retries reached with error. Last error: Traceback (most recent call last):\n  File \"script_1f5bbfca_1755537137.py\", line 92, in <module>\n    main()\n  File \"script_1f5bbfca_1755537137.py\", line 79, in main\n    detections = postprocess(output_data, frame.shape[0], frame.shape[1])\n  File \"script_1f5bbfca_1755537137.py\", line 44, in postprocess\n    num_detections = int(output_data[0][0])  # Correct access to the scalar value for number of detections\nTypeError: only size-1 arrays can be converted to Python scalars.",
          "calculatedTotalCost": 0.0,
          "costDetails": {},
          "completionTokens": 0,
          "totalTokens": 0,
          "environment": "default",
          "promptTokens": 0,
          "endTime": null,
          "model": null,
          "modelParameters": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null,
          "latency": null
        }
      ],
      "environment": "default",
      "createdAt": "2025-08-18T17:09:59.403Z",
      "updatedAt": "2025-08-18T17:12:31.088Z"
    },
    {
      "id": "9f9be779",
      "timestamp": "2025-08-18T17:08:43.373000+00:00",
      "name": "9f9b_tpu_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import cv2\nimport numpy as np\nfrom tflite_runtime.interpreter import Interpreter, load_delegate\n\n# Configuration parameters\nmodel_path  = \"/home/mendel/tinyml_autopilot/models/edgetpu_detect.tflite\"\nlabel_path  = \"/home/mendel/tinyml_autopilot/models/labelmap.txt\"\ninput_path  = \"/home/mendel/tinyml_autopilot/data//sheeps.mp4\"\noutput_path  = \"/home/mendel/tinyml_autopilot/results/sheeps_detections.mp4\"\nconfidence_threshold  = 0.5\n\n# Phase 1: Setup\ndef load_labels(label_file):\n    with open(label_file, 'r') as f:\n        return [line.strip() for line in f.readlines()]\n\ntry:\n    interpreter = Interpreter(model_path=model_path,\n                              experimental_delegates=[load_delegate('libedgetpu.so.1.0')])\nexcept ValueError:\n    interpreter = Interpreter(model_path=model_path,\n                              experimental_delegates=[load_delegate('/usr/lib/aarch64-linux-gnu/libedgetpu.so.1.0')])\n\ninterpreter.allocate_tensors()\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\n# Load labels\nlabels = load_labels(label_path)\n\n# Phase 2: Input Acquisition & Preprocessing Loop\ncap = cv2.VideoCapture(input_path)\nfps = cap.get(cv2.CAP_PROP_FPS)\nframe_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\nframe_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n\nout = cv2.VideoWriter(output_path, cv2.VideoWriter_fourcc(*'mp4v'), fps, (frame_width, frame_height))\n\nfloating_model = (input_details[0]['dtype'] == np.float32)\n\nwhile cap.isOpened():\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    # Preprocess the input image\n    input_image = cv2.resize(frame, (input_details[0]['shape'][2], input_details[0]['shape'][1]))\n    input_data = np.expand_dims(input_image, axis=0)\n\n    if floating_model:\n        input_data = (np.float32(input_data) - 127.5) / 127.5\n\n    # Phase 3: Inference\n    interpreter.set_tensor(input_details[0]['index'], input_data)\n    interpreter.invoke()\n\n    # Phase 4: Output Interpretation & Handling Loop\n    boxes = interpreter.get_tensor(output_details[0]['index'])[0]  # Bounding box coordinates of detected objects\n    classes = interpreter.get_tensor(output_details[1]['index'])[0]  # Class index of detected objects\n    scores = interpreter.get_tensor(output_details[2]['index'])[0]  # Confidence of detected objects\n\n    for i in range(len(scores)):\n        if scores[i] >= confidence_threshold:\n            ymin, xmin, ymax, xmax = boxes[i]\n            (left, right, top, bottom) = (xmin * frame_width, xmax * frame_width,\n                                          ymin * frame_height, ymax * frame_height)\n\n            # Draw bounding box and label on the image\n            cv2.rectangle(frame, (int(left), int(top)), (int(right), int(bottom)), (0, 255, 0), 2)\n            text = f'{labels[int(classes[i])]}: {int(scores[i] * 100)}%'\n            cv2.putText(frame, text, (int(left), int(top) - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n\n    # Write the frame with detections\n    out.write(frame)\n\n# Phase 5: Cleanup\ncap.release()\nout.release()\ncv2.destroyAllWindows()"
      },
      "session_id": "phi4_7854_tpusg_batch",
      "metadata": {
        "num_run": 26
      },
      "tags": [
        "benchmark",
        "phi4:latest",
        "tpu_sketch_generator"
      ],
      "latency": 17.135,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-20-08-43-374980_chatcmpl-d8a4f5d5-22ef-415f-a5a8-01d78528123c",
          "traceId": "9f9be779",
          "type": "GENERATION",
          "name": "9f9b_tpusg_gen_attempt#1",
          "startTime": "2025-08-18T17:08:43.374000+00:00",
          "endTime": "2025-08-18T17:09:00.509000+00:00",
          "model": "phi4:latest",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 17135.0,
          "costDetails": {},
          "completionTokens": 816,
          "totalTokens": 2622,
          "environment": "default",
          "promptTokens": 1806,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "environment": "default",
      "createdAt": "2025-08-18T17:08:43.894Z",
      "updatedAt": "2025-08-18T17:09:58.562Z"
    },
    {
      "id": "73cf8b27",
      "timestamp": "2025-08-18T17:07:30.797000+00:00",
      "name": "73cf_tpu_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import cv2\nimport numpy as np\nfrom tflite_runtime.interpreter import Interpreter, load_delegate\n\n# Configuration Parameters\nmodel_path  = \"/home/mendel/tinyml_autopilot/models/edgetpu_detect.tflite\"\nlabel_path  = \"/home/mendel/tinyml_autopilot/models/labelmap.txt\"\ninput_path  = \"/home/mendel/tinyml_autopilot/data//sheeps.mp4\"\noutput_path  = \"/home/mendel/tinyml_autopilot/results/sheeps_detections.mp4\"\nconfidence_threshold  = 0.5\n\n# Phase 1: Setup\ndef load_labels(path):\n    with open(path, 'r') as file:\n        return [line.strip() for line in file.readlines()]\n\ntry:\n    labels = load_labels(label_path)\nexcept FileNotFoundError:\n    print(\"Label path not found. Proceeding without label map.\")\n    labels = []\n\n# Load interpreter with EdgeTPU\ntry:\n    interpreter = Interpreter(\n        model_path=model_path,\n        experimental_delegates=[load_delegate('libedgetpu.so.1.0')]\n    )\nexcept RuntimeError as e:\n    print(f\"Failed to load Edge TPU delegate: {e}\")\n    try:\n        interpreter = Interpreter(\n            model_path=model_path,\n            experimental_delegates=[load_delegate('/usr/lib/aarch64-linux-gnu/libedgetpu.so.1.0')]\n        )\n    except Exception as e:\n        raise RuntimeError(\"Unable to load the TensorFlow Lite model with EdgeTPU.\") from e\n\ninterpreter.allocate_tensors()\n\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\nfloating_model = input_details[0]['dtype'] == np.float32\n\n# Phase 2: Input Acquisition & Preprocessing Loop\ncap = cv2.VideoCapture(input_path)\nfourcc = cv2.VideoWriter_fourcc(*'XVID')\nout = cv2.VideoWriter(output_path, fourcc, 20.0, (640, 480))\n\nwhile cap.isOpened():\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    # Preprocess the input image\n    frame_resized = cv2.resize(frame, (300, 300))\n    input_data = np.expand_dims(frame_resized, axis=0)\n    \n    if floating_model:\n        input_data = (np.float32(input_data) - 127.5) / 127.5\n\n    # Phase 3: Inference\n    interpreter.set_tensor(input_details[0]['index'], input_data)\n    interpreter.invoke()\n\n    # Phase 4: Output Interpretation & Handling Loop\n    boxes = interpreter.get_tensor(output_details[0]['index'])[0]  # Bounding box coordinates\n    classes = interpreter.get_tensor(output_details[1]['index'])[0]  # Class index\n    scores = interpreter.get_tensor(output_details[2]['index'])[0]  # Confidence\n\n    for i in range(len(scores)):\n        if scores[i] >= confidence_threshold:\n            ymin, xmin, ymax, xmax = boxes[i]\n            (left, right, top, bottom) = (xmin * frame.shape[1], xmax * frame.shape[1],\n                                          ymin * frame.shape[0], ymax * frame.shape[0])\n            \n            # Draw bounding box and label on the image\n            cv2.rectangle(frame, (int(left), int(top)), (int(right), int(bottom)), (10, 255, 0), 2)\n            if labels:\n                object_name = labels[int(classes[i])]\n            else:\n                object_name = f'ID {int(classes[i])}'\n            \n            label = f'{object_name}: {int(scores[i] * 100)}%'\n            cv2.putText(frame, label, (int(left), int(top) - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 0), 1)\n\n    # Write the frame with detections\n    out.write(frame)\n    \n    # Display the resulting frame\n    cv2.imshow('Object detector', frame)\n    if cv2.waitKey(1) & 0xFF == ord('q'):\n        break\n\n# Phase 5: Cleanup\ncap.release()\nout.release()\ncv2.destroyAllWindows()"
      },
      "session_id": "phi4_7854_tpusg_batch",
      "metadata": {
        "num_run": 25
      },
      "tags": [
        "benchmark",
        "phi4:latest",
        "tpu_sketch_generator"
      ],
      "latency": 18.078,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-20-07-30-799733_chatcmpl-05ec5dcb-54ee-4153-a677-d6ae944588a1",
          "traceId": "73cf8b27",
          "type": "GENERATION",
          "name": "73cf_tpusg_gen_attempt#1",
          "startTime": "2025-08-18T17:07:30.799000+00:00",
          "endTime": "2025-08-18T17:07:48.877000+00:00",
          "model": "phi4:latest",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 18078.0,
          "costDetails": {},
          "completionTokens": 869,
          "totalTokens": 2675,
          "environment": "default",
          "promptTokens": 1806,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "environment": "default",
      "createdAt": "2025-08-18T17:07:31.320Z",
      "updatedAt": "2025-08-18T17:08:43.006Z"
    },
    {
      "id": "beb3229a",
      "timestamp": "2025-08-18T17:06:12.776000+00:00",
      "name": "beb3_tpu_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import cv2\nimport numpy as np\nfrom tflite_runtime.interpreter import Interpreter, load_delegate\n\n# Configuration Parameters\nmodel_path  = \"/home/mendel/tinyml_autopilot/models/edgetpu_detect.tflite\"\nlabel_path  = \"/home/mendel/tinyml_autopilot/models/labelmap.txt\"\ninput_path  = \"/home/mendel/tinyml_autopilot/data//sheeps.mp4\"\noutput_path  = \"/home/mendel/tinyml_autopilot/results/sheeps_detections.mp4\"\nconfidence_threshold  = 0.5\n\n# Load Labels\nwith open(label_path, 'r') as f:\n    labels = [line.strip() for line in f.readlines()]\n\ntry:\n    interpreter = Interpreter(\n        model_path=model_path,\n        experimental_delegates=[load_delegate('libedgetpu.so.1.0')]\n    )\nexcept ValueError:\n    interpreter = Interpreter(\n        model_path=model_path,\n        experimental_delegates=[load_delegate('/usr/lib/aarch64-linux-gnu/libedgetpu.so.1.0')]\n    )\n\ninterpreter.allocate_tensors()\n\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\n# Read video\ncap = cv2.VideoCapture(input_path)\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\nout = cv2.VideoWriter(output_path, fourcc, 30.0, (int(cap.get(3)), int(cap.get(4))))\n\nwhile cap.isOpened():\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    # Preprocess the input image\n    height, width, _ = frame.shape\n    input_frame = cv2.resize(frame, (input_details[0]['shape'][2], input_details[0]['shape'][1]))\n    input_data = np.expand_dims(input_frame, axis=0)\n\n    # Check if model uses floating point inputs\n    floating_model = (input_details[0]['dtype'] == np.float32)\n    if floating_model:\n        input_data = (np.float32(input_data) - 127.5) / 127.5\n\n    # Run inference\n    interpreter.set_tensor(input_details[0]['index'], input_data)\n    interpreter.invoke()\n\n    # Get output details\n    boxes = interpreter.get_tensor(output_details[0]['index'])[0]  # Bounding box coordinates of detected objects\n    classes = interpreter.get_tensor(output_details[1]['index'])[0]  # Class index of detected objects\n    scores = interpreter.get_tensor(output_details[2]['index'])[0]  # Confidence of detected objects\n\n    for i in range(len(scores)):\n        if scores[i] >= confidence_threshold:\n            ymin, xmin, ymax, xmax = boxes[i]\n            (left, right, top, bottom) = (xmin * width, xmax * width, ymin * height, ymax * height)\n\n            # Draw rectangle and label on the image\n            cv2.rectangle(frame, (int(left), int(top)), (int(right), int(bottom)), (0, 255, 0), 2)\n            object_name = labels[int(classes[i])]\n            label = f'{object_name}: {scores[i]:.2f}'\n            label_size, base_line = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.7, 2)\n            top = max(top, label_size[1])\n            cv2.rectangle(frame, (int(left), int(top - round(1.5 * label_size[1]))),\n                          (int(left + round(1.5 * label_size[0])), int(top + base_line)), (255, 255, 255), cv2.FILLED)\n            cv2.putText(frame, label, (int(left), int(top + base_line)),\n                        cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 0), 2)\n\n    # Write frame to output video\n    out.write(frame)\n\n# Release resources\ncap.release()\nout.release()\ncv2.destroyAllWindows()"
      },
      "session_id": "phi4_7854_tpusg_batch",
      "metadata": {
        "num_run": 24
      },
      "tags": [
        "benchmark",
        "phi4:latest",
        "tpu_sketch_generator"
      ],
      "latency": 18.845,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-20-06-12-778105_chatcmpl-d418ddf4-42f7-40e9-a92e-8cedb9530e61",
          "traceId": "beb3229a",
          "type": "GENERATION",
          "name": "beb3_tpusg_gen_attempt#1",
          "startTime": "2025-08-18T17:06:12.778000+00:00",
          "endTime": "2025-08-18T17:06:31.623000+00:00",
          "model": "phi4:latest",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 18845.0,
          "costDetails": {},
          "completionTokens": 904,
          "totalTokens": 2710,
          "environment": "default",
          "promptTokens": 1806,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "environment": "default",
      "createdAt": "2025-08-18T17:06:13.297Z",
      "updatedAt": "2025-08-18T17:07:29.955Z"
    },
    {
      "id": "fdf6ab4e",
      "timestamp": "2025-08-18T17:02:52.140000+00:00",
      "name": "fdf6_tpu_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import os\nimport cv2\nimport numpy as np\nfrom tflite_runtime.interpreter import Interpreter, load_delegate\n\n# CONFIGURATION PARAMETERS\nmodel_path      = \"/home/mendel/tinyml_autopilot/models/edgetpu_detect.tflite\"\nlabel_path      = \"/home/mendel/tinyml_autopilot/models/labelmap.txt\"\ninput_path      = \"/home/mendel/tinyml_autopilot/data//sheeps.mp4\"\noutput_path     = \"/home/mendel/tinyml_autopilot/results/sheeps_detections.mp4\"\nconfidence_threshold    = 0.5\n\n# Phase 1: Setup\ndef load_labels(label_file):\n    with open(label_file, 'r', encoding='utf-8') as file:\n        return [line.strip() for line in file.readlines()]\n\ntry:\n    interpreter = Interpreter(\n        model_path=model_path,\n        experimental_delegates=[load_delegate('libedgetpu.so.1.0')]\n    )\nexcept ValueError:\n    interpreter = Interpreter(\n        model_path=model_path,\n        experimental_delegates=[load_delegate('/usr/lib/aarch64-linux-gnu/libedgetpu.so.1.0')]\n    )\n\ninterpreter.allocate_tensors()\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\nlabels = load_labels(label_path)\nnum_classes = len(labels)\n\n# Preprocess input image\ndef preprocess_image(image):\n    input_shape = input_details[0]['shape']\n    image = cv2.resize(image, (input_shape[1], input_shape[2]))\n    image = np.expand_dims(image, axis=0).astype(input_details[0]['dtype'])\n    if input_details[0]['dtype'] == np.float32:\n        image = (image - 127.5) / 127.5\n    return image\n\n# Phase 4: Output Interpretation & Handling Loop\ndef postprocess(output_data, frame, width, height):\n    # Extract number of detections as a scalar value using np.squeeze\n    num_detections = int(np.squeeze(output_data[0]))\n\n    boxes = output_data[1]\n    classes = output_data[2].astype(np.int64)\n    scores = np.squeeze(output_data[3])  # Ensure scores are squeezed\n\n    for i in range(num_detections):\n        if scores[i] >= confidence_threshold:\n            ymin, xmin, ymax, xmax = boxes[i]\n            left, right, top, bottom = (xmin * width), (xmax * width), (ymin * height), (ymax * height)\n            label = labels[classes[i]]\n\n            # Draw bounding box and label\n            cv2.rectangle(frame, (int(left), int(top)), (int(right), int(bottom)), (0, 255, 0), 2)\n            cv2.putText(frame, f'{label}: {scores[i]:.2f}', (int(left), int(top) - 10),\n                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n\n# Phase 2: Input Acquisition & Preprocessing Loop\ncap = cv2.VideoCapture(input_path)\nwidth = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))  # Convert float to int using int()\nheight = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\nout = cv2.VideoWriter(output_path, cv2.VideoWriter_fourcc(*'mp4v'), 20.0, (width, height))\n\nwhile cap.isOpened():\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    input_data = preprocess_image(frame)\n\n    # Phase 3: Inference\n    interpreter.set_tensor(input_details[0]['index'], input_data)\n    interpreter.invoke()\n\n    # Phase 4.1: Get Output Tensor(s)\n    boxes = np.squeeze(interpreter.get_tensor(output_details[0]['index']))\n    classes = np.squeeze(interpreter.get_tensor(output_details[1]['index'])).astype(np.int64)\n    scores = np.squeeze(interpreter.get_tensor(output_details[2]['index']))\n\n    # Prepare output data, ensuring num_detections is squeezed\n    num_detections = int(np.squeeze(interpreter.get_tensor(output_details[3]['index'])))\n\n    # Phase 4.2: Post-process and draw results\n    output_data = [num_detections, boxes, classes, scores]\n    postprocess(output_data, frame, width, height)\n\n    out.write(frame)\n\ncap.release()\nout.release()"
      },
      "session_id": "phi4_7854_tpusg_batch",
      "metadata": {
        "num_run": 23
      },
      "tags": [
        "benchmark",
        "phi4:latest",
        "tpu_sketch_generator"
      ],
      "latency": 141.765,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-20-02-52-141730_chatcmpl-8def70bc-1de3-4a4d-86be-4bb8d77c6574",
          "traceId": "fdf6ab4e",
          "type": "GENERATION",
          "name": "fdf6_tpusg_gen_attempt#1",
          "startTime": "2025-08-18T17:02:52.141000+00:00",
          "endTime": "2025-08-18T17:03:10.136000+00:00",
          "model": "phi4:latest",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 17995.0,
          "costDetails": {},
          "completionTokens": 865,
          "totalTokens": 2671,
          "environment": "default",
          "promptTokens": 1806,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-20-03-17-998149_chatcmpl-9807a400-410b-49d1-ab42-29edb7669974",
          "traceId": "fdf6ab4e",
          "type": "GENERATION",
          "name": "fdf6_tpusg_gen_attempt#2",
          "startTime": "2025-08-18T17:03:17.998000+00:00",
          "endTime": "2025-08-18T17:03:40.132000+00:00",
          "model": "phi4:latest",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 22134.0,
          "costDetails": {},
          "completionTokens": 1084,
          "totalTokens": 3132,
          "environment": "default",
          "promptTokens": 2048,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-20-03-48-430584_chatcmpl-eda4e4e1-b7cd-4b40-aef6-3b58ef700b69",
          "traceId": "fdf6ab4e",
          "type": "GENERATION",
          "name": "fdf6_tpusg_gen_attempt#3",
          "startTime": "2025-08-18T17:03:48.430000+00:00",
          "endTime": "2025-08-18T17:04:11.117000+00:00",
          "model": "phi4:latest",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 22687.0,
          "costDetails": {},
          "completionTokens": 1114,
          "totalTokens": 3162,
          "environment": "default",
          "promptTokens": 2048,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-20-04-19-481897_chatcmpl-26c88437-c04e-4ee6-add2-5170aae4653d",
          "traceId": "fdf6ab4e",
          "type": "GENERATION",
          "name": "fdf6_tpusg_gen_attempt#4",
          "startTime": "2025-08-18T17:04:19.481000+00:00",
          "endTime": "2025-08-18T17:04:42.429000+00:00",
          "model": "phi4:latest",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 22948.0,
          "costDetails": {},
          "completionTokens": 1127,
          "totalTokens": 3175,
          "environment": "default",
          "promptTokens": 2048,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-20-04-50-718510_chatcmpl-f6cf38e1-2ed5-4fe3-930a-911a4cdd1d52",
          "traceId": "fdf6ab4e",
          "type": "GENERATION",
          "name": "fdf6_tpusg_gen_attempt#5",
          "startTime": "2025-08-18T17:04:50.718000+00:00",
          "endTime": "2025-08-18T17:05:13.906000+00:00",
          "model": "phi4:latest",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 23188.0,
          "costDetails": {},
          "completionTokens": 1142,
          "totalTokens": 3190,
          "environment": "default",
          "promptTokens": 2048,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "environment": "default",
      "createdAt": "2025-08-18T17:02:52.661Z",
      "updatedAt": "2025-08-18T17:06:11.898Z"
    },
    {
      "id": "8d315ff9",
      "timestamp": "2025-08-18T17:01:35.614000+00:00",
      "name": "8d31_tpu_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import cv2\nimport numpy as np\nfrom tflite_runtime.interpreter import Interpreter, load_delegate\n\n# CONFIGURATION PARAMETERS\nmodel_path  = \"/home/mendel/tinyml_autopilot/models/edgetpu_detect.tflite\"\nlabel_path  = \"/home/mendel/tinyml_autopilot/models/labelmap.txt\"\ninput_path  = \"/home/mendel/tinyml_autopilot/data//sheeps.mp4\"\noutput_path  = \"/home/mendel/tinyml_autopilot/results/sheeps_detections.mp4\"\nconfidence_threshold  = 0.5\n\n# Phase 1: Setup\ndef load_labels(label_file):\n    with open(label_file, 'r') as f:\n        return [line.strip() for line in f.readlines()]\n\nlabels = load_labels(label_path)\n\ntry:\n    interpreter = Interpreter(\n        model_path=model_path,\n        experimental_delegates=[load_delegate('libedgetpu.so.1.0')]\n    )\nexcept ValueError:\n    interpreter = Interpreter(\n        model_path=model_path,\n        experimental_delegates=[load_delegate('/usr/lib/aarch64-linux-gnu/libedgetpu.so.1.0')]\n    )\n\ninterpreter.allocate_tensors()\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\nfloating_model = input_details[0]['dtype'] == np.float32\n\n# Phase 2: Input Acquisition & Preprocessing Loop\ncap = cv2.VideoCapture(input_path)\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\nout = cv2.VideoWriter(output_path, fourcc, 30.0, (int(cap.get(3)), int(cap.get(4))))\n\nwhile cap.isOpened():\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    # Preprocess\n    input_shape = input_details[0]['shape']\n    img = cv2.resize(frame, (input_shape[1], input_shape[2]))\n    img = np.expand_dims(img, axis=0)\n\n    if floating_model:\n        img = (np.float32(img) - 127.5) / 127.5\n\n    # Phase 3: Inference\n    interpreter.set_tensor(input_details[0]['index'], img)\n    interpreter.invoke()\n\n    # Phase 4: Output Interpretation & Handling Loop\n    boxes = interpreter.get_tensor(output_details[0]['index'])[0]  # Bounding box coordinates\n    classes = interpreter.get_tensor(output_details[1]['index'])[0]  # Class index\n    scores = interpreter.get_tensor(output_details[2]['index'])[0]  # Confidence\n\n    for i in range(len(scores)):\n        if scores[i] >= confidence_threshold:\n            ymin, xmin, ymax, xmax = boxes[i]\n            (left, right, top, bottom) = (xmin * frame.shape[1], xmax * frame.shape[1],\n                                          ymin * frame.shape[0], ymax * frame.shape[0])\n            \n            left, right, top, bottom = int(left), int(right), int(top), int(bottom)\n            cv2.rectangle(frame, (left, top), (right, bottom), (255, 0, 0), 2)\n            label = f\"{labels[int(classes[i])]}: {int(scores[i] * 100)}%\"\n            cv2.putText(frame, label, (left, top - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 0, 0), 1)\n\n    out.write(frame)\n    cv2.imshow('Frame', frame)\n\n    if cv2.waitKey(1) & 0xFF == ord('q'):\n        break\n\n# Phase 5: Cleanup\ncap.release()\nout.release()\ncv2.destroyAllWindows()"
      },
      "session_id": "phi4_7854_tpusg_batch",
      "metadata": {
        "num_run": 22
      },
      "tags": [
        "benchmark",
        "phi4:latest",
        "tpu_sketch_generator"
      ],
      "latency": 17.53,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-20-01-35-615779_chatcmpl-fc8788bc-147b-4728-ae2a-e1bf03e45bbd",
          "traceId": "8d315ff9",
          "type": "GENERATION",
          "name": "8d31_tpusg_gen_attempt#1",
          "startTime": "2025-08-18T17:01:35.615000+00:00",
          "endTime": "2025-08-18T17:01:53.145000+00:00",
          "model": "phi4:latest",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 17530.0,
          "costDetails": {},
          "completionTokens": 828,
          "totalTokens": 2634,
          "environment": "default",
          "promptTokens": 1806,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "environment": "default",
      "createdAt": "2025-08-18T17:01:36.137Z",
      "updatedAt": "2025-08-18T17:02:52.006Z"
    },
    {
      "id": "55620860",
      "timestamp": "2025-08-18T16:59:45.093000+00:00",
      "name": "5562_tpu_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import os\nimport cv2\nimport numpy as np\nfrom tflite_runtime.interpreter import Interpreter, load_delegate\n\n# Configuration Parameters\nmodel_path  = \"/home/mendel/tinyml_autopilot/models/edgetpu_detect.tflite\"\nlabel_path  = \"/home/mendel/tinyml_autopilot/models/labelmap.txt\"\ninput_path  = \"/home/mendel/tinyml_autopilot/data//sheeps.mp4\"\noutput_path  = \"/home/mendel/tinyml_autopilot/results/sheeps_detections.mp4\"\nconfidence_threshold  = 0.5\n\n# Phase 1: Setup\ndef load_labels(label_file):\n    with open(label_file, 'r', encoding='utf-8') as file:\n        return [line.strip() for line in file.readlines()]\n\nlabels = load_labels(label_path)\n\ntry:\n    interpreter = Interpreter(\n        model_path=model_path,\n        experimental_delegates=[load_delegate('libedgetpu.so.1.0')]\n    )\nexcept ValueError:\n    try:\n        interpreter = Interpreter(\n            model_path=model_path,\n            experimental_delegates=[load_delegate('/usr/lib/aarch64-linux-gnu/libedgetpu.so.1.0')]\n        )\n    except Exception as e:\n        print(f\"Failed to load TFLite model with EdgeTPU: {e}\")\n        exit(1)\n\ninterpreter.allocate_tensors()\n\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\nfloating_model = (input_details[0]['dtype'] == np.float32)\nheight, width = input_details[0]['shape'][1], input_details[0]['shape'][2]\n\n# Phase 4.3: Post-processing function\ndef filter_detections(output_data):\n    boxes = output_data[0][0]\n    classes = output_data[1][0].astype(int)\n    scores = output_data[2][0]\n    \n    results = []\n    for i in range(len(scores)):\n        if scores[i] >= confidence_threshold:\n            ymin, xmin, ymax, xmax = boxes[i]\n            left, right, top, bottom = int(xmin * width), int(xmax * width), int(ymin * height), int(ymax * height)\n            results.append((int(classes[i]), float(scores[i]), left, top, right, bottom))\n    return results\n\n# Phase 2: Input Acquisition & Preprocessing Loop\ncap = cv2.VideoCapture(input_path)\n\nif not cap.isOpened():\n    print(\"Error opening video stream or file\")\n    exit(1)\n\n# Initialize VideoWriter before the loop\nframe_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\nframe_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\nout = cv2.VideoWriter(output_path, fourcc, 30.0, (frame_width, frame_height))\n\nwhile True:\n    ret, frame = cap.read()\n    if not ret:\n        break\n    \n    # Resize and normalize the frame to model input size\n    img_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n    frame_resized = cv2.resize(img_rgb, (width, height))\n    \n    input_data = np.expand_dims(frame_resized, axis=0)\n\n    if floating_model:\n        input_data = (np.float32(input_data) - 127.5) / 127.5\n\n    # Phase 3: Inference\n    interpreter.set_tensor(input_details[0]['index'], input_data)\n    interpreter.invoke()\n\n    # Phase 4.1: Get Output Tensor(s)\n    output_data = [\n        interpreter.get_tensor(output_details[i]['index']) for i in range(len(output_details))\n    ]\n\n    # Phase 4.2 & 4.3: Interpret and Post-process Results\n    detections = filter_detections(output_data)\n\n    # Draw bounding boxes on the frame\n    for detection in detections:\n        class_id, score, left, top, right, bottom = detection\n        label = f\"{labels[class_id]}: {score:.2f}\"\n        \n        cv2.rectangle(frame, (left, top), (right, bottom), (0, 255, 0), 2)\n        cv2.putText(frame, label, (left, top - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n\n    # Phase 4.4: Handle Output\n    out.write(frame)\n    \n    if cv2.waitKey(1) & 0xFF == ord('q'):  # Exit on pressing 'q'\n        break\n\n# Release resources\ncap.release()\nout.release()\ncv2.destroyAllWindows()"
      },
      "session_id": "phi4_7854_tpusg_batch",
      "metadata": {
        "num_run": 21
      },
      "tags": [
        "benchmark",
        "phi4:latest",
        "tpu_sketch_generator"
      ],
      "latency": 52.044,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-19-59-45-095360_chatcmpl-4bdd59a1-fc7e-4ea6-ad67-a9fc55119c3d",
          "traceId": "55620860",
          "type": "GENERATION",
          "name": "5562_tpusg_gen_attempt#1",
          "startTime": "2025-08-18T16:59:45.095000+00:00",
          "endTime": "2025-08-18T17:00:05.497000+00:00",
          "model": "phi4:latest",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 20402.0,
          "costDetails": {},
          "completionTokens": 991,
          "totalTokens": 2797,
          "environment": "default",
          "promptTokens": 1806,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-20-00-13-783651_chatcmpl-70775c7f-e3dd-4e95-8b0a-4391a20d958e",
          "traceId": "55620860",
          "type": "GENERATION",
          "name": "5562_tpusg_gen_attempt#2",
          "startTime": "2025-08-18T17:00:13.783000+00:00",
          "endTime": "2025-08-18T17:00:37.139000+00:00",
          "model": "phi4:latest",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 23356.0,
          "costDetails": {},
          "completionTokens": 1149,
          "totalTokens": 3197,
          "environment": "default",
          "promptTokens": 2048,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "environment": "default",
      "createdAt": "2025-08-18T16:59:45.616Z",
      "updatedAt": "2025-08-18T17:01:35.342Z"
    },
    {
      "id": "3cc6623d",
      "timestamp": "2025-08-18T16:58:30.597000+00:00",
      "name": "3cc6_tpu_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import cv2\nimport numpy as np\nfrom tflite_runtime.interpreter import Interpreter, load_delegate\n\n# Configuration parameters\nmodel_path  = \"/home/mendel/tinyml_autopilot/models/edgetpu_detect.tflite\"\nlabel_path  = \"/home/mendel/tinyml_autopilot/models/labelmap.txt\"\ninput_path  = \"/home/mendel/tinyml_autopilot/data//sheeps.mp4\"\noutput_path  = \"/home/mendel/tinyml_autopilot/results/sheeps_detections.mp4\"\nconfidence_threshold  = 0.5\n\n# Load labels\nwith open(label_path, 'r') as f:\n    labels = [line.strip() for line in f.readlines()]\n\n# Load the Edge TPU model\ntry:\n    interpreter = Interpreter(\n        model_path=model_path,\n        experimental_delegates=[load_delegate('libedgetpu.so.1.0')]\n    )\nexcept ValueError:\n    interpreter = Interpreter(\n        model_path=model_path,\n        experimental_delegates=[load_delegate('/usr/lib/aarch64-linux-gnu/libedgetpu.so.1.0')]\n    )\n\ninterpreter.allocate_tensors()\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\n# Get input image size\n_, input_height, input_width, _ = input_details[0]['shape']\n\n# Open the video file or stream\ncap = cv2.VideoCapture(input_path)\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\nout = cv2.VideoWriter(output_path, fourcc, 30.0, (int(cap.get(3)), int(cap.get(4))))\n\nwhile cap.isOpened():\n    ret, frame = cap.read()\n    \n    if not ret:\n        break\n    \n    # Preprocess the image\n    frame_resized = cv2.resize(frame, (input_width, input_height))\n    input_data = np.expand_dims(frame_resized, axis=0)\n\n    # Set the tensor and run inference\n    interpreter.set_tensor(input_details[0]['index'], input_data)\n    interpreter.invoke()\n\n    # Get output details\n    boxes = interpreter.get_tensor(output_details[0]['index'])[0]  # Bounding box coordinates of detected objects\n    classes = interpreter.get_tensor(output_details[1]['index'])[0]  # Class index of detected objects\n    scores = interpreter.get_tensor(output_details[2]['index'])[0]  # Confidence of detected objects\n    \n    for i in range(len(scores)):\n        if scores[i] >= confidence_threshold:\n            ymin, xmin, ymax, xmax = boxes[i]\n            (left, right, top, bottom) = (xmin * frame.shape[1], xmax * frame.shape[1],\n                                          ymin * frame.shape[0], ymax * frame.shape[0])\n            \n            # Draw bounding box and label on the image\n            cv2.rectangle(frame, (int(left), int(top)), (int(right), int(bottom)), (0, 255, 0), 2)\n            label = f\"{labels[int(classes[i])]}: {int(scores[i]*100)}%\"\n            cv2.putText(frame, label, (int(left), int(top-5)), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n\n    # Write the frame with bounding boxes\n    out.write(frame)\n    \n    # Display the frame for debugging purposes (optional)\n    # cv2.imshow('Frame', frame)\n    # if cv2.waitKey(1) & 0xFF == ord('q'):\n    #     break\n\n# Release resources\ncap.release()\nout.release()\ncv2.destroyAllWindows()"
      },
      "session_id": "phi4_7854_tpusg_batch",
      "metadata": {
        "num_run": 20
      },
      "tags": [
        "benchmark",
        "phi4:latest",
        "tpu_sketch_generator"
      ],
      "latency": 15.716,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-19-58-30-598824_chatcmpl-8ab352a5-4907-491e-88fe-34cebab450dc",
          "traceId": "3cc6623d",
          "type": "GENERATION",
          "name": "3cc6_tpusg_gen_attempt#1",
          "startTime": "2025-08-18T16:58:30.598000+00:00",
          "endTime": "2025-08-18T16:58:46.314000+00:00",
          "model": "phi4:latest",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 15716.0,
          "costDetails": {},
          "completionTokens": 746,
          "totalTokens": 2552,
          "environment": "default",
          "promptTokens": 1806,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "environment": "default",
      "createdAt": "2025-08-18T16:58:31.118Z",
      "updatedAt": "2025-08-18T16:59:44.370Z"
    },
    {
      "id": "2d663362",
      "timestamp": "2025-08-18T16:57:17.066000+00:00",
      "name": "2d66_tpu_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import cv2\nimport numpy as np\nfrom tflite_runtime.interpreter import Interpreter, load_delegate\n\n# Configuration Parameters\nmodel_path  = \"/home/mendel/tinyml_autopilot/models/edgetpu_detect.tflite\"\nlabel_path  = \"/home/mendel/tinyml_autopilot/models/labelmap.txt\"\ninput_path  = \"/home/mendel/tinyml_autopilot/data//sheeps.mp4\"\noutput_path  = \"/home/mendel/tinyml_autopilot/results/sheeps_detections.mp4\"\nconfidence_threshold  = 0.5\n\n# Phase 1: Setup\ndef load_labels(label_path):\n    with open(label_path, 'r') as f:\n        return [line.strip() for line in f.readlines()]\n\nlabels = load_labels(label_path)\n\ntry:\n    interpreter = Interpreter(\n        model_path=model_path,\n        experimental_delegates=[load_delegate('libedgetpu.so.1.0')]\n    )\nexcept ValueError:\n    interpreter = Interpreter(\n        model_path=model_path,\n        experimental_delegates=[load_delegate('/usr/lib/aarch64-linux-gnu/libedgetpu.so.1.0')]\n    )\n\ninterpreter.allocate_tensors()\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\n# Phase 2: Input Acquisition & Preprocessing Loop\ncap = cv2.VideoCapture(input_path)\nframe_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\nframe_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\nout = cv2.VideoWriter(output_path, cv2.VideoWriter_fourcc(*'mp4v'), 30, (frame_width, frame_height))\n\nwhile cap.isOpened():\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    input_frame = cv2.resize(frame, (input_details[0]['shape'][2], input_details[0]['shape'][1]))\n    input_data = np.expand_dims(input_frame, axis=0).astype(np.uint8)\n\n    # Phase 3: Inference\n    interpreter.set_tensor(input_details[0]['index'], input_data)\n    interpreter.invoke()\n\n    # Phase 4: Output Interpretation & Handling Loop\n    boxes = interpreter.get_tensor(output_details[0]['index'])[0]  # Bounding box coordinates of detected objects\n    classes = interpreter.get_tensor(output_details[1]['index'])[0]  # Class index of detected objects\n    scores = interpreter.get_tensor(output_details[2]['index'])[0]  # Confidence of detected objects\n\n    for i in range(len(scores)):\n        if scores[i] >= confidence_threshold:\n            ymin, xmin, ymax, xmax = boxes[i]\n            (left, right, top, bottom) = (xmin * frame_width, xmax * frame_width,\n                                          ymin * frame_height, ymax * frame_height)\n            cv2.rectangle(frame, (int(left), int(top)), (int(right), int(bottom)), (0, 255, 0), 2)\n            label = f\"{labels[int(classes[i])]}: {int(scores[i]*100)}%\"\n            cv2.putText(frame, label, (int(left), int(top-10)), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n\n    out.write(frame)\n\ncap.release()\nout.release()\ncv2.destroyAllWindows()\n\n# Phase 5: Cleanup"
      },
      "session_id": "phi4_7854_tpusg_batch",
      "metadata": {
        "num_run": 19
      },
      "tags": [
        "benchmark",
        "phi4:latest",
        "tpu_sketch_generator"
      ],
      "latency": 14.696,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-19-57-17-067852_chatcmpl-7a2960d4-b1c8-4dd6-b996-8be8a6fc79aa",
          "traceId": "2d663362",
          "type": "GENERATION",
          "name": "2d66_tpusg_gen_attempt#1",
          "startTime": "2025-08-18T16:57:17.067000+00:00",
          "endTime": "2025-08-18T16:57:31.763000+00:00",
          "model": "phi4:latest",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 14696.0,
          "costDetails": {},
          "completionTokens": 693,
          "totalTokens": 2499,
          "environment": "default",
          "promptTokens": 1806,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "environment": "default",
      "createdAt": "2025-08-18T16:57:17.589Z",
      "updatedAt": "2025-08-18T16:58:29.710Z"
    },
    {
      "id": "f7ee6d7c",
      "timestamp": "2025-08-18T16:56:00.524000+00:00",
      "name": "f7ee_tpu_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import cv2\nimport numpy as np\nfrom tflite_runtime.interpreter import Interpreter, load_delegate\n\n# Configuration Parameters\nmodel_path  = \"/home/mendel/tinyml_autopilot/models/edgetpu_detect.tflite\"\nlabel_path  = \"/home/mendel/tinyml_autopilot/models/labelmap.txt\"\ninput_path  = \"/home/mendel/tinyml_autopilot/data//sheeps.mp4\"\noutput_path  = \"/home/mendel/tinyml_autopilot/results/sheeps_detections.mp4\"\nconfidence_threshold  = 0.5\n\n# Load labels\ndef load_labels(path):\n    with open(path, 'r') as file:\n        return [line.strip() for line in file.readlines()]\n\nlabels = load_labels(label_path)\n\n# Phase 1: Setup\ntry:\n    interpreter = Interpreter(\n        model_path=model_path,\n        experimental_delegates=[load_delegate('libedgetpu.so.1.0')]\n    )\nexcept Exception as e:\n    print(f\"Failed to load EdgeTPU delegate with default path, trying alternative path: {e}\")\n    try:\n        interpreter = Interpreter(\n            model_path=model_path,\n            experimental_delegates=[load_delegate('/usr/lib/aarch64-linux-gnu/libedgetpu.so.1.0')]\n        )\n    except Exception as e:\n        raise RuntimeError(f\"Failed to load EdgeTPU delegate: {e}\")\n\ninterpreter.allocate_tensors()\n\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\n# Phase 2: Input Acquisition & Preprocessing Loop\ncap = cv2.VideoCapture(input_path)\nfourcc = cv2.VideoWriter_fourcc(*'XVID')\nout = cv2.VideoWriter(output_path, fourcc, 20.0, (int(cap.get(3)), int(cap.get(4))))\n\nfloating_model = input_details[0]['dtype'] == np.float32\n\nwhile cap.isOpened():\n    ret, frame = cap.read()\n    if not ret:\n        break\n    \n    # Preprocess the image\n    img_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n    img_resized = cv2.resize(img_rgb, (input_details[0]['shape'][2], input_details[0]['shape'][1]))\n    \n    input_data = np.expand_dims(img_resized, axis=0)\n\n    if floating_model:\n        input_data = (np.float32(input_data) - 127.5) / 127.5\n\n    # Phase 3: Inference\n    interpreter.set_tensor(input_details[0]['index'], input_data)\n    interpreter.invoke()\n\n    # Phase 4: Output Interpretation & Handling Loop\n    boxes = interpreter.get_tensor(output_details[0]['index'])[0] # Bounding box coordinates of detected objects\n    classes = interpreter.get_tensor(output_details[1]['index'])[0] # Class index of detected objects\n    scores = interpreter.get_tensor(output_details[2]['index'])[0] # Confidence of detected objects\n\n    height, width, _ = frame.shape\n    for i in range(len(scores)):\n        if scores[i] > confidence_threshold:\n            ymin, xmin, ymax, xmax = boxes[i]\n            left, right, top, bottom = int(xmin * width), int(xmax * width), int(ymin * height), int(ymax * height)\n            \n            # Draw bounding box and label on the frame\n            cv2.rectangle(frame, (left, top), (right, bottom), (0, 255, 0), 2)\n            label = f\"{labels[int(classes[i])]}: {int(scores[i] * 100)}%\"\n            cv2.putText(frame, label, (left, top - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n\n    # Write the frame with detection boxes\n    out.write(frame)\n\n# Phase 5: Cleanup\ncap.release()\nout.release()\ncv2.destroyAllWindows()"
      },
      "session_id": "phi4_7854_tpusg_batch",
      "metadata": {
        "num_run": 18
      },
      "tags": [
        "benchmark",
        "phi4:latest",
        "tpu_sketch_generator"
      ],
      "latency": 18.645,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-19-56-00-526273_chatcmpl-0b3e0af7-b17e-441b-9d96-8c25565bc91c",
          "traceId": "f7ee6d7c",
          "type": "GENERATION",
          "name": "f7ee_tpusg_gen_attempt#1",
          "startTime": "2025-08-18T16:56:00.526000+00:00",
          "endTime": "2025-08-18T16:56:19.171000+00:00",
          "model": "phi4:latest",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 18645.0,
          "costDetails": {},
          "completionTokens": 900,
          "totalTokens": 2706,
          "environment": "default",
          "promptTokens": 1806,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "environment": "default",
      "createdAt": "2025-08-18T16:56:01.046Z",
      "updatedAt": "2025-08-18T16:57:16.870Z"
    },
    {
      "id": "36b6d2df",
      "timestamp": "2025-08-18T16:54:44.988000+00:00",
      "name": "36b6_tpu_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import cv2\nimport numpy as np\nfrom tflite_runtime.interpreter import Interpreter, load_delegate\n\n# Configuration Parameters\nmodel_path  = \"/home/mendel/tinyml_autopilot/models/edgetpu_detect.tflite\"\nlabel_path  = \"/home/mendel/tinyml_autopilot/models/labelmap.txt\"\ninput_path  = \"/home/mendel/tinyml_autopilot/data//sheeps.mp4\"\noutput_path  = \"/home/mendel/tinyml_autopilot/results/sheeps_detections.mp4\"\nconfidence_threshold  = 0.5\n\n# Phase 1: Setup\ndef load_labels(label_file):\n    with open(label_file, 'r') as file:\n        return [line.strip() for line in file.readlines()]\n\nlabels = load_labels(label_path)\n\ntry:\n    interpreter = Interpreter(\n        model_path=model_path,\n        experimental_delegates=[load_delegate('libedgetpu.so.1.0')]\n    )\nexcept Exception as e:\n    try:\n        interpreter = Interpreter(\n            model_path=model_path,\n            experimental_delegates=[load_delegate('/usr/lib/aarch64-linux-gnu/libedgetpu.so.1.0')]\n        )\n    except Exception as e:\n        raise RuntimeError(\"Failed to load the Edge TPU delegate.\") from e\n\ninterpreter.allocate_tensors()\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\n# Phase 2: Input Acquisition & Preprocessing Loop\ncap = cv2.VideoCapture(input_path)\nframe_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\nframe_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\nout = cv2.VideoWriter(output_path, fourcc, 30.0, (frame_width, frame_height))\n\nfloating_model = (input_details[0]['dtype'] == np.float32)\n\nwhile cap.isOpened():\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    # Preprocess\n    input_frame = cv2.resize(frame, (300, 300))\n    input_data = np.expand_dims(input_frame, axis=0)\n    \n    if floating_model:\n        input_data = (np.float32(input_data) - 127.5) / 127.5\n\n    interpreter.set_tensor(input_details[0]['index'], input_data)\n\n    # Phase 3: Inference\n    interpreter.invoke()\n\n    # Phase 4: Output Interpretation & Handling Loop\n    boxes = interpreter.get_tensor(output_details[0]['index'])[0] # Bounding box coordinates of detected objects\n    classes = interpreter.get_tensor(output_details[1]['index'])[0] # Class index of detected objects\n    scores = interpreter.get_tensor(output_details[2]['index'])[0] # Confidence of detected objects\n\n    for i in range(len(scores)):\n        if scores[i] >= confidence_threshold:\n            ymin, xmin, ymax, xmax = boxes[i]\n            (left, right, top, bottom) = (xmin * frame_width, xmax * frame_width,\n                                          ymin * frame_height, ymax * frame_height)\n\n            label = f\"{labels[int(classes[i])]}: {int(scores[i] * 100)}%\"\n            cv2.rectangle(frame, (int(left), int(top)), (int(right), int(bottom)), (0, 255, 0), 2)\n            cv2.putText(frame, label, (int(left), int(top - 5)), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n\n    out.write(frame)\n\n# Phase 5: Cleanup\ncap.release()\nout.release()\ncv2.destroyAllWindows()"
      },
      "session_id": "phi4_7854_tpusg_batch",
      "metadata": {
        "num_run": 17
      },
      "tags": [
        "benchmark",
        "phi4:latest",
        "tpu_sketch_generator"
      ],
      "latency": 17.521,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-19-54-44-989482_chatcmpl-ad499ce1-6b91-4fbc-97dc-a441fad3a060",
          "traceId": "36b6d2df",
          "type": "GENERATION",
          "name": "36b6_tpusg_gen_attempt#1",
          "startTime": "2025-08-18T16:54:44.989000+00:00",
          "endTime": "2025-08-18T16:55:02.510000+00:00",
          "model": "phi4:latest",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 17521.0,
          "costDetails": {},
          "completionTokens": 840,
          "totalTokens": 2646,
          "environment": "default",
          "promptTokens": 1806,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "environment": "default",
      "createdAt": "2025-08-18T16:54:45.510Z",
      "updatedAt": "2025-08-18T16:56:00.141Z"
    },
    {
      "id": "35ae5126",
      "timestamp": "2025-08-18T16:51:39.433000+00:00",
      "name": "35ae_tpu_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import os\nimport numpy as np\nimport cv2  # Importing OpenCV for video capture and writing\nfrom tflite_runtime.interpreter import Interpreter, load_delegate\n\n# Phase 1: Setup\n\n# Paths and Parameters\nmodel_path   = \"/home/mendel/tinyml_autopilot/models/edgetpu_detect.tflite\"\nlabel_path   = \"/home/mendel/tinyml_autopilot/models/labelmap.txt\"\ninput_path   = \"/home/mendel/tinyml_autopilot/data//sheeps.mp4\"\noutput_path   = \"/home/mendel/tinyml_autopilot/results/sheeps_detections.mp4\"\n\n# Load Labels\nwith open(label_path, 'r') as f:\n    labels = [line.strip() for line in f.readlines()]\n\n# Load Interpreter with EdgeTPU\ntry:\n    interpreter = Interpreter(model_path=model_path,\n                              experimental_delegates=[load_delegate('libedgetpu.so.1.0')])\nexcept ValueError:\n    interpreter = Interpreter(model_path=model_path,\n                              experimental_delegates=[load_delegate('/usr/lib/aarch64-linux-gnu/libedgetpu.so.1.0')])\n\ninterpreter.allocate_tensors()\n\n# Get Model Details\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\nfloating_model = (input_details[0]['dtype'] == np.float32)\n\n# Phase 2: Input Acquisition & Preprocessing Loop\n\ncap = cv2.VideoCapture(input_path)\nif not cap.isOpened():\n    print(\"Error opening video stream or file\")\n    exit(1)\n\nwidth = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\nheight = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\nfps = cap.get(cv2.CAP_PROP_FPS)\n\n# Obtain input dimensions from the model\ninput_height, input_width = input_details[0]['shape'][1], input_details[0]['shape'][2]\n\nfourcc = cv2.VideoWriter_fourcc(*'XVID')\nout = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n\nwhile cap.isOpened():\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    # Preprocess Data\n    # Resize the frame to match model input dimensions\n    frame_resized = cv2.resize(frame, (input_width, input_height))\n    frame_rgb = cv2.cvtColor(frame_resized, cv2.COLOR_BGR2RGB)\n    input_data = np.expand_dims(frame_rgb, axis=0)\n\n    if floating_model:\n        input_data = (np.float32(input_data) - 127.5) / 127.5\n\n    # Phase 3: Inference\n    interpreter.set_tensor(input_details[0]['index'], input_data)\n    interpreter.invoke()\n\n    # Phase 4: Output Interpretation & Handling Loop\n    boxes = interpreter.get_tensor(output_details[0]['index'])[0]  # Bounding box coordinates\n    classes = interpreter.get_tensor(output_details[1]['index'])[0]  # Class index\n    scores = interpreter.get_tensor(output_details[2]['index'])[0]  # Confidence\n\n    for i in range(len(scores)):\n        if scores[i] > 0.5:  # Confidence threshold\n            ymin, xmin, ymax, xmax = boxes[i]\n            (left, right, top, bottom) = (xmin * width, xmax * width,\n                                          ymin * height, ymax * height)\n            left, right, top, bottom = int(left), int(right), int(top), int(bottom)\n\n            # Draw bounding box and label\n            cv2.rectangle(frame, (left, top), (right, bottom), (0, 255, 0), 2)\n            object_name = labels[int(classes[i])]\n            label = f'{object_name}: {int(scores[i]*100)}%'\n            label_size, base_line = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.7, 2)\n            top = max(top, label_size[1])\n            cv2.rectangle(frame, (left, top - round(1.5*label_size[1])), \n                          (left + round(1.5*label_size[0]), top + base_line), (255, 255, 255), cv2.FILLED)\n            cv2.putText(frame, label, (left, top), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 0), 2)\n\n    # Write the frame with detections\n    out.write(frame)\n\n# Release resources\ncap.release()\nout.release()"
      },
      "session_id": "phi4_7854_tpusg_batch",
      "metadata": {
        "num_run": 16
      },
      "tags": [
        "benchmark",
        "phi4:latest",
        "tpu_sketch_generator"
      ],
      "latency": 127.691,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-19-51-39-434507_chatcmpl-726f2907-c544-42c2-9a9b-3ea5c24a2aa7",
          "traceId": "35ae5126",
          "type": "GENERATION",
          "name": "35ae_tpusg_gen_attempt#1",
          "startTime": "2025-08-18T16:51:39.434000+00:00",
          "endTime": "2025-08-18T16:51:55.643000+00:00",
          "model": "phi4:latest",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 16209.0,
          "costDetails": {},
          "completionTokens": 774,
          "totalTokens": 2580,
          "environment": "default",
          "promptTokens": 1806,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-19-52-03-519779_chatcmpl-c06fb1fc-7a5a-4b89-b193-8f8e3154a949",
          "traceId": "35ae5126",
          "type": "GENERATION",
          "name": "35ae_tpusg_gen_attempt#2",
          "startTime": "2025-08-18T16:52:03.519000+00:00",
          "endTime": "2025-08-18T16:52:29.180000+00:00",
          "model": "phi4:latest",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 25661.0,
          "costDetails": {},
          "completionTokens": 1272,
          "totalTokens": 3320,
          "environment": "default",
          "promptTokens": 2048,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-19-52-29-183944_chatcmpl-eca30a40-2b7d-4f57-8f2e-6fd229031030",
          "traceId": "35ae5126",
          "type": "GENERATION",
          "name": "35ae_tpusg_gen_attempt#3",
          "startTime": "2025-08-18T16:52:29.183000+00:00",
          "endTime": "2025-08-18T16:52:48.841000+00:00",
          "model": "phi4:latest",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 19658.0,
          "costDetails": {},
          "completionTokens": 957,
          "totalTokens": 2915,
          "environment": "default",
          "promptTokens": 1958,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-19-52-53-344794_chatcmpl-149a8972-3b3b-48aa-a999-4a50b8a75e52",
          "traceId": "35ae5126",
          "type": "GENERATION",
          "name": "35ae_tpusg_gen_attempt#4",
          "startTime": "2025-08-18T16:52:53.344000+00:00",
          "endTime": "2025-08-18T16:53:15.035000+00:00",
          "model": "phi4:latest",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 21691.0,
          "costDetails": {},
          "completionTokens": 1062,
          "totalTokens": 3110,
          "environment": "default",
          "promptTokens": 2048,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-19-53-23-145036_chatcmpl-d93f1265-b4cb-4396-adcf-d5f350132c8c",
          "traceId": "35ae5126",
          "type": "GENERATION",
          "name": "35ae_tpusg_gen_attempt#5",
          "startTime": "2025-08-18T16:53:23.145000+00:00",
          "endTime": "2025-08-18T16:53:47.125000+00:00",
          "model": "phi4:latest",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 23980.0,
          "costDetails": {},
          "completionTokens": 1184,
          "totalTokens": 3232,
          "environment": "default",
          "promptTokens": 2048,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "environment": "default",
      "createdAt": "2025-08-18T16:51:39.953Z",
      "updatedAt": "2025-08-18T16:54:44.648Z"
    },
    {
      "id": "29474ff9",
      "timestamp": "2025-08-18T16:49:52.903000+00:00",
      "name": "2947_tpu_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import cv2\nimport numpy as np\nfrom tflite_runtime.interpreter import Interpreter, load_delegate\n\ndef main():\n    # Paths and parameters\n    model_path = '/home/mendel/tinyml_autopilot/models/edgetpu_detect.tflite'\n    label_path = '/home/mendel/tinyml_autopilot/models/labelmap.txt'\n    input_path = '/home/mendel/tinyml_autopilot/data/sheeps.mp4'\n    output_path = '/home/mendel/tinyml_autopilot/results/sheeps_detections.mp4'\n    confidence_threshold = 0.5\n\n    # Load labels\n    with open(label_path, 'r') as f:\n        labels = [line.strip() for line in f.readlines()]\n\n    # Load TFLite model and allocate tensors.\n    try:\n        interpreter = Interpreter(\n            model_path=model_path,\n            experimental_delegates=[load_delegate('libedgetpu.so.1.0')]\n        )\n    except ValueError:\n        interpreter = Interpreter(\n            model_path=model_path,\n            experimental_delegates=[load_delegate('/usr/lib/aarch64-linux-gnu/libedgetpu.so.1.0')]\n        )\n    \n    interpreter.allocate_tensors()\n\n    # Get input and output details\n    input_details = interpreter.get_input_details()\n    output_details = interpreter.get_output_details()\n\n    model_height, model_width, _ = input_details[0]['shape'][1:4]\n    floating_model = (input_details[0]['dtype'] == np.float32)\n\n    # Open video file or stream\n    cap = cv2.VideoCapture(input_path)\n    \n    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n    out = cv2.VideoWriter(output_path, fourcc, 20.0, (int(cap.get(3)), int(cap.get(4))))\n\n    while cap.isOpened():\n        ret, frame = cap.read()\n        if not ret:\n            break\n\n        # Resize the image to match model's expected input size\n        resized_frame = cv2.resize(frame, (model_width, model_height))\n\n        # Preprocess the image\n        frame_rgb = cv2.cvtColor(resized_frame, cv2.COLOR_BGR2RGB)\n        input_data = np.expand_dims(frame_rgb, axis=0)\n\n        if floating_model:\n            input_data = (np.float32(input_data) - 127.5) / 127.5\n\n        # Perform inference\n        interpreter.set_tensor(input_details[0]['index'], input_data)\n        interpreter.invoke()\n\n        # Retrieve detection results\n        boxes = interpreter.get_tensor(output_details[0]['index'])[0]  # Bounding box coordinates of detected objects\n        classes = interpreter.get_tensor(output_details[1]['index'])[0]  # Class index of detected objects\n        scores = interpreter.get_tensor(output_details[2]['index'])[0]   # Confidence of detected objects\n\n        # Visualization and saving output video frame by frame\n        for i in range(len(scores)):\n            if scores[i] >= confidence_threshold:\n                ymin, xmin, ymax, xmax = boxes[i]\n                \n                # Scale bounding box coordinates back to original image size\n                (left, right, top, bottom) = (\n                    int(xmin * cap.get(3)), \n                    int(xmax * cap.get(3)), \n                    int(ymin * cap.get(4)), \n                    int(ymax * cap.get(4))\n                )\n\n                # Draw rectangle and label on the frame\n                cv2.rectangle(frame, (left, top), (right, bottom), (0, 255, 0), 2)\n                label = f'{labels[int(classes[i])]}: {int(scores[i] * 100)}%'\n                cv2.putText(frame, label, (left, top - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n\n        # Write the frame with detections to the output video\n        out.write(frame)\n\n    # Release resources\n    cap.release()\n    out.release()\n\nif __name__ == \"__main__\":\n    main()"
      },
      "session_id": "phi4_7854_tpusg_batch",
      "metadata": {
        "num_run": 15
      },
      "tags": [
        "benchmark",
        "phi4:latest",
        "tpu_sketch_generator"
      ],
      "latency": 47.89,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-19-49-52-905401_chatcmpl-43c57dce-7a4e-4c48-a142-07342fb568fb",
          "traceId": "29474ff9",
          "type": "GENERATION",
          "name": "2947_tpusg_gen_attempt#1",
          "startTime": "2025-08-18T16:49:52.905000+00:00",
          "endTime": "2025-08-18T16:50:10.953000+00:00",
          "model": "phi4:latest",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 18048.0,
          "costDetails": {},
          "completionTokens": 870,
          "totalTokens": 2676,
          "environment": "default",
          "promptTokens": 1806,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-19-50-19-034421_chatcmpl-9fdd1216-98bf-4185-8858-d7243a0f5f93",
          "traceId": "29474ff9",
          "type": "GENERATION",
          "name": "2947_tpusg_gen_attempt#2",
          "startTime": "2025-08-18T16:50:19.034000+00:00",
          "endTime": "2025-08-18T16:50:40.795000+00:00",
          "model": "phi4:latest",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 21761.0,
          "costDetails": {},
          "completionTokens": 1058,
          "totalTokens": 3106,
          "environment": "default",
          "promptTokens": 2048,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "environment": "default",
      "createdAt": "2025-08-18T16:49:53.432Z",
      "updatedAt": "2025-08-18T16:51:38.821Z"
    },
    {
      "id": "02ddc4db",
      "timestamp": "2025-08-18T16:47:58.023000+00:00",
      "name": "02dd_tpu_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import cv2\nimport numpy as np\nfrom tflite_runtime.interpreter import Interpreter, load_delegate\n\n# Configuration parameters from the prompt\nmodel_path   = \"/home/mendel/tinyml_autopilot/models/edgetpu_detect.tflite\"\nlabel_path   = \"/home/mendel/tinyml_autopilot/models/labelmap.txt\"\ninput_path   = \"/home/mendel/tinyml_autopilot/data//sheeps.mp4\"\noutput_path   = \"/home/mendel/tinyml_autopilot/results/sheeps_detections.mp4\"\nconfidence_threshold   = 0.5\n\n# Phase 1: Setup\n# Load labels if necessary\ndef load_labels(label_file):\n    with open(label_file, 'r') as file:\n        return [line.strip() for line in file.readlines()]\n\nlabels = load_labels(label_path)\n\n# Load interpreter with EdgeTPU delegate\ntry:\n    interpreter = Interpreter(model_path=model_path,\n                              experimental_delegates=[load_delegate('libedgetpu.so.1.0')])\nexcept Exception as e:\n    print(f\"Error loading TPU delegate: {e}\")\n    try:\n        interpreter = Interpreter(model_path=model_path,\n                                  experimental_delegates=[load_delegate('/usr/lib/aarch64-linux-gnu/libedgetpu.so.1.0')])\n    except Exception as e:\n        raise RuntimeError(\"Failed to load EdgeTPU delegate.\") from e\n\ninterpreter.allocate_tensors()\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\n# Phase 2: Input Acquisition & Preprocessing Loop\ncap = cv2.VideoCapture(input_path)\nif not cap.isOpened():\n    raise FileNotFoundError(f\"Unable to open video file {input_path}\")\n\n# Get input frame dimensions and define codec and output size\nframe_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\nframe_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\nfps = int(cap.get(cv2.CAP_PROP_FPS))\n\n# Define the codec and create VideoWriter object\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')  # 'mp4v' for .mp4 files\nout = cv2.VideoWriter(output_path, fourcc, fps, (frame_width, frame_height))\n\nwhile True:\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    # Preprocess the frame\n    input_data = cv2.resize(frame, (input_details[0]['shape'][2], input_details[0]['shape'][1]))\n    input_data = np.expand_dims(input_data, axis=0).astype(np.uint8)\n\n    # Quantization handling if necessary\n    floating_model = (input_details[0]['dtype'] == np.float32)\n    if floating_model:\n        input_scale, input_zero_point = input_details[0][\"quantization\"]\n        input_data = (input_data / 255.0 - input_zero_point) / input_scale\n\n    # Phase 3: Inference\n    interpreter.set_tensor(input_details[0]['index'], input_data)\n    interpreter.invoke()\n\n    # Phase 4: Output Interpretation & Handling Loop\n    boxes = interpreter.get_tensor(output_details[0]['index'])[0]  # Bounding box coordinates of detected objects\n    classes = interpreter.get_tensor(output_details[1]['index'])[0]  # Class index of detected objects\n    scores = interpreter.get_tensor(output_details[2]['index'])[0]  # Confidence of detected objects\n\n    for i in range(len(scores)):\n        if scores[i] >= confidence_threshold:\n            ymin, xmin, ymax, xmax = boxes[i]\n            (left, right, top, bottom) = (xmin * frame_width, xmax * frame_width,\n                                          ymin * frame_height, ymax * frame_height)\n\n            # Draw bounding box and label on the frame\n            cv2.rectangle(frame, (int(left), int(top)), (int(right), int(bottom)), (0, 255, 0), 2)\n            label = f\"{labels[int(classes[i])]}: {int(scores[i] * 100)}%\"\n            label_size, base_line = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.7, 2)\n            top = max(top, label_size[1])\n            cv2.rectangle(frame, (int(left), int(top - label_size[1])), \n                          (int(left + label_size[0]), int(top + base_line)), (255, 255, 255), cv2.FILLED)\n            cv2.putText(frame, label, (int(left), int(top)), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 0), 2)\n\n    # Write the frame with detections\n    out.write(frame)\n\n    # Display the resulting frame (optional)\n    cv2.imshow('Frame', frame)\n    if cv2.waitKey(1) & 0xFF == ord('q'):\n        break\n\n# Release everything if job is finished\ncap.release()\nout.release()\ncv2.destroyAllWindows()"
      },
      "session_id": "phi4_7854_tpusg_batch",
      "metadata": {
        "num_run": 14
      },
      "tags": [
        "benchmark",
        "phi4:latest",
        "tpu_sketch_generator"
      ],
      "latency": 54.999,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-19-47-58-025512_chatcmpl-51fdd170-abf5-4d3d-aa4b-eb3c2a3c0488",
          "traceId": "02ddc4db",
          "type": "GENERATION",
          "name": "02dd_tpusg_gen_attempt#1",
          "startTime": "2025-08-18T16:47:58.025000+00:00",
          "endTime": "2025-08-18T16:48:19.097000+00:00",
          "model": "phi4:latest",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 21072.0,
          "costDetails": {},
          "completionTokens": 1026,
          "totalTokens": 2832,
          "environment": "default",
          "promptTokens": 1806,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-19-48-27-508500_chatcmpl-01d67ec1-146c-4b98-a44f-02b33d1c2f6e",
          "traceId": "02ddc4db",
          "type": "GENERATION",
          "name": "02dd_tpusg_gen_attempt#2",
          "startTime": "2025-08-18T16:48:27.508000+00:00",
          "endTime": "2025-08-18T16:48:53.024000+00:00",
          "model": "phi4:latest",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 25516.0,
          "costDetails": {},
          "completionTokens": 1268,
          "totalTokens": 3316,
          "environment": "default",
          "promptTokens": 2048,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "environment": "default",
      "createdAt": "2025-08-18T16:47:58.544Z",
      "updatedAt": "2025-08-18T16:49:51.767Z"
    },
    {
      "id": "21676f84",
      "timestamp": "2025-08-18T16:45:01.492000+00:00",
      "name": "2167_tpu_sketch_generator",
      "input": null,
      "output": {
        "status": "failed",
        "last_code": "import cv2\nimport numpy as np\nfrom tflite_runtime.interpreter import Interpreter\n\n# Set up the path to the model file\nmodel_path  = \"/home/mendel/tinyml_autopilot/models/edgetpu_detect.tflite\"\n\n# Load labels from a text file\nlabel_path  = \"/home/mendel/tinyml_autopilot/models/labelmap.txt\"\ntry:\n    with open(label_path, 'r') as f:\n        labels = [line.strip() for line in f.readlines()]\nexcept FileNotFoundError:\n    print(f\"Error: Label file '{label_path}' not found. Please ensure it's available.\")\n    raise\n\n# Set up TFLite interpreter\ninterpreter = Interpreter(model_path=model_path)\ninterpreter.allocate_tensors()\n\ninput_details  = \"/home/mendel/tinyml_autopilot/data//sheeps.mp4\"\noutput_details  = \"/home/mendel/tinyml_autopilot/results/sheeps_detections.mp4\"\n\ncap = cv2.VideoCapture('/home/mendel/tf_files/video1.mp4')\n\nif not cap.isOpened():\n    print(\"Error: Could not open video.\")\n    exit(1)\n\nheight, width = None, None\n\nwhile cap.isOpened():\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    input_size = tuple(input_details[0]['shape'][1:3])\n    frame_resized = cv2.resize(frame, input_size)\n\n    # Prepare input data\n    input_data = np.expand_dims(cv2.cvtColor(frame_resized, cv2.COLOR_BGR2RGB), axis=0)\n    \n    if input_details[0]['dtype'] == np.uint8:\n        scale, zero_point = input_details[0]['quantization']\n        input_data = (input_data / 255.0).astype(np.float32) * scale + zero_point\n\n    interpreter.set_tensor(input_details[0]['index'], input_data)\n    interpreter.invoke()\n\n    # Retrieve output data\n    boxes = interpreter.get_tensor(output_details[0]['index'])[0]\n    classes = interpreter.get_tensor(output_details[1]['index'])[0]\n    scores = interpreter.get_tensor(output_details[2]['index'])[0]\n\n    if height is None or width is None:\n        height, width, _ = frame.shape\n\n    for i in range(len(scores)):\n        if scores[i] >= 0.5:  # Confidence threshold\n            ymin, xmin, ymax, xmax = boxes[i]\n            left, right, top, bottom = (xmin * width, xmax * width,\n                                        ymin * height, ymax * height)\n\n            label = f\"{labels[int(classes[i])]}: {int(scores[i]*100)}%\"\n            cv2.rectangle(frame, (int(left), int(top)), (int(right), int(bottom)), (0, 255, 0), 2)\n            cv2.putText(frame, label, (int(left), int(top)-10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 1)\n\n    cv2.imshow('Object detector', frame)\n    \n    if cv2.waitKey(1) == ord('q'):\n        break\n\ncap.release()\ncv2.destroyAllWindows()\n\n# Save output video\noutput_video = cv2.VideoWriter('/home/mendel/tf_files/output.mp4',\n                               cv2.VideoWriter_fourcc(*'mp4v'),\n                               20.0, (int(cap.get(3)), int(cap.get(4))))\n\nwhile cap.isOpened():\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    input_size = tuple(input_details[0]['shape'][1:3])\n    frame_resized = cv2.resize(frame, input_size)\n\n    input_data = np.expand_dims(cv2.cvtColor(frame_resized, cv2.COLOR_BGR2RGB), axis=0)\n    \n    if input_details[0]['dtype'] == np.uint8:\n        scale, zero_point = input_details[0]['quantization']\n        input_data = (input_data / 255.0).astype(np.float32) * scale + zero_point\n\n    interpreter.set_tensor(input_details[0]['index'], input_data)\n    interpreter.invoke()\n\n    # Retrieve output data\n    boxes = interpreter.get_tensor(output_details[0]['index'])[0]\n    classes = interpreter.get_tensor(output_details[1]['index'])[0]\n    scores = interpreter.get_tensor(output_details[2]['index'])[0]\n\n    for i in range(len(scores)):\n        if scores[i] >= 0.5:  # Confidence threshold\n            ymin, xmin, ymax, xmax = boxes[i]\n            left, right, top, bottom = (xmin * width, xmax * width,\n                                        ymin * height, ymax * height)\n\n            label = f\"{labels[int(classes[i])]}: {int(scores[i]*100)}%\"\n            cv2.rectangle(frame, (int(left), int(top)), (int(right), int(bottom)), (0, 255, 0), 2)\n            cv2.putText(frame, label, (int(left), int(top)-10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 1)\n\n    output_video.write(frame)\n\noutput_video.release()",
        "last_error": "Max retries reached with error. Last error: Error: Could not open video.."
      },
      "session_id": "phi4_7854_tpusg_batch",
      "metadata": {
        "num_run": 13
      },
      "tags": [
        "benchmark",
        "phi4:latest",
        "tpu_sketch_generator"
      ],
      "latency": 170.289,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-19-45-01-494214_chatcmpl-0e66c6be-fd57-4885-8b80-301a1b3babb3",
          "traceId": "21676f84",
          "type": "GENERATION",
          "name": "2167_tpusg_gen_attempt#1",
          "startTime": "2025-08-18T16:45:01.494000+00:00",
          "endTime": "2025-08-18T16:45:23.127000+00:00",
          "model": "phi4:latest",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 21633.0,
          "costDetails": {},
          "completionTokens": 1057,
          "totalTokens": 2863,
          "environment": "default",
          "promptTokens": 1806,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-19-45-31-119996_chatcmpl-6aae513c-95ef-41ea-b27e-ca0c243e467e",
          "traceId": "21676f84",
          "type": "GENERATION",
          "name": "2167_tpusg_gen_attempt#2",
          "startTime": "2025-08-18T16:45:31.119000+00:00",
          "endTime": "2025-08-18T16:46:00.941000+00:00",
          "model": "phi4:latest",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 29822.0,
          "costDetails": {},
          "completionTokens": 1494,
          "totalTokens": 3542,
          "environment": "default",
          "promptTokens": 2048,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-19-46-08-922516_chatcmpl-0dc71ff3-c543-46f1-aff4-a46399e2d6bb",
          "traceId": "21676f84",
          "type": "GENERATION",
          "name": "2167_tpusg_gen_attempt#3",
          "startTime": "2025-08-18T16:46:08.922000+00:00",
          "endTime": "2025-08-18T16:46:37.002000+00:00",
          "model": "phi4:latest",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 28080.0,
          "costDetails": {},
          "completionTokens": 1399,
          "totalTokens": 3447,
          "environment": "default",
          "promptTokens": 2048,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-19-46-44-567563_chatcmpl-3c3d0159-da48-4901-98f2-9dab5d1c2e6c",
          "traceId": "21676f84",
          "type": "GENERATION",
          "name": "2167_tpusg_gen_attempt#4",
          "startTime": "2025-08-18T16:46:44.567000+00:00",
          "endTime": "2025-08-18T16:47:09.183000+00:00",
          "model": "phi4:latest",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 24616.0,
          "costDetails": {},
          "completionTokens": 1217,
          "totalTokens": 3265,
          "environment": "default",
          "promptTokens": 2048,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-19-47-16-776200_chatcmpl-52e713f1-4daa-4271-8918-4b79b8f12e15",
          "traceId": "21676f84",
          "type": "GENERATION",
          "name": "2167_tpusg_gen_attempt#5",
          "startTime": "2025-08-18T16:47:16.776000+00:00",
          "endTime": "2025-08-18T16:47:43.891000+00:00",
          "model": "phi4:latest",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 27115.0,
          "costDetails": {},
          "completionTokens": 1354,
          "totalTokens": 3402,
          "environment": "default",
          "promptTokens": 2048,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "d2608a3f-1a29-4c49-abf6-710decf12a9c",
          "traceId": "21676f84",
          "type": "SPAN",
          "name": "error_21_tpusg_failure_signal_tpu_sketch_generator",
          "startTime": "2025-08-18T16:47:51.783000+00:00",
          "level": "ERROR",
          "statusMessage": "Failed: Max retries reached with error. Last error: Error: Could not open video..",
          "calculatedTotalCost": 0.0,
          "costDetails": {},
          "completionTokens": 0,
          "totalTokens": 0,
          "environment": "default",
          "promptTokens": 0,
          "endTime": null,
          "model": null,
          "modelParameters": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null,
          "latency": null
        }
      ],
      "environment": "default",
      "createdAt": "2025-08-18T16:45:02.012Z",
      "updatedAt": "2025-08-18T16:47:57.282Z"
    },
    {
      "id": "1d5cf198",
      "timestamp": "2025-08-18T16:43:41.953000+00:00",
      "name": "1d5c_tpu_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import cv2\nimport numpy as np\nfrom tflite_runtime.interpreter import Interpreter, load_delegate\n\n# Configuration parameters\nmodel_path  = \"/home/mendel/tinyml_autopilot/models/edgetpu_detect.tflite\"\nlabel_path  = \"/home/mendel/tinyml_autopilot/models/labelmap.txt\"\ninput_path  = \"/home/mendel/tinyml_autopilot/data//sheeps.mp4\"\noutput_path  = \"/home/mendel/tinyml_autopilot/results/sheeps_detections.mp4\"\nconfidence_threshold  = 0.5\n\n# Phase 1: Setup\n# Load labels\nwith open(label_path, 'r') as f:\n    labels = [line.strip() for line in f.readlines()]\n\n# Load interpreter with EdgeTPU\ntry:\n    interpreter = Interpreter(\n        model_path=model_path,\n        experimental_delegates=[load_delegate('libedgetpu.so.1.0')]\n    )\nexcept Exception as e:\n    print(f\"Failed to load Edge TPU delegate from 'libedgetpu.so.1.0': {e}\")\n    try:\n        interpreter = Interpreter(\n            model_path=model_path,\n            experimental_delegates=[load_delegate('/usr/lib/aarch64-linux-gnu/libedgetpu.so.1.0')]\n        )\n    except Exception as e:\n        raise RuntimeError(f\"Failed to load Edge TPU delegate: {e}\")\n\ninterpreter.allocate_tensors()\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\n# Phase 2: Input Acquisition & Preprocessing Loop\ncap = cv2.VideoCapture(input_path)\nif not cap.isOpened():\n    raise IOError(\"Cannot open video file\")\n\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\nout = cv2.VideoWriter(output_path, fourcc, 20.0, (int(cap.get(3)), int(cap.get(4))))\n\nwhile True:\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    input_data = cv2.resize(frame, (input_details[0]['shape'][2], input_details[0]['shape'][1]))\n    input_data = np.expand_dims(input_data, axis=0)\n\n    # Quantization Handling\n    floating_model = (input_details[0]['dtype'] == np.float32)\n    if floating_model:\n        input_data = (np.float32(input_data) - 127.5) / 127.5\n\n    interpreter.set_tensor(input_details[0]['index'], input_data)\n\n    # Phase 3: Inference\n    interpreter.invoke()\n\n    # Phase 4: Output Interpretation & Handling Loop\n    boxes = interpreter.get_tensor(output_details[0]['index'])[0]  # Bounding box coordinates of detected objects\n    classes = interpreter.get_tensor(output_details[1]['index'])[0]  # Class index of detected objects\n    scores = interpreter.get_tensor(output_details[2]['index'])[0]  # Confidence of detected objects\n\n    for i in range(len(scores)):\n        if scores[i] >= confidence_threshold:\n            ymin, xmin, ymax, xmax = boxes[i]\n            (left, right, top, bottom) = (xmin * frame.shape[1], xmax * frame.shape[1],\n                                          ymin * frame.shape[0], ymax * frame.shape[0])\n            left, right, top, bottom = int(left), int(right), int(top), int(bottom)\n\n            # Draw bounding box and label on the image\n            cv2.rectangle(frame, (left, top), (right, bottom), (0, 255, 0), 2)\n            object_name = labels[int(classes[i])]\n            label = f\"{object_name}: {int(scores[i] * 100)}%\"\n            label_size, base_line = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.7, 2)\n            top = max(top, label_size[1])\n            cv2.rectangle(frame, (left, top - round(1.5 * label_size[1])), (left + round(1.5 * label_size[0]), top + base_line), (255, 255, 255), cv2.FILLED)\n            cv2.putText(frame, label, (left, top), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 0), 2)\n\n    # Handle Output\n    out.write(frame)\n\n# Phase 5: Cleanup\ncap.release()\nout.release()\ncv2.destroyAllWindows()"
      },
      "session_id": "phi4_7854_tpusg_batch",
      "metadata": {
        "num_run": 12
      },
      "tags": [
        "benchmark",
        "phi4:latest",
        "tpu_sketch_generator"
      ],
      "latency": 20.644,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-19-43-41-954566_chatcmpl-216ff7e5-7494-4770-b4cc-283f56aec4ed",
          "traceId": "1d5cf198",
          "type": "GENERATION",
          "name": "1d5c_tpusg_gen_attempt#1",
          "startTime": "2025-08-18T16:43:41.954000+00:00",
          "endTime": "2025-08-18T16:44:02.598000+00:00",
          "model": "phi4:latest",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 20644.0,
          "costDetails": {},
          "completionTokens": 1007,
          "totalTokens": 2813,
          "environment": "default",
          "promptTokens": 1806,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "environment": "default",
      "createdAt": "2025-08-18T16:43:42.473Z",
      "updatedAt": "2025-08-18T16:45:00.673Z"
    },
    {
      "id": "dd8e89bc",
      "timestamp": "2025-08-18T16:42:22.431000+00:00",
      "name": "dd8e_tpu_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import cv2\nimport numpy as np\nfrom tflite_runtime.interpreter import Interpreter, load_delegate\n\n# Configuration parameters\nmodel_path  = \"/home/mendel/tinyml_autopilot/models/edgetpu_detect.tflite\"\nlabel_path  = \"/home/mendel/tinyml_autopilot/models/labelmap.txt\"\ninput_path  = \"/home/mendel/tinyml_autopilot/data//sheeps.mp4\"\noutput_path  = \"/home/mendel/tinyml_autopilot/results/sheeps_detections.mp4\"\nconfidence_threshold  = 0.5\n\n# Phase 1: Setup\ndef load_labels(label_file):\n    with open(label_file, 'r') as file:\n        return [line.strip() for line in file.readlines()]\n\nlabels = load_labels(label_path)\n\ntry:\n    interpreter = Interpreter(\n        model_path=model_path,\n        experimental_delegates=[load_delegate('libedgetpu.so.1.0')]\n    )\nexcept Exception as e:\n    print(f\"Error loading EdgeTPU delegate: {e}\")\n    try:\n        interpreter = Interpreter(\n            model_path=model_path,\n            experimental_delegates=[load_delegate('/usr/lib/aarch64-linux-gnu/libedgetpu.so.1.0')]\n        )\n    except Exception as e:\n        raise RuntimeError(\"Failed to load the TFLite model with EdgeTPU delegate.\") from e\n\ninterpreter.allocate_tensors()\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\nfloating_model = input_details[0]['dtype'] == np.float32\ninput_shape = input_details[0]['shape']\n\n# Phase 2: Input Acquisition & Preprocessing Loop\ncap = cv2.VideoCapture(input_path)\nout = None\n\nwhile cap.isOpened():\n    ret, frame = cap.read()\n    if not ret:\n        break\n    \n    # Resize and prepare the frame for model input\n    image_size = input_shape[1:3]\n    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n    frame_resized = cv2.resize(frame_rgb, (image_size[1], image_size[0]))\n    input_data = np.expand_dims(frame_resized, axis=0)\n\n    if floating_model:\n        input_data = (np.float32(input_data) - 127.5) / 127.5\n\n    # Phase 3: Inference\n    interpreter.set_tensor(input_details[0]['index'], input_data)\n    interpreter.invoke()\n\n    # Phase 4: Output Interpretation & Handling Loop\n    boxes = interpreter.get_tensor(output_details[0]['index'])[0]  # Bounding box coordinates of detected objects\n    classes = interpreter.get_tensor(output_details[1]['index'])[0]  # Class index of detected objects\n    scores = interpreter.get_tensor(output_details[2]['index'])[0]  # Confidence of detected objects\n\n    height, width, _ = frame.shape\n    for i in range(len(scores)):\n        if scores[i] > confidence_threshold:\n            ymin, xmin, ymax, xmax = boxes[i]\n            (left, right, top, bottom) = (xmin * width, xmax * width, ymin * height, ymax * height)\n            object_name = labels[int(classes[i])]\n            \n            # Draw the bounding box on the frame\n            cv2.rectangle(frame, (int(left), int(top)), (int(right), int(bottom)), (0, 255, 0), 2)\n            label = f\"{object_name}: {int(scores[i] * 100)}%\"\n            label_size, base_line = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.7, 2)\n            \n            top = max(top, label_size[1])\n            cv2.rectangle(frame, (int(left), int(top - label_size[1])), \n                          (int(left + label_size[0]), int(top + base_line)), \n                          (255, 255, 255), cv2.FILLED)\n            cv2.putText(frame, label, (int(left), int(top)), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 0), 2)\n\n    if out is None:\n        fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n        out = cv2.VideoWriter(output_path, fourcc, 20.0, (width, height))\n    \n    out.write(frame)\n    cv2.imshow('Object detector', frame)\n    if cv2.waitKey(1) & 0xFF == ord('q'):\n        break\n\n# Phase 5: Cleanup\ncap.release()\nif out is not None:\n    out.release()\ncv2.destroyAllWindows()"
      },
      "session_id": "phi4_7854_tpusg_batch",
      "metadata": {
        "num_run": 11
      },
      "tags": [
        "benchmark",
        "phi4:latest",
        "tpu_sketch_generator"
      ],
      "latency": 20.542,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-19-42-22-433361_chatcmpl-6a900713-ca7f-4656-a739-a32d937d94cf",
          "traceId": "dd8e89bc",
          "type": "GENERATION",
          "name": "dd8e_tpusg_gen_attempt#1",
          "startTime": "2025-08-18T16:42:22.433000+00:00",
          "endTime": "2025-08-18T16:42:42.975000+00:00",
          "model": "phi4:latest",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 20542.0,
          "costDetails": {},
          "completionTokens": 1001,
          "totalTokens": 2807,
          "environment": "default",
          "promptTokens": 1806,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "environment": "default",
      "createdAt": "2025-08-18T16:42:22.955Z",
      "updatedAt": "2025-08-18T16:43:41.611Z"
    },
    {
      "id": "121f20a3",
      "timestamp": "2025-08-18T16:40:35.490000+00:00",
      "name": "121f_tpu_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import cv2\nimport numpy as np\nfrom tflite_runtime.interpreter import Interpreter, load_delegate\n\n# CONFIGURATION PARAMETERS\nmodel_path   = \"/home/mendel/tinyml_autopilot/models/edgetpu_detect.tflite\"\nlabel_path   = \"/home/mendel/tinyml_autopilot/models/labelmap.txt\"\ninput_path   = \"/home/mendel/tinyml_autopilot/data//sheeps.mp4\"\noutput_path   = \"/home/mendel/tinyml_autopilot/results/sheeps_detections.mp4\"\nconfidence_threshold   = 0.5\n\n# Phase 1: Setup\ntry:\n    interpreter = Interpreter(\n        model_path=model_path,\n        experimental_delegates=[load_delegate('libedgetpu.so.1.0')]\n    )\nexcept Exception as e:\n    print(f\"Failed to load Edge TPU delegate with error: {e}\")\n    try:\n        interpreter = Interpreter(\n            model_path=model_path,\n            experimental_delegates=[load_delegate('/usr/lib/aarch64-linux-gnu/libedgetpu.so.1.0')]\n        )\n    except Exception as e:\n        raise RuntimeError(f\"Failed to load Edge TPU delegate with error: {e}\")\n\ninterpreter.allocate_tensors()\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\n# Load labels\nwith open(label_path, 'r') as f:\n    labels = [line.strip() for line in f.readlines()]\n\n# Open the input video file and get its properties\ncap = cv2.VideoCapture(input_path)\nwidth = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\nheight = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\nfps = cap.get(cv2.CAP_PROP_FPS)\n\n# Define codec and create VideoWriter object for output\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\nout = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n\n# Phase 2: Input Acquisition & Preprocessing Loop\nwhile cap.isOpened():\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    # Preprocess input data\n    input_frame = cv2.resize(frame, (input_details[0]['shape'][2], input_details[0]['shape'][1]))\n    input_data = np.expand_dims(input_frame, axis=0)\n\n    if input_details[0]['dtype'] == np.float32:\n        input_data = (np.float32(input_data) - 127.5) / 127.5\n\n    # Phase 3: Inference\n    interpreter.set_tensor(input_details[0]['index'], input_data)\n    interpreter.invoke()\n\n    # Phase 4: Output Interpretation & Handling Loop\n    boxes = interpreter.get_tensor(output_details[0]['index'])[0]  # Bounding box coordinates of detected objects\n    classes = interpreter.get_tensor(output_details[1]['index'])[0]  # Class index of detected objects\n    scores = interpreter.get_tensor(output_details[2]['index'])[0]  # Confidence of detected objects\n\n    for i in range(len(scores)):\n        if scores[i] > confidence_threshold:\n            ymin, xmin, ymax, xmax = boxes[i]\n            (left, right, top, bottom) = (xmin * width, xmax * width, ymin * height, ymax * height)\n            \n            # Clip bounding box coordinates to image dimensions\n            left = max(0, int(left))\n            right = min(width, int(right))\n            top = max(0, int(top))\n            bottom = min(height, int(bottom))\n\n            label = f\"{labels[int(classes[i])]}: {int(scores[i] * 100)}%\"\n            \n            cv2.rectangle(frame, (left, top), (right, bottom), (10, 255, 0), 2)\n            cv2.putText(frame, label, (left, top - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (10, 255, 0), 2)\n\n    # Handle Output\n    out.write(frame)\n\n# Phase 5: Cleanup\ncap.release()\nout.release()\ncv2.destroyAllWindows()"
      },
      "session_id": "phi4_7854_tpusg_batch",
      "metadata": {
        "num_run": 10
      },
      "tags": [
        "benchmark",
        "phi4:latest",
        "tpu_sketch_generator"
      ],
      "latency": 46.995,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-19-40-35-492183_chatcmpl-93cd197c-f90b-4d08-b463-e9238361c062",
          "traceId": "121f20a3",
          "type": "GENERATION",
          "name": "121f_tpusg_gen_attempt#1",
          "startTime": "2025-08-18T16:40:35.492000+00:00",
          "endTime": "2025-08-18T16:40:53.062000+00:00",
          "model": "phi4:latest",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 17570.0,
          "costDetails": {},
          "completionTokens": 844,
          "totalTokens": 2650,
          "environment": "default",
          "promptTokens": 1806,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-19-41-01-368156_chatcmpl-68a32b3b-c29f-4cac-ac34-6291c3456ef4",
          "traceId": "121f20a3",
          "type": "GENERATION",
          "name": "121f_tpusg_gen_attempt#2",
          "startTime": "2025-08-18T16:41:01.368000+00:00",
          "endTime": "2025-08-18T16:41:22.487000+00:00",
          "model": "phi4:latest",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 21119.0,
          "costDetails": {},
          "completionTokens": 1030,
          "totalTokens": 3078,
          "environment": "default",
          "promptTokens": 2048,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "environment": "default",
      "createdAt": "2025-08-18T16:40:36.010Z",
      "updatedAt": "2025-08-18T16:42:20.740Z"
    },
    {
      "id": "ee8202a6",
      "timestamp": "2025-08-18T16:39:17.949000+00:00",
      "name": "ee82_tpu_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import cv2\nfrom tflite_runtime.interpreter import Interpreter, load_delegate\nimport numpy as np\n\n# Configuration Parameters\nmodel_path  = \"/home/mendel/tinyml_autopilot/models/edgetpu_detect.tflite\"\nlabel_path  = \"/home/mendel/tinyml_autopilot/models/labelmap.txt\"\ninput_path  = \"/home/mendel/tinyml_autopilot/data//sheeps.mp4\"\noutput_path  = \"/home/mendel/tinyml_autopilot/results/sheeps_detections.mp4\"\nconfidence_threshold  = 0.5\n\n# Load Labels\ndef load_labels(label_path):\n    with open(label_path, 'r') as file:\n        return [line.strip() for line in file.readlines()]\n\nlabels = load_labels(label_path)\n\n# Load Interpreter with EdgeTPU\ntry:\n    interpreter = Interpreter(\n        model_path=model_path,\n        experimental_delegates=[load_delegate('libedgetpu.so.1.0')]\n    )\nexcept RuntimeError as e:\n    try:\n        interpreter = Interpreter(\n            model_path=model_path,\n            experimental_delegates=[load_delegate('/usr/lib/aarch64-linux-gnu/libedgetpu.so.1.0')]\n        )\n    except Exception as inner_e:\n        raise RuntimeError(\"Failed to load Edge TPU delegate\") from inner_e\n\ninterpreter.allocate_tensors()\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\n# Video Capture\ncap = cv2.VideoCapture(input_path)\nfourcc = cv2.VideoWriter_fourcc(*'XVID')\nout = cv2.VideoWriter(output_path, fourcc, 20.0, (int(cap.get(3)), int(cap.get(4))))\n\nwhile cap.isOpened():\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    # Preprocess Input\n    input_frame = cv2.resize(frame, (input_details[0]['shape'][2], input_details[0]['shape'][1]))\n    input_data = np.expand_dims(input_frame, axis=0)\n\n    floating_model = (input_details[0]['dtype'] == np.float32)\n    if floating_model:\n        input_data = (np.float32(input_data) - 127.5) / 127.5\n\n    # Set Input Tensor\n    interpreter.set_tensor(input_details[0]['index'], input_data)\n\n    # Run Inference\n    interpreter.invoke()\n\n    # Get Output Details\n    boxes = interpreter.get_tensor(output_details[0]['index'])[0]  # Bounding box coordinates of detected objects\n    classes = interpreter.get_tensor(output_details[1]['index'])[0]  # Class index of detected objects\n    scores = interpreter.get_tensor(output_details[2]['index'])[0]  # Confidence of detected objects\n\n    # Interpret Results\n    for i in range(len(scores)):\n        if scores[i] >= confidence_threshold:\n            ymin, xmin, ymax, xmax = boxes[i]\n            (left, right, top, bottom) = (xmin * frame.shape[1], xmax * frame.shape[1],\n                                          ymin * frame.shape[0], ymax * frame.shape[0])\n\n            # Draw bounding box and label\n            cv2.rectangle(frame, (int(left), int(top)), (int(right), int(bottom)), (10, 255, 0), 2)\n            object_name = labels[int(classes[i])]\n            label = f\"{object_name}: {int(scores[i] * 100)}%\"\n            cv2.putText(frame, label, (int(left), int(top) - 10),\n                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 0, 0), 1)\n\n    # Write frame to output video\n    out.write(frame)\n    cv2.imshow('Frame', frame)\n    if cv2.waitKey(1) & 0xFF == ord('q'):\n        break\n\n# Cleanup\ncap.release()\nout.release()\ncv2.destroyAllWindows()"
      },
      "session_id": "phi4_7854_tpusg_batch",
      "metadata": {
        "num_run": 9
      },
      "tags": [
        "benchmark",
        "phi4:latest",
        "tpu_sketch_generator"
      ],
      "latency": 18.283,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-19-39-17-951053_chatcmpl-2ba2ecb2-6aa5-47c6-bef8-284d9b8fa938",
          "traceId": "ee8202a6",
          "type": "GENERATION",
          "name": "ee82_tpusg_gen_attempt#1",
          "startTime": "2025-08-18T16:39:17.951000+00:00",
          "endTime": "2025-08-18T16:39:36.234000+00:00",
          "model": "phi4:latest",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 18283.0,
          "costDetails": {},
          "completionTokens": 880,
          "totalTokens": 2686,
          "environment": "default",
          "promptTokens": 1806,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "environment": "default",
      "createdAt": "2025-08-18T16:39:18.470Z",
      "updatedAt": "2025-08-18T16:40:35.590Z"
    },
    {
      "id": "81b4f741",
      "timestamp": "2025-08-18T16:38:01.929000+00:00",
      "name": "81b4_tpu_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import cv2\nimport numpy as np\nfrom tflite_runtime.interpreter import Interpreter, load_delegate\n\n# CONFIGURATION PARAMETERS\nmodel_path  = \"/home/mendel/tinyml_autopilot/models/edgetpu_detect.tflite\"\nlabel_path  = \"/home/mendel/tinyml_autopilot/models/labelmap.txt\"\ninput_path  = \"/home/mendel/tinyml_autopilot/data//sheeps.mp4\"\noutput_path  = \"/home/mendel/tinyml_autopilot/results/sheeps_detections.mp4\"\nconfidence_threshold  = 0.5\n\n# Phase 1: Setup\ndef load_labels(path):\n    with open(path, 'r') as file:\n        return [line.strip() for line in file.readlines()]\n\nlabels = load_labels(label_path)\n\ntry:\n    interpreter = Interpreter(\n        model_path=model_path,\n        experimental_delegates=[load_delegate('libedgetpu.so.1.0')]\n    )\nexcept ValueError:\n    try:\n        interpreter = Interpreter(\n            model_path=model_path,\n            experimental_delegates=[load_delegate('/usr/lib/aarch64-linux-gnu/libedgetpu.so.1.0')]\n        )\n    except Exception as e:\n        print(\"Failed to load TFLite model with Edge TPU: \", str(e))\n        exit(1)\n\ninterpreter.allocate_tensors()\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\n# Phase 2: Input Acquisition & Preprocessing Loop\ncap = cv2.VideoCapture(input_path)\nfps = cap.get(cv2.CAP_PROP_FPS)\nwidth = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\nheight = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n\noutput_video = cv2.VideoWriter(\n    output_path,\n    cv2.VideoWriter_fourcc(*'mp4v'),\n    fps,\n    (width, height)\n)\n\nwhile cap.isOpened():\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    input_frame = cv2.resize(frame, (300, 300))\n    input_data = np.expand_dims(input_frame, axis=0).astype(np.uint8)\n\n    # Phase 3: Inference\n    interpreter.set_tensor(input_details[0]['index'], input_data)\n    interpreter.invoke()\n\n    # Phase 4: Output Interpretation & Handling Loop\n    boxes = interpreter.get_tensor(output_details[0]['index'])[0]  # Bounding box coordinates\n    classes = interpreter.get_tensor(output_details[1]['index'])[0]  # Class index\n    scores = interpreter.get_tensor(output_details[2]['index'])[0]  # Confidence\n\n    for i in range(len(scores)):\n        if scores[i] > confidence_threshold:\n            ymin, xmin, ymax, xmax = boxes[i]\n            (left, right, top, bottom) = (xmin * width, xmax * width, \n                                          ymin * height, ymax * height)\n            \n            label = f\"{labels[int(classes[i])]}: {int(scores[i]*100)}%\"\n            frame = cv2.rectangle(frame, (int(left), int(top)), (int(right), int(bottom)), (0, 255, 0), 2)\n            frame = cv2.putText(frame, label, (int(left), int(top)-10), \n                                cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 1)\n\n    output_video.write(frame)\n\n# Phase 5: Cleanup\ncap.release()\noutput_video.release()\ncv2.destroyAllWindows()"
      },
      "session_id": "phi4_7854_tpusg_batch",
      "metadata": {
        "num_run": 8
      },
      "tags": [
        "benchmark",
        "phi4:latest",
        "tpu_sketch_generator"
      ],
      "latency": 16.88,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-19-38-01-930936_chatcmpl-5d1c0550-47a5-4850-a525-2140c0c91486",
          "traceId": "81b4f741",
          "type": "GENERATION",
          "name": "81b4_tpusg_gen_attempt#1",
          "startTime": "2025-08-18T16:38:01.930000+00:00",
          "endTime": "2025-08-18T16:38:18.810000+00:00",
          "model": "phi4:latest",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 16880.0,
          "costDetails": {},
          "completionTokens": 806,
          "totalTokens": 2612,
          "environment": "default",
          "promptTokens": 1806,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "environment": "default",
      "createdAt": "2025-08-18T16:38:02.447Z",
      "updatedAt": "2025-08-18T16:39:16.616Z"
    },
    {
      "id": "e3dfb98e",
      "timestamp": "2025-08-18T16:34:59.012000+00:00",
      "name": "e3df_tpu_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import cv2\nimport numpy as np\nfrom tflite_runtime.interpreter import Interpreter, load_delegate\n\n# Configuration Parameters\nmodel_path  = \"/home/mendel/tinyml_autopilot/models/edgetpu_detect.tflite\"\ninput_path  = \"/home/mendel/tinyml_autopilot/data//sheeps.mp4\"\noutput_path  = \"/home/mendel/tinyml_autopilot/results/sheeps_detections.mp4\"\nlabel_path  = \"/home/mendel/tinyml_autopilot/models/labelmap.txt\"\n\n# Phase 1: Setup\ntry:\n    interpreter = Interpreter(\n        model_path=model_path,\n        experimental_delegates=[load_delegate('libedgetpu.so.1.0')]\n    )\nexcept ValueError:\n    interpreter = Interpreter(\n        model_path=model_path,\n        experimental_delegates=[load_delegate('/usr/lib/aarch64-linux-gnu/libedgetpu.so.1.0')]\n    )\n\ninterpreter.allocate_tensors()\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\n# Get input size\n_, input_height, input_width, _ = input_details[0]['shape']\n\n# Phase 2: Input Acquisition & Preprocessing Loop\ncap = cv2.VideoCapture(input_path)\nfourcc = cv2.VideoWriter_fourcc(*'XVID')\nout = cv2.VideoWriter(output_path, fourcc, 20.0, (int(cap.get(3)), int(cap.get(4))))\n\nwhile cap.isOpened():\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    # Preprocess the input\n    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n    frame_resized = cv2.resize(frame_rgb, (input_width, input_height))\n    input_data = np.expand_dims(frame_resized, axis=0)\n\n    # Quantization Handling\n    floating_model = input_details[0]['dtype'] == np.float32\n    if floating_model:\n        input_data = (np.float32(input_data) - 127.5) / 127.5\n\n    # Phase 3: Inference\n    interpreter.set_tensor(input_details[0]['index'], input_data)\n    interpreter.invoke()\n\n    # Phase 4: Output Interpretation & Handling Loop\n    boxes = interpreter.get_tensor(output_details[0]['index'])[0]  # Bounding box coordinates of detected objects\n    classes = interpreter.get_tensor(output_details[1]['index'])[0]  # Class index of detected objects\n    scores = interpreter.get_tensor(output_details[2]['index'])[0]  # Confidence of detected objects\n\n    # Loop over all detections and draw detection box if confidence is above minimum threshold\n    for i in range(len(scores)):\n        if scores[i] > 0.5:  # Minimum confidence threshold\n            ymin, xmin, ymax, xmax = boxes[i]\n            (left, right, top, bottom) = (xmin * frame.shape[1], xmax * frame.shape[1],\n                                          ymin * frame.shape[0], ymax * frame.shape[0])\n\n            cv2.rectangle(frame, (int(left), int(top)), (int(right), int(bottom)), (10, 255, 0), 2)\n\n    # Write the frame with detection boxes\n    out.write(frame)\n\n# Phase 5: Cleanup\ncap.release()\nout.release()\ncv2.destroyAllWindows()"
      },
      "session_id": "phi4_7854_tpusg_batch",
      "metadata": {
        "num_run": 7
      },
      "tags": [
        "benchmark",
        "phi4:latest",
        "tpu_sketch_generator"
      ],
      "latency": 125.347,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-19-34-59-013734_chatcmpl-5b63e917-e973-4811-be92-cf37fa9fb7a3",
          "traceId": "e3dfb98e",
          "type": "GENERATION",
          "name": "e3df_tpusg_gen_attempt#1",
          "startTime": "2025-08-18T16:34:59.013000+00:00",
          "endTime": "2025-08-18T16:35:15.938000+00:00",
          "model": "phi4:latest",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 16925.0,
          "costDetails": {},
          "completionTokens": 810,
          "totalTokens": 2616,
          "environment": "default",
          "promptTokens": 1806,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-19-35-24-401910_chatcmpl-609fb774-f5d9-43fe-9e43-1000426c49e4",
          "traceId": "e3dfb98e",
          "type": "GENERATION",
          "name": "e3df_tpusg_gen_attempt#2",
          "startTime": "2025-08-18T16:35:24.401000+00:00",
          "endTime": "2025-08-18T16:35:46.270000+00:00",
          "model": "phi4:latest",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 21869.0,
          "costDetails": {},
          "completionTokens": 1070,
          "totalTokens": 3118,
          "environment": "default",
          "promptTokens": 2048,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-19-35-54-057461_chatcmpl-327e20c3-80c6-4fe9-9870-d47996ebf722",
          "traceId": "e3dfb98e",
          "type": "GENERATION",
          "name": "e3df_tpusg_gen_attempt#3",
          "startTime": "2025-08-18T16:35:54.057000+00:00",
          "endTime": "2025-08-18T16:36:15.981000+00:00",
          "model": "phi4:latest",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 21924.0,
          "costDetails": {},
          "completionTokens": 1075,
          "totalTokens": 3123,
          "environment": "default",
          "promptTokens": 2048,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-19-36-23-981944_chatcmpl-9f0e8d69-3dd2-4381-bde9-56d4bce6f8f8",
          "traceId": "e3dfb98e",
          "type": "GENERATION",
          "name": "e3df_tpusg_gen_attempt#4",
          "startTime": "2025-08-18T16:36:23.981000+00:00",
          "endTime": "2025-08-18T16:36:45.625000+00:00",
          "model": "phi4:latest",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 21644.0,
          "costDetails": {},
          "completionTokens": 1057,
          "totalTokens": 3105,
          "environment": "default",
          "promptTokens": 2048,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-19-36-45-632401_chatcmpl-cffc6f16-a1fd-49aa-92a3-fd11101c1625",
          "traceId": "e3dfb98e",
          "type": "GENERATION",
          "name": "e3df_tpusg_gen_attempt#5",
          "startTime": "2025-08-18T16:36:45.632000+00:00",
          "endTime": "2025-08-18T16:37:04.360000+00:00",
          "model": "phi4:latest",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 18728.0,
          "costDetails": {},
          "completionTokens": 905,
          "totalTokens": 2864,
          "environment": "default",
          "promptTokens": 1959,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "environment": "default",
      "createdAt": "2025-08-18T16:34:59.531Z",
      "updatedAt": "2025-08-18T16:38:01.767Z"
    },
    {
      "id": "83db3009",
      "timestamp": "2025-08-18T16:33:40.396000+00:00",
      "name": "83db_tpu_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import os\nimport cv2\nimport numpy as np\nfrom tflite_runtime.interpreter import Interpreter, load_delegate\n\n# Configuration Parameters\nmodel_path  = \"/home/mendel/tinyml_autopilot/models/edgetpu_detect.tflite\"\nlabel_path  = \"/home/mendel/tinyml_autopilot/models/labelmap.txt\"\ninput_path  = \"/home/mendel/tinyml_autopilot/data//sheeps.mp4\"\noutput_path  = \"/home/mendel/tinyml_autopilot/results/sheeps_detections.mp4\"\nconfidence_threshold  = 0.5\n\n# Phase 1: Setup\ndef load_labels(label_file):\n    with open(label_file, 'r') as file:\n        return [line.strip() for line in file.readlines()]\n\ntry:\n    interpreter = Interpreter(\n        model_path=model_path,\n        experimental_delegates=[load_delegate('libedgetpu.so.1.0')]\n    )\nexcept Exception as e:\n    print(\"Failed to load EdgeTPU delegate:\", str(e))\n    try:\n        interpreter = Interpreter(\n            model_path=model_path,\n            experimental_delegates=[load_delegate('/usr/lib/aarch64-linux-gnu/libedgetpu.so.1.0')]\n        )\n    except Exception as e:\n        raise RuntimeError(\"Could not initialize the TFLite interpreter with EdgeTPU delegate.\") from e\n\ninterpreter.allocate_tensors()\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\n# Load labels\nlabels = load_labels(label_path)\n\n# Phase 2: Input Acquisition & Preprocessing Loop\ncap = cv2.VideoCapture(input_path)\nframe_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\nframe_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n\nout = cv2.VideoWriter(output_path, \n                      cv2.VideoWriter_fourcc(*'mp4v'), \n                      30.0, (frame_width, frame_height))\n\nfloating_model = input_details[0]['dtype'] == np.float32\n\nwhile cap.isOpened():\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    # Preprocess the image\n    img_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n    img_resized = cv2.resize(img_rgb, (input_details[0]['shape'][2], input_details[0]['shape'][1]))\n    \n    if floating_model:\n        img_normalized = (np.float32(img_resized) - 127.5) / 127.5\n    else:\n        img_normalized = np.uint8(img_resized)\n\n    input_data = np.expand_dims(img_normalized, axis=0)\n\n    # Phase 3: Inference\n    interpreter.set_tensor(input_details[0]['index'], input_data)\n    interpreter.invoke()\n\n    # Phase 4: Output Interpretation & Handling Loop\n    boxes = interpreter.get_tensor(output_details[0]['index'])[0]  # Bounding box coordinates of detected objects\n    classes = interpreter.get_tensor(output_details[1]['index'])[0]  # Class index of detected objects\n    scores = interpreter.get_tensor(output_details[2]['index'])[0]  # Confidence of detected objects\n\n    for i in range(len(scores)):\n        if scores[i] >= confidence_threshold:\n            ymin, xmin, ymax, xmax = boxes[i]\n            (left, right, top, bottom) = (xmin * frame_width, xmax * frame_width,\n                                          ymin * frame_height, ymax * frame_height)\n            \n            # Ensure the bounding box is within the frame dimensions\n            left, right, top, bottom = max(0, int(left)), min(frame_width, int(right)), \\\n                                       max(0, int(top)), min(frame_height, int(bottom))\n            \n            label = f\"{labels[int(classes[i])]}: {int(scores[i] * 100)}%\"\n            cv2.rectangle(frame, (left, top), (right, bottom), (10, 255, 0), 2)\n            cv2.putText(frame, label, (left, top - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (10, 255, 0), 2)\n\n    # Handle output\n    out.write(frame)\n\n# Phase 5: Cleanup\ncap.release()\nout.release()\ncv2.destroyAllWindows()"
      },
      "session_id": "phi4_7854_tpusg_batch",
      "metadata": {
        "num_run": 6
      },
      "tags": [
        "benchmark",
        "phi4:latest",
        "tpu_sketch_generator"
      ],
      "latency": 19.837,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-19-33-40-397685_chatcmpl-ed0ed0bf-6a46-4769-a1b3-b0977b926c67",
          "traceId": "83db3009",
          "type": "GENERATION",
          "name": "83db_tpusg_gen_attempt#1",
          "startTime": "2025-08-18T16:33:40.397000+00:00",
          "endTime": "2025-08-18T16:34:00.234000+00:00",
          "model": "phi4:latest",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 19837.0,
          "costDetails": {},
          "completionTokens": 961,
          "totalTokens": 2767,
          "environment": "default",
          "promptTokens": 1806,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "environment": "default",
      "createdAt": "2025-08-18T16:33:40.916Z",
      "updatedAt": "2025-08-18T16:34:58.632Z"
    },
    {
      "id": "4321629e",
      "timestamp": "2025-08-18T16:31:51.888000+00:00",
      "name": "4321_tpu_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import cv2\nimport numpy as np\nfrom tflite_runtime.interpreter import Interpreter, load_delegate\n\n# Configuration Parameters\nmodel_path  = \"/home/mendel/tinyml_autopilot/models/edgetpu_detect.tflite\"\nlabel_path  = \"/home/mendel/tinyml_autopilot/models/labelmap.txt\"\ninput_path  = \"/home/mendel/tinyml_autopilot/data//sheeps.mp4\"\noutput_path  = \"/home/mendel/tinyml_autopilot/results/sheeps_detections.mp4\"\nconfidence_threshold  = 0.5\n\n# Phase 1: Setup\ndef load_labels(path):\n    with open(path, 'r') as file:\n        return [line.strip() for line in file.readlines()]\n\nlabels = load_labels(label_path)\n\ntry:\n    interpreter = Interpreter(\n        model_path=model_path,\n        experimental_delegates=[load_delegate('libedgetpu.so.1.0')]\n    )\nexcept ValueError:\n    interpreter = Interpreter(\n        model_path=model_path,\n        experimental_delegates=[load_delegate('/usr/lib/aarch64-linux-gnu/libedgetpu.so.1.0')]\n    )\n\ninterpreter.allocate_tensors()\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\n# Get expected input size\nexpected_height, expected_width = input_details[0]['shape'][1:3]\n\n# Phase 2: Input Acquisition & Preprocessing Loop\ncap = cv2.VideoCapture(input_path)\nframe_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\nframe_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n\nout = None\n\nwhile cap.isOpened():\n    ret, frame = cap.read()\n    if not ret:\n        break\n    \n    # Resize frame to match model's expected input size\n    resized_frame = cv2.resize(frame, (expected_width, expected_height))\n    \n    input_data = np.expand_dims(resized_frame, axis=0)\n\n    # Phase 3: Inference\n    interpreter.set_tensor(input_details[0]['index'], input_data)\n    interpreter.invoke()\n\n    # Phase 4: Output Interpretation & Handling Loop\n    boxes = interpreter.get_tensor(output_details[0]['index'])[0]  # Bounding box coordinates of detected objects\n    classes = interpreter.get_tensor(output_details[1]['index'])[0]  # Class index of detected objects\n    scores = interpreter.get_tensor(output_details[2]['index'])[0]  # Confidence of detected objects\n\n    # Filter out low-confidence detections\n    for i in range(len(scores)):\n        if scores[i] >= confidence_threshold:\n            ymin, xmin, ymax, xmax = boxes[i]\n            (left, right, top, bottom) = (xmin * frame_width, xmax * frame_width,\n                                          ymin * frame_height, ymax * frame_height)\n            \n            # Draw bounding box and label on the frame\n            cv2.rectangle(frame, (int(left), int(top)), (int(right), int(bottom)), (0, 255, 0), 2)\n            object_name = labels[int(classes[i])]\n            label = f'{object_name}: {int(scores[i]*100)}%'\n            label_size, base_line = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.7, 2)\n            top = max(top, label_size[1])\n            cv2.rectangle(frame, (int(left), int(top) - round(1.5*label_size[1])), \n                          (int(left) + round(1.5*label_size[0]), int(top) + base_line), \n                          (255, 255, 255), cv2.FILLED)\n            cv2.putText(frame, label, (int(left), int(top) + base_line), \n                        cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 0), 2)\n\n    # Initialize video writer\n    if out is None:\n        fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n        out = cv2.VideoWriter(output_path, fourcc, 20.0, (frame_width, frame_height))\n\n    out.write(frame)\n    \n    # Display the resulting frame\n    cv2.imshow('Object detector', frame)\n\n    if cv2.waitKey(1) & 0xFF == ord('q'):\n        break\n\ncap.release()\nout.release()\ncv2.destroyAllWindows()"
      },
      "session_id": "phi4_7854_tpusg_batch",
      "metadata": {
        "num_run": 5
      },
      "tags": [
        "benchmark",
        "phi4:latest",
        "tpu_sketch_generator"
      ],
      "latency": 49.074,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-19-31-51-890160_chatcmpl-8deaee44-4bd6-409a-ac8f-a0df75e026c4",
          "traceId": "4321629e",
          "type": "GENERATION",
          "name": "4321_tpusg_gen_attempt#1",
          "startTime": "2025-08-18T16:31:51.890000+00:00",
          "endTime": "2025-08-18T16:32:09.821000+00:00",
          "model": "phi4:latest",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 17931.0,
          "costDetails": {},
          "completionTokens": 851,
          "totalTokens": 2657,
          "environment": "default",
          "promptTokens": 1806,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-19-32-18-281601_chatcmpl-5a8cff9e-6244-47df-9525-e10f29d9914b",
          "traceId": "4321629e",
          "type": "GENERATION",
          "name": "4321_tpusg_gen_attempt#2",
          "startTime": "2025-08-18T16:32:18.281000+00:00",
          "endTime": "2025-08-18T16:32:40.964000+00:00",
          "model": "phi4:latest",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 22683.0,
          "costDetails": {},
          "completionTokens": 1113,
          "totalTokens": 3161,
          "environment": "default",
          "promptTokens": 2048,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "environment": "default",
      "createdAt": "2025-08-18T16:31:52.408Z",
      "updatedAt": "2025-08-18T16:33:40.071Z"
    },
    {
      "id": "cfa493f2",
      "timestamp": "2025-08-18T16:30:07.384000+00:00",
      "name": "cfa4_tpu_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import cv2\nimport numpy as np\nfrom tflite_runtime.interpreter import Interpreter, load_delegate\n\ndef load_labels(label_path):\n    with open(label_path, 'r') as file:\n        return [line.strip() for line in file.readlines()]\n\ndef main():\n    # Phase 1: Setup\n    model_path = '/home/mendel/tinyml_autopilot/models/edgetpu_detect.tflite'\n    label_path = '/home/mendel/tinyml_autopilot/models/labelmap.txt'\n    input_path = '/home/mendel/tinyml_autopilot/data/sheeps.mp4'\n    output_path = '/home/mendel/tinyml_autopilot/results/sheeps_detections.mp4'\n    confidence_threshold = 0.5\n\n    labels = load_labels(label_path)\n\n    # Load interpreter with EdgeTPU\n    try:\n        interpreter = Interpreter(model_path=model_path, experimental_delegates=[load_delegate('libedgetpu.so.1.0')])\n    except RuntimeError:\n        try:\n            interpreter = Interpreter(model_path=model_path, experimental_delegates=[load_delegate('/usr/lib/aarch64-linux-gnu/libedgetpu.so.1.0')])\n        except Exception as e:\n            print(f\"Failed to load EdgeTPU delegate: {e}\")\n            return\n\n    interpreter.allocate_tensors()\n    input_details = interpreter.get_input_details()\n    output_details = interpreter.get_output_details()\n\n    # Phase 2: Input Acquisition & Preprocessing Loop\n    cap = cv2.VideoCapture(input_path)\n    if not cap.isOpened():\n        print(\"Error opening video stream or file\")\n        return\n\n    frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n    frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n\n    out = cv2.VideoWriter(output_path, cv2.VideoWriter_fourcc(*'mp4v'), 30, (frame_width, frame_height))\n\n    # Get input size from model\n    input_size = input_details[0]['shape'][1:3]\n\n    while cap.isOpened():\n        ret, frame = cap.read()\n        if not ret:\n            break\n\n        # Resize the frame to match the model's expected input size\n        resized_frame = cv2.resize(frame, (input_size[1], input_size[0]))\n\n        # Preprocess input\n        frame_rgb = cv2.cvtColor(resized_frame, cv2.COLOR_BGR2RGB)\n        input_data = np.expand_dims(frame_rgb, axis=0)\n\n        interpreter.set_tensor(input_details[0]['index'], input_data)\n        \n        # Phase 3: Inference\n        interpreter.invoke()\n\n        # Phase 4: Output Interpretation & Handling Loop\n        boxes = interpreter.get_tensor(output_details[0]['index'])[0]\n        classes = interpreter.get_tensor(output_details[1]['index'])[0]\n        scores = interpreter.get_tensor(output_details[2]['index'])[0]\n\n        for i in range(len(scores)):\n            if scores[i] >= confidence_threshold:\n                ymin, xmin, ymax, xmax = boxes[i]\n                (left, right, top, bottom) = (xmin * frame_width, xmax * frame_width,\n                                              ymin * frame_height, ymax * frame_height)\n                left, right, top, bottom = int(left), int(right), int(top), int(bottom)\n\n                cv2.rectangle(frame, (left, top), (right, bottom), (0, 255, 0), 2)\n                label = f\"{labels[int(classes[i])]}: {int(scores[i]*100)}%\"\n                cv2.putText(frame, label, (left, top - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n\n        out.write(frame)\n\n    cap.release()\n    out.release()\n\nif __name__ == \"__main__\":\n    main()"
      },
      "session_id": "phi4_7854_tpusg_batch",
      "metadata": {
        "num_run": 4
      },
      "tags": [
        "benchmark",
        "phi4:latest",
        "tpu_sketch_generator"
      ],
      "latency": 45.982,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-19-30-07-386197_chatcmpl-1dbafdab-c5f1-4ef7-baad-62cefad5adfe",
          "traceId": "cfa493f2",
          "type": "GENERATION",
          "name": "cfa4_tpusg_gen_attempt#1",
          "startTime": "2025-08-18T16:30:07.386000+00:00",
          "endTime": "2025-08-18T16:30:24.152000+00:00",
          "model": "phi4:latest",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 16766.0,
          "costDetails": {},
          "completionTokens": 801,
          "totalTokens": 2607,
          "environment": "default",
          "promptTokens": 1806,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-19-30-32-595441_chatcmpl-ee710824-1efc-4c2d-aff9-bc3075dfd734",
          "traceId": "cfa493f2",
          "type": "GENERATION",
          "name": "cfa4_tpusg_gen_attempt#2",
          "startTime": "2025-08-18T16:30:32.595000+00:00",
          "endTime": "2025-08-18T16:30:53.368000+00:00",
          "model": "phi4:latest",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 20773.0,
          "costDetails": {},
          "completionTokens": 1003,
          "totalTokens": 3051,
          "environment": "default",
          "promptTokens": 2048,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "environment": "default",
      "createdAt": "2025-08-18T16:30:07.904Z",
      "updatedAt": "2025-08-18T16:31:51.097Z"
    },
    {
      "id": "b860ea2b",
      "timestamp": "2025-08-18T16:28:23.853000+00:00",
      "name": "b860_tpu_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import cv2\nfrom tflite_runtime.interpreter import Interpreter, load_delegate\nimport numpy as np\n\n# CONFIGURATION PARAMETERS\nmodel_path  = \"/home/mendel/tinyml_autopilot/models/edgetpu_detect.tflite\"\nlabel_path  = \"/home/mendel/tinyml_autopilot/models/labelmap.txt\"\ninput_path  = \"/home/mendel/tinyml_autopilot/data//sheeps.mp4\"\noutput_path  = \"/home/mendel/tinyml_autopilot/results/sheeps_detections.mp4\"\nconfidence_threshold  = 0.5\n\n# Phase 1: Setup\ndef load_labels(path):\n    with open(path, 'r') as file:\n        return [line.strip() for line in file.readlines()]\n\ntry:\n    labels = load_labels(label_path)\nexcept Exception as e:\n    print(f\"Error loading label map: {e}\")\n    labels = []\n\n# Load the Edge TPU interpreter\ntry:\n    interpreter = Interpreter(\n        model_path=model_path,\n        experimental_delegates=[load_delegate('libedgetpu.so.1.0')]\n    )\nexcept ValueError:\n    try:\n        interpreter = Interpreter(\n            model_path=model_path,\n            experimental_delegates=[load_delegate('/usr/lib/aarch64-linux-gnu/libedgetpu.so.1.0')]\n        )\n    except Exception as e:\n        print(f\"Failed to load Edge TPU delegate: {e}\")\n        raise\n\ninterpreter.allocate_tensors()\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\n# Get model input details\nheight, width = input_details[0]['shape'][1:3]  # Model expects 300x300\n\nfloating_model = (input_details[0]['dtype'] == np.float32)\n\n# Phase 2: Input Acquisition & Preprocessing Loop\ncap = cv2.VideoCapture(input_path)\nif not cap.isOpened():\n    print(\"Error opening video file\")\n    raise SystemExit\n\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\nout = cv2.VideoWriter(output_path, fourcc, 20.0, (width, height))\n\nwhile True:\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    # Resize image to match model's expected input size\n    resized_image = cv2.resize(frame, (width, height))\n    input_data = np.expand_dims(resized_image, axis=0)\n\n    if floating_model:\n        input_data = (np.float32(input_data) - 127.5) / 127.5\n\n    # Phase 3: Inference\n    interpreter.set_tensor(input_details[0]['index'], input_data)\n    interpreter.invoke()\n\n    # Phase 4: Output Interpretation & Handling Loop\n    boxes = interpreter.get_tensor(output_details[0]['index'])[0]  # Bounding box coordinates of detected objects\n    classes = interpreter.get_tensor(output_details[1]['index'])[0]  # Class index of detected objects\n    scores = interpreter.get_tensor(output_details[2]['index'])[0]  # Confidence of detected objects\n\n    for i in range(len(scores)):\n        if scores[i] >= confidence_threshold:\n            ymin, xmin, ymax, xmax = boxes[i]\n            \n            left, right, top, bottom = (xmin * width, xmax * width,\n                                        ymin * height, ymax * height)\n            \n            label_index = int(classes[i])\n            if labels:\n                label = f\"{labels[label_index]}: {int(scores[i] * 100)}%\"\n            else:\n                label = f\"ID {label_index}: {int(scores[i] * 100)}%\"\n\n            cv2.rectangle(frame, (int(left), int(top)), (int(right), int(bottom)), (0, 255, 0), 2)\n            cv2.putText(frame, label, (int(left), int(top) - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 1)\n\n    # Display the resulting frame\n    out.write(frame)\n\n# Cleanup Phase 5: Release resources\ncap.release()\nout.release()\ncv2.destroyAllWindows()"
      },
      "session_id": "phi4_7854_tpusg_batch",
      "metadata": {
        "num_run": 3
      },
      "tags": [
        "benchmark",
        "phi4:latest",
        "tpu_sketch_generator"
      ],
      "latency": 49.51,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-19-28-23-854880_chatcmpl-5571b45b-1232-48d4-8bd8-1dccf7d85bb4",
          "traceId": "b860ea2b",
          "type": "GENERATION",
          "name": "b860_tpusg_gen_attempt#1",
          "startTime": "2025-08-18T16:28:23.854000+00:00",
          "endTime": "2025-08-18T16:28:42.270000+00:00",
          "model": "phi4:latest",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 18416.0,
          "costDetails": {},
          "completionTokens": 887,
          "totalTokens": 2693,
          "environment": "default",
          "promptTokens": 1806,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-19-28-50-767782_chatcmpl-3203761c-3c29-4e2b-889a-4cbf8e835320",
          "traceId": "b860ea2b",
          "type": "GENERATION",
          "name": "b860_tpusg_gen_attempt#2",
          "startTime": "2025-08-18T16:28:50.767000+00:00",
          "endTime": "2025-08-18T16:29:13.364000+00:00",
          "model": "phi4:latest",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 22597.0,
          "costDetails": {},
          "completionTokens": 1109,
          "totalTokens": 3157,
          "environment": "default",
          "promptTokens": 2048,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "environment": "default",
      "createdAt": "2025-08-18T16:28:24.371Z",
      "updatedAt": "2025-08-18T16:30:07.144Z"
    },
    {
      "id": "b5e3e32f",
      "timestamp": "2025-08-18T16:27:08.318000+00:00",
      "name": "b5e3_tpu_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import cv2\nfrom tflite_runtime.interpreter import Interpreter, load_delegate\nimport numpy as np\n\n# Configuration Parameters\nmodel_path  = \"/home/mendel/tinyml_autopilot/models/edgetpu_detect.tflite\"\nlabel_path  = \"/home/mendel/tinyml_autopilot/models/labelmap.txt\"\ninput_path  = \"/home/mendel/tinyml_autopilot/data//sheeps.mp4\"\noutput_path  = \"/home/mendel/tinyml_autopilot/results/sheeps_detections.mp4\"\nconfidence_threshold  = 0.5\n\n# Phase 1: Setup\ndef load_labels(path):\n    with open(path, 'r') as file:\n        return [line.strip() for line in file.readlines()]\n\nlabels = load_labels(label_path)\n\ntry:\n    interpreter = Interpreter(\n        model_path=model_path,\n        experimental_delegates=[load_delegate('libedgetpu.so.1.0')]\n    )\nexcept ValueError:\n    print(\"Failed to load Edge TPU delegate, falling back to CPU.\")\n    interpreter = Interpreter(model_path=model_path)\n\ninterpreter.allocate_tensors()\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\n# Phase 2: Input Acquisition & Preprocessing Loop\ncap = cv2.VideoCapture(input_path)\nframe_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\nframe_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\nout = cv2.VideoWriter(output_path, cv2.VideoWriter_fourcc(*'mp4v'), 30, (frame_width, frame_height))\n\ndef preprocess_frame(frame):\n    input_shape = input_details[0]['shape']\n    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n    frame_resized = cv2.resize(frame_rgb, (input_shape[1], input_shape[2]))\n    return np.expand_dims(frame_resized, axis=0)\n\n# Phase 3: Inference\ndef set_input_tensor(input_data):\n    interpreter.set_tensor(input_details[0]['index'], input_data)\n\ndef get_output_tensors():\n    boxes = interpreter.get_tensor(output_details[0]['index'])[0] # Bounding box coordinates of detected objects\n    classes = interpreter.get_tensor(output_details[1]['index'])[0] # Class index of detected objects\n    scores = interpreter.get_tensor(output_details[2]['index'])[0] # Confidence of detected objects\n    count = int(interpreter.get_tensor(output_details[3]['index'])[0]) # Number of objects detected\n    return boxes, classes, scores, count\n\n# Phase 4: Output Interpretation & Handling Loop\ndef interpret_output(boxes, classes, scores, frame):\n    for i in range(len(scores)):\n        if scores[i] >= confidence_threshold:\n            ymin, xmin, ymax, xmax = boxes[i]\n            (left, right, top, bottom) = (xmin * frame.shape[1], xmax * frame.shape[1],\n                                          ymin * frame.shape[0], ymax * frame.shape[0])\n            cv2.rectangle(frame, (int(left), int(top)), (int(right), int(bottom)), (0, 255, 0), 2)\n            label = f'{labels[int(classes[i])]}: {int(scores[i] * 100)}%'\n            cv2.putText(frame, label, (int(left), int(top - 5)), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n\n# Main processing loop\nwhile cap.isOpened():\n    ret, frame = cap.read()\n    if not ret:\n        break\n    \n    input_data = preprocess_frame(frame)\n    set_input_tensor(input_data)\n    interpreter.invoke()\n\n    boxes, classes, scores, count = get_output_tensors()\n    interpret_output(boxes, classes, scores, frame)\n\n    out.write(frame)\n\ncap.release()\nout.release()\ncv2.destroyAllWindows()"
      },
      "session_id": "phi4_7854_tpusg_batch",
      "metadata": {
        "num_run": 2
      },
      "tags": [
        "benchmark",
        "phi4:latest",
        "tpu_sketch_generator"
      ],
      "latency": 16.722,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-19-27-08-320510_chatcmpl-f7e1c2a1-612e-4e4e-822a-19c81e4b4abc",
          "traceId": "b5e3e32f",
          "type": "GENERATION",
          "name": "b5e3_tpusg_gen_attempt#1",
          "startTime": "2025-08-18T16:27:08.320000+00:00",
          "endTime": "2025-08-18T16:27:25.042000+00:00",
          "model": "phi4:latest",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 16722.0,
          "costDetails": {},
          "completionTokens": 799,
          "totalTokens": 2605,
          "environment": "default",
          "promptTokens": 1806,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "environment": "default",
      "createdAt": "2025-08-18T16:27:08.836Z",
      "updatedAt": "2025-08-18T16:28:23.087Z"
    },
    {
      "id": "a6b3d80c",
      "timestamp": "2025-08-18T16:25:22.692000+00:00",
      "name": "a6b3_tpu_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import cv2\nimport numpy as np\nfrom tflite_runtime.interpreter import Interpreter, load_delegate\n\n# Configuration Parameters\nmodel_path  = \"/home/mendel/tinyml_autopilot/models/edgetpu_detect.tflite\"\nlabel_path  = \"/home/mendel/tinyml_autopilot/models/labelmap.txt\"\ninput_path  = \"/home/mendel/tinyml_autopilot/data//sheeps.mp4\"\noutput_path  = \"/home/mendel/tinyml_autopilot/results/sheeps_detections.mp4\"\nconfidence_threshold  = 0.5\n\n# Load labels\ndef load_labels(label_file):\n    with open(label_file, 'r') as file:\n        return [line.strip() for line in file.readlines()]\n\nlabels = load_labels(label_path)\n\ntry:\n    interpreter = Interpreter(\n        model_path=model_path,\n        experimental_delegates=[load_delegate('libedgetpu.so.1.0')]\n    )\nexcept ValueError:\n    interpreter = Interpreter(\n        model_path=model_path,\n        experimental_delegates=[load_delegate('/usr/lib/aarch64-linux-gnu/libedgetpu.so.1.0')]\n    )\n\ninterpreter.allocate_tensors()\n\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\n# Determine if the model is floating-point or integer\nfloating_model = input_details[0]['dtype'] == np.float32\n\ncap = cv2.VideoCapture(input_path)\nframe_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\nframe_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n\nout = cv2.VideoWriter(output_path, cv2.VideoWriter_fourcc(*'mp4v'), 30.0, (frame_width, frame_height))\n\nwhile cap.isOpened():\n    ret, frame = cap.read()\n    if not ret:\n        break\n    \n    # Preprocess the frame\n    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n    frame_resized = cv2.resize(frame_rgb, (input_details[0]['shape'][2], input_details[0]['shape'][1]))\n    \n    input_data = np.expand_dims(frame_resized, axis=0)\n\n    if floating_model:\n        input_data = input_data.astype(np.float32)\n        input_data = (np.float32(input_data) - 127.5) / 127.5\n    else:\n        # Convert to UINT8 without normalization\n        input_data = input_data.astype(np.uint8)\n\n    # Inference\n    interpreter.set_tensor(input_details[0]['index'], input_data)\n    interpreter.invoke()\n\n    # Output Interpretation & Handling Loop\n    boxes = interpreter.get_tensor(output_details[0]['index'])[0]  # Bounding box coordinates of detected objects\n    classes = interpreter.get_tensor(output_details[1]['index'])[0]  # Class index of detected objects\n    scores = interpreter.get_tensor(output_details[2]['index'])[0]  # Confidence of detected objects\n\n    for i in range(len(scores)):\n        if scores[i] >= confidence_threshold:\n            ymin, xmin, ymax, xmax = boxes[i]\n            (left, right, top, bottom) = (xmin * frame_width, xmax * frame_width,\n                                          ymin * frame_height, ymax * frame_height)\n\n            left, right, top, bottom = int(left), int(right), int(top), int(bottom)\n            \n            label = f\"{labels[int(classes[i])]}: {int(scores[i]*100)}%\"\n            cv2.rectangle(frame, (left, top), (right, bottom), (10, 255, 0), 2)\n            cv2.putText(frame, label, (left, top-15), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (10, 255, 0), 2)\n\n    out.write(frame)\n\ncap.release()\nout.release()\n\n# Cleanup\ncv2.destroyAllWindows()"
      },
      "session_id": "phi4_7854_tpusg_batch",
      "metadata": {
        "num_run": 1
      },
      "tags": [
        "benchmark",
        "phi4:latest",
        "tpu_sketch_generator"
      ],
      "latency": 46.967,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-19-25-22-694062_chatcmpl-2111d3b7-d169-432a-8494-af060bf6efb9",
          "traceId": "a6b3d80c",
          "type": "GENERATION",
          "name": "a6b3_tpusg_gen_attempt#1",
          "startTime": "2025-08-18T16:25:22.694000+00:00",
          "endTime": "2025-08-18T16:25:41.057000+00:00",
          "model": "phi4:latest",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 18363.0,
          "costDetails": {},
          "completionTokens": 884,
          "totalTokens": 2690,
          "environment": "default",
          "promptTokens": 1806,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-19-25-49-595794_chatcmpl-9de80593-7383-404f-98ad-48557d9557a8",
          "traceId": "a6b3d80c",
          "type": "GENERATION",
          "name": "a6b3_tpusg_gen_attempt#2",
          "startTime": "2025-08-18T16:25:49.595000+00:00",
          "endTime": "2025-08-18T16:26:09.661000+00:00",
          "model": "phi4:latest",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 20066.0,
          "costDetails": {},
          "completionTokens": 973,
          "totalTokens": 3021,
          "environment": "default",
          "promptTokens": 2048,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "environment": "default",
      "createdAt": "2025-08-18T16:25:23.210Z",
      "updatedAt": "2025-08-18T16:27:07.432Z"
    }
  ],
  "meta": {
    "total_items": 30
  }
}
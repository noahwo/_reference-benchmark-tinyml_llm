{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setting up\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ref: https://langfuse.com/docs/query-traces\n",
    "import os\n",
    "import json\n",
    "from langfuse import Langfuse\n",
    "import json\n",
    "import os\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "LOCAL_HOST = True\n",
    "\n",
    "\"\"\"Define session_id\"\"\"\n",
    "# session_id=\"qwen2.5-coder_f4d4_dp_batch\"\n",
    "session_id_list = [\n",
    "    # \"qwen2.5-coder:32b_4e11_tpsg_batch\",\n",
    "    # \"qwen2.5-coder:32b_ae24_tpsg_batch\"\n",
    "    \"phi4_7854_psg_batch\",\n",
    "    \"phi4_7854_tpusg_batch\"\n",
    "]\n",
    "\n",
    "\n",
    "\"\"\"Define paths\"\"\"\n",
    " \n",
    "parent_dir = os.path.dirname(os.getcwd())\n",
    "\n",
    " \n",
    "# date = os.path.basename(parent_dir)\n",
    "tex_dir = os.path.join(parent_dir, \"tex\")\n",
    "processed_data_dir = os.path.join(parent_dir, \"processed_data\")\n",
    "raw_export_dir = os.path.join(parent_dir, \"raw_export\")\n",
    "ipynb_dir = os.path.join(parent_dir, \"ipynb\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Export raw data\n",
    "\n",
    "Langfuse added a limit of 20 API invocations per minute. https://langfuse.com/faq/all/api-limits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching traces for session phi4_7854_psg_batch...\n",
      "Fetching observation data for time-21-00-24-217285_chatcmpl-a733c311-dfb8-4c6c-a296-5adaed94998d...\n",
      "Fetching observation data for time-21-00-39-809353_chatcmpl-5165542c-b0b0-4534-b7e7-d24d872743c7...\n",
      "Fetching observation data for time-21-00-59-493097_chatcmpl-1bdff160-dde9-490c-8e1c-342d5c268379...\n",
      "Fetching observation data for time-21-01-21-330265_chatcmpl-efc292bb-5547-498e-8712-03ae7ac59386...\n",
      "Fetching observation data for time-21-01-44-791886_chatcmpl-c94fb0fc-c71a-4996-8441-37bfcd790bbf...\n",
      "Fetching observation data for 406b4350-71cb-450e-82cf-b3e659916d78...\n",
      "Fetching observation data for time-20-59-29-707123_chatcmpl-3d3f99a1-b154-487f-8f3d-8ad3cb74e0c1...\n",
      "Fetching observation data for time-20-58-00-156978_chatcmpl-9dd3eb68-f943-4081-9ea1-f375529f63e7...\n",
      "Fetching observation data for time-20-58-15-189956_chatcmpl-bb9f7a31-ce36-4d9b-97fe-742064137c17...\n",
      "Fetching observation data for time-20-58-32-611304_chatcmpl-5edf159e-baba-4979-bd0c-d194b78f78ef...\n",
      "Fetching observation data for time-20-56-10-610559_chatcmpl-6a3a1769-1892-48c9-8ce4-cd2efe35a8f6...\n",
      "Fetching observation data for time-20-56-26-134235_chatcmpl-0c797e6f-a710-43e3-a8a8-e426c5e0e720...\n",
      "Fetching observation data for time-20-56-49-905369_chatcmpl-c00e205e-60ca-41bb-8c9e-199ebd560a26...\n",
      "Fetching observation data for time-20-57-04-061326_chatcmpl-243b3e03-f85d-40ef-878c-cc2e8c964d3c...\n",
      "Fetching observation data for time-20-57-26-980807_chatcmpl-eb53b15b-71a3-4931-b904-d0b53ad6d1c7...\n",
      "Fetching observation data for 83c7c3bb-2081-494b-9987-4450aa475c6d...\n",
      "Fetching observation data for time-20-55-15-988788_chatcmpl-b68e1f8b-789f-4331-aa79-929ca26921fb...\n",
      "Fetching observation data for time-20-53-25-462169_chatcmpl-507557aa-cbb0-4880-8222-639608ffe2ce...\n",
      "Fetching observation data for time-20-53-35-172398_chatcmpl-4aa4525e-0719-42ae-af6c-e18253a24b4e...\n",
      "Fetching observation data for time-20-53-55-206744_chatcmpl-dde16a2c-9b24-4b9c-97d1-6a0ebdd3c41d...\n",
      "Fetching observation data for time-20-54-16-036028_chatcmpl-d4c30b00-e1d0-42fb-a344-f3329eca00b5...\n",
      "Fetching observation data for time-20-54-39-283473_chatcmpl-08807409-5e90-4d68-8549-f73302130e12...\n",
      "Fetching observation data for time-20-51-00-893401_chatcmpl-a2c98820-68f5-45c7-8bfd-3797ada7b5e1...\n",
      "Fetching observation data for time-20-51-18-300270_chatcmpl-cc05789d-7b4a-4a24-b9cd-92021a712b14...\n",
      "Fetching observation data for time-20-51-38-329358_chatcmpl-dca52b08-4405-4594-82b5-bf3e4ef3f2b1...\n",
      "Fetching observation data for time-20-52-01-255132_chatcmpl-ba8ddd83-81f1-4715-af6a-38536cb8a4cf...\n",
      "Fetching observation data for time-20-52-24-621520_chatcmpl-adccdc9e-dc36-4d74-b2fe-134eb223956d...\n",
      "Fetching observation data for time-20-49-08-244391_chatcmpl-8489bbd0-4dbd-49af-a003-1483dabe9897...\n",
      "Fetching observation data for time-20-49-21-102090_chatcmpl-a4fccf6e-968f-46aa-9914-2ab3b67d8d74...\n",
      "Fetching observation data for time-20-49-41-570494_chatcmpl-6853808a-79ec-44c5-bc5e-22fb54b7e1af...\n",
      "Fetching observation data for time-20-50-01-731520_chatcmpl-602b6ea8-6c0f-4729-ae83-3b194b70a2f2...\n",
      "Fetching observation data for time-20-47-09-717424_chatcmpl-51754aed-07a0-4434-ad45-7ddbd22d829f...\n",
      "Fetching observation data for time-20-47-19-816771_chatcmpl-125d7714-f63e-4e86-810e-b30987147ae8...\n",
      "Fetching observation data for time-20-47-38-413030_chatcmpl-380a4c1c-dd0d-4ba2-99a0-c4daa55b83bd...\n",
      "Fetching observation data for time-20-47-55-258359_chatcmpl-5d7cc3e9-c8f1-4af5-8dca-986f1ce8df2d...\n",
      "Fetching observation data for time-20-48-12-217325_chatcmpl-2118553f-6b4e-48d7-a97e-478507c3c80f...\n",
      "Fetching observation data for time-20-46-17-205400_chatcmpl-85bcb725-c020-4ccc-a3fc-bf865c922222...\n",
      "Fetching observation data for time-20-45-05-685434_chatcmpl-86836a58-bfa6-40c4-81b2-77b081ff96d0...\n",
      "Fetching observation data for time-20-45-20-472821_chatcmpl-e79a42d6-0b81-4f5f-b080-b379d83ebacb...\n",
      "Fetching observation data for time-20-43-56-187574_chatcmpl-1560ca74-ca16-4358-995d-52fb35f66fbc...\n",
      "Fetching observation data for time-20-44-10-105272_chatcmpl-b8594f45-5fc8-41bc-8b21-a5187ed9d739...\n",
      "Fetching observation data for time-20-42-38-663465_chatcmpl-2a24832b-9f4c-44c8-9360-8bbb2e4959ac...\n",
      "Fetching observation data for time-20-42-52-062480_chatcmpl-26e64bd4-7de4-408f-b792-da2e14368438...\n",
      "Fetching observation data for time-20-43-16-422952_chatcmpl-749fc7ce-bd65-4b2b-a416-c0fc356b6402...\n",
      "Fetching observation data for time-20-41-23-156886_chatcmpl-8b9abad8-2a4c-4d0c-be9c-7e94cb9e414d...\n",
      "Fetching observation data for time-20-41-39-016648_chatcmpl-b0c57d5b-47c6-42b7-a342-af5210c42505...\n",
      "Fetching observation data for time-20-40-31-636616_chatcmpl-8d4ea992-6b1a-4b6f-a832-852cd6d1f619...\n",
      "Fetching observation data for time-20-38-58-984670_chatcmpl-fc4826d6-998f-4d13-bc90-577b88dba97a...\n",
      "Fetching observation data for time-20-39-12-241271_chatcmpl-5c07cc46-ceca-4860-b517-dea511e91a33...\n",
      "Fetching observation data for time-20-39-33-341962_chatcmpl-85b509cd-82ad-44d0-89fa-22039fd12a22...\n",
      "Fetching observation data for time-20-37-43-340910_chatcmpl-9a4bdcac-2acd-48ca-be8a-9fb899c03b50...\n",
      "Fetching observation data for time-20-37-59-086597_chatcmpl-1635d449-2acc-4404-8b3d-0989769ae10a...\n",
      "Fetching observation data for time-20-35-42-786822_chatcmpl-987b7e1e-50d2-4bea-94d7-1d4df5c41878...\n",
      "Fetching observation data for time-20-36-00-542490_chatcmpl-514520de-a13d-4d34-b14d-0bf57d51f5ba...\n",
      "Fetching observation data for time-20-36-24-581098_chatcmpl-963bd2fa-4dc2-4ca3-800f-905dbdd93274...\n",
      "Fetching observation data for time-20-36-44-228434_chatcmpl-396699b1-3131-4de5-a9eb-de036745f4b7...\n",
      "Fetching observation data for time-20-34-07-142156_chatcmpl-b3524ebb-05a3-40ce-beeb-3024c507aa58...\n",
      "Fetching observation data for time-20-34-22-422546_chatcmpl-bdc67e52-0ca5-48c8-baa7-d67fe5af01a3...\n",
      "Fetching observation data for time-20-34-44-268069_chatcmpl-8ef24001-8c4c-494e-9f70-97101848e53b...\n",
      "Fetching observation data for time-20-31-57-515652_chatcmpl-c9e7c4f9-5a7e-4f5c-986d-3e251341988b...\n",
      "Fetching observation data for time-20-32-40-052926_chatcmpl-01221e88-ea08-4309-b1b4-2aa3d51ce692...\n",
      "Fetching observation data for time-20-33-06-024746_chatcmpl-30d07125-e337-4f55-a1dd-d5ddb050cbd4...\n",
      "Fetching observation data for time-20-33-30-791332_chatcmpl-f4526ed2-56d4-46cf-a1ef-b4e46c3a4bf7...\n",
      "Fetching observation data for time-20-30-00-709121_chatcmpl-f49f570e-6b50-41d5-bde6-9cb4f5f1b8f4...\n",
      "Fetching observation data for time-20-30-19-267915_chatcmpl-318f8602-b8a3-4fe9-90cb-bb4cfc6a42d2...\n",
      "Fetching observation data for time-20-30-42-533268_chatcmpl-1a4c80b5-b447-48e7-9913-b83c2af1a544...\n",
      "Fetching observation data for time-20-31-04-432011_chatcmpl-2df05995-edac-4c95-bd94-6ebdb2233178...\n",
      "Fetching observation data for time-20-31-27-703558_chatcmpl-184aee82-729f-4dcc-9384-9e5acd1b223e...\n",
      "Fetching observation data for db8250ff-d1f5-49d9-adc7-f37e989fa632...\n",
      "Fetching observation data for time-20-28-50-939142_chatcmpl-995a8acd-aaca-4dfe-8bab-971f2d3abfd5...\n",
      "Fetching observation data for time-20-29-04-496190_chatcmpl-88eb580e-2437-4a17-9760-8e2d4efb7c68...\n",
      "Fetching observation data for time-20-27-35-169631_chatcmpl-beafdd87-46c7-49a2-aa8e-f83a7df58224...\n",
      "Fetching observation data for time-20-27-49-354703_chatcmpl-6bbb953b-f0c9-4fd7-a013-bdaa2559ca44...\n",
      "Fetching observation data for time-20-25-57-938684_chatcmpl-1a8da50a-160e-430e-987c-baddf554cc66...\n",
      "Fetching observation data for time-20-26-13-053791_chatcmpl-48fcc8c2-7dd0-4a2d-a7c2-f0229130177e...\n",
      "Fetching observation data for time-20-26-34-582836_chatcmpl-07c6ce2b-1208-411c-a362-96d6069daf55...\n",
      "Fetching observation data for time-20-24-16-360118_chatcmpl-410a5bf5-27f3-492b-af92-cd287d9bba18...\n",
      "Fetching observation data for time-20-24-32-549091_chatcmpl-985d88e7-4e0a-4636-89e0-246d662adb98...\n",
      "Fetching observation data for time-20-24-55-886475_chatcmpl-95fa0892-281f-4176-9d89-051c0e937956...\n",
      "Fetching observation data for time-20-22-43-751293_chatcmpl-c138cccc-1db9-42f3-95e3-d4efb0b7d86d...\n",
      "Fetching observation data for time-20-22-58-313259_chatcmpl-68532b48-d651-4d9e-9565-3c19aff6e7c1...\n",
      "Fetching observation data for time-20-23-16-670781_chatcmpl-f3fe9af6-afed-476a-b66d-6f7005f51322...\n",
      "Fetching observation data for time-20-20-59-167390_chatcmpl-3ad42db8-2f0f-4360-8d38-caca22bb353f...\n",
      "Fetching observation data for time-20-21-12-035015_chatcmpl-8831a7e0-f836-4306-af00-5aaf664fd8b3...\n",
      "Fetching observation data for time-20-21-33-749511_chatcmpl-6105157b-7019-4ace-b693-ae7e3dc66949...\n",
      "Fetching observation data for time-20-21-52-152978_chatcmpl-5e974775-d207-4fb1-af1e-5a960817a03e...\n",
      "Fetching observation data for time-20-22-14-015881_chatcmpl-5952236f-3d15-4531-811e-50d27978d2fa...\n",
      "Fetching observation data for 3dbc4302-c94c-4480-bbd6-5b44401392be...\n",
      "Fetching observation data for time-20-19-55-577846_chatcmpl-558fad93-ba1f-4bc4-8f06-12deabc2863f...\n",
      "Fetching observation data for time-20-18-41-982451_chatcmpl-35b3e745-c13b-4614-a861-2b59fd8cf638...\n",
      "Fetching observation data for time-20-18-56-701972_chatcmpl-8a1729eb-616a-4e33-bec8-5c1abbaa2544...\n",
      "Fetching observation data for time-20-16-21-405824_chatcmpl-29709031-6fb0-44b9-ae17-a9d3797964e4...\n",
      "Fetching observation data for time-20-16-35-485763_chatcmpl-d169a64a-f776-46d8-8c46-6751207a4eee...\n",
      "Fetching observation data for time-20-16-56-745643_chatcmpl-5f1399a3-8a61-4b14-93d0-cad5e1f1dbec...\n",
      "Fetching observation data for time-20-17-23-446423_chatcmpl-6182bdde-7c65-4885-819f-260e8e820585...\n",
      "Fetching observation data for time-20-17-42-100159_chatcmpl-97405d53-5b93-44ef-a615-82cb59e00da9...\n",
      "Raw JSON saved to: /home/han/Projects/reference-benchmark-tinyml_llm/data_analysis/2025/08.20/raw_export/raw_phi4_7854_psg_batch.json\n",
      "Fetching traces for session phi4_7854_tpusg_batch...\n",
      "Fetching observation data for time-20-15-01-727336_chatcmpl-b335b920-32d6-4450-9bff-3b19c91a6be4...\n",
      "Fetching observation data for time-20-13-45-023613_chatcmpl-c98a0a81-4930-4fa1-a158-5aea2364ede4...\n",
      "Fetching observation data for time-20-12-31-019060_chatcmpl-db2cc6c2-5b61-4711-8684-7c939b2fecd5...\n",
      "Fetching observation data for time-20-09-58-885273_chatcmpl-797c5b3b-ccee-4e19-a4d3-73397f3ab046...\n",
      "Fetching observation data for time-20-10-23-343126_chatcmpl-11d8d6ab-2ffb-45fd-920a-ad5226e7ea1e...\n",
      "Fetching observation data for time-20-10-53-840444_chatcmpl-95ae6f6b-ba6a-4d75-a61a-e14e3aea0517...\n",
      "Fetching observation data for time-20-11-23-481999_chatcmpl-dbbbef3d-a8a7-45d4-abfb-aa44184d7d59...\n",
      "Fetching observation data for time-20-11-53-476589_chatcmpl-656ffb86-16ea-49f6-8200-5933eceb470e...\n",
      "Fetching observation data for ef222939-3ff6-4d40-86e3-c6d33f97d21c...\n",
      "Fetching observation data for time-20-08-43-374980_chatcmpl-d8a4f5d5-22ef-415f-a5a8-01d78528123c...\n",
      "Fetching observation data for time-20-07-30-799733_chatcmpl-05ec5dcb-54ee-4153-a677-d6ae944588a1...\n",
      "Fetching observation data for time-20-06-12-778105_chatcmpl-d418ddf4-42f7-40e9-a92e-8cedb9530e61...\n",
      "Fetching observation data for time-20-02-52-141730_chatcmpl-8def70bc-1de3-4a4d-86be-4bb8d77c6574...\n",
      "Fetching observation data for time-20-03-17-998149_chatcmpl-9807a400-410b-49d1-ab42-29edb7669974...\n",
      "Fetching observation data for time-20-03-48-430584_chatcmpl-eda4e4e1-b7cd-4b40-aef6-3b58ef700b69...\n",
      "Fetching observation data for time-20-04-19-481897_chatcmpl-26c88437-c04e-4ee6-add2-5170aae4653d...\n",
      "Fetching observation data for time-20-04-50-718510_chatcmpl-f6cf38e1-2ed5-4fe3-930a-911a4cdd1d52...\n",
      "Fetching observation data for time-20-01-35-615779_chatcmpl-fc8788bc-147b-4728-ae2a-e1bf03e45bbd...\n",
      "Fetching observation data for time-19-59-45-095360_chatcmpl-4bdd59a1-fc7e-4ea6-ad67-a9fc55119c3d...\n",
      "Fetching observation data for time-20-00-13-783651_chatcmpl-70775c7f-e3dd-4e95-8b0a-4391a20d958e...\n",
      "Fetching observation data for time-19-58-30-598824_chatcmpl-8ab352a5-4907-491e-88fe-34cebab450dc...\n",
      "Fetching observation data for time-19-57-17-067852_chatcmpl-7a2960d4-b1c8-4dd6-b996-8be8a6fc79aa...\n",
      "Fetching observation data for time-19-56-00-526273_chatcmpl-0b3e0af7-b17e-441b-9d96-8c25565bc91c...\n",
      "Fetching observation data for time-19-54-44-989482_chatcmpl-ad499ce1-6b91-4fbc-97dc-a441fad3a060...\n",
      "Fetching observation data for time-19-51-39-434507_chatcmpl-726f2907-c544-42c2-9a9b-3ea5c24a2aa7...\n",
      "Fetching observation data for time-19-52-03-519779_chatcmpl-c06fb1fc-7a5a-4b89-b193-8f8e3154a949...\n",
      "Fetching observation data for time-19-52-29-183944_chatcmpl-eca30a40-2b7d-4f57-8f2e-6fd229031030...\n",
      "Fetching observation data for time-19-52-53-344794_chatcmpl-149a8972-3b3b-48aa-a999-4a50b8a75e52...\n",
      "Fetching observation data for time-19-53-23-145036_chatcmpl-d93f1265-b4cb-4396-adcf-d5f350132c8c...\n",
      "Fetching observation data for time-19-49-52-905401_chatcmpl-43c57dce-7a4e-4c48-a142-07342fb568fb...\n",
      "Fetching observation data for time-19-50-19-034421_chatcmpl-9fdd1216-98bf-4185-8858-d7243a0f5f93...\n",
      "Fetching observation data for time-19-47-58-025512_chatcmpl-51fdd170-abf5-4d3d-aa4b-eb3c2a3c0488...\n",
      "Fetching observation data for time-19-48-27-508500_chatcmpl-01d67ec1-146c-4b98-a44f-02b33d1c2f6e...\n",
      "Fetching observation data for time-19-45-01-494214_chatcmpl-0e66c6be-fd57-4885-8b80-301a1b3babb3...\n",
      "Fetching observation data for time-19-45-31-119996_chatcmpl-6aae513c-95ef-41ea-b27e-ca0c243e467e...\n",
      "Fetching observation data for time-19-46-08-922516_chatcmpl-0dc71ff3-c543-46f1-aff4-a46399e2d6bb...\n",
      "Fetching observation data for time-19-46-44-567563_chatcmpl-3c3d0159-da48-4901-98f2-9dab5d1c2e6c...\n",
      "Fetching observation data for time-19-47-16-776200_chatcmpl-52e713f1-4daa-4271-8918-4b79b8f12e15...\n",
      "Fetching observation data for d2608a3f-1a29-4c49-abf6-710decf12a9c...\n",
      "Fetching observation data for time-19-43-41-954566_chatcmpl-216ff7e5-7494-4770-b4cc-283f56aec4ed...\n",
      "Fetching observation data for time-19-42-22-433361_chatcmpl-6a900713-ca7f-4656-a739-a32d937d94cf...\n",
      "Fetching observation data for time-19-40-35-492183_chatcmpl-93cd197c-f90b-4d08-b463-e9238361c062...\n",
      "Fetching observation data for time-19-41-01-368156_chatcmpl-68a32b3b-c29f-4cac-ac34-6291c3456ef4...\n",
      "Fetching observation data for time-19-39-17-951053_chatcmpl-2ba2ecb2-6aa5-47c6-bef8-284d9b8fa938...\n",
      "Fetching observation data for time-19-38-01-930936_chatcmpl-5d1c0550-47a5-4850-a525-2140c0c91486...\n",
      "Fetching observation data for time-19-34-59-013734_chatcmpl-5b63e917-e973-4811-be92-cf37fa9fb7a3...\n",
      "Fetching observation data for time-19-35-24-401910_chatcmpl-609fb774-f5d9-43fe-9e43-1000426c49e4...\n",
      "Fetching observation data for time-19-35-54-057461_chatcmpl-327e20c3-80c6-4fe9-9870-d47996ebf722...\n",
      "Fetching observation data for time-19-36-23-981944_chatcmpl-9f0e8d69-3dd2-4381-bde9-56d4bce6f8f8...\n",
      "Fetching observation data for time-19-36-45-632401_chatcmpl-cffc6f16-a1fd-49aa-92a3-fd11101c1625...\n",
      "Fetching observation data for time-19-33-40-397685_chatcmpl-ed0ed0bf-6a46-4769-a1b3-b0977b926c67...\n",
      "Fetching observation data for time-19-31-51-890160_chatcmpl-8deaee44-4bd6-409a-ac8f-a0df75e026c4...\n",
      "Fetching observation data for time-19-32-18-281601_chatcmpl-5a8cff9e-6244-47df-9525-e10f29d9914b...\n",
      "Fetching observation data for time-19-30-07-386197_chatcmpl-1dbafdab-c5f1-4ef7-baad-62cefad5adfe...\n",
      "Fetching observation data for time-19-30-32-595441_chatcmpl-ee710824-1efc-4c2d-aff9-bc3075dfd734...\n",
      "Fetching observation data for time-19-28-23-854880_chatcmpl-5571b45b-1232-48d4-8bd8-1dccf7d85bb4...\n",
      "Fetching observation data for time-19-28-50-767782_chatcmpl-3203761c-3c29-4e2b-889a-4cbf8e835320...\n",
      "Fetching observation data for time-19-27-08-320510_chatcmpl-f7e1c2a1-612e-4e4e-822a-19c81e4b4abc...\n",
      "Fetching observation data for time-19-25-22-694062_chatcmpl-2111d3b7-d169-432a-8494-af060bf6efb9...\n",
      "Fetching observation data for time-19-25-49-595794_chatcmpl-9de80593-7383-404f-98ad-48557d9557a8...\n",
      "Raw JSON saved to: /home/han/Projects/reference-benchmark-tinyml_llm/data_analysis/2025/08.20/raw_export/raw_phi4_7854_tpusg_batch.json\n"
     ]
    }
   ],
   "source": [
    "# ALTERNATIVE TO 2.\n",
    "import os\n",
    "import json\n",
    "from time import sleep\n",
    "from langfuse import Langfuse\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "\n",
    "# LANGFUSE_SERVICE_PUBLIC_KEY = \"pk-lf-559a2c0f-ee29-4c32-944c-bf73b5f0ce28\"\n",
    "# LANGFUSE_SERVICE_SECRET_KEY = \"sk-lf-75f8bf7f-a5db-4756-b0dd-d758a2a292c8\"\n",
    "# LANGFUSE_SERVICE_HOST = \"https://langfuse.hann.fi\"\n",
    "\n",
    "\n",
    "if LOCAL_HOST:\n",
    "    langfuse = Langfuse(\n",
    "        secret_key=\"sk-lf-75f8bf7f-a5db-4756-b0dd-d758a2a292c8\",\n",
    "        public_key=\"pk-lf-559a2c0f-ee29-4c32-944c-bf73b5f0ce28\",\n",
    "        host=\"https://langfuse.hann.fi\",\n",
    "    )\n",
    "else:\n",
    "    langfuse = Langfuse(\n",
    "        secret_key=LANGFUSE_SERVICE_SECRET_KEY,\n",
    "        public_key=LANGFUSE_SERVICE_PUBLIC_KEY,\n",
    "        host=LANGFUSE_SERVICE_HOST,\n",
    "    )\n",
    "\n",
    "API_invok_count = 0\n",
    "query_range_num_run = {\"start\": 0, \"end\": 1}\n",
    "\n",
    "\n",
    "class CustomJSONEncoder(json.JSONEncoder):\n",
    "    def __init__(self, *args, LOCAL_HOST=True, **kwargs):\n",
    "        self.LOCAL_HOST = LOCAL_HOST\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "    def default(self, obj):\n",
    "        if isinstance(obj, datetime):\n",
    "            return obj.isoformat()\n",
    "        if hasattr(obj, \"__dict__\"):\n",
    "            data = obj.__dict__.copy()\n",
    "            if \"observations\" in data:\n",
    "                data[\"observations\"] = [\n",
    "                    fetch_observation_data(obs, self.LOCAL_HOST)\n",
    "                    for obs in data[\"observations\"]\n",
    "                ]\n",
    "\n",
    "            return data\n",
    "        return super().default(obj)\n",
    "\n",
    "\n",
    "def fetch_observation_data(observation_id, LOCAL_HOST):\n",
    "    \"\"\"\n",
    "    Fetches observation data from Langfuse and returns its dictionary representation.\n",
    "    \"\"\"\n",
    "    print(f\"Fetching observation data for {observation_id}...\")\n",
    "    global API_invok_count\n",
    "    if API_invok_count >= 0 and not LOCAL_HOST:\n",
    "        print(\"Waiting for 3 seconds to fetch observation data...\")\n",
    "        for _ in tqdm(range(3), desc=\"Progress\", unit=\"s\"):\n",
    "            sleep(1)\n",
    "        API_invok_count = 0\n",
    "\n",
    "    observation_response = langfuse.fetch_observation(observation_id)\n",
    "    API_invok_count += 1\n",
    "\n",
    "    return observation_response.data.dict()\n",
    "\n",
    "\n",
    "def fetch_and_save_complete_data(session_id_list, raw_export_dir, LOCAL_HOST):\n",
    "    \"\"\"\n",
    "    Fetches complete trace data for each session ID and saves it to JSON files.\n",
    "\n",
    "    Parameters:\n",
    "        session_id_list (list): List of session IDs to process.\n",
    "        raw_export_dir (str): Directory path to save raw JSON files.\n",
    "    \"\"\"\n",
    "\n",
    "    def save_complete_data(session_id):\n",
    "        global API_invok_count\n",
    "        if API_invok_count >= 0 and not LOCAL_HOST:\n",
    "            print(\"Waiting for 4 seconds to fetch traces...\")\n",
    "            for _ in tqdm(range(4), desc=\"Progress\", unit=\"s\"):\n",
    "                sleep(1)\n",
    "            API_invok_count = 0\n",
    "\n",
    "        fetch_traces_response = langfuse.fetch_traces(session_id=session_id)\n",
    "        API_invok_count += 1\n",
    "\n",
    "        print(f\"Fetching traces for session {session_id}...\")\n",
    "        # Create directories if they don't exist\n",
    "        os.makedirs(raw_export_dir, exist_ok=True)\n",
    "\n",
    "        # Save complete data to JSON file\n",
    "        # if session_id.startswith(\"da0a\"):\n",
    "        #     session_id = \"phi4_\" + session_id\n",
    "        if \"tpsg\" in session_id:\n",
    "            session_id_ = session_id.replace(\"tpsg\", \"tpusg\")\n",
    "        else:\n",
    "            session_id_ = session_id\n",
    "            \n",
    "        raw_path = os.path.join(raw_export_dir, f\"raw_{session_id_}.json\")\n",
    "        with open(raw_path, \"w\") as f:\n",
    "            json.dump(fetch_traces_response, f, cls=CustomJSONEncoder, indent=2)\n",
    "\n",
    "        print(f\"Raw JSON saved to: {raw_path}\")\n",
    "\n",
    "    for session_id in session_id_list:\n",
    "        save_complete_data(session_id)\n",
    "\n",
    "\n",
    "fetch_and_save_complete_data(session_id_list, raw_export_dir, LOCAL_HOST)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Trim data\n",
    "\n",
    "Here also intercept the runs with fatal errors that need to be excluded from the analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPAN error_37_psg_failure_signal_py_sketch_generator: Failed. Last error: Max retries reached with failure. Last error from execution: cv2.error: OpenCV(4.10.0) /io/opencv/modules/dnn/src/tflite/tflite_importer.cpp:118: error: (-213:The function/feature is not implemented) Parse tensor with type UINT8 in function 'parseTensor'\n",
      "\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/wuguangh/Projects/tinyml-autopilot/results/object_detection/sketches/tmp_20250818210159_psg_phi4:latest/tmp_20250818210159_psg_phi4:latest.py\", line 16, in <module>\n",
      "    interpreter = cv2.dnn_DetectionModel(model_path)\n",
      "SystemError: <class 'cv2.dnn.DetectionModel'> returned a result with an exception set\n",
      "\n",
      "SPAN error_8b_psg_failure_signal_py_sketch_generator: Failed. Last error: Max retries reached with failure. Last error from execution:   File \"/home/wuguangh/Projects/tinyml-autopilot/results/object_detection/sketches/tmp_20250818205753_psg_phi4:latest/tmp_20250818205753_psg_phi4:latest.py\", line 1\n",
      "    <code>\n",
      "    ^\n",
      "SyntaxError: invalid syntax\n",
      "\n",
      "SPAN error_95_psg_failure_signal_py_sketch_generator: Failed. Last error: Max retries reached with failure. Last error from execution: INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/wuguangh/Projects/tinyml-autopilot/results/object_detection/sketches/tmp_20250818203150_psg_phi4:latest/tmp_20250818203150_psg_phi4:latest.py\", line 78, in <module>\n",
      "    interpreter.set_tensor(input_details[0]['index'], input_data)\n",
      "  File \"/home/wuguangh/.conda/envs/tinyml/lib/python3.10/site-packages/tflite_runtime/interpreter.py\", line 720, in set_tensor\n",
      "    self._interpreter.SetTensor(tensor_index, value)\n",
      "ValueError: Cannot set tensor: Dimension mismatch. Got 1 but expected 300 for dimension 1 of input 175.\n",
      "\n",
      "SPAN error_c9_psg_failure_signal_py_sketch_generator: Failed. Last error: Max retries reached with failure. Last error from execution: INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/wuguangh/Projects/tinyml-autopilot/results/object_detection/sketches/tmp_20250818202236_psg_phi4:latest/tmp_20250818202236_psg_phi4:latest.py\", line 50, in <module>\n",
      "    interpreter.set_tensor(input_details[0]['index'], blob)\n",
      "  File \"/home/wuguangh/.conda/envs/tinyml/lib/python3.10/site-packages/tflite_runtime/interpreter.py\", line 720, in set_tensor\n",
      "    self._interpreter.SetTensor(tensor_index, value)\n",
      "ValueError: Cannot set tensor: Got value of type FLOAT32 but expected type UINT8 for input 175, name: normalized_input_image_tensor \n",
      "\n",
      "Successfully processed and saved trimmed data for session phi4_7854_psg_batch\n",
      "SPAN error_00_tpusg_failure_signal_tpu_sketch_generator: Failed: Max retries reached with error. Last error: Traceback (most recent call last):\n",
      "  File \"script_1f5bbfca_1755537137.py\", line 92, in <module>\n",
      "    main()\n",
      "  File \"script_1f5bbfca_1755537137.py\", line 79, in main\n",
      "    detections = postprocess(output_data, frame.shape[0], frame.shape[1])\n",
      "  File \"script_1f5bbfca_1755537137.py\", line 44, in postprocess\n",
      "    num_detections = int(output_data[0][0])  # Correct access to the scalar value for number of detections\n",
      "TypeError: only size-1 arrays can be converted to Python scalars.\n",
      "SPAN error_21_tpusg_failure_signal_tpu_sketch_generator: Failed: Max retries reached with error. Last error: Error: Could not open video..\n",
      "Successfully processed and saved trimmed data for session phi4_7854_tpusg_batch\n",
      "Total 0 traces skipped. They are []\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "skipped_traces = []\n",
    "\n",
    "\n",
    "def process_existing_observation(observation):\n",
    "    \"\"\"\n",
    "    Processes an existing observation dictionary by trimming unwanted keys.\n",
    "    \"\"\"\n",
    "    unwanted_observation_keys = [\n",
    "        \"completionStartTime\",\n",
    "        \"metadata\",\n",
    "        \"timeToFirstToken\",\n",
    "        \"createdAt\",\n",
    "        \"usageDetails\",\n",
    "        \"usage\",\n",
    "        \"projectId\",\n",
    "        \"unit\",\n",
    "        \"updatedAt\",\n",
    "        \"version\",\n",
    "        \"parentObservationId\",\n",
    "        \"promptId\",\n",
    "        \"promptName\",\n",
    "        \"promptVersion\",\n",
    "        \"modelId\",\n",
    "        \"inputPrice\",\n",
    "        \"outputPrice\",\n",
    "        \"totalPrice\",\n",
    "        # \"modelParameters\",\n",
    "        \"input\",\n",
    "        \"output\",\n",
    "    ]\n",
    "\n",
    "    # If observation is a dictionary containing observation data\n",
    "    if isinstance(observation, dict):\n",
    "        trimmed_observation = {\n",
    "            k: v for k, v in observation.items() if k not in unwanted_observation_keys\n",
    "        }\n",
    "        return trimmed_observation\n",
    "    return observation\n",
    "\n",
    "\n",
    "def trim_data(data):\n",
    "    \"\"\"\n",
    "    Recursively trims the data structure.\n",
    "    \"\"\"\n",
    "\n",
    "    if isinstance(data, dict):\n",
    "        # Process the current dictionary\n",
    "        unwanted_trace_keys = [\n",
    "            \"release\",\n",
    "            \"version\",\n",
    "            \"user_id\",\n",
    "            \"public\",\n",
    "            \"html_path\",\n",
    "            \"scores\",\n",
    "            \"bookmarked\",\n",
    "            \"projectId\",\n",
    "            \"externalId\",\n",
    "            \"page\",\n",
    "            \"limit\",\n",
    "            \"total_pages\",\n",
    "        ]\n",
    "\n",
    "        # If this is a trace that contains observations, check for fatal errors\n",
    "        if \"observations\" in data:\n",
    "            # Check for SPAN observations with fatal errors before processing\n",
    "            skip_trace = False\n",
    "            for obs in data[\"observations\"]:\n",
    "                if isinstance(obs, dict) and obs.get(\"name\").startswith(\"error\"):\n",
    "                    status_message = obs.get(\"statusMessage\", \"\")\n",
    "                    ob_name = obs.get(\"name\")\n",
    "                    print(f\"SPAN {ob_name}: {status_message}\")\n",
    "\n",
    "                    if \"Fatal error\" in status_message:\n",
    "                        print(f\"Found Fatal error in SPAN observation, skipping trace\")\n",
    "                        skip_trace = True\n",
    "                        skipped_traces.append(data[\"name\"])\n",
    "                        break\n",
    "\n",
    "            if skip_trace:\n",
    "                return None  # Signal to skip this trace\n",
    "\n",
    "        # Create a new dictionary with wanted keys and recursively process values\n",
    "        trimmed_data = {}\n",
    "        for key, value in data.items():\n",
    "            if key not in unwanted_trace_keys:\n",
    "                if key == \"observations\":\n",
    "                    # Special handling for observations\n",
    "                    trimmed_data[key] = [\n",
    "                        process_existing_observation(obs) for obs in value\n",
    "                    ]\n",
    "                elif isinstance(value, (dict, list)):\n",
    "                    # Recursively process nested structures\n",
    "                    trimmed_data[key] = trim_data(value)\n",
    "                else:\n",
    "                    trimmed_data[key] = value\n",
    "\n",
    "        return trimmed_data\n",
    "\n",
    "    elif isinstance(data, list):\n",
    "        # Recursively process each item in the list\n",
    "        processed_items = []\n",
    "        for item in data:\n",
    "            processed_item = trim_data(item)\n",
    "            if processed_item is not None:  # Only add items that weren't filtered out\n",
    "                processed_items.append(processed_item)\n",
    "        return processed_items\n",
    "\n",
    "    else:\n",
    "        # Return non-dict, non-list values as is\n",
    "        return data\n",
    "\n",
    "\n",
    "def read_and_trim_data(session_id_list, raw_export_dir, trimmed_export_dir):\n",
    "    \"\"\"\n",
    "    Reads complete data from JSON files, trims the data, and saves the trimmed data to new JSON files.\n",
    "    \"\"\"\n",
    "    os.makedirs(trimmed_export_dir, exist_ok=True)\n",
    "\n",
    "    for session_id in session_id_list:\n",
    "        try:\n",
    "            if session_id.startswith(\"da0a\"):\n",
    "                session_id = \"phi4_\" + session_id\n",
    "            # Read raw data\n",
    "            if \"tpsg\" in session_id:\n",
    "                session_id_ = session_id.replace(\"tpsg\", \"tpusg\")\n",
    "            else:\n",
    "                session_id_ = session_id\n",
    "            raw_path = os.path.join(raw_export_dir, f\"raw_{session_id_}.json\")\n",
    "            with open(raw_path, \"r\") as f:\n",
    "                data = json.load(f)\n",
    "\n",
    "            # Process and trim the data\n",
    "            trimmed_data = trim_data(data)\n",
    "\n",
    "            # If the entire data was filtered out (unlikely but possible)\n",
    "            if trimmed_data is None:\n",
    "                print(\n",
    "                    f\"All traces in session {session_id} were filtered due to fatal errors\"\n",
    "                )\n",
    "                continue\n",
    "\n",
    "            # Save trimmed data\n",
    "            if \"tpsg\" in session_id:\n",
    "                session_id_ = session_id.replace(\"tpsg\", \"tpusg\")\n",
    "            else:\n",
    "                session_id_ = session_id\n",
    "            trimmed_path = os.path.join(\n",
    "                trimmed_export_dir, f\"trimmed_{session_id_}.json\"\n",
    "            )\n",
    "            with open(trimmed_path, \"w\") as f:\n",
    "                json.dump(trimmed_data, f, indent=2)\n",
    "\n",
    "            print(\n",
    "                f\"Successfully processed and saved trimmed data for session {session_id}\"\n",
    "            )\n",
    "\n",
    "            # Optional: Verify trimming worked\n",
    "            # print(f\"Verifying trimmed data for session {session_id}...\")\n",
    "            # verify_trimming(trimmed_path)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing session {session_id}: {str(e)}\")\n",
    "\n",
    "\n",
    "def verify_trimming(trimmed_path):\n",
    "    \"\"\"\n",
    "    Verifies that the trimmed data doesn't contain unwanted keys.\n",
    "    \"\"\"\n",
    "    with open(trimmed_path, \"r\") as f:\n",
    "        trimmed_data = json.load(f)\n",
    "\n",
    "    unwanted_keys = [\n",
    "        \"release\",\n",
    "        \"version\",\n",
    "        \"user_id\",\n",
    "        \"public\",\n",
    "        \"html_path\",\n",
    "        \"scores\",\n",
    "        \"bookmarked\",\n",
    "        \"projectId\",\n",
    "        \"externalId\",\n",
    "        \"page\",\n",
    "        \"limit\",\n",
    "        \"total_pages\",\n",
    "        \"completionStartTime\",\n",
    "        \"metadata\",\n",
    "        \"usageDetails\",\n",
    "        \"timeToFirstToken\",\n",
    "        \"createdAt\",\n",
    "        \"completionTokens\",\n",
    "        \"promptTokens\",\n",
    "        \"projectId\",\n",
    "        \"unit\",\n",
    "        \"updatedAt\",\n",
    "        \"version\",\n",
    "        # \"statusMessage\",\n",
    "        \"parentObservationId\",\n",
    "        \"promptId\",\n",
    "        \"promptName\",\n",
    "        \"promptVersion\",\n",
    "        \"modelId\",\n",
    "        \"inputPrice\",\n",
    "        \"outputPrice\",\n",
    "        \"totalPrice\",\n",
    "        \"calculatedInputCost\",\n",
    "        \"calculatedOutputCost\",\n",
    "        \"calculatedTotalCost\",\n",
    "    ]\n",
    "\n",
    "    def check_keys(obj):\n",
    "        if isinstance(obj, dict):\n",
    "            for key in obj.keys():\n",
    "                if key in unwanted_keys:\n",
    "                    print(f\"Warning: Found unwanted key '{key}' in trimmed data\")\n",
    "            for value in obj.values():\n",
    "                check_keys(value)\n",
    "        elif isinstance(obj, list):\n",
    "            for item in obj:\n",
    "                check_keys(item)\n",
    "\n",
    "    check_keys(trimmed_data)\n",
    "    print(\"Verification complete\")\n",
    "\n",
    "\n",
    "# Usage example:\n",
    "read_and_trim_data(session_id_list, raw_export_dir, raw_export_dir)\n",
    "print(f\"Total {len(skipped_traces)} traces skipped. They are {skipped_traces}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Generate CSV files from JSON\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing session phi4_7854_psg_batch, simple id phi4_7854. Look for /home/han/Projects/reference-benchmark-tinyml_llm/data_analysis/2025/08.20/raw_export/trimmed_phi4_7854_psg_batch.json\n",
      "/home/han/Projects/reference-benchmark-tinyml_llm/data_analysis/2025/08.20/processed_data/phi4_7854/clean_phi4_7854_psg_batch.csv\n",
      "Successfully saved CSV to: /home/han/Projects/reference-benchmark-tinyml_llm/data_analysis/2025/08.20/processed_data/phi4_7854/clean_phi4_7854_psg_batch.csv\n",
      "Processing session phi4_7854_tpusg_batch, simple id phi4_7854. Look for /home/han/Projects/reference-benchmark-tinyml_llm/data_analysis/2025/08.20/raw_export/trimmed_phi4_7854_tpusg_batch.json\n",
      "/home/han/Projects/reference-benchmark-tinyml_llm/data_analysis/2025/08.20/processed_data/phi4_7854/clean_phi4_7854_tpusg_batch.csv\n",
      "Successfully saved CSV to: /home/han/Projects/reference-benchmark-tinyml_llm/data_analysis/2025/08.20/processed_data/phi4_7854/clean_phi4_7854_tpusg_batch.csv\n"
     ]
    }
   ],
   "source": [
    "import traceback\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from dateutil import parser\n",
    "\n",
    "\n",
    "def json_to_csv(session_id):\n",
    "    \"\"\"\n",
    "    Convert JSON trace data to CSV format with aggregated metrics.\n",
    "\n",
    "    Args:\n",
    "        session_id (str): Identifier for the session to process\n",
    "    \"\"\"\n",
    "\n",
    "    def extract_observation_details(observations, trace_id):\n",
    "        \"\"\"Extract and aggregate metrics from observations\"\"\"\n",
    "        metrics = {\n",
    "            \"status\": None,\n",
    "            \"latency\": 0,\n",
    "            \"total_tokens\": 0,\n",
    "            \"prompt_tokens\": 0,\n",
    "            \"completion_tokens\": 0,\n",
    "            \"total_cost\": 0,\n",
    "            \"input_cost\": 0,\n",
    "            \"output_cost\": 0,\n",
    "            \"parameters\": set(),\n",
    "        }\n",
    "\n",
    "        # Process GENERATION observations\n",
    "        for obs in (o for o in observations if o[\"type\"] == \"GENERATION\"):\n",
    "            metrics[\"total_tokens\"] += obs[\"totalTokens\"]\n",
    "            metrics[\"prompt_tokens\"] += obs[\"promptTokens\"]\n",
    "            metrics[\"completion_tokens\"] += obs[\"completionTokens\"]\n",
    "            metrics[\"latency\"] += obs[\"latency\"]\n",
    "            for key, value in obs[\"modelParameters\"].items():\n",
    "                metrics[\"parameters\"].add(key + \":\" + value)\n",
    "\n",
    "            # Add costs if present\n",
    "            for cost_type in [\"Total\", \"Input\", \"Output\"]:\n",
    "                key = f\"calculated{cost_type}Cost\"\n",
    "                metric_key = cost_type.lower() + \"_cost\"\n",
    "                if obs.get(key) is not None:\n",
    "                    metrics[metric_key] += obs[key]\n",
    "        if len(metrics[\"parameters\"]) == 0:\n",
    "            metrics[\"parameters\"] = \"N/A\"\n",
    "        # Process SPAN observations for status\n",
    "        status_indicators = [\n",
    "            obs[\"name\"]\n",
    "            for obs in observations\n",
    "            if obs[\"type\"] == \"SPAN\" and \"start_\" not in obs[\"name\"]\n",
    "        ]\n",
    "\n",
    "        #  if later than 2025-05-19, use status_signal_from_output\n",
    "        if datetime.now() > datetime(2025, 5, 19):\n",
    "            pass\n",
    "        else:\n",
    "            # Determine status\n",
    "            success_signals = sum(\"end_\" in name for name in status_indicators)\n",
    "            failure_signals = sum(\n",
    "                \"failure_signal\" in name for name in status_indicators\n",
    "            )\n",
    "\n",
    "            if success_signals + failure_signals > 1:\n",
    "                raise ValueError(\n",
    "                    f\"Multiple status indicators found in trace {trace_id}\"\n",
    "                )\n",
    "\n",
    "            metrics[\"status\"] = (\n",
    "                \"success\"\n",
    "                if success_signals\n",
    "                else \"failure\" if failure_signals else \"unknown\"\n",
    "            )\n",
    "\n",
    "        metrics[\"prompt_cost\"] = metrics.pop(\"input_cost\")\n",
    "        metrics[\"completion_cost\"] = metrics.pop(\"output_cost\")\n",
    "        metrics[\"latency\"] = round(metrics[\"latency\"] / 1000, 2)\n",
    "        return metrics\n",
    "\n",
    "    def cal_time(trace):\n",
    "        time_diff = datetime.fromisoformat(\n",
    "            trace[\"updatedAt\"].replace(\"Z\", \"+00:00\")\n",
    "        ) - datetime.fromisoformat(trace[\"createdAt\"].replace(\"Z\", \"+00:00\"))\n",
    "        seconds_diff = time_diff.total_seconds()\n",
    "        return seconds_diff\n",
    "\n",
    "    try:\n",
    "\n",
    "        if session_id.startswith(\"da0a\"):\n",
    "            session_id = \"phi4_\" + session_id\n",
    "        simple_session_id = session_id.rsplit(\"_\", 2)[0]\n",
    "\n",
    "        \n",
    "        # Load JSON data\n",
    "        if \"tpsg\" in session_id:\n",
    "                session_id_ = session_id.replace(\"tpsg\", \"tpusg\")\n",
    "        else:\n",
    "                session_id_ = session_id\n",
    "        trimmed_path = os.path.join(raw_export_dir, f\"trimmed_{session_id_}.json\")\n",
    "        print(\n",
    "            f\"Processing session {session_id}, simple id {simple_session_id}. Look for {trimmed_path}\"\n",
    "        )\n",
    "        with open(trimmed_path, \"r\") as file:\n",
    "            traces = json.load(file)[\"data\"]\n",
    "\n",
    "        # Process traces\n",
    "        rows = [\n",
    "            {\n",
    "                \"num_run\": trace[\"metadata\"][\"num_run\"],\n",
    "                \"name\": trace[\"name\"],\n",
    "                \"trace_id\": trace[\"id\"],\n",
    "                \"batch_id\": trace[\"session_id\"],\n",
    "                # \"latency\": cal_time(trace),\n",
    "                # \"latency\": round(trace[\"latency\"], 2),\n",
    "                **extract_observation_details(\n",
    "                    trace[\"observations\"],\n",
    "                    trace[\"id\"],\n",
    "                ),\n",
    "                \"status\": (\n",
    "                    \"failure\"\n",
    "                    if trace[\"output\"][\"status\"].lower() == \"failed\"\n",
    "                    else \"success\"\n",
    "                ),\n",
    "                \"tags\": trace[\"tags\"],\n",
    "                \"timestamp\": int(parser.isoparse(trace[\"timestamp\"]).timestamp()),\n",
    "            }\n",
    "            for trace in traces\n",
    "        ]\n",
    "        # print(rows)\n",
    "        # print(rows)\n",
    "        # Create and save DataFrame\n",
    "        df = pd.DataFrame(rows).sort_values(\"num_run\")\n",
    "\n",
    "        output_dir = os.path.join(processed_data_dir, f\"{simple_session_id}\")\n",
    "        if not os.path.exists(output_dir):\n",
    "            os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "        output_path = os.path.join(output_dir, f\"clean_{session_id_}.csv\")\n",
    "\n",
    "        print(output_path)\n",
    "        df.to_csv(output_path, index=False)\n",
    "        print(f\"Successfully saved CSV to: {output_path}\")\n",
    "\n",
    "    except FileNotFoundError as e:\n",
    "        print(\n",
    "            f\"FileNotFoundError: For session {session_id} not found. Looked for {trimmed_path}\\nError info: \\n{e}\\n\\nTraceback: {traceback.format_exc()}\"\n",
    "        )\n",
    "    except json.JSONDecodeError:\n",
    "        print(f\"Error: Invalid JSON format in input file for session {session_id}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing session {session_id}: {str(e)}\")\n",
    "\n",
    "\n",
    "# Example usage\n",
    "for session_id in session_id_list:\n",
    "    json_to_csv(session_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CSV Generation with Generation Counts\n",
    "\n",
    "This section creates CSV files similar to the langfuse_export section 3, but adds a column for the number of generation attempts used for each trace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing sessions: ['phi4_7854_psg_batch', 'phi4_7854_tpusg_batch']\n",
      "Looking for raw files in: /home/han/Projects/reference-benchmark-tinyml_llm/data_analysis/2025/08.20/raw_export\n",
      "Will save CSV files to: /home/han/Projects/reference-benchmark-tinyml_llm/data_analysis/2025/08.20/processed_data\n",
      "Processing session phi4_7854_psg_batch, simple id phi4_7854. Look for /home/han/Projects/reference-benchmark-tinyml_llm/data_analysis/2025/08.20/raw_export/trimmed_phi4_7854_psg_batch.json\n",
      "/home/han/Projects/reference-benchmark-tinyml_llm/data_analysis/2025/08.20/processed_data/phi4_7854/clean_phi4_7854_psg_batch.csv\n",
      "Successfully saved CSV to: /home/han/Projects/reference-benchmark-tinyml_llm/data_analysis/2025/08.20/processed_data/phi4_7854/clean_phi4_7854_psg_batch.csv\n",
      "Processing session phi4_7854_tpusg_batch, simple id phi4_7854. Look for /home/han/Projects/reference-benchmark-tinyml_llm/data_analysis/2025/08.20/raw_export/trimmed_phi4_7854_tpusg_batch.json\n",
      "/home/han/Projects/reference-benchmark-tinyml_llm/data_analysis/2025/08.20/processed_data/phi4_7854/clean_phi4_7854_tpusg_batch.csv\n",
      "Successfully saved CSV to: /home/han/Projects/reference-benchmark-tinyml_llm/data_analysis/2025/08.20/processed_data/phi4_7854/clean_phi4_7854_tpusg_batch.csv\n"
     ]
    }
   ],
   "source": [
    "import traceback\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "from datetime import datetime\n",
    "from dateutil import parser\n",
    "\n",
    "# Setup paths - same as langfuse_export\n",
    "parent_dir = os.path.dirname(os.getcwd())\n",
    "raw_export_dir = os.path.join(parent_dir, \"raw_export\")\n",
    "processed_data_dir = os.path.join(parent_dir, \"processed_data\")\n",
    "\n",
    "\n",
    "# Get session id list from data directory\n",
    "session_id_list = []\n",
    "\n",
    "for root, dirs, files in os.walk(raw_export_dir):\n",
    "    for file in files:\n",
    "        file_path = os.path.join(root, file)\n",
    "        if \"trimmed_\" in file_path:\n",
    "            session_id = file_path.split('trimmed_')[1].rstrip('.json')\n",
    "            session_id_list.append(session_id)\n",
    "\n",
    "print(f\"Processing sessions: {session_id_list}\")\n",
    "print(f\"Looking for raw files in: {raw_export_dir}\")\n",
    "print(f\"Will save CSV files to: {processed_data_dir}\")\n",
    "\n",
    "\n",
    "def json_to_csv_weighted(session_id):\n",
    "    \"\"\"\n",
    "    Convert JSON trace data to CSV format with aggregated metrics.\n",
    "    Upgraded version that includes generation_count column.\n",
    "\n",
    "    Args:\n",
    "        session_id (str): Identifier for the session to process\n",
    "    \"\"\"\n",
    "\n",
    "    def extract_observation_details(observations, trace_id):\n",
    "        \"\"\"Extract and aggregate metrics from observations\"\"\"\n",
    "        metrics = {\n",
    "            \"status\": None,\n",
    "            \"latency\": 0,\n",
    "            \"total_tokens\": 0,\n",
    "            \"prompt_tokens\": 0,\n",
    "            \"completion_tokens\": 0,\n",
    "            \"total_cost\": 0,\n",
    "            \"input_cost\": 0,\n",
    "            \"output_cost\": 0,\n",
    "            \"parameters\": set(),\n",
    "            \"generation_count\": 0,  # New field for generation count\n",
    "        }\n",
    "\n",
    "        # Count generations and process GENERATION observations\n",
    "        for obs in (o for o in observations if o[\"type\"] == \"GENERATION\"):\n",
    "            metrics[\"generation_count\"] += 1\n",
    "            metrics[\"total_tokens\"] += obs[\"totalTokens\"]\n",
    "            metrics[\"prompt_tokens\"] += obs[\"promptTokens\"]\n",
    "            metrics[\"completion_tokens\"] += obs[\"completionTokens\"]\n",
    "            metrics[\"latency\"] += obs[\"latency\"]\n",
    "            for key, value in obs[\"modelParameters\"].items():\n",
    "                metrics[\"parameters\"].add(key + \":\" + value)\n",
    "\n",
    "            # Add costs if present\n",
    "            for cost_type in [\"Total\", \"Input\", \"Output\"]:\n",
    "                key = f\"calculated{cost_type}Cost\"\n",
    "                metric_key = cost_type.lower() + \"_cost\"\n",
    "                if obs.get(key) is not None:\n",
    "                    metrics[metric_key] += obs[key]\n",
    "                    \n",
    "        if len(metrics[\"parameters\"]) == 0:\n",
    "            metrics[\"parameters\"] = \"N/A\"\n",
    "            \n",
    "        # Process SPAN observations for status\n",
    "        status_indicators = [\n",
    "            obs[\"name\"]\n",
    "            for obs in observations\n",
    "            if obs[\"type\"] == \"SPAN\" and \"start_\" not in obs[\"name\"]\n",
    "        ]\n",
    "\n",
    "        #  if later than 2025-05-19, use status_signal_from_output\n",
    "        if datetime.now() > datetime(2025, 5, 19):\n",
    "            pass\n",
    "        else:\n",
    "            # Determine status\n",
    "            success_signals = sum(\"end_\" in name for name in status_indicators)\n",
    "            failure_signals = sum(\n",
    "                \"failure_signal\" in name for name in status_indicators\n",
    "            )\n",
    "\n",
    "            if success_signals + failure_signals > 1:\n",
    "                raise ValueError(\n",
    "                    f\"Multiple status indicators found in trace {trace_id}\"\n",
    "                )\n",
    "\n",
    "            metrics[\"status\"] = (\n",
    "                \"success\"\n",
    "                if success_signals\n",
    "                else \"failure\" if failure_signals else \"unknown\"\n",
    "            )\n",
    "\n",
    "        metrics[\"prompt_cost\"] = metrics.pop(\"input_cost\")\n",
    "        metrics[\"completion_cost\"] = metrics.pop(\"output_cost\")\n",
    "        metrics[\"latency\"] = round(metrics[\"latency\"] / 1000, 2)\n",
    "        return metrics\n",
    "\n",
    "    def cal_time(trace):\n",
    "        time_diff = datetime.fromisoformat(\n",
    "            trace[\"updatedAt\"].replace(\"Z\", \"+00:00\")\n",
    "        ) - datetime.fromisoformat(trace[\"createdAt\"].replace(\"Z\", \"+00:00\"))\n",
    "        seconds_diff = time_diff.total_seconds()\n",
    "        return seconds_diff\n",
    "\n",
    "    try:\n",
    "        if session_id.startswith(\"da0a\"):\n",
    "            session_id = \"phi4_\" + session_id\n",
    "        simple_session_id = session_id.rsplit(\"_\", 2)[0]\n",
    "\n",
    "        # Load JSON data\n",
    "        if \"tpsg\" in session_id:\n",
    "            session_id_ = session_id.replace(\"tpsg\", \"tpusg\")\n",
    "        else:\n",
    "            session_id_ = session_id\n",
    "        trimmed_path = os.path.join(raw_export_dir, f\"trimmed_{session_id_}.json\")\n",
    "        print(\n",
    "            f\"Processing session {session_id}, simple id {simple_session_id}. Look for {trimmed_path}\"\n",
    "        )\n",
    "        with open(trimmed_path, \"r\") as file:\n",
    "            traces = json.load(file)[\"data\"]\n",
    "\n",
    "        # Process traces\n",
    "        rows = [\n",
    "            {\n",
    "                \"num_run\": trace[\"metadata\"][\"num_run\"],\n",
    "                \"name\": trace[\"name\"],\n",
    "                \"trace_id\": trace[\"id\"],\n",
    "                \"batch_id\": trace[\"session_id\"],\n",
    "                # \"latency\": cal_time(trace),\n",
    "                # \"latency\": round(trace[\"latency\"], 2),\n",
    "                **extract_observation_details(\n",
    "                    trace[\"observations\"],\n",
    "                    trace[\"id\"],\n",
    "                ),\n",
    "                \"status\": (\n",
    "                    \"failure\"\n",
    "                    if trace[\"output\"][\"status\"].lower() == \"failed\"\n",
    "                    else \"success\"\n",
    "                ),\n",
    "                \"tags\": trace[\"tags\"],\n",
    "                \"timestamp\": int(parser.isoparse(trace[\"timestamp\"]).timestamp()),\n",
    "            }\n",
    "            for trace in traces\n",
    "        ]\n",
    "        \n",
    "        # Create and save DataFrame\n",
    "        df = pd.DataFrame(rows).sort_values(\"num_run\")\n",
    "\n",
    "        output_dir = os.path.join(processed_data_dir, f\"{simple_session_id}\")\n",
    "        if not os.path.exists(output_dir):\n",
    "            os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "        output_path = os.path.join(output_dir, f\"clean_{session_id_}.csv\")\n",
    "\n",
    "        print(output_path)\n",
    "        df.to_csv(output_path, index=False)\n",
    "        print(f\"Successfully saved CSV to: {output_path}\")\n",
    "\n",
    "    except FileNotFoundError as e:\n",
    "        print(\n",
    "            f\"FileNotFoundError: For session {session_id} not found. Looked for {trimmed_path}\\nError info: \\n{e}\\n\\nTraceback: {traceback.format_exc()}\"\n",
    "        )\n",
    "    except json.JSONDecodeError:\n",
    "        print(f\"Error: Invalid JSON format in input file for session {session_id}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing session {session_id}: {str(e)}\")\n",
    "\n",
    "\n",
    "\n",
    "# Example usage\n",
    "for session_id in session_id_list:\n",
    "    json_to_csv_weighted(session_id)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llmdev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
  "data": [
    {
      "id": "db154ae7",
      "timestamp": "2025-08-19T19:45:46.364000+00:00",
      "name": "db15_tpu_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import numpy as np\nfrom tflite_runtime.interpreter import Interpreter, load_delegate\nimport cv2\n\n# Phase 1: Setup\nmodel_path  = \"/home/mendel/tinyml_autopilot/models/edgetpu_detect.tflite\"\nlabel_path  = \"/home/mendel/tinyml_autopilot/models/labelmap.txt\"\ninput_path  = \"/home/mendel/tinyml_autopilot/data//sheeps.mp4\"\noutput_path  = \"/home/mendel/tinyml_autopilot/results/sheeps_detections.mp4\"\nconfidence_threshold  = 0.5\n\n# Load Labels\nwith open(label_path, 'r') as f:\n    labels = [line.strip() for line in f.readlines()]\n\n# Load Interpreter with EdgeTPU\ntry:\n    interpreter = Interpreter(model_path=model_path, experimental_delegates=[load_delegate('libedgetpu.so.1.0')])\nexcept Exception as e:\n    try:\n        interpreter = Interpreter(model_path=model_path, experimental_delegates=[load_delegate('/usr/lib/aarch64-linux-gnu/libedgetpu.so.1.0')])\n    except Exception as e:\n        print(f\"Error loading EdgeTPU delegate: {e}\")\n        exit(1)\n\ninterpreter.allocate_tensors()\n\n# Get Model Details\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\ninput_shape = input_details[0]['shape']\nfloating_model = (input_details[0]['dtype'] == np.float32)\nheight, width = input_shape[1], input_shape[2]\n\n# Phase 2: Input Acquisition & Preprocessing Loop\ncap = cv2.VideoCapture(input_path)\nfourcc = cv2.VideoWriter_fourcc(*'XVID')\nout = cv2.VideoWriter(output_path, fourcc, 20.0, (width, height))\n\nwhile cap.isOpened():\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    # Preprocess Data\n    input_data = cv2.resize(frame, (width, height))\n    input_data = np.expand_dims(input_data, axis=0)\n    \n    if floating_model:\n        input_data = (np.float32(input_data) - 127.5) / 127.5\n\n    # Phase 3: Inference\n    interpreter.set_tensor(input_details[0]['index'], input_data)\n    interpreter.invoke()\n\n    # Phase 4: Output Interpretation & Handling Loop\n    boxes = interpreter.get_tensor(output_details[0]['index'])[0]  # Bounding box coordinates\n    classes = interpreter.get_tensor(output_details[1]['index'])[0]  # Class indices\n    scores = interpreter.get_tensor(output_details[2]['index'])[0]   # Confidence scores\n\n    for i in range(len(scores)):\n        if scores[i] > confidence_threshold:\n            y_min, x_min, y_max, x_max = boxes[i]\n            label = labels[int(classes[i])]\n            cv2.rectangle(frame, (int(x_min * width), int(y_min * height)), (int(x_max * width), int(y_max * height)), (0, 255, 0), 2)\n            cv2.putText(frame, f'{label}: {scores[i]:.2f}', (int(x_min * width), int(y_min * height) - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n\n    out.write(frame)\n\n# Phase 5: Cleanup\ncap.release()\nout.release()\ncv2.destroyAllWindows()"
      },
      "session_id": "qwen2.5-coder:14b_5d09_tpusg_batch",
      "metadata": {
        "num_run": 30
      },
      "tags": [
        "benchmark",
        "qwen2.5-coder:14b",
        "tpu_sketch_generator"
      ],
      "latency": 12.65,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-22-45-46-365947_chatcmpl-80f8846a-a64c-4e29-b2e3-e3f9ff3ed7ff",
          "traceId": "db154ae7",
          "type": "GENERATION",
          "name": "db15_tpusg_gen_attempt#1",
          "startTime": "2025-08-19T19:45:46.365000+00:00",
          "endTime": "2025-08-19T19:45:59.015000+00:00",
          "model": "qwen2.5-coder:14b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 12650.0,
          "promptTokens": 1833,
          "environment": "default",
          "completionTokens": 743,
          "costDetails": {},
          "totalTokens": 2576,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "updatedAt": "2025-08-19T19:46:52.590Z",
      "environment": "default",
      "createdAt": "2025-08-19T19:45:46.881Z"
    },
    {
      "id": "bc7ad8d8",
      "timestamp": "2025-08-19T19:44:08.762000+00:00",
      "name": "bc7a_tpu_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "# Phase 1: Setup\n\n# Imports\ntry:\n    from tflite_runtime.interpreter import Interpreter, load_delegate\nexcept ImportError:\n    try:\n        from tensorflow.lite import Interpreter\n        from tensorflow.lite.experimental import load_delegate\n    except ImportError:\n        raise ImportError(\"Cannot import TensorFlow Lite interpreter or delegate.\")\n\nimport numpy as np\nimport cv2\nimport os\n\n# Paths/Parameters\nmodel_path   = \"/home/mendel/tinyml_autopilot/models/edgetpu_detect.tflite\"\nlabel_path   = \"/home/mendel/tinyml_autopilot/models/labelmap.txt\"\ninput_path   = \"/home/mendel/tinyml_autopilot/data//sheeps.mp4\"\noutput_path   = \"/home/mendel/tinyml_autopilot/results/sheeps_detections.mp4\"\nconfidence_threshold   = 0.5\n\n# Load Labels\nwith open(label_path, 'r') as f:\n    labels = [line.strip() for line in f.readlines()]\n\n# Load Interpreter with EdgeTPU\ntry:\n    interpreter = Interpreter(model_path=model_path, \n                              experimental_delegates=[load_delegate('libedgetpu.so.1')])\nexcept Exception as e:\n    print(f\"Error loading libedgetpu: {e}\")\n    try:\n        interpreter = Interpreter(model_path=model_path, \n                                  experimental_delegates=[load_delegate('/usr/lib/aarch64-linux-gnu/libedgetpu.so.1')])\n    except Exception as e:\n        raise ImportError(\"Cannot load EdgeTPU delegate.\")\n\ninterpreter.allocate_tensors()\n\n# Get Model Details\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\n# Phase 2: Input Acquisition & Preprocessing Loop\n\n# Acquire Input Data\ncap = cv2.VideoCapture(input_path)\nif not cap.isOpened():\n    raise IOError(\"Cannot open video file\")\n\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\nout = cv2.VideoWriter(output_path, fourcc, 30.0, (int(cap.get(3)), int(cap.get(4))))\n\nwhile True:\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    # Preprocess Data\n    input_data = cv2.resize(frame, tuple(input_details[0]['shape'][1:3]))\n    input_data = np.expand_dims(input_data, axis=0).astype(np.float32)\n\n    # Quantization Handling\n    floating_model = (input_details[0]['dtype'] == np.float32)\n    if floating_model:\n        input_data = (np.float32(input_data) - 127.5) / 127.5\n\n    # Ensure the input data has the correct type\n    input_data = input_data.astype(input_details[0]['dtype'])\n\n    # Phase 3: Inference\n\n    interpreter.set_tensor(input_details[0]['index'], input_data)\n    interpreter.invoke()\n\n    # Phase 4: Output Interpretation & Handling Loop\n\n    # Get Output Tensor(s)\n    boxes = interpreter.get_tensor(output_details[0]['index'])[0]\n    classes = interpreter.get_tensor(output_details[1]['index'])[0]\n    scores = interpreter.get_tensor(output_details[2]['index'])[0]\n\n    # Interpret Results\n    for i in range(len(scores)):\n        if scores[i] > confidence_threshold:\n            class_name = labels[int(classes[i])]\n            ymin, xmin, ymax, xmax = boxes[i]\n            height, width, _ = frame.shape\n            xmin = int(xmin * width)\n            xmax = int(xmax * width)\n            ymin = int(ymin * height)\n            ymax = int(ymax * height)\n\n            # Draw bounding box and label\n            cv2.rectangle(frame, (xmin, ymin), (xmax, ymax), (0, 255, 0), 2)\n            cv2.putText(frame, f\"{class_name}: {int(scores[i]*100)}%\", \n                        (xmin, ymin - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n\n    # Handle Output\n    out.write(frame)\n\n# Phase 5: Cleanup\n\ncap.release()\nout.release()\ncv2.destroyAllWindows()"
      },
      "session_id": "qwen2.5-coder:14b_5d09_tpusg_batch",
      "metadata": {
        "num_run": 29
      },
      "tags": [
        "benchmark",
        "qwen2.5-coder:14b",
        "tpu_sketch_generator"
      ],
      "latency": 39.57,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-22-44-08-763336_chatcmpl-f5f40d7b-2725-4317-8e7e-66f699b245ba",
          "traceId": "bc7ad8d8",
          "type": "GENERATION",
          "name": "bc7a_tpusg_gen_attempt#1",
          "startTime": "2025-08-19T19:44:08.763000+00:00",
          "endTime": "2025-08-19T19:44:23.219000+00:00",
          "model": "qwen2.5-coder:14b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 14456.0,
          "promptTokens": 1833,
          "environment": "default",
          "completionTokens": 857,
          "costDetails": {},
          "totalTokens": 2690,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-22-44-31-171755_chatcmpl-71f309c1-d917-41c8-a648-b2aac3681be3",
          "traceId": "bc7ad8d8",
          "type": "GENERATION",
          "name": "bc7a_tpusg_gen_attempt#2",
          "startTime": "2025-08-19T19:44:31.171000+00:00",
          "endTime": "2025-08-19T19:44:48.333000+00:00",
          "model": "qwen2.5-coder:14b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 17162.0,
          "promptTokens": 2048,
          "environment": "default",
          "completionTokens": 1027,
          "costDetails": {},
          "totalTokens": 3075,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "updatedAt": "2025-08-19T19:45:45.959Z",
      "environment": "default",
      "createdAt": "2025-08-19T19:44:09.278Z"
    },
    {
      "id": "5bae661e",
      "timestamp": "2025-08-19T19:43:02.214000+00:00",
      "name": "5bae_tpu_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import numpy as np\nfrom tflite_runtime.interpreter import Interpreter, load_delegate\nimport cv2\n\n# Phase 1: Setup\nmodel_path  = \"/home/mendel/tinyml_autopilot/models/edgetpu_detect.tflite\"\nlabel_path  = \"/home/mendel/tinyml_autopilot/models/labelmap.txt\"\ninput_path  = \"/home/mendel/tinyml_autopilot/data//sheeps.mp4\"\noutput_path  = \"/home/mendel/tinyml_autopilot/results/sheeps_detections.mp4\"\nconfidence_threshold  = 0.5\n\n# Load labels\nwith open(label_path, 'r') as f:\n    labels = [line.strip() for line in f.readlines()]\n\n# Load interpreter with EdgeTPU acceleration\ntry:\n    interpreter = Interpreter(model_path=model_path, \n                              experimental_delegates=[load_delegate('libedgetpu.so.1.0')])\nexcept Exception as e:\n    print(f\"Error loading EdgeTPU delegate: {e}\")\n    try:\n        interpreter = Interpreter(model_path=model_path, \n                                  experimental_delegates=[load_delegate('/usr/lib/aarch64-linux-gnu/libedgetpu.so.1.0')])\n    except Exception as e:\n        print(f\"Fallback Error loading EdgeTPU delegate: {e}\")\n        exit()\n\ninterpreter.allocate_tensors()\n\n# Get model details\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\nfloating_model = (input_details[0]['dtype'] == np.float32)\nheight, width = input_details[0]['shape'][1], input_details[0]['shape'][2]\n\n# Phase 2: Input Acquisition & Preprocessing Loop\ncap = cv2.VideoCapture(input_path)\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\nout = cv2.VideoWriter(output_path, fourcc, 30.0, (width, height))\n\nwhile cap.isOpened():\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    # Preprocess data\n    input_data = cv2.resize(frame, (width, height))\n    input_data = np.expand_dims(input_data, axis=0)\n    if floating_model:\n        input_data = (np.float32(input_data) - 127.5) / 127.5\n\n    # Phase 3: Inference\n    interpreter.set_tensor(input_details[0]['index'], input_data)\n    interpreter.invoke()\n\n    # Phase 4: Output Interpretation & Handling Loop\n    boxes = interpreter.get_tensor(output_details[0]['index'])[0]\n    classes = interpreter.get_tensor(output_details[1]['index'])[0]\n    scores = interpreter.get_tensor(output_details[2]['index'])[0]\n\n    for i in range(len(scores)):\n        if scores[i] > confidence_threshold:\n            box = boxes[i]\n            class_id = int(classes[i])\n            score = scores[i]\n            label = labels[class_id]\n\n            # Draw bounding box and label\n            y1, x1, y2, x2 = map(int, box * [height, width, height, width])\n            cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n            cv2.putText(frame, f'{label}: {score:.2f}', (x1, y1 - 10),\n                        cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n\n    out.write(frame)\n\ncap.release()\nout.release()\ncv2.destroyAllWindows()"
      },
      "session_id": "qwen2.5-coder:14b_5d09_tpusg_batch",
      "metadata": {
        "num_run": 28
      },
      "tags": [
        "benchmark",
        "qwen2.5-coder:14b",
        "tpu_sketch_generator"
      ],
      "latency": 12.787,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-22-43-02-216539_chatcmpl-6cfe0ef8-13be-43d2-9d40-27b2a372bcd2",
          "traceId": "5bae661e",
          "type": "GENERATION",
          "name": "5bae_tpusg_gen_attempt#1",
          "startTime": "2025-08-19T19:43:02.216000+00:00",
          "endTime": "2025-08-19T19:43:15.003000+00:00",
          "model": "qwen2.5-coder:14b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 12787.0,
          "promptTokens": 1833,
          "environment": "default",
          "completionTokens": 750,
          "costDetails": {},
          "totalTokens": 2583,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "updatedAt": "2025-08-19T19:44:08.403Z",
      "environment": "default",
      "createdAt": "2025-08-19T19:43:02.732Z"
    },
    {
      "id": "ddeb0d8c",
      "timestamp": "2025-08-19T19:41:07.219000+00:00",
      "name": "ddeb_tpu_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import os\nimport time\nfrom tflite_runtime.interpreter import Interpreter, load_delegate\nimport numpy as np\nimport cv2\n\n# Define necessary variables using the provided paths and parameters\nmodel_path    = \"/home/mendel/tinyml_autopilot/models/edgetpu_detect.tflite\"\nlabel_path    = \"/home/mendel/tinyml_autopilot/models/labelmap.txt\"\ninput_path    = \"/home/mendel/tinyml_autopilot/data//sheeps.mp4\"\noutput_path    = \"/home/mendel/tinyml_autopilot/results/sheeps_detections.mp4\"\nconfidence_threshold    = 0.5\n\n# Load labels\nlabels = []\nwith open(label_path, 'r') as f:\n    for line in f:\n        labels.append(line.strip())\n\n# Load Interpreter with EdgeTPU acceleration\ntry:\n    interpreter = Interpreter(model_path=model_path, \n                            experimental_delegates=[load_delegate('libedgetpu.so.1.0')])\nexcept Exception as e:\n    try:\n        interpreter = Interpreter(model_path=model_path, \n                                experimental_delegates=[load_delegate('/usr/lib/aarch64-linux-gnu/libedgetpu.so.1.0')])\n    except Exception as e:\n        print(f\"Failed to load EdgeTPU delegate: {e}\")\n        exit(1)\n\ninterpreter.allocate_tensors()\n\n# Get Model Details\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\n# Phase 2: Input Acquisition & Preprocessing Loop\ncap = cv2.VideoCapture(input_path)\nif not cap.isOpened():\n    print(f\"Error opening video file {input_path}\")\n    exit(1)\n\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\nout = cv2.VideoWriter(output_path, fourcc, 20.0, (640, 480))\n\nwhile cap.isOpened():\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    # Preprocess Data\n    input_data = cv2.resize(frame, tuple(input_details[0]['shape'][1:3][::-1]))  # Resize to match the model's expected input size\n    input_data = np.expand_dims(input_data, axis=0)\n    \n    # Convert to UINT8 if the model expects it\n    if input_details[0]['dtype'] == np.uint8:\n        input_data = input_data.astype(np.uint8)\n\n    # Quantization Handling\n    floating_model = (input_details[0]['dtype'] == np.float32)\n    if floating_model:\n        input_data = (np.float32(input_data) - 127.5) / 127.5\n\n    # Phase 3: Inference\n    interpreter.set_tensor(input_details[0]['index'], input_data)\n    interpreter.invoke()\n\n    # Phase 4: Output Interpretation & Handling Loop\n    boxes = interpreter.get_tensor(output_details[0]['index'])[0]\n    classes = interpreter.get_tensor(output_details[1]['index'])[0]\n    scores = interpreter.get_tensor(output_details[2]['index'])[0]\n\n    for box, cls, score in zip(boxes, classes, scores):\n        if score >= confidence_threshold:\n            label_name = labels[int(cls)]\n            ymin, xmin, ymax, xmax = map(int, box * [480, 640, 480, 640])\n            cv2.rectangle(frame, (xmin, ymin), (xmax, ymax), (10, 255, 0), 2)\n            label_text = f'{label_name}: {score:.2f}'\n            cv2.putText(frame, label_text, (xmin, ymin - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (10, 255, 0), 2)\n\n    # Handle Output\n    out.write(frame)\n\n# Release resources\ncap.release()\nout.release()\ncv2.destroyAllWindows()"
      },
      "session_id": "qwen2.5-coder:14b_5d09_tpusg_batch",
      "metadata": {
        "num_run": 27
      },
      "tags": [
        "benchmark",
        "qwen2.5-coder:14b",
        "tpu_sketch_generator"
      ],
      "latency": 60.757,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-22-41-07-221834_chatcmpl-21826881-09fe-4573-9668-80811c68fc64",
          "traceId": "ddeb0d8c",
          "type": "GENERATION",
          "name": "ddeb_tpusg_gen_attempt#1",
          "startTime": "2025-08-19T19:41:07.221000+00:00",
          "endTime": "2025-08-19T19:41:20.432000+00:00",
          "model": "qwen2.5-coder:14b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 13211.0,
          "promptTokens": 1833,
          "environment": "default",
          "completionTokens": 778,
          "costDetails": {},
          "totalTokens": 2611,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-22-41-28-307068_chatcmpl-0dd12f66-0875-4304-b275-237a59bc1bf2",
          "traceId": "ddeb0d8c",
          "type": "GENERATION",
          "name": "ddeb_tpusg_gen_attempt#2",
          "startTime": "2025-08-19T19:41:28.307000+00:00",
          "endTime": "2025-08-19T19:41:44.424000+00:00",
          "model": "qwen2.5-coder:14b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 16117.0,
          "promptTokens": 2048,
          "environment": "default",
          "completionTokens": 960,
          "costDetails": {},
          "totalTokens": 3008,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-22-41-52-327985_chatcmpl-c88d6b1c-16f5-4d03-8aef-a0e90d606342",
          "traceId": "ddeb0d8c",
          "type": "GENERATION",
          "name": "ddeb_tpusg_gen_attempt#3",
          "startTime": "2025-08-19T19:41:52.327000+00:00",
          "endTime": "2025-08-19T19:42:07.978000+00:00",
          "model": "qwen2.5-coder:14b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 15651.0,
          "promptTokens": 2048,
          "environment": "default",
          "completionTokens": 931,
          "costDetails": {},
          "totalTokens": 2979,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "updatedAt": "2025-08-19T19:43:01.464Z",
      "environment": "default",
      "createdAt": "2025-08-19T19:41:07.736Z"
    },
    {
      "id": "bd25c4d1",
      "timestamp": "2025-08-19T19:39:30.956000+00:00",
      "name": "bd25_tpu_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import os\nimport time\nimport numpy as np\nfrom tflite_runtime.interpreter import Interpreter, load_delegate\n\n# Configuration parameters\nmodel_path   = \"/home/mendel/tinyml_autopilot/models/edgetpu_detect.tflite\"\nlabel_path   = \"/home/mendel/tinyml_autopilot/models/labelmap.txt\"\ninput_path   = \"/home/mendel/tinyml_autopilot/data//sheeps.mp4\"\noutput_path   = \"/home/mendel/tinyml_autopilot/results/sheeps_detections.mp4\"\nconfidence_threshold   = 0.5\n\n# Load labels\nwith open(label_path, 'r') as f:\n    labels = [line.strip() for line in f.readlines()]\n\n# Load interpreter with EdgeTPU delegate\ntry:\n    interpreter = Interpreter(model_path=model_path, \n                            experimental_delegates=[load_delegate('libedgetpu.so.1.0')])\nexcept Exception as e:\n    try:\n        interpreter = Interpreter(model_path=model_path, \n                                experimental_delegates=[load_delegate('/usr/lib/aarch64-linux-gnu/libedgetpu.so.1.0')])\n    except Exception as e:\n        raise Exception(\"Failed to load EdgeTPU delegate: \" + str(e))\ninterpreter.allocate_tensors()\n\n# Get model details\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\nfloating_model = (input_details[0]['dtype'] == np.float32)\n\n# Input acquisition & preprocessing loop\nimport cv2\n\ncap = cv2.VideoCapture(input_path)\nif not cap.isOpened():\n    raise Exception(\"Error: Could not open video file.\")\n\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\nout = cv2.VideoWriter(output_path, fourcc, 20.0, (int(cap.get(3)), int(cap.get(4))))\n\nwhile True:\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    # Preprocess data\n    input_data = cv2.resize(frame, (input_details[0]['shape'][2], input_details[0]['shape'][1]))\n    input_data = np.expand_dims(input_data, axis=0)  # Add batch dimension\n\n    if floating_model:\n        input_data = (np.float32(input_data) - 127.5) / 127.5\n    else:\n        input_data = np.uint8(input_data)\n\n    # Set input tensor and run inference\n    interpreter.set_tensor(input_details[0]['index'], input_data)\n    interpreter.invoke()\n\n    # Get output tensors\n    boxes = interpreter.get_tensor(output_details[0]['index'])[0]\n    classes = interpreter.get_tensor(output_details[1]['index'])[0]\n    scores = interpreter.get_tensor(output_details[2]['index'])[0]\n\n    # Interpret results\n    for i in range(len(scores)):\n        if scores[i] > confidence_threshold:\n            (ymin, xmin, ymax, xmax) = boxes[i]\n            label = labels[int(classes[i])]\n            cv2.rectangle(frame, (int(xmin * frame.shape[1]), int(ymin * frame.shape[0])),\n                          (int(xmax * frame.shape[1]), int(ymax * frame.shape[0])), (0, 255, 0), 2)\n            cv2.putText(frame, f'{label}: {scores[i]:.2f}', \n                        (int(xmin * frame.shape[1]), int(ymin * frame.shape[0]) - 10),\n                        cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n\n    # Handle output\n    out.write(frame)\n\ncap.release()\nout.release()\ncv2.destroyAllWindows()"
      },
      "session_id": "qwen2.5-coder:14b_5d09_tpusg_batch",
      "metadata": {
        "num_run": 26
      },
      "tags": [
        "benchmark",
        "qwen2.5-coder:14b",
        "tpu_sketch_generator"
      ],
      "latency": 36.968,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-22-39-30-957977_chatcmpl-5f4ca8c1-a3cf-4746-9187-5e7059bd2b3e",
          "traceId": "bd25c4d1",
          "type": "GENERATION",
          "name": "bd25_tpusg_gen_attempt#1",
          "startTime": "2025-08-19T19:39:30.957000+00:00",
          "endTime": "2025-08-19T19:39:44.053000+00:00",
          "model": "qwen2.5-coder:14b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 13096.0,
          "promptTokens": 1833,
          "environment": "default",
          "completionTokens": 770,
          "costDetails": {},
          "totalTokens": 2603,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-22-39-51-934875_chatcmpl-0c003d9e-cf1c-4921-9548-82e972b982ae",
          "traceId": "bd25c4d1",
          "type": "GENERATION",
          "name": "bd25_tpusg_gen_attempt#2",
          "startTime": "2025-08-19T19:39:51.934000+00:00",
          "endTime": "2025-08-19T19:40:07.925000+00:00",
          "model": "qwen2.5-coder:14b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 15991.0,
          "promptTokens": 2048,
          "environment": "default",
          "completionTokens": 952,
          "costDetails": {},
          "totalTokens": 3000,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "updatedAt": "2025-08-19T19:41:05.628Z",
      "environment": "default",
      "createdAt": "2025-08-19T19:39:31.472Z"
    },
    {
      "id": "5dbb0383",
      "timestamp": "2025-08-19T19:38:17.730000+00:00",
      "name": "5dbb_tpu_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import os\nimport numpy as np\nfrom tflite_runtime.interpreter import Interpreter, load_delegate\n\n# Configuration Parameters\nmodel_path  = \"/home/mendel/tinyml_autopilot/models/edgetpu_detect.tflite\"\nlabel_path  = \"/home/mendel/tinyml_autopilot/models/labelmap.txt\"\ninput_path  = \"/home/mendel/tinyml_autopilot/data//sheeps.mp4\"\noutput_path  = \"/home/mendel/tinyml_autopilot/results/sheeps_detections.mp4\"\nconfidence_threshold  = 0.5\n\n# Load labels\nwith open(label_path, 'r') as f:\n    labels = [line.strip() for line in f.readlines()]\n\n# Load interpreter with EdgeTPU delegate\ntry:\n    interpreter = Interpreter(model_path=model_path,\n                              experimental_delegates=[load_delegate('libedgetpu.so.1.0')])\nexcept Exception as e:\n    try:\n        interpreter = Interpreter(model_path=model_path,\n                                  experimental_delegates=[load_delegate('/usr/lib/aarch64-linux-gnu/libedgetpu.so.1.0')])\n    except Exception as e:\n        raise ImportError(\"Failed to load EdgeTPU delegate: {}\".format(e))\ninterpreter.allocate_tensors()\n\n# Get model details\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\ninput_shape = input_details[0]['shape']\ninput_dtype = input_details[0]['dtype']\n\n# Video capture setup\nimport cv2\ncap = cv2.VideoCapture(input_path)\nif not cap.isOpened():\n    raise IOError(\"Could not open video file\")\n\nframe_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\nframe_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\nfps = cap.get(cv2.CAP_PROP_FPS)\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\nout = cv2.VideoWriter(output_path, fourcc, fps, (frame_width, frame_height))\n\nwhile True:\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    # Preprocess data\n    input_data = cv2.resize(frame, (input_shape[1], input_shape[2]))\n    input_data = cv2.cvtColor(input_data, cv2.COLOR_BGR2RGB)\n    input_data = np.expand_dims(input_data, axis=0).astype(input_dtype)\n\n    if input_dtype == np.float32:\n        input_data = (np.float32(input_data) - 127.5) / 127.5\n\n    # Set input tensor\n    interpreter.set_tensor(input_details[0]['index'], input_data)\n    # Run inference\n    interpreter.invoke()\n\n    # Get output tensors\n    boxes = interpreter.get_tensor(output_details[0]['index'])[0]\n    classes = interpreter.get_tensor(output_details[1]['index'])[0]\n    scores = interpreter.get_tensor(output_details[2]['index'])[0]\n\n    for i in range(len(scores)):\n        if scores[i] > confidence_threshold:\n            box = boxes[i]\n            class_id = int(classes[i])\n            label = labels[class_id]\n            score = scores[i]\n\n            # Draw bounding box and label\n            y1, x1, y2, x2 = map(int, box)\n            cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n            cv2.putText(frame, f'{label} {score:.2f}', (x1, y1 - 10),\n                        cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n\n    # Write frame to output video\n    out.write(frame)\n\ncap.release()\nout.release()\nprint(\"Video processing complete.\")"
      },
      "session_id": "qwen2.5-coder:14b_5d09_tpusg_batch",
      "metadata": {
        "num_run": 25
      },
      "tags": [
        "benchmark",
        "qwen2.5-coder:14b",
        "tpu_sketch_generator"
      ],
      "latency": 13.336,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-22-38-17-731921_chatcmpl-10d1bb1c-4101-4b51-a585-34ffa3d66170",
          "traceId": "5dbb0383",
          "type": "GENERATION",
          "name": "5dbb_tpusg_gen_attempt#1",
          "startTime": "2025-08-19T19:38:17.731000+00:00",
          "endTime": "2025-08-19T19:38:31.067000+00:00",
          "model": "qwen2.5-coder:14b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 13336.0,
          "promptTokens": 1833,
          "environment": "default",
          "completionTokens": 786,
          "costDetails": {},
          "totalTokens": 2619,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "updatedAt": "2025-08-19T19:39:28.475Z",
      "environment": "default",
      "createdAt": "2025-08-19T19:38:18.247Z"
    },
    {
      "id": "354d52fe",
      "timestamp": "2025-08-19T19:36:39.155000+00:00",
      "name": "354d_tpu_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import os\nimport numpy as np\nfrom tflite_runtime.interpreter import Interpreter, load_delegate\n\n# Define paths and parameters\nmodel_path   = \"/home/mendel/tinyml_autopilot/models/edgetpu_detect.tflite\"\nlabel_path   = \"/home/mendel/tinyml_autopilot/models/labelmap.txt\"\ninput_path   = \"/home/mendel/tinyml_autopilot/data//sheeps.mp4\"\noutput_path   = \"/home/mendel/tinyml_autopilot/results/sheeps_detections.mp4\"\nconfidence_threshold   = 0.5\n\n# Load labels\nwith open(label_path, 'r') as f:\n    labels = [line.strip() for line in f.readlines()]\n\n# Load interpreter with EdgeTPU acceleration\ntry:\n    interpreter = Interpreter(model_path=model_path, experimental_delegates=[load_delegate('libedgetpu.so.1.0')])\nexcept Exception as e:\n    print(f\"Failed to load delegate: {e}\")\n    try:\n        interpreter = Interpreter(model_path=model_path, experimental_delegates=[load_delegate('/usr/lib/aarch64-linux-gnu/libedgetpu.so.1.0')])\n    except Exception as e:\n        print(f\"Failed to load alternate delegate path: {e}\")\n        raise SystemExit(\"Could not initialize EdgeTPU delegate\")\n\ninterpreter.allocate_tensors()\n\n# Get model details\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\n# Phase 2: Input Acquisition & Preprocessing Loop\nimport cv2\n\n# Open video capture\ncap = cv2.VideoCapture(input_path)\nif not cap.isOpened():\n    raise SystemExit(\"Could not open video file\")\n\nwidth = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\nheight = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\nfps = cap.get(cv2.CAP_PROP_FPS)\n\n# Define the codec and create VideoWriter object\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\nout = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n\nwhile True:\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    # Preprocess data\n    input_data = cv2.resize(frame, (input_details[0]['shape'][1], input_details[0]['shape'][2]))\n    input_data = cv2.cvtColor(input_data, cv2.COLOR_BGR2RGB)\n    input_data = np.expand_dims(input_data, axis=0)\n\n    # Convert to UINT8 if required\n    if input_details[0]['dtype'] == np.uint8:\n        input_data = input_data.astype(np.uint8)\n\n    # Set input tensor(s)\n    interpreter.set_tensor(input_details[0]['index'], input_data)\n\n    # Run inference\n    interpreter.invoke()\n\n    # Phase 4: Output Interpretation & Handling Loop\n    # Get output tensor(s)\n    boxes = interpreter.get_tensor(output_details[0]['index'])[0]\n    classes = interpreter.get_tensor(output_details[1]['index'])[0]\n    scores = interpreter.get_tensor(output_details[2]['index'])[0]\n\n    # Interpret results\n    for i in range(len(scores)):\n        if scores[i] > confidence_threshold:\n            class_id = int(classes[i])\n            label = labels[class_id]\n            box = boxes[i]\n            ymin, xmin, ymax, xmax = box\n\n            # Convert to pixel coordinates\n            xmin = int(xmin * width)\n            ymin = int(ymin * height)\n            xmax = int(xmax * width)\n            ymax = int(ymax * height)\n\n            # Draw bounding box and label\n            cv2.rectangle(frame, (xmin, ymin), (xmax, ymax), (0, 255, 0), 2)\n            cv2.putText(frame, f'{label}: {scores[i]:.2f}', (xmin, ymin - 10),\n                        cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n\n    # Write the frame with bounding boxes to the output video\n    out.write(frame)\n\n# Phase 5: Cleanup\ncap.release()\nout.release()\ncv2.destroyAllWindows()"
      },
      "session_id": "qwen2.5-coder:14b_5d09_tpusg_batch",
      "metadata": {
        "num_run": 24
      },
      "tags": [
        "benchmark",
        "qwen2.5-coder:14b",
        "tpu_sketch_generator"
      ],
      "latency": 40.03,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-22-36-39-156147_chatcmpl-2b809069-321d-4314-8803-67582406a404",
          "traceId": "354d52fe",
          "type": "GENERATION",
          "name": "354d_tpusg_gen_attempt#1",
          "startTime": "2025-08-19T19:36:39.156000+00:00",
          "endTime": "2025-08-19T19:36:54.332000+00:00",
          "model": "qwen2.5-coder:14b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 15176.0,
          "promptTokens": 1833,
          "environment": "default",
          "completionTokens": 896,
          "costDetails": {},
          "totalTokens": 2729,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-22-37-02-234351_chatcmpl-fd857d37-3c89-4da4-81ff-59b1e4edc86a",
          "traceId": "354d52fe",
          "type": "GENERATION",
          "name": "354d_tpusg_gen_attempt#2",
          "startTime": "2025-08-19T19:37:02.234000+00:00",
          "endTime": "2025-08-19T19:37:19.186000+00:00",
          "model": "qwen2.5-coder:14b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 16952.0,
          "promptTokens": 2048,
          "environment": "default",
          "completionTokens": 1013,
          "costDetails": {},
          "totalTokens": 3061,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "updatedAt": "2025-08-19T19:38:17.317Z",
      "environment": "default",
      "createdAt": "2025-08-19T19:36:39.673Z"
    },
    {
      "id": "33e5091e",
      "timestamp": "2025-08-19T19:34:39.533000+00:00",
      "name": "33e5_tpu_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import os\nimport numpy as np\nimport time\nfrom tflite_runtime.interpreter import Interpreter, load_delegate\nimport cv2\n\n# Phase 1: Setup\n# 1.1. Imports and Paths/Parameters\nmodel_path    = \"/home/mendel/tinyml_autopilot/models/edgetpu_detect.tflite\"\nlabel_path    = \"/home/mendel/tinyml_autopilot/models/labelmap.txt\"\ninput_path    = \"/home/mendel/tinyml_autopilot/data//sheeps.mp4\"\noutput_path    = \"/home/mendel/tinyml_autopilot/results/sheeps_detections.mp4\"\nconfidence_threshold    = 0.5\n\n# 1.2. Load Labels\nwith open(label_path, 'r') as f:\n    labels = [line.strip() for line in f.readlines()]\n\n# 1.3. Initialize Interpreter\ntry:\n    interpreter = Interpreter(model_path=model_path, \n                            experimental_delegates=[load_delegate('libedgetpu.so.1.0')])\nexcept Exception as e:\n    print(f\"Error loading EdgeTPU delegate: {e}\")\n    try:\n        interpreter = Interpreter(model_path=model_path, \n                                experimental_delegates=[load_delegate('/usr/lib/aarch64-linux-gnu/libedgetpu.so.1.0')])\n    except Exception as e:\n        raise RuntimeError(\"Failed to load EdgeTPU delegate\") from e\n\ninterpreter.allocate_tensors()\n\n# 1.4. Get Model Details\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\n# Phase 2: Input Acquisition & Preprocessing Loop\n# 2.1. Acquire Input Data\ncap = cv2.VideoCapture(input_path)\n\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\nout = cv2.VideoWriter(output_path, fourcc, 30.0, (640, 480))\n\nwhile cap.isOpened():\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    # Load and preprocess the image\n    def load_image(image, target_size=(300, 300)):\n        image = cv2.resize(image, target_size)\n        return np.array(image)\n\n    input_data = load_image(frame)\n    input_shape = input_details[0]['shape']\n    input_dtype = input_details[0]['dtype']\n\n    # 2.2. Preprocess Data\n    if len(input_data.shape) == 3:\n        input_data = np.expand_dims(input_data, axis=0)\n\n    input_data = np.resize(input_data, input_shape)\n    input_data = input_data.astype(input_dtype)\n\n    # 2.3. Quantization Handling\n    floating_model = (input_details[0]['dtype'] == np.float32)\n    if floating_model:\n        input_data = (np.float32(input_data) - 127.5) / 127.5\n\n    # Phase 3: Inference\n    interpreter.set_tensor(input_details[0]['index'], input_data)\n    interpreter.invoke()\n\n    # Phase 4: Output Interpretation & Handling Loop\n    # 4.1. Get Output Tensor(s)\n    boxes = interpreter.get_tensor(output_details[0]['index'])[0]   # Bounding boxes\n    classes = interpreter.get_tensor(output_details[1]['index'])[0] # Class indices\n    scores = interpreter.get_tensor(output_details[2]['index'])[0]  # Confidence scores\n\n    # 4.2. Interpret Results\n    detections = []\n    for i in range(len(scores)):\n        if scores[i] > confidence_threshold:\n            label = labels[int(classes[i])]\n            detection = {\n                'label': label,\n                'score': float(scores[i]),\n                'box': boxes[i]\n            }\n            detections.append(detection)\n\n    # 4.3. Post-processing\n    def draw_detections(image, detections):\n        for detection in detections:\n            box = detection['box']\n            label = detection['label']\n            score = detection['score']\n            x1, y1, x2, y2 = map(int, box)\n            cv2.rectangle(image, (x1, y1), (x2, y2), (0, 0, 255), 3)\n            cv2.putText(image, f\"{label} {score:.2f}\", (x1, y1 - 20), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 0, 255), 2)\n        return image\n\n    frame = draw_detections(frame, detections)\n\n    # Write the output frame\n    out.write(frame)\n\n# Release resources\ncap.release()\nout.release()\ncv2.destroyAllWindows()"
      },
      "session_id": "qwen2.5-coder:14b_5d09_tpusg_batch",
      "metadata": {
        "num_run": 23
      },
      "tags": [
        "benchmark",
        "qwen2.5-coder:14b",
        "tpu_sketch_generator"
      ],
      "latency": 65.8,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-22-34-39-535178_chatcmpl-9d18d9ff-babe-4c09-af2d-b7a4b57b9818",
          "traceId": "33e5091e",
          "type": "GENERATION",
          "name": "33e5_tpusg_gen_attempt#1",
          "startTime": "2025-08-19T19:34:39.535000+00:00",
          "endTime": "2025-08-19T19:34:54.252000+00:00",
          "model": "qwen2.5-coder:14b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 14717.0,
          "promptTokens": 1833,
          "environment": "default",
          "completionTokens": 873,
          "costDetails": {},
          "totalTokens": 2706,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-22-34-58-608895_chatcmpl-4b9c1d2d-474d-4f2c-8ba7-b35b559687ad",
          "traceId": "33e5091e",
          "type": "GENERATION",
          "name": "33e5_tpusg_gen_attempt#2",
          "startTime": "2025-08-19T19:34:58.608000+00:00",
          "endTime": "2025-08-19T19:35:18.342000+00:00",
          "model": "qwen2.5-coder:14b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 19734.0,
          "promptTokens": 2048,
          "environment": "default",
          "completionTokens": 1196,
          "costDetails": {},
          "totalTokens": 3244,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-22-35-26-042489_chatcmpl-1997e629-4f6d-4022-9513-827f7d781476",
          "traceId": "33e5091e",
          "type": "GENERATION",
          "name": "33e5_tpusg_gen_attempt#3",
          "startTime": "2025-08-19T19:35:26.042000+00:00",
          "endTime": "2025-08-19T19:35:45.335000+00:00",
          "model": "qwen2.5-coder:14b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 19293.0,
          "promptTokens": 2048,
          "environment": "default",
          "completionTokens": 1167,
          "costDetails": {},
          "totalTokens": 3215,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "updatedAt": "2025-08-19T19:36:38.754Z",
      "environment": "default",
      "createdAt": "2025-08-19T19:34:40.052Z"
    },
    {
      "id": "66175eb1",
      "timestamp": "2025-08-19T19:33:32.850000+00:00",
      "name": "6617_tpu_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import numpy as np\nimport time\nfrom tflite_runtime.interpreter import Interpreter, load_delegate\n\n# Phase 1: Setup\n\n# Paths/Parameters\nmodel_path  = \"/home/mendel/tinyml_autopilot/models/edgetpu_detect.tflite\"\nlabel_path  = \"/home/mendel/tinyml_autopilot/models/labelmap.txt\"\ninput_path  = \"/home/mendel/tinyml_autopilot/data//sheeps.mp4\"\noutput_path  = \"/home/mendel/tinyml_autopilot/results/sheeps_detections.mp4\"\nconfidence_threshold  = 0.5\n\n# Load Labels\nwith open(label_path, 'r') as f:\n    labels = [line.strip() for line in f.readlines()]\n\n# Load Interpreter with EdgeTPU\ntry:\n    interpreter = Interpreter(model_path=model_path, \n                              experimental_delegates=[load_delegate('libedgetpu.so.1.0')])\nexcept Exception as e:\n    try:\n        interpreter = Interpreter(model_path=model_path, \n                                  experimental_delegates=[load_delegate('/usr/lib/aarch64-linux-gnu/libedgetpu.so.1.0')])\n    except Exception as e:\n        raise Exception(\"Failed to load EdgeTPU delegate: \" + str(e))\ninterpreter.allocate_tensors()\n\n# Get Model Details\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\nheight = input_details[0]['shape'][1]\nwidth = input_details[0]['shape'][2]\n\n# Phase 2: Input Acquisition & Preprocessing Loop\n\nimport cv2\n\ncap = cv2.VideoCapture(input_path)\nif not cap.isOpened():\n    raise Exception(\"Error opening video file\")\n\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\nout = cv2.VideoWriter(output_path, fourcc, 30.0, (width, height))\n\nwhile True:\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    # Preprocess Data\n    input_data = cv2.resize(frame, (width, height))\n    input_data = input_data.reshape((1, height, width, 3)).astype(np.uint8)\n\n    # Quantization Handling\n    floating_model = (input_details[0]['dtype'] == np.float32)\n    if floating_model:\n        input_data = (np.float32(input_data) - 127.5) / 127.5\n\n    # Phase 3: Inference\n    interpreter.set_tensor(input_details[0]['index'], input_data)\n    interpreter.invoke()\n\n    # Phase 4: Output Interpretation & Handling Loop\n\n    # Get Output Tensor(s)\n    boxes = interpreter.get_tensor(output_details[0]['index'])[0]  # Bounding boxes\n    classes = interpreter.get_tensor(output_details[1]['index'])[0]  # Class indices\n    scores = interpreter.get_tensor(output_details[2]['index'])[0]   # Confidence scores\n\n    # Interpret Results & Post-processing\n    for box, cls, score in zip(boxes, classes, scores):\n        if score >= confidence_threshold:\n            label = labels[int(cls)]\n            ymin, xmin, ymax, xmax = map(int, box)\n            cv2.rectangle(frame, (xmin, ymin), (xmax, ymax), (0, 255, 0), 2)\n            cv2.putText(frame, f'{label}: {score:.2f}', (xmin, ymin - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n\n    # Handle Output\n    out.write(frame)\n\n# Phase 5: Cleanup\n\ncap.release()\nout.release()\ncv2.destroyAllWindows()"
      },
      "session_id": "qwen2.5-coder:14b_5d09_tpusg_batch",
      "metadata": {
        "num_run": 22
      },
      "tags": [
        "benchmark",
        "qwen2.5-coder:14b",
        "tpu_sketch_generator"
      ],
      "latency": 13.106,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-22-33-32-852271_chatcmpl-681196fa-0308-4d2a-b6ae-ee2424fc50f8",
          "traceId": "66175eb1",
          "type": "GENERATION",
          "name": "6617_tpusg_gen_attempt#1",
          "startTime": "2025-08-19T19:33:32.852000+00:00",
          "endTime": "2025-08-19T19:33:45.958000+00:00",
          "model": "qwen2.5-coder:14b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 13106.0,
          "promptTokens": 1833,
          "environment": "default",
          "completionTokens": 771,
          "costDetails": {},
          "totalTokens": 2604,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "updatedAt": "2025-08-19T19:34:39.197Z",
      "environment": "default",
      "createdAt": "2025-08-19T19:33:33.370Z"
    },
    {
      "id": "f9734663",
      "timestamp": "2025-08-19T19:31:56.202000+00:00",
      "name": "f973_tpu_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import numpy as np\nfrom tflite_runtime.interpreter import Interpreter, load_delegate\nimport cv2\n\n# Configuration parameters\nmodel_path   = \"/home/mendel/tinyml_autopilot/models/edgetpu_detect.tflite\"\nlabel_path   = \"/home/mendel/tinyml_autopilot/models/labelmap.txt\"\ninput_path   = \"/home/mendel/tinyml_autopilot/data//sheeps.mp4\"\noutput_path   = \"/home/mendel/tinyml_autopilot/results/sheeps_detections.mp4\"\nconfidence_threshold   = 0.5\n\n# Load labels\nwith open(label_path, 'r') as f:\n    labels = [line.strip() for line in f.readlines()]\n\n# Load interpreter with EdgeTPU acceleration\ntry:\n    interpreter = Interpreter(model_path=model_path, experimental_delegates=[load_delegate('libedgetpu.so.1.0')])\nexcept Exception as e:\n    try:\n        interpreter = Interpreter(model_path=model_path, experimental_delegates=[load_delegate('/usr/lib/aarch64-linux-gnu/libedgetpu.so.1.0')])\n    except Exception as e:\n        print(f\"Failed to load EdgeTPU delegate: {e}\")\n        exit(1)\n\ninterpreter.allocate_tensors()\n\n# Get model details\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\nheight = input_details[0]['shape'][1]\nwidth = input_details[0]['shape'][2]\nfloating_model = (input_details[0]['dtype'] == np.float32)\ninput_mean = 127.5\ninput_std = 127.5\n\n# Open video capture\ncap = cv2.VideoCapture(input_path)\nif not cap.isOpened():\n    print(\"Error opening video file\")\n    exit(1)\n\n# Define the codec and create VideoWriter object\nfourcc = cv2.VideoWriter_fourcc(*'XVID')\nout = cv2.VideoWriter(output_path, fourcc, 30.0, (int(cap.get(cv2.CAP_PROP_FRAME_WIDTH)), int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))))\n\nwhile True:\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    # Preprocess image\n    input_data = cv2.resize(frame, (width, height))\n    input_data = np.expand_dims(input_data, axis=0)\n    input_data = input_data.astype(np.float32)\n\n    if floating_model:\n        input_data = (input_data - input_mean) / input_std\n    else:\n        input_data = (input_data - input_mean) / input_std * 255.0\n        input_data = input_data.astype(np.uint8)\n\n    # Set input tensor\n    interpreter.set_tensor(input_details[0]['index'], input_data)\n\n    # Run inference\n    interpreter.invoke()\n\n    # Get output tensors\n    boxes = interpreter.get_tensor(output_details[0]['index'])[0]  # Bounding box coordinates\n    classes = interpreter.get_tensor(output_details[1]['index'])[0]  # Class index\n    scores = interpreter.get_tensor(output_details[2]['index'])[0]   # Confidence scores\n\n    # Post-process results\n    for i in range(len(scores)):\n        if scores[i] > confidence_threshold:\n            class_name = labels[int(classes[i])]\n            box = boxes[i]\n            ymin, xmin, ymax, xmax = map(int, box)\n            cv2.rectangle(frame, (xmin, ymin), (xmax, ymax), (0, 255, 0), 2)\n            cv2.putText(frame, f'{class_name}: {scores[i]:.2f}', (xmin, ymin - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n\n    # Write the frame with detection results\n    out.write(frame)\n\n    if cv2.waitKey(1) & 0xFF == ord('q'):\n        break\n\n# Release everything when done\ncap.release()\nout.release()\ncv2.destroyAllWindows()"
      },
      "session_id": "qwen2.5-coder:14b_5d09_tpusg_batch",
      "metadata": {
        "num_run": 21
      },
      "tags": [
        "benchmark",
        "qwen2.5-coder:14b",
        "tpu_sketch_generator"
      ],
      "latency": 38.818,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-22-31-56-203458_chatcmpl-98b66e27-a89b-40d4-8c36-ca3cb0b6005a",
          "traceId": "f9734663",
          "type": "GENERATION",
          "name": "f973_tpusg_gen_attempt#1",
          "startTime": "2025-08-19T19:31:56.203000+00:00",
          "endTime": "2025-08-19T19:32:09.936000+00:00",
          "model": "qwen2.5-coder:14b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 13733.0,
          "promptTokens": 1833,
          "environment": "default",
          "completionTokens": 806,
          "costDetails": {},
          "totalTokens": 2639,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-22-32-17-753721_chatcmpl-09d2d1ad-6816-4f3d-a601-a27cb883b53c",
          "traceId": "f9734663",
          "type": "GENERATION",
          "name": "f973_tpusg_gen_attempt#2",
          "startTime": "2025-08-19T19:32:17.753000+00:00",
          "endTime": "2025-08-19T19:32:35.021000+00:00",
          "model": "qwen2.5-coder:14b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 17268.0,
          "promptTokens": 2048,
          "environment": "default",
          "completionTokens": 1033,
          "costDetails": {},
          "totalTokens": 3081,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "updatedAt": "2025-08-19T19:33:32.364Z",
      "environment": "default",
      "createdAt": "2025-08-19T19:31:56.722Z"
    },
    {
      "id": "3e1d724a",
      "timestamp": "2025-08-19T19:30:45.465000+00:00",
      "name": "3e1d_tpu_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import os\nimport numpy as np\nfrom tflite_runtime.interpreter import Interpreter, load_delegate\n\n# Configuration parameters\nmodel_path  = \"/home/mendel/tinyml_autopilot/models/edgetpu_detect.tflite\"\nlabel_path  = \"/home/mendel/tinyml_autopilot/models/labelmap.txt\"\ninput_path  = \"/home/mendel/tinyml_autopilot/data//sheeps.mp4\"\noutput_path  = \"/home/mendel/tinyml_autopilot/results/sheeps_detections.mp4\"\nconfidence_threshold  = 0.5\n\n# Load labels\nlabels = []\nwith open(label_path, 'r') as f:\n    for line in f:\n        labels.append(line.strip())\n\n# Load interpreter with EdgeTPU acceleration\ntry:\n    interpreter = Interpreter(model_path=model_path,\n                              experimental_delegates=[load_delegate('libedgetpu.so.1.0')])\nexcept Exception as e:\n    print(f\"Failed to load EdgeTPU delegate: {e}\")\n    try:\n        interpreter = Interpreter(model_path=model_path,\n                                  experimental_delegates=[load_delegate('/usr/lib/aarch64-linux-gnu/libedgetpu.so.1.0')])\n    except Exception as e:\n        print(f\"Failed to load alternative EdgeTPU delegate: {e}\")\n        exit(1)\n\ninterpreter.allocate_tensors()\n\n# Get model details\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\n# Phase 2: Input Acquisition & Preprocessing Loop\nimport cv2\n\ncap = cv2.VideoCapture(input_path)\nif not cap.isOpened():\n    print(\"Error opening video file\")\n    exit(1)\n\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\nout = cv2.VideoWriter(output_path, fourcc, 30.0, (int(cap.get(3)), int(cap.get(4))))\n\nwhile True:\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    # Preprocess data\n    input_data = cv2.resize(frame, (input_details[0]['shape'][1], input_details[0]['shape'][2]))\n    input_data = input_data[np.newaxis].astype(input_details[0]['dtype'])\n    \n    # Quantization handling\n    if input_details[0]['dtype'] == np.float32:\n        input_data = (np.float32(input_data) - 127.5) / 127.5\n\n    # Phase 3: Inference\n    interpreter.set_tensor(input_details[0]['index'], input_data)\n    interpreter.invoke()\n\n    # Phase 4: Output Interpretation & Handling Loop\n    # Get output tensor(s)\n    boxes = interpreter.get_tensor(output_details[0]['index'])[0]\n    classes = interpreter.get_tensor(output_details[1]['index'])[0]\n    scores = interpreter.get_tensor(output_details[2]['index'])[0]\n\n    # Interpret results and post-processing\n    for i in range(len(scores)):\n        if scores[i] > confidence_threshold:\n            box = boxes[i]\n            class_id = int(classes[i])\n            label = labels[class_id]\n            score = scores[i]\n\n            y1, x1, y2, x2 = box.astype(int)\n            cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n            cv2.putText(frame, f'{label}: {score:.2f}', (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n\n    # Handle output\n    out.write(frame)\n\n# Phase 5: Cleanup\ncap.release()\nout.release()\ncv2.destroyAllWindows()"
      },
      "session_id": "qwen2.5-coder:14b_5d09_tpusg_batch",
      "metadata": {
        "num_run": 20
      },
      "tags": [
        "benchmark",
        "qwen2.5-coder:14b",
        "tpu_sketch_generator"
      ],
      "latency": 13.296,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-22-30-45-467174_chatcmpl-becc8aa4-90c8-4dd5-af26-6608c844ce09",
          "traceId": "3e1d724a",
          "type": "GENERATION",
          "name": "3e1d_tpusg_gen_attempt#1",
          "startTime": "2025-08-19T19:30:45.467000+00:00",
          "endTime": "2025-08-19T19:30:58.763000+00:00",
          "model": "qwen2.5-coder:14b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 13296.0,
          "promptTokens": 1833,
          "environment": "default",
          "completionTokens": 782,
          "costDetails": {},
          "totalTokens": 2615,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "updatedAt": "2025-08-19T19:31:55.625Z",
      "environment": "default",
      "createdAt": "2025-08-19T19:30:45.984Z"
    },
    {
      "id": "80dc9a6a",
      "timestamp": "2025-08-19T19:29:36.589000+00:00",
      "name": "80dc_tpu_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import os\nimport numpy as np\nfrom tflite_runtime.interpreter import Interpreter, load_delegate\n\n# Configuration parameters\nmodel_path  = \"/home/mendel/tinyml_autopilot/models/edgetpu_detect.tflite\"\nlabel_path  = \"/home/mendel/tinyml_autopilot/models/labelmap.txt\"\ninput_path  = \"/home/mendel/tinyml_autopilot/data//sheeps.mp4\"\noutput_path  = \"/home/mendel/tinyml_autopilot/results/sheeps_detections.mp4\"\nconfidence_threshold  = 0.5\n\n# Load labels\nlabels = []\nwith open(label_path, 'r') as f:\n    for line in f:\n        labels.append(line.strip())\n\n# Load interpreter with EdgeTPU acceleration\ntry:\n    interpreter = Interpreter(model_path=model_path,\n                              experimental_delegates=[load_delegate('libedgetpu.so.1.0')])\nexcept Exception as e:\n    print(f\"Failed to load EdgeTPU delegate with default path: {e}\")\n    try:\n        interpreter = Interpreter(model_path=model_path,\n                                  experimental_delegates=[load_delegate('/usr/lib/aarch64-linux-gnu/libedgetpu.so.1.0')])\n    except Exception as e:\n        print(f\"Failed to load EdgeTPU delegate with alternative path: {e}\")\n        raise e\n\ninterpreter.allocate_tensors()\n\n# Get model details\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\n# Input acquisition & preprocessing loop\nimport cv2\n\ncap = cv2.VideoCapture(input_path)\nif not cap.isOpened():\n    print(f\"Error: Could not open video file {input_path}\")\n    exit()\n\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\nout = cv2.VideoWriter(output_path, fourcc, 30.0, (640, 480))\n\nwhile cap.isOpened():\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    # Preprocess data\n    input_data = cv2.resize(frame, (input_details[0]['shape'][1], input_details[0]['shape'][2]))\n    input_data = cv2.cvtColor(input_data, cv2.COLOR_BGR2RGB)\n    input_data = np.expand_dims(input_data, axis=0)\n\n    # Quantization handling\n    floating_model = (input_details[0]['dtype'] == np.float32)\n    if floating_model:\n        input_data = (np.float32(input_data) - 127.5) / 127.5\n\n    # Set input tensor(s)\n    interpreter.set_tensor(input_details[0]['index'], input_data)\n\n    # Run inference\n    interpreter.invoke()\n\n    # Get output tensor(s)\n    boxes = interpreter.get_tensor(output_details[0]['index'])[0]  # Bounding box coordinates\n    classes = interpreter.get_tensor(output_details[1]['index'])[0]  # Class indices\n    scores = interpreter.get_tensor(output_details[2]['index'])[0]  # Confidence scores\n\n    # Interpret results\n    for i in range(len(scores)):\n        if scores[i] > confidence_threshold:\n            class_name = labels[int(classes[i])]\n            ymin, xmin, ymax, xmax = boxes[i]\n            height, width, _ = frame.shape\n            ymin = int(ymin * height)\n            xmin = int(xmin * width)\n            ymax = int(ymax * height)\n            xmax = int(xmax * width)\n\n            # Draw bounding box and label on the frame\n            cv2.rectangle(frame, (xmin, ymin), (xmax, ymax), (0, 255, 0), 2)\n            cv2.putText(frame, f'{class_name}: {scores[i]:.2f}', (xmin, ymin - 10),\n                        cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n\n    # Write the frame to the output video\n    out.write(frame)\n\n# Cleanup\ncap.release()\nout.release()\ncv2.destroyAllWindows()"
      },
      "session_id": "qwen2.5-coder:14b_5d09_tpusg_batch",
      "metadata": {
        "num_run": 19
      },
      "tags": [
        "benchmark",
        "qwen2.5-coder:14b",
        "tpu_sketch_generator"
      ],
      "latency": 14.241,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-22-29-36-591177_chatcmpl-f5489b0c-c7bd-4c3a-8f9c-bb6bd27114a6",
          "traceId": "80dc9a6a",
          "type": "GENERATION",
          "name": "80dc_tpusg_gen_attempt#1",
          "startTime": "2025-08-19T19:29:36.591000+00:00",
          "endTime": "2025-08-19T19:29:50.832000+00:00",
          "model": "qwen2.5-coder:14b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 14241.0,
          "promptTokens": 1833,
          "environment": "default",
          "completionTokens": 843,
          "costDetails": {},
          "totalTokens": 2676,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "updatedAt": "2025-08-19T19:30:44.260Z",
      "environment": "default",
      "createdAt": "2025-08-19T19:29:37.107Z"
    },
    {
      "id": "bfec330f",
      "timestamp": "2025-08-19T19:28:24.652000+00:00",
      "name": "bfec_tpu_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import numpy as np\nfrom tflite_runtime.interpreter import Interpreter, load_delegate\nimport cv2\n\n# Configuration parameters\nmodel_path  = \"/home/mendel/tinyml_autopilot/models/edgetpu_detect.tflite\"\nlabel_path  = \"/home/mendel/tinyml_autopilot/models/labelmap.txt\"\ninput_path  = \"/home/mendel/tinyml_autopilot/data//sheeps.mp4\"\noutput_path  = \"/home/mendel/tinyml_autopilot/results/sheeps_detections.mp4\"\nconfidence_threshold  = 0.5\n\n# Load labels\nwith open(label_path, 'r') as f:\n    labels = [line.strip() for line in f.readlines()]\n\n# Load interpreter with EdgeTPU delegate\ntry:\n    interpreter = Interpreter(model_path=model_path, experimental_delegates=[load_delegate('libedgetpu.so.1.0')])\nexcept Exception as e:\n    try:\n        interpreter = Interpreter(model_path=model_path, experimental_delegates=[load_delegate('/usr/lib/aarch64-linux-gnu/libedgetpu.so.1.0')])\n    except Exception as e:\n        print(f\"Failed to load EdgeTPU delegate: {e}\")\n        exit(1)\n\ninterpreter.allocate_tensors()\n\n# Get model details\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\n# Phase 2: Input Acquisition & Preprocessing Loop\ncap = cv2.VideoCapture(input_path)\nif not cap.isOpened():\n    print(\"Error opening video file\")\n    exit(1)\n\nwidth = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\nheight = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\nfps = cap.get(cv2.CAP_PROP_FPS)\n\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\nout = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n\nwhile True:\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    # Preprocess data\n    input_data = cv2.resize(frame, (input_details[0]['shape'][1], input_details[0]['shape'][2]))\n    input_data = input_data[:, :, ::-1]  # BGR to RGB\n    input_data = np.expand_dims(input_data, axis=0)\n\n    if input_details[0]['dtype'] == np.float32:\n        input_data = (np.float32(input_data) - 127.5) / 127.5\n\n    # Phase 3: Inference\n    interpreter.set_tensor(input_details[0]['index'], input_data)\n    interpreter.invoke()\n\n    # Phase 4: Output Interpretation & Handling Loop\n    boxes = interpreter.get_tensor(output_details[0]['index'])[0]\n    classes = interpreter.get_tensor(output_details[1]['index'])[0]\n    scores = interpreter.get_tensor(output_details[2]['index'])[0]\n\n    for i in range(len(scores)):\n        if scores[i] > confidence_threshold:\n            y_min, x_min, y_max, x_max = boxes[i]\n            class_id = int(classes[i])\n\n            label_text = f'{labels[class_id]}: {scores[i]:.2f}'\n            cv2.rectangle(frame, (int(x_min * width), int(y_min * height)), \n                          (int(x_max * width), int(y_max * height)), (0, 255, 0), 2)\n            cv2.putText(frame, label_text, (int(x_min * width), int(y_min * height) - 10),\n                        cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n\n    out.write(frame)\n\ncap.release()\nout.release()\ncv2.destroyAllWindows()"
      },
      "session_id": "qwen2.5-coder:14b_5d09_tpusg_batch",
      "metadata": {
        "num_run": 18
      },
      "tags": [
        "benchmark",
        "qwen2.5-coder:14b",
        "tpu_sketch_generator"
      ],
      "latency": 13.291,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-22-28-24-655439_chatcmpl-7c58904c-9370-4517-aac8-7a151601bc11",
          "traceId": "bfec330f",
          "type": "GENERATION",
          "name": "bfec_tpusg_gen_attempt#1",
          "startTime": "2025-08-19T19:28:24.655000+00:00",
          "endTime": "2025-08-19T19:28:37.946000+00:00",
          "model": "qwen2.5-coder:14b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 13291.0,
          "promptTokens": 1833,
          "environment": "default",
          "completionTokens": 781,
          "costDetails": {},
          "totalTokens": 2614,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "updatedAt": "2025-08-19T19:29:36.012Z",
      "environment": "default",
      "createdAt": "2025-08-19T19:28:25.175Z"
    },
    {
      "id": "f3a3e7b4",
      "timestamp": "2025-08-19T19:26:49.030000+00:00",
      "name": "f3a3_tpu_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import numpy as np\nimport cv2\nfrom tflite_runtime.interpreter import Interpreter, load_delegate\n\n# Phase 1: Setup\n# Paths/Parameters\nmodel_path   = \"/home/mendel/tinyml_autopilot/models/edgetpu_detect.tflite\"\nlabel_path   = \"/home/mendel/tinyml_autopilot/models/labelmap.txt\"\ninput_path   = \"/home/mendel/tinyml_autopilot/data//sheeps.mp4\"\noutput_path   = \"/home/mendel/tinyml_autopilot/results/sheeps_detections.mp4\"\nconfidence_threshold   = 0.5\n\n# Load Labels\nwith open(label_path, 'r') as f:\n    labels = [line.strip() for line in f.readlines()]\n\n# Load Interpreter with EdgeTPU\ntry:\n    interpreter = Interpreter(model_path=model_path, \n                            experimental_delegates=[load_delegate('libedgetpu.so.1.0')])\nexcept Exception as e:\n    try:\n        interpreter = Interpreter(model_path=model_path, \n                                experimental_delegates=[load_delegate('/usr/lib/aarch64-linux-gnu/libedgetpu.so.1.0')])\n    except Exception as e:\n        raise RuntimeError(\"Failed to load EdgeTPU delegate: \" + str(e))\n\ninterpreter.allocate_tensors()\n\n# Get Model Details\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\ninput_shape = input_details[0]['shape']\nfloating_model = (input_details[0]['dtype'] == np.float32)\n\n# Phase 2: Input Acquisition & Preprocessing Loop\ncap = cv2.VideoCapture(input_path)\nif not cap.isOpened():\n    raise RuntimeError(\"Failed to open video file\")\n\nwhile True:\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    # Resize and preprocess the frame\n    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n    resized_frame = cv2.resize(frame_rgb, (input_shape[1], input_shape[2]))\n\n    # Convert to UINT8 if necessary\n    input_data = np.expand_dims(resized_frame, axis=0).astype(np.uint8)\n\n    if floating_model:\n        input_data = (input_data - 127.5) / 127.5\n\n    # Phase 3: Inference\n    interpreter.set_tensor(input_details[0]['index'], input_data)\n    interpreter.invoke()\n\n    # Phase 4: Output Interpretation & Handling Loop\n    # Get Output Tensor(s)\n    boxes = interpreter.get_tensor(output_details[0]['index'])[0]  # Bounding boxes\n    classes = interpreter.get_tensor(output_details[1]['index'])[0]  # Class IDs\n    scores = interpreter.get_tensor(output_details[2]['index'])[0]  # Confidence scores\n\n    # Interpret Results and Post-processing\n    for i in range(len(scores)):\n        if scores[i] > confidence_threshold:\n            class_name = labels[int(classes[i])]\n            box = boxes[i]\n            y1, x1, y2, x2 = int(box[0] * frame.shape[0]), int(box[1] * frame.shape[1]), int(box[2] * frame.shape[0]), int(box[3] * frame.shape[1])\n            cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n            cv2.putText(frame, f'{class_name}: {scores[i]:.2f}', (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n\n    # Handle Output\n    cv2.imshow('Detection', frame)\n    if cv2.waitKey(1) & 0xFF == ord('q'):\n        break\n\n# Phase 5: Cleanup\ncap.release()\ncv2.destroyAllWindows()"
      },
      "session_id": "qwen2.5-coder:14b_5d09_tpusg_batch",
      "metadata": {
        "num_run": 17
      },
      "tags": [
        "benchmark",
        "qwen2.5-coder:14b",
        "tpu_sketch_generator"
      ],
      "latency": 38.7,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-22-26-49-031309_chatcmpl-20fad46e-5fb1-4a1c-a13a-7133c73fbb1c",
          "traceId": "f3a3e7b4",
          "type": "GENERATION",
          "name": "f3a3_tpusg_gen_attempt#1",
          "startTime": "2025-08-19T19:26:49.031000+00:00",
          "endTime": "2025-08-19T19:27:02.684000+00:00",
          "model": "qwen2.5-coder:14b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 13653.0,
          "promptTokens": 1833,
          "environment": "default",
          "completionTokens": 806,
          "costDetails": {},
          "totalTokens": 2639,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-22-27-10-494002_chatcmpl-4f284deb-a72d-471a-9948-a6db56a84b21",
          "traceId": "f3a3e7b4",
          "type": "GENERATION",
          "name": "f3a3_tpusg_gen_attempt#2",
          "startTime": "2025-08-19T19:27:10.494000+00:00",
          "endTime": "2025-08-19T19:27:27.731000+00:00",
          "model": "qwen2.5-coder:14b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 17237.0,
          "promptTokens": 2048,
          "environment": "default",
          "completionTokens": 1026,
          "costDetails": {},
          "totalTokens": 3074,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "updatedAt": "2025-08-19T19:28:22.241Z",
      "environment": "default",
      "createdAt": "2025-08-19T19:26:49.548Z"
    },
    {
      "id": "64d7d106",
      "timestamp": "2025-08-19T19:25:14.150000+00:00",
      "name": "64d7_tpu_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import os\nimport time\nimport numpy as np\nfrom tflite_runtime.interpreter import Interpreter, load_delegate\n\n# Configuration parameters\nmodel_path   = \"/home/mendel/tinyml_autopilot/models/edgetpu_detect.tflite\"\nlabel_path   = \"/home/mendel/tinyml_autopilot/models/labelmap.txt\"\ninput_path   = \"/home/mendel/tinyml_autopilot/data//sheeps.mp4\"\noutput_path   = \"/home/mendel/tinyml_autopilot/results/sheeps_detections.mp4\"\nconfidence_threshold   = 0.5\n\n# Load labels\nwith open(label_path, 'r') as f:\n    labels = [line.strip() for line in f.readlines()]\n\n# Load interpreter with EdgeTPU\ntry:\n    interpreter = Interpreter(model_path=model_path, \n                              experimental_delegates=[load_delegate('libedgetpu.so.1')])\nexcept Exception as e:\n    print(f\"Error loading EdgeTPU delegate: {e}\")\n    try:\n        interpreter = Interpreter(model_path=model_path, \n                                  experimental_delegates=[load_delegate('/usr/lib/aarch64-linux-gnu/libedgetpu.so.1')])\n    except Exception as e:\n        raise RuntimeError(\"Failed to load EdgeTPU delegate\") from e\n\ninterpreter.allocate_tensors()\n\n# Get model details\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\n# Phase 2: Input Acquisition & Preprocessing Loop\nimport cv2\n\ncap = cv2.VideoCapture(input_path)\nif not cap.isOpened():\n    raise RuntimeError(\"Could not open video file\")\n\nframe_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\nframe_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\nout = cv2.VideoWriter(output_path, fourcc, 30.0, (frame_width, frame_height))\n\nwhile True:\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    # Preprocess data\n    input_data = cv2.resize(frame, (input_details[0]['shape'][1], input_details[0]['shape'][2]))\n    input_data = np.expand_dims(input_data, axis=0)  # Add batch dimension\n    input_data = input_data.astype(np.uint8)  # Ensure the data type is UINT8\n\n    # Phase 3: Inference\n    interpreter.set_tensor(input_details[0]['index'], input_data)\n    interpreter.invoke()\n\n    # Phase 4: Output Interpretation & Handling Loop\n    boxes = interpreter.get_tensor(output_details[0]['index'])[0]\n    classes = interpreter.get_tensor(output_details[1]['index'])[0]\n    scores = interpreter.get_tensor(output_details[2]['index'])[0]\n\n    for i in range(len(scores)):\n        if scores[i] > confidence_threshold:\n            ymin, xmin, ymax, xmax = boxes[i]\n            ymin *= frame_height\n            xmin *= frame_width\n            ymax *= frame_height\n            xmax *= frame_width\n\n            label = labels[int(classes[i])]\n            cv2.rectangle(frame, (int(xmin), int(ymin)), (int(xmax), int(ymax)), (0, 255, 0), 2)\n            cv2.putText(frame, f'{label}: {scores[i]:.2f}', (int(xmin), int(ymin) - 10), \n                        cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n\n    out.write(frame)\n\n# Phase 5: Cleanup\ncap.release()\nout.release()\ncv2.destroyAllWindows()"
      },
      "session_id": "qwen2.5-coder:14b_5d09_tpusg_batch",
      "metadata": {
        "num_run": 16
      },
      "tags": [
        "benchmark",
        "qwen2.5-coder:14b",
        "tpu_sketch_generator"
      ],
      "latency": 36.624,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-22-25-14-152396_chatcmpl-8b8d6236-1766-413d-8694-b27b885533e5",
          "traceId": "64d7d106",
          "type": "GENERATION",
          "name": "64d7_tpusg_gen_attempt#1",
          "startTime": "2025-08-19T19:25:14.152000+00:00",
          "endTime": "2025-08-19T19:25:26.983000+00:00",
          "model": "qwen2.5-coder:14b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 12831.0,
          "promptTokens": 1833,
          "environment": "default",
          "completionTokens": 754,
          "costDetails": {},
          "totalTokens": 2587,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-22-25-34-902902_chatcmpl-b1732307-e874-4eee-a5d3-a87d79d1223a",
          "traceId": "64d7d106",
          "type": "GENERATION",
          "name": "64d7_tpusg_gen_attempt#2",
          "startTime": "2025-08-19T19:25:34.902000+00:00",
          "endTime": "2025-08-19T19:25:50.776000+00:00",
          "model": "qwen2.5-coder:14b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 15874.0,
          "promptTokens": 2048,
          "environment": "default",
          "completionTokens": 918,
          "costDetails": {},
          "totalTokens": 2966,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "updatedAt": "2025-08-19T19:26:48.301Z",
      "environment": "default",
      "createdAt": "2025-08-19T19:25:14.670Z"
    },
    {
      "id": "37a9eba9",
      "timestamp": "2025-08-19T19:24:01.585000+00:00",
      "name": "37a9_tpu_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "# Phase 1: Setup\n\n# Imports\nfrom tflite_runtime.interpreter import Interpreter, load_delegate\nimport numpy as np\nimport cv2\nimport time\n\n# Paths/Parameters\nmodel_path  = \"/home/mendel/tinyml_autopilot/models/edgetpu_detect.tflite\"\nlabel_path  = \"/home/mendel/tinyml_autopilot/models/labelmap.txt\"\ninput_path  = \"/home/mendel/tinyml_autopilot/data//sheeps.mp4\"\noutput_path  = \"/home/mendel/tinyml_autopilot/results/sheeps_detections.mp4\"\nconfidence_threshold  = 0.5\n\n# Load Labels\nwith open(label_path, 'r') as f:\n    labels = [line.strip() for line in f.readlines()]\n\n# Load Interpreter with EdgeTPU\ntry:\n    interpreter = Interpreter(model_path=model_path,\n                                experimental_delegates=[load_delegate('libedgetpu.so.1.0')])\nexcept Exception as e:\n    try:\n        interpreter = Interpreter(model_path=model_path,\n                                  experimental_delegates=[load_delegate('/usr/lib/aarch64-linux-gnu/libedgetpu.so.1.0')])\n    except Exception as e:\n        print(\"Failed to load EdgeTPU delegate:\", e)\n        exit(1)\n\ninterpreter.allocate_tensors()\n\n# Get Model Details\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\n# Phase 2: Input Acquisition & Preprocessing Loop\n\n# Acquire Input Data\ncap = cv2.VideoCapture(input_path)\nif not cap.isOpened():\n    print(\"Error opening video file\")\n    exit(1)\n\n# Get video dimensions and create output video writer\nwidth = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\nheight = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\nout = cv2.VideoWriter(output_path, fourcc, 30.0, (width, height))\n\n# Preprocess Data\ninput_shape = input_details[0]['shape']\ninput_dtype = input_details[0]['dtype']\n\nwhile True:\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    # Resize and preprocess frame\n    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n    frame_resized = cv2.resize(frame_rgb, (input_shape[1], input_shape[2]))\n    input_data = np.expand_dims(frame_resized, axis=0)\n\n    # Quantization Handling\n    if input_dtype == np.float32:\n        input_data = (np.float32(input_data) - 127.5) / 127.5\n\n    # Phase 3: Inference\n    interpreter.set_tensor(input_details[0]['index'], input_data)\n    interpreter.invoke()\n\n    # Phase 4: Output Interpretation & Handling Loop\n\n    # Get Output Tensor(s)\n    boxes = interpreter.get_tensor(output_details[0]['index'])\n    classes = interpreter.get_tensor(output_details[1]['index'])\n    scores = interpreter.get_tensor(output_details[2]['index'])\n\n    # Interpret Results\n    for i in range(len(scores[0])):\n        if scores[0][i] > confidence_threshold:\n            ymin, xmin, ymax, xmax = boxes[0][i]\n            label_id = int(classes[0][i])\n            label = labels[label_id]\n\n            # Post-processing: scale coordinates and draw bounding box\n            xmin = int(xmin * width)\n            xmax = int(xmax * width)\n            ymin = int(ymin * height)\n            ymax = int(ymax * height)\n\n            cv2.rectangle(frame, (xmin, ymin), (xmax, ymax), (0, 255, 0), 2)\n            cv2.putText(frame, f'{label}: {scores[0][i]:.2f}', (xmin, ymin - 10),\n                        cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n\n    # Handle Output\n    out.write(frame)\n\n# Phase 5: Cleanup\n\ncap.release()\nout.release()\ncv2.destroyAllWindows()"
      },
      "session_id": "qwen2.5-coder:14b_5d09_tpusg_batch",
      "metadata": {
        "num_run": 15
      },
      "tags": [
        "benchmark",
        "qwen2.5-coder:14b",
        "tpu_sketch_generator"
      ],
      "latency": 14.627,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-22-24-01-587634_chatcmpl-3619b0c0-346f-42b9-99ec-17452bf32c87",
          "traceId": "37a9eba9",
          "type": "GENERATION",
          "name": "37a9_tpusg_gen_attempt#1",
          "startTime": "2025-08-19T19:24:01.587000+00:00",
          "endTime": "2025-08-19T19:24:16.214000+00:00",
          "model": "qwen2.5-coder:14b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 14627.0,
          "promptTokens": 1833,
          "environment": "default",
          "completionTokens": 867,
          "costDetails": {},
          "totalTokens": 2700,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "updatedAt": "2025-08-19T19:25:13.764Z",
      "environment": "default",
      "createdAt": "2025-08-19T19:24:02.104Z"
    },
    {
      "id": "32a68251",
      "timestamp": "2025-08-19T19:22:53.844000+00:00",
      "name": "32a6_tpu_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import os\nimport numpy as np\nfrom tflite_runtime.interpreter import Interpreter, load_delegate\n\n# Phase 1: Setup\n# Paths/Parameters\nmodel_path  = \"/home/mendel/tinyml_autopilot/models/edgetpu_detect.tflite\"\nlabel_path  = \"/home/mendel/tinyml_autopilot/models/labelmap.txt\"\ninput_path  = \"/home/mendel/tinyml_autopilot/data//sheeps.mp4\"\noutput_path  = \"/home/mendel/tinyml_autopilot/results/sheeps_detections.mp4\"\nconfidence_threshold  = 0.5\n\n# Load Labels\nwith open(label_path, 'r') as f:\n    labels = [line.strip() for line in f.readlines()]\n\n# Load Interpreter with EdgeTPU\ntry:\n    interpreter = Interpreter(model_path=model_path,\n                            experimental_delegates=[load_delegate('libedgetpu.so.1')])\nexcept Exception as e:\n    try:\n        interpreter = Interpreter(model_path=model_path,\n                                experimental_delegates=[load_delegate('/usr/lib/aarch64-linux-gnu/libedgetpu.so.1')])\n    except Exception as e:\n        print(f\"Failed to load EdgeTPU delegate: {e}\")\n        exit(1)\n\ninterpreter.allocate_tensors()\n\n# Get Model Details\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\nfloating_model = (input_details[0]['dtype'] == np.float32)\nheight = input_details[0]['shape'][1]\nwidth = input_details[0]['shape'][2]\n\n# Phase 2: Input Acquisition & Preprocessing Loop\nimport cv2\n\ncap = cv2.VideoCapture(input_path)\nif not cap.isOpened():\n    print(f\"Error opening video file {input_path}\")\n    exit(1)\n\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\nout = cv2.VideoWriter(output_path, fourcc, 20.0, (width, height))\n\nwhile cap.isOpened():\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    # Preprocess Data\n    input_data = cv2.resize(frame, (width, height))\n    input_data = np.expand_dims(input_data, axis=0)\n    if floating_model:\n        input_data = (np.float32(input_data) - 127.5) / 127.5\n\n    # Phase 3: Inference\n    interpreter.set_tensor(input_details[0]['index'], input_data)\n    interpreter.invoke()\n\n    # Phase 4: Output Interpretation & Handling Loop\n    boxes = interpreter.get_tensor(output_details[0]['index'])[0]\n    classes = interpreter.get_tensor(output_details[1]['index'])[0]\n    scores = interpreter.get_tensor(output_details[2]['index'])[0]\n\n    for i in range(len(scores)):\n        if scores[i] > confidence_threshold:\n            class_id = int(classes[i])\n            label = labels[class_id]\n            box = boxes[i]\n            y_min, x_min, y_max, x_max = box\n            cv2.rectangle(frame, (int(x_min * width), int(y_min * height)), \n                           (int(x_max * width), int(y_max * height)), (0, 255, 0), 2)\n            cv2.putText(frame, f'{label}: {scores[i]:.2f}', (int(x_min * width), int(y_min * height) - 10),\n                        cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n\n    # Handle Output\n    out.write(frame)\n\n# Phase 5: Cleanup\ncap.release()\nout.release()\ncv2.destroyAllWindows()"
      },
      "session_id": "qwen2.5-coder:14b_5d09_tpusg_batch",
      "metadata": {
        "num_run": 14
      },
      "tags": [
        "benchmark",
        "qwen2.5-coder:14b",
        "tpu_sketch_generator"
      ],
      "latency": 13.09,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-22-22-53-845227_chatcmpl-a411586e-478e-404b-8dfc-4ab7482c7b0a",
          "traceId": "32a68251",
          "type": "GENERATION",
          "name": "32a6_tpusg_gen_attempt#1",
          "startTime": "2025-08-19T19:22:53.845000+00:00",
          "endTime": "2025-08-19T19:23:06.935000+00:00",
          "model": "qwen2.5-coder:14b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 13090.0,
          "promptTokens": 1833,
          "environment": "default",
          "completionTokens": 771,
          "costDetails": {},
          "totalTokens": 2604,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "updatedAt": "2025-08-19T19:24:00.707Z",
      "environment": "default",
      "createdAt": "2025-08-19T19:22:54.364Z"
    },
    {
      "id": "78140b12",
      "timestamp": "2025-08-19T19:21:25.007000+00:00",
      "name": "7814_tpu_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import numpy as np\nfrom tflite_runtime.interpreter import Interpreter, load_delegate\n\n# Phase 1: Setup\nmodel_path  = \"/home/mendel/tinyml_autopilot/models/edgetpu_detect.tflite\"\nlabel_path  = \"/home/mendel/tinyml_autopilot/models/labelmap.txt\"\ninput_path  = \"/home/mendel/tinyml_autopilot/data//sheeps.mp4\"\noutput_path  = \"/home/mendel/tinyml_autopilot/results/sheeps_detections.mp4\"\nconfidence_threshold  = 0.5\n\n# Load Labels\nwith open(label_path, 'r') as f:\n    labels = [line.strip() for line in f.readlines()]\n\n# Load Interpreter with EdgeTPU\ntry:\n    interpreter = Interpreter(model_path=model_path,\n                              experimental_delegates=[load_delegate('libedgetpu.so.1.0')])\nexcept Exception as e:\n    try:\n        interpreter = Interpreter(model_path=model_path,\n                                  experimental_delegates=[load_delegate('/usr/lib/aarch64-linux-gnu/libedgetpu.so.1.0')])\n    except Exception as e:\n        print(f\"Failed to load EdgeTPU delegate: {e}\")\n        exit(1)\n\ninterpreter.allocate_tensors()\n\n# Get Model Details\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\n# Phase 2: Input Acquisition & Preprocessing Loop\nimport cv2\n\ncap = cv2.VideoCapture(input_path)\nif not cap.isOpened():\n    print(f\"Error opening video file: {input_path}\")\n    exit(1)\n\nwhile cap.isOpened():\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    # Preprocess Data\n    height, width, channels = input_details[0]['shape'][1:4]\n    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n    frame_resized = cv2.resize(frame_rgb, (width, height))\n    input_data = np.expand_dims(frame_resized, axis=0)\n\n    # Quantization Handling\n    floating_model = (input_details[0]['dtype'] == np.float32)\n    if floating_model:\n        input_data = (np.float32(input_data) - 127.5) / 127.5\n\n    # Phase 3: Inference\n    interpreter.set_tensor(input_details[0]['index'], input_data)\n    interpreter.invoke()\n\n    # Phase 4: Output Interpretation & Handling Loop\n    # Get Output Tensor(s)\n    boxes = interpreter.get_tensor(output_details[0]['index'])[0]  # Bounding boxes\n    classes = interpreter.get_tensor(output_details[1]['index'])[0]  # Class IDs\n    scores = interpreter.get_tensor(output_details[2]['index'])[0]   # Confidence scores\n\n    for i in range(len(scores)):\n        if scores[i] > confidence_threshold:\n            class_name = labels[int(classes[i])]\n            ymin, xmin, ymax, xmax = boxes[i]\n            # Scale coordinates back to original dimensions\n            xmin = int(xmin * width)\n            xmax = int(xmax * width)\n            ymin = int(ymin * height)\n            ymax = int(ymax * height)\n\n            # Draw bounding box and label on the frame\n            cv2.rectangle(frame, (xmin, ymin), (xmax, ymax), (0, 255, 0), 2)\n            cv2.putText(frame, f'{class_name}: {scores[i]:.2f}',\n                        (xmin, ymin - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n\n    # Display the resulting frame\n    cv2.imshow('Frame', frame)\n\n    if cv2.waitKey(1) & 0xFF == ord('q'):\n        break\n\n# Phase 5: Cleanup\ncap.release()\ncv2.destroyAllWindows()"
      },
      "session_id": "qwen2.5-coder:14b_5d09_tpusg_batch",
      "metadata": {
        "num_run": 13
      },
      "tags": [
        "benchmark",
        "qwen2.5-coder:14b",
        "tpu_sketch_generator"
      ],
      "latency": 33.204,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-22-21-25-010167_chatcmpl-6659b786-a296-4866-a29b-6bceeec19b21",
          "traceId": "78140b12",
          "type": "GENERATION",
          "name": "7814_tpusg_gen_attempt#1",
          "startTime": "2025-08-19T19:21:25.010000+00:00",
          "endTime": "2025-08-19T19:21:38.931000+00:00",
          "model": "qwen2.5-coder:14b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 13921.0,
          "promptTokens": 1833,
          "environment": "default",
          "completionTokens": 819,
          "costDetails": {},
          "totalTokens": 2652,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-22-21-42-688910_chatcmpl-9f1d6a6d-d204-4cbd-ac4b-e6c5967267b2",
          "traceId": "78140b12",
          "type": "GENERATION",
          "name": "7814_tpusg_gen_attempt#2",
          "startTime": "2025-08-19T19:21:42.688000+00:00",
          "endTime": "2025-08-19T19:21:58.214000+00:00",
          "model": "qwen2.5-coder:14b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 15526.0,
          "promptTokens": 2048,
          "environment": "default",
          "completionTokens": 923,
          "costDetails": {},
          "totalTokens": 2971,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "updatedAt": "2025-08-19T19:22:52.676Z",
      "environment": "default",
      "createdAt": "2025-08-19T19:21:25.524Z"
    },
    {
      "id": "342edfa8",
      "timestamp": "2025-08-19T19:19:53.019000+00:00",
      "name": "342e_tpu_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import os\nimport numpy as np\nfrom tflite_runtime.interpreter import Interpreter, load_delegate\n\n# Phase 1: Setup\nmodel_path  = \"/home/mendel/tinyml_autopilot/models/edgetpu_detect.tflite\"\nlabel_path  = \"/home/mendel/tinyml_autopilot/models/labelmap.txt\"\ninput_path  = \"/home/mendel/tinyml_autopilot/data//sheeps.mp4\"\noutput_path  = \"/home/mendel/tinyml_autopilot/results/sheeps_detections.mp4\"\nconfidence_threshold  = 0.5\n\n# Load Labels\nwith open(label_path, 'r') as f:\n    labels = [line.strip() for line in f.readlines()]\n\n# Load Interpreter with EdgeTPU\ntry:\n    interpreter = Interpreter(model_path=model_path, \n                            experimental_delegates=[load_delegate('libedgetpu.so.1.0')])\nexcept Exception as e:\n    print(f\"Failed to load EdgeTPU delegate: {e}\")\n    try:\n        interpreter = Interpreter(model_path=model_path, \n                                experimental_delegates=[load_delegate('/usr/lib/aarch64-linux-gnu/libedgetpu.so.1.0')])\n    except Exception as e:\n        print(f\"Failed to load alternative EdgeTPU delegate: {e}\")\n        raise\n\ninterpreter.allocate_tensors()\n\n# Get Model Details\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\n# Phase 2: Input Acquisition & Preprocessing Loop\nimport cv2\n\ncap = cv2.VideoCapture(input_path)\nif not cap.isOpened():\n    print(\"Error opening video file\")\n    exit()\n\nwhile True:\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    # Preprocess Data\n    input_data = cv2.resize(frame, (input_details[0]['shape'][1], input_details[0]['shape'][2]))\n    input_data = np.expand_dims(input_data, axis=0)\n    \n    if input_details[0]['dtype'] == np.float32:\n        input_data = (np.float32(input_data) - 127.5) / 127.5\n\n    # Phase 3: Inference\n    interpreter.set_tensor(input_details[0]['index'], input_data)\n    interpreter.invoke()\n\n    # Phase 4: Output Interpretation & Handling Loop\n    boxes = interpreter.get_tensor(output_details[0]['index'])[0]\n    classes = interpreter.get_tensor(output_details[1]['index'])[0]  # Removed label_offset\n    scores = interpreter.get_tensor(output_details[2]['index'])[0]\n\n    for i in range(len(scores)):\n        if scores[i] > confidence_threshold:\n            class_name = labels[int(classes[i])]\n            box = boxes[i]\n            x_min, y_min, x_max, y_max = int(box[1] * frame.shape[1]), int(box[0] * frame.shape[0]), int(box[3] * frame.shape[1]), int(box[2] * frame.shape[0])\n            \n            # Draw bounding box and label\n            cv2.rectangle(frame, (x_min, y_min), (x_max, y_max), (0, 255, 0), 2)\n            cv2.putText(frame, f'{class_name}: {scores[i]:.2f}', (x_min, y_min - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n\n    # Handle Output\n    cv2.imshow('Object Detection', frame)\n    if cv2.waitKey(1) & 0xFF == ord('q'):\n        break\n\n# Phase 5: Cleanup\ncap.release()\ncv2.destroyAllWindows()"
      },
      "session_id": "qwen2.5-coder:14b_5d09_tpusg_batch",
      "metadata": {
        "num_run": 12
      },
      "tags": [
        "benchmark",
        "qwen2.5-coder:14b",
        "tpu_sketch_generator"
      ],
      "latency": 36.241,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-22-19-53-021678_chatcmpl-c811f292-0c6b-4809-ad32-ae835b9089f9",
          "traceId": "342edfa8",
          "type": "GENERATION",
          "name": "342e_tpusg_gen_attempt#1",
          "startTime": "2025-08-19T19:19:53.021000+00:00",
          "endTime": "2025-08-19T19:20:06.211000+00:00",
          "model": "qwen2.5-coder:14b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 13190.0,
          "promptTokens": 1833,
          "environment": "default",
          "completionTokens": 776,
          "costDetails": {},
          "totalTokens": 2609,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-22-20-14-254853_chatcmpl-c91a208b-fdb5-4eb0-a70a-f871055ee8cc",
          "traceId": "342edfa8",
          "type": "GENERATION",
          "name": "342e_tpusg_gen_attempt#2",
          "startTime": "2025-08-19T19:20:14.254000+00:00",
          "endTime": "2025-08-19T19:20:29.262000+00:00",
          "model": "qwen2.5-coder:14b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 15008.0,
          "promptTokens": 2048,
          "environment": "default",
          "completionTokens": 890,
          "costDetails": {},
          "totalTokens": 2938,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "updatedAt": "2025-08-19T19:21:23.172Z",
      "environment": "default",
      "createdAt": "2025-08-19T19:19:53.539Z"
    },
    {
      "id": "f47c7afe",
      "timestamp": "2025-08-19T19:18:16.183000+00:00",
      "name": "f47c_tpu_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import os\nimport time\nimport numpy as np\nfrom tflite_runtime.interpreter import Interpreter, load_delegate\n\n# Phase 1: Setup\nmodel_path  = \"/home/mendel/tinyml_autopilot/models/edgetpu_detect.tflite\"\nlabel_path  = \"/home/mendel/tinyml_autopilot/models/labelmap.txt\"\ninput_path  = \"/home/mendel/tinyml_autopilot/data//sheeps.mp4\"\noutput_path  = \"/home/mendel/tinyml_autopilot/results/sheeps_detections.mp4\"\nconfidence_threshold  = 0.5\n\n# Load Labels\nwith open(label_path, 'r') as f:\n    labels = [line.strip() for line in f.readlines()]\n\n# Load Interpreter with EdgeTPU\ntry:\n    interpreter = Interpreter(model_path=model_path, \n                              experimental_delegates=[load_delegate('libedgetpu.so.1.0')])\nexcept Exception as e:\n    try:\n        interpreter = Interpreter(model_path=model_path, \n                                  experimental_delegates=[load_delegate('/usr/lib/aarch64-linux-gnu/libedgetpu.so.1.0')])\n    except Exception as e:\n        print(f\"Failed to load EdgeTPU delegate: {e}\")\n        exit(1)\n\ninterpreter.allocate_tensors()\n\n# Get Model Details\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\nheight = input_details[0]['shape'][1]\nwidth = input_details[0]['shape'][2]\nfloating_model = (input_details[0]['dtype'] == np.float32)\ninput_mean = 127.5\ninput_std = 127.5\n\n# Phase 2: Input Acquisition & Preprocessing Loop\nimport cv2\n\ncap = cv2.VideoCapture(input_path)\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\nout = cv2.VideoWriter(output_path, fourcc, cap.get(cv2.CAP_PROP_FPS), (int(cap.get(cv2.CAP_PROP_FRAME_WIDTH)), int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))))\n\nwhile True:\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    # Preprocess Data\n    input_data = cv2.resize(frame, (width, height))\n    input_data = input_data[np.newaxis].astype(np.uint8)\n\n    if floating_model:\n        input_data = (np.float32(input_data) - input_mean) / input_std\n\n    # Phase 3: Inference\n    interpreter.set_tensor(input_details[0]['index'], input_data)\n    interpreter.invoke()\n\n    # Phase 4: Output Interpretation & Handling Loop\n    boxes = interpreter.get_tensor(output_details[0]['index'])[0]\n    classes = interpreter.get_tensor(output_details[1]['index'])[0]\n    scores = interpreter.get_tensor(output_details[2]['index'])[0]\n\n    for i in range(len(scores)):\n        if scores[i] > confidence_threshold:\n            box = boxes[i]\n            class_id = int(classes[i])\n            label = labels[class_id]\n            score = scores[i]\n            y_min, x_min, y_max, x_max = box\n\n            # Draw bounding box and label on the frame\n            cv2.rectangle(frame, (int(x_min * width), int(y_min * height)), \n                          (int(x_max * width), int(y_max * height)), (0, 255, 0), 2)\n            label_text = f'{label}: {score:.2f}'\n            cv2.putText(frame, label_text, (int(x_min * width), int(y_min * height) - 10),\n                        cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n\n    # Write the frame with detection results to the output video\n    out.write(frame)\n\n# Phase 5: Cleanup\ncap.release()\nout.release()\ncv2.destroyAllWindows()"
      },
      "session_id": "qwen2.5-coder:14b_5d09_tpusg_batch",
      "metadata": {
        "num_run": 11
      },
      "tags": [
        "benchmark",
        "qwen2.5-coder:14b",
        "tpu_sketch_generator"
      ],
      "latency": 37.983,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-22-18-16-184992_chatcmpl-80926246-607a-490d-a3f1-75afc671b13b",
          "traceId": "f47c7afe",
          "type": "GENERATION",
          "name": "f47c_tpusg_gen_attempt#1",
          "startTime": "2025-08-19T19:18:16.184000+00:00",
          "endTime": "2025-08-19T19:18:29.706000+00:00",
          "model": "qwen2.5-coder:14b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 13522.0,
          "promptTokens": 1833,
          "environment": "default",
          "completionTokens": 798,
          "costDetails": {},
          "totalTokens": 2631,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-22-18-37-620656_chatcmpl-13e790fc-4b55-4cc6-a5af-7227df43a556",
          "traceId": "f47c7afe",
          "type": "GENERATION",
          "name": "f47c_tpusg_gen_attempt#2",
          "startTime": "2025-08-19T19:18:37.620000+00:00",
          "endTime": "2025-08-19T19:18:54.167000+00:00",
          "model": "qwen2.5-coder:14b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 16547.0,
          "promptTokens": 2048,
          "environment": "default",
          "completionTokens": 987,
          "costDetails": {},
          "totalTokens": 3035,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "updatedAt": "2025-08-19T19:19:51.870Z",
      "environment": "default",
      "createdAt": "2025-08-19T19:18:16.705Z"
    },
    {
      "id": "ef606ba7",
      "timestamp": "2025-08-19T19:17:02.238000+00:00",
      "name": "ef60_tpu_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import os\nimport numpy as np\nfrom tflite_runtime.interpreter import Interpreter, load_delegate\n\n# Phase 1: Setup\n# Paths/Parameters\nmodel_path  = \"/home/mendel/tinyml_autopilot/models/edgetpu_detect.tflite\"\nlabel_path  = \"/home/mendel/tinyml_autopilot/models/labelmap.txt\"\ninput_path  = \"/home/mendel/tinyml_autopilot/data//sheeps.mp4\"\noutput_path  = \"/home/mendel/tinyml_autopilot/results/sheeps_detections.mp4\"\nconfidence_threshold  = 0.5\n\n# Load Labels\nwith open(label_path, 'r') as f:\n    labels = [line.strip() for line in f.readlines()]\n\n# Load Interpreter with EdgeTPU\ntry:\n    interpreter = Interpreter(model_path=model_path, \n                              experimental_delegates=[load_delegate('libedgetpu.so.1')])\nexcept Exception as e:\n    print(f\"Failed to load delegate: {e}\")\n    try:\n        interpreter = Interpreter(model_path=model_path, \n                                  experimental_delegates=[load_delegate('/usr/lib/aarch64-linux-gnu/libedgetpu.so.1')])\n    except Exception as e:\n        print(f\"Failed to load alternative delegate: {e}\")\n        exit(1)\n\ninterpreter.allocate_tensors()\n\n# Get Model Details\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\n# Phase 2: Input Acquisition & Preprocessing Loop\nimport cv2\n\ncap = cv2.VideoCapture(input_path)\nif not cap.isOpened():\n    print(\"Error opening video file\")\n    exit(1)\n\nframe_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\nframe_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\nfps = cap.get(cv2.CAP_PROP_FPS)\n\n# Define the codec and create VideoWriter object\nfourcc = cv2.VideoWriter_fourcc(*'XVID')\nout = cv2.VideoWriter(output_path, fourcc, fps, (frame_width, frame_height))\n\nwhile True:\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    # Preprocess Data\n    input_data = cv2.resize(frame, (input_details[0]['shape'][1], input_details[0]['shape'][2]))\n    input_data = cv2.cvtColor(input_data, cv2.COLOR_BGR2RGB)\n    input_data = np.expand_dims(input_data, axis=0)\n\n    # Quantization Handling\n    floating_model = (input_details[0]['dtype'] == np.float32)\n    if floating_model:\n        input_data = (np.float32(input_data) - 127.5) / 127.5\n\n    # Phase 3: Inference\n    interpreter.set_tensor(input_details[0]['index'], input_data)\n    interpreter.invoke()\n\n    # Phase 4: Output Interpretation & Handling Loop\n    # Get Output Tensor(s)\n    boxes = interpreter.get_tensor(output_details[0]['index'])[0]  # Assuming the first output is bounding boxes\n    classes = interpreter.get_tensor(output_details[1]['index'])[0]  # Assuming the second output is class indices\n    scores = interpreter.get_tensor(output_details[2]['index'])[0]  # Assuming the third output is confidence scores\n\n    # Interpret Results and Post-processing\n    for i in range(len(scores)):\n        if scores[i] > confidence_threshold:\n            label = labels[int(classes[i])]\n            ymin, xmin, ymax, xmax = boxes[i]\n            ymin = int(ymin * frame_height)\n            xmin = int(xmin * frame_width)\n            ymax = int(ymax * frame_height)\n            xmax = int(xmax * frame_width)\n\n            # Draw bounding box and label\n            cv2.rectangle(frame, (xmin, ymin), (xmax, ymax), (0, 255, 0), 2)\n            cv2.putText(frame, f'{label}: {scores[i]:.2f}', (xmin, ymin - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n\n    # Handle Output\n    out.write(frame)\n\n# Phase 5: Cleanup\ncap.release()\nout.release()\ncv2.destroyAllWindows()"
      },
      "session_id": "qwen2.5-coder:14b_5d09_tpusg_batch",
      "metadata": {
        "num_run": 10
      },
      "tags": [
        "benchmark",
        "qwen2.5-coder:14b",
        "tpu_sketch_generator"
      ],
      "latency": 15.066,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-22-17-02-240868_chatcmpl-4eabc228-2521-4922-9b3f-0b642e50f5ef",
          "traceId": "ef606ba7",
          "type": "GENERATION",
          "name": "ef60_tpusg_gen_attempt#1",
          "startTime": "2025-08-19T19:17:02.240000+00:00",
          "endTime": "2025-08-19T19:17:17.306000+00:00",
          "model": "qwen2.5-coder:14b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 15066.0,
          "promptTokens": 1833,
          "environment": "default",
          "completionTokens": 889,
          "costDetails": {},
          "totalTokens": 2722,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "updatedAt": "2025-08-19T19:18:14.952Z",
      "environment": "default",
      "createdAt": "2025-08-19T19:17:02.759Z"
    },
    {
      "id": "b72e8100",
      "timestamp": "2025-08-19T19:15:31.281000+00:00",
      "name": "b72e_tpu_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import numpy as np\nfrom tflite_runtime.interpreter import Interpreter, load_delegate\n\n# Phase 1: Setup\nmodel_path  = \"/home/mendel/tinyml_autopilot/models/edgetpu_detect.tflite\"\nlabel_path  = \"/home/mendel/tinyml_autopilot/models/labelmap.txt\"\ninput_path  = \"/home/mendel/tinyml_autopilot/data//sheeps.mp4\"\noutput_path  = \"/home/mendel/tinyml_autopilot/results/sheeps_detections.mp4\"\nconfidence_threshold  = 0.5\n\n# Load labels\nwith open(label_path, 'r') as f:\n    labels = [line.strip() for line in f.readlines()]\n\n# Load interpreter with EdgeTPU acceleration\ntry:\n    interpreter = Interpreter(model_path=model_path, experimental_delegates=[load_delegate('libedgetpu.so.1')])\nexcept Exception as e:\n    try:\n        interpreter = Interpreter(model_path=model_path, experimental_delegates=[load_delegate('/usr/lib/aarch64-linux-gnu/libedgetpu.so.1')])\n    except Exception as e:\n        raise RuntimeError(\"Failed to load EdgeTPU delegate: \" + str(e))\n\ninterpreter.allocate_tensors()\n\n# Get model details\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\n# Phase 2: Input Acquisition & Preprocessing Loop\nimport cv2\n\n# Open the video file\ncap = cv2.VideoCapture(input_path)\nif not cap.isOpened():\n    raise IOError(\"Cannot open video\")\n\nwhile True:\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    # Resize and preprocess the frame\n    height, width, channels = frame.shape\n    input_shape = input_details[0]['shape'][1:3]\n    resized_frame = cv2.resize(frame, (input_shape[1], input_shape[0]))\n    rgb_frame = cv2.cvtColor(resized_frame, cv2.COLOR_BGR2RGB)\n    \n    # Convert the frame to UINT8 if necessary\n    input_data = np.expand_dims(rgb_frame, axis=0).astype(input_details[0]['dtype'])\n\n    # Phase 3: Inference\n    interpreter.set_tensor(input_details[0]['index'], input_data)\n    interpreter.invoke()\n\n    # Phase 4: Output Interpretation & Handling Loop\n    output_boxes = interpreter.get_tensor(output_details[0]['index'])[0]\n    output_classes = interpreter.get_tensor(output_details[1]['index'])[0]\n    output_scores = interpreter.get_tensor(output_details[2]['index'])[0]\n\n    for i in range(len(output_scores)):\n        if output_scores[i] > confidence_threshold:\n            class_name = labels[int(output_classes[i])]\n            y_min, x_min, y_max, x_max = [int(val * height) for val in output_boxes[i]]\n            cv2.rectangle(frame, (x_min, y_min), (x_max, y_max), (0, 255, 0), 2)\n            label_text = f'{class_name}: {output_scores[i]:.2f}'\n            cv2.putText(frame, label_text, (x_min, y_min - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n\n    # Display the frame\n    cv2.imshow('Object Detection', frame)\n    if cv2.waitKey(1) & 0xFF == ord('q'):\n        break\n\n# Phase 5: Cleanup\ncap.release()\ncv2.destroyAllWindows()"
      },
      "session_id": "qwen2.5-coder:14b_5d09_tpusg_batch",
      "metadata": {
        "num_run": 9
      },
      "tags": [
        "benchmark",
        "qwen2.5-coder:14b",
        "tpu_sketch_generator"
      ],
      "latency": 35.879,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-22-15-31-283724_chatcmpl-fd9a07f1-a59c-47bf-81ba-a951a832d88a",
          "traceId": "b72e8100",
          "type": "GENERATION",
          "name": "b72e_tpusg_gen_attempt#1",
          "startTime": "2025-08-19T19:15:31.283000+00:00",
          "endTime": "2025-08-19T19:15:43.859000+00:00",
          "model": "qwen2.5-coder:14b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 12576.0,
          "promptTokens": 1833,
          "environment": "default",
          "completionTokens": 737,
          "costDetails": {},
          "totalTokens": 2570,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-22-15-51-771960_chatcmpl-3934aef9-5d27-41d2-a212-545108a61b56",
          "traceId": "b72e8100",
          "type": "GENERATION",
          "name": "b72e_tpusg_gen_attempt#2",
          "startTime": "2025-08-19T19:15:51.771000+00:00",
          "endTime": "2025-08-19T19:16:07.162000+00:00",
          "model": "qwen2.5-coder:14b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 15391.0,
          "promptTokens": 2048,
          "environment": "default",
          "completionTokens": 915,
          "costDetails": {},
          "totalTokens": 2963,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "updatedAt": "2025-08-19T19:17:01.507Z",
      "environment": "default",
      "createdAt": "2025-08-19T19:15:31.815Z"
    },
    {
      "id": "ef5c36a5",
      "timestamp": "2025-08-19T19:13:30.668000+00:00",
      "name": "ef5c_tpu_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import numpy as np\nfrom tflite_runtime.interpreter import Interpreter, load_delegate\n\n# Configuration parameters\nmodel_path    = \"/home/mendel/tinyml_autopilot/models/edgetpu_detect.tflite\"\nlabel_path    = \"/home/mendel/tinyml_autopilot/models/labelmap.txt\"\ninput_path    = \"/home/mendel/tinyml_autopilot/data//sheeps.mp4\"\noutput_path    = \"/home/mendel/tinyml_autopilot/results/sheeps_detections.mp4\"\nconfidence_threshold    = 0.5\n\n# Load labels\nwith open(label_path, 'r') as f:\n    labels = [line.strip() for line in f.readlines()]\n\n# Load interpreter with EdgeTPU delegate\ntry:\n    interpreter = Interpreter(model_path=model_path, experimental_delegates=[load_delegate('libedgetpu.so.1.0')])\nexcept Exception as e:\n    print(f\"Failed to load EdgeTPU delegate: {e}\")\n    try:\n        interpreter = Interpreter(model_path=model_path, experimental_delegates=[load_delegate('/usr/lib/aarch64-linux-gnu/libedgetpu.so.1.0')])\n    except Exception as e:\n        print(f\"Failed to load EdgeTPU delegate from alternate path: {e}\")\n        raise\n\ninterpreter.allocate_tensors()\n\n# Get model details\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\ninput_shape = input_details[0]['shape']\ninput_dtype = input_details[0]['dtype']\n\n# Input acquisition and preprocessing\nimport cv2\n\ncap = cv2.VideoCapture(input_path)\nfourcc = cv2.VideoWriter_fourcc(*'XVID')\nout = cv2.VideoWriter(output_path, fourcc, 30.0, (int(cap.get(3)), int(cap.get(4))))\n\nwhile cap.isOpened():\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    # Preprocess data\n    input_data = cv2.resize(frame, (input_shape[1], input_shape[2]))\n    input_data = cv2.cvtColor(input_data, cv2.COLOR_BGR2RGB)\n    input_data = np.expand_dims(input_data, axis=0).astype(input_dtype)\n\n    if input_dtype == np.float32:\n        input_data = (np.float32(input_data) - 127.5) / 127.5\n\n    # Set input tensor\n    interpreter.set_tensor(input_details[0]['index'], input_data)\n\n    # Run inference\n    interpreter.invoke()\n\n    # Get output tensor(s)\n    boxes = interpreter.get_tensor(output_details[0]['index'])\n    classes = interpreter.get_tensor(output_details[1]['index'])\n    scores = interpreter.get_tensor(output_details[2]['index'])\n\n    # Interpret results\n    num_detections = int(interpreter.get_tensor(output_details[3]['index']))\n    for i in range(num_detections):\n        if scores[0, i] > confidence_threshold:  # Fix: Access the first element of the scores array\n            box = boxes[0, i]\n            class_id = int(classes[0, i])\n            label = labels[class_id]\n            score = scores[0, i]\n\n            # Draw bounding box and label\n            y1, x1, y2, x2 = box\n            cv2.rectangle(frame, (int(x1), int(y1)), (int(x2), int(y2)), (0, 255, 0), 2)\n            cv2.putText(frame, f'{label}: {score:.2f}', (int(x1), int(y1) - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n\n    # Handle output\n    out.write(frame)\n\ncap.release()\nout.release()\ncv2.destroyAllWindows()"
      },
      "session_id": "qwen2.5-coder:14b_5d09_tpusg_batch",
      "metadata": {
        "num_run": 8
      },
      "tags": [
        "benchmark",
        "qwen2.5-coder:14b",
        "tpu_sketch_generator"
      ],
      "latency": 61.928,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-22-13-30-669718_chatcmpl-6b16f1bf-8569-4298-ae62-1b9be5b47cc0",
          "traceId": "ef5c36a5",
          "type": "GENERATION",
          "name": "ef5c_tpusg_gen_attempt#1",
          "startTime": "2025-08-19T19:13:30.669000+00:00",
          "endTime": "2025-08-19T19:13:43.835000+00:00",
          "model": "qwen2.5-coder:14b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 13166.0,
          "promptTokens": 1833,
          "environment": "default",
          "completionTokens": 776,
          "costDetails": {},
          "totalTokens": 2609,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-22-13-52-119668_chatcmpl-28f6fabb-a7e3-4348-9369-9bc54b597885",
          "traceId": "ef5c36a5",
          "type": "GENERATION",
          "name": "ef5c_tpusg_gen_attempt#2",
          "startTime": "2025-08-19T19:13:52.119000+00:00",
          "endTime": "2025-08-19T19:14:09.670000+00:00",
          "model": "qwen2.5-coder:14b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 17551.0,
          "promptTokens": 2048,
          "environment": "default",
          "completionTokens": 1052,
          "costDetails": {},
          "totalTokens": 3100,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-22-14-17-762258_chatcmpl-e7d57934-ae93-44f1-9ae9-22a484b47f4d",
          "traceId": "ef5c36a5",
          "type": "GENERATION",
          "name": "ef5c_tpusg_gen_attempt#3",
          "startTime": "2025-08-19T19:14:17.762000+00:00",
          "endTime": "2025-08-19T19:14:32.597000+00:00",
          "model": "qwen2.5-coder:14b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 14835.0,
          "promptTokens": 2048,
          "environment": "default",
          "completionTokens": 880,
          "costDetails": {},
          "totalTokens": 2928,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "updatedAt": "2025-08-19T19:15:29.907Z",
      "environment": "default",
      "createdAt": "2025-08-19T19:13:31.189Z"
    },
    {
      "id": "f8843675",
      "timestamp": "2025-08-19T19:10:36.972000+00:00",
      "name": "f884_tpu_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import os\nimport numpy as np\nfrom tflite_runtime.interpreter import Interpreter, load_delegate\n\n# Configuration parameters\nmodel_path      = \"/home/mendel/tinyml_autopilot/models/edgetpu_detect.tflite\"\nlabel_path      = \"/home/mendel/tinyml_autopilot/models/labelmap.txt\"\ninput_path      = \"/home/mendel/tinyml_autopilot/data//sheeps.mp4\"\noutput_path      = \"/home/mendel/tinyml_autopilot/results/sheeps_detections.mp4\"\nconfidence_threshold      = 0.5\n\n# Load labels\nwith open(label_path, 'r') as f:\n    labels = [line.strip() for line in f.readlines()]\n\n# Load TFLite model with EdgeTPU delegate\ntry:\n    interpreter = Interpreter(model_path=model_path,\n                              experimental_delegates=[load_delegate('libedgetpu.so.1.0')])\nexcept Exception as e:\n    print(f\"Failed to load EdgeTPU delegate with error: {e}\")\n    try:\n        interpreter = Interpreter(model_path=model_path,\n                                  experimental_delegates=[load_delegate('/usr/lib/aarch64-linux-gnu/libedgetpu.so.1.0')])\n    except Exception as e:\n        print(f\"Failed to load alternative EdgeTPU delegate with error: {e}\")\n        exit(1)\n\ninterpreter.allocate_tensors()\n\n# Get model details\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\n# Check if the input is a float model\nfloating_model = (input_details[0]['dtype'] == np.float32)\n\n# Initialize video capture\nimport cv2\ncap = cv2.VideoCapture(input_path)\nif not cap.isOpened():\n    print(\"Error: Could not open video file.\")\n    exit(1)\n\n# Get the frame dimensions\nframe_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\nframe_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n\n# Define the codec and create VideoWriter object\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\nout = cv2.VideoWriter(output_path, fourcc, 30.0, (frame_width, frame_height))\n\nwhile cap.isOpened():\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    # Preprocess the frame\n    input_data = cv2.resize(frame, (input_details[0]['shape'][1], input_details[0]['shape'][2]))\n    input_data = np.expand_dims(input_data, axis=0)\n\n    if floating_model:\n        input_data = (np.float32(input_data) - 127.5) / 127.5\n\n    # Set the tensor\n    interpreter.set_tensor(input_details[0]['index'], input_data)\n\n    # Run inference\n    interpreter.invoke()\n\n    # Get output details\n    boxes = interpreter.get_tensor(output_details[0]['index'])\n    classes = interpreter.get_tensor(output_details[1]['index'])\n    scores = interpreter.get_tensor(output_details[2]['index'])\n\n    # Interpret results\n    num_detections = int(interpreter.get_tensor(output_details[3]['index']))\n    for i in range(num_detections):\n        if scores[0][i] > confidence_threshold:  # Ensure correct indexing\n            y1, x1, y2, x2 = boxes[0][i] * [frame_height, frame_width, frame_height, frame_width]\n            label = labels[int(classes[0][i])]\n            cv2.rectangle(frame, (int(x1), int(y1)), (int(x2), int(y2)), (0, 255, 0), 2)\n            cv2.putText(frame, f'{label}: {scores[0][i]:.2f}', (int(x1), int(y1) - 10),\n                        cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n\n    # Write the frame with detections\n    out.write(frame)\n\n# Release everything when done\ncap.release()\nout.release()\ncv2.destroyAllWindows()"
      },
      "session_id": "qwen2.5-coder:14b_5d09_tpusg_batch",
      "metadata": {
        "num_run": 7
      },
      "tags": [
        "benchmark",
        "qwen2.5-coder:14b",
        "tpu_sketch_generator"
      ],
      "latency": 115.246,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-22-10-36-973825_chatcmpl-ac1275ee-031e-4934-941c-f9e8b0c62dbb",
          "traceId": "f8843675",
          "type": "GENERATION",
          "name": "f884_tpusg_gen_attempt#1",
          "startTime": "2025-08-19T19:10:36.973000+00:00",
          "endTime": "2025-08-19T19:10:51.068000+00:00",
          "model": "qwen2.5-coder:14b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 14095.0,
          "promptTokens": 1833,
          "environment": "default",
          "completionTokens": 833,
          "costDetails": {},
          "totalTokens": 2666,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-22-10-59-155414_chatcmpl-acfc29c7-b102-4258-801b-56ab6c15d3fc",
          "traceId": "f8843675",
          "type": "GENERATION",
          "name": "f884_tpusg_gen_attempt#2",
          "startTime": "2025-08-19T19:10:59.155000+00:00",
          "endTime": "2025-08-19T19:11:15.332000+00:00",
          "model": "qwen2.5-coder:14b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 16177.0,
          "promptTokens": 2048,
          "environment": "default",
          "completionTokens": 964,
          "costDetails": {},
          "totalTokens": 3012,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-22-11-23-596485_chatcmpl-ef519506-15f1-444a-8a8f-287160379cd2",
          "traceId": "f8843675",
          "type": "GENERATION",
          "name": "f884_tpusg_gen_attempt#3",
          "startTime": "2025-08-19T19:11:23.596000+00:00",
          "endTime": "2025-08-19T19:11:41.232000+00:00",
          "model": "qwen2.5-coder:14b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 17636.0,
          "promptTokens": 2048,
          "environment": "default",
          "completionTokens": 1056,
          "costDetails": {},
          "totalTokens": 3104,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-22-11-49-319930_chatcmpl-be2dd6ef-90be-4cb0-b306-c930e48ef653",
          "traceId": "f8843675",
          "type": "GENERATION",
          "name": "f884_tpusg_gen_attempt#4",
          "startTime": "2025-08-19T19:11:49.319000+00:00",
          "endTime": "2025-08-19T19:12:05.890000+00:00",
          "model": "qwen2.5-coder:14b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 16571.0,
          "promptTokens": 2048,
          "environment": "default",
          "completionTokens": 989,
          "costDetails": {},
          "totalTokens": 3037,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-22-12-13-973987_chatcmpl-795adb69-8054-4e3e-8d1b-61b0fcb45352",
          "traceId": "f8843675",
          "type": "GENERATION",
          "name": "f884_tpusg_gen_attempt#5",
          "startTime": "2025-08-19T19:12:13.973000+00:00",
          "endTime": "2025-08-19T19:12:32.219000+00:00",
          "model": "qwen2.5-coder:14b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 18246.0,
          "promptTokens": 2048,
          "environment": "default",
          "completionTokens": 1097,
          "costDetails": {},
          "totalTokens": 3145,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "updatedAt": "2025-08-19T19:13:29.811Z",
      "environment": "default",
      "createdAt": "2025-08-19T19:10:37.492Z"
    },
    {
      "id": "58f2b702",
      "timestamp": "2025-08-19T19:09:25.287000+00:00",
      "name": "58f2_tpu_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import os\nimport time\nimport numpy as np\nfrom tflite_runtime.interpreter import Interpreter, load_delegate\n\n# Configuration parameters\nmodel_path  = \"/home/mendel/tinyml_autopilot/models/edgetpu_detect.tflite\"\nlabel_path  = \"/home/mendel/tinyml_autopilot/models/labelmap.txt\"\ninput_path  = \"/home/mendel/tinyml_autopilot/data//sheeps.mp4\"\noutput_path  = \"/home/mendel/tinyml_autopilot/results/sheeps_detections.mp4\"\nconfidence_threshold  = 0.5\n\n# Load labels\nwith open(label_path, 'r') as f:\n    labels = [line.strip() for line in f.readlines()]\n\n# Load TFLite model and allocate tensors with EdgeTPU delegate\ntry:\n    interpreter = Interpreter(model_path=model_path,\n                            experimental_delegates=[load_delegate('libedgetpu.so.1')])\nexcept Exception as e:\n    try:\n        interpreter = Interpreter(model_path=model_path,\n                                experimental_delegates=[load_delegate('/usr/lib/aarch64-linux-gnu/libedgetpu.so.1')])\n    except Exception as e:\n        raise RuntimeError(f\"Failed to load EdgeTPU delegate: {e}\")\n\ninterpreter.allocate_tensors()\n\n# Get model details\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\n# Check if model uses floating point inputs\nfloating_model = (input_details[0]['dtype'] == np.float32)\n\n# Phase 2: Input Acquisition & Preprocessing Loop\nimport cv2\n\ncap = cv2.VideoCapture(input_path)\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\nout = cv2.VideoWriter(output_path, fourcc, cap.get(cv2.CAP_PROP_FPS), (int(cap.get(3)), int(cap.get(4))))\n\nwhile cap.isOpened():\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    # Preprocess data\n    input_data = cv2.resize(frame, (input_details[0]['shape'][1], input_details[0]['shape'][2]))\n    input_data = cv2.cvtColor(input_data, cv2.COLOR_BGR2RGB)\n    input_data = np.expand_dims(input_data, axis=0)\n\n    if floating_model:\n        input_data = (np.float32(input_data) - 127.5) / 127.5\n\n    # Phase 3: Inference\n    interpreter.set_tensor(input_details[0]['index'], input_data)\n    interpreter.invoke()\n\n    # Phase 4: Output Interpretation & Handling Loop\n    # Get output tensors\n    boxes = interpreter.get_tensor(output_details[0]['index'])[0]\n    classes = interpreter.get_tensor(output_details[1]['index'])[0]\n    scores = interpreter.get_tensor(output_details[2]['index'])[0]\n\n    # Post-processing\n    for i in range(len(scores)):\n        if scores[i] > confidence_threshold:\n            class_id = int(classes[i])\n            label = labels[class_id]\n            box = boxes[i]\n            ymin, xmin, ymax, xmax = map(int, box * [frame.shape[0], frame.shape[1], frame.shape[0], frame.shape[1]])\n            cv2.rectangle(frame, (xmin, ymin), (xmax, ymax), (0, 255, 0), 2)\n            cv2.putText(frame, f'{label}: {scores[i]:.2f}', (xmin, ymin - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n\n    # Write the frame to output video\n    out.write(frame)\n\ncap.release()\nout.release()\n\nprint(f\"Output saved to {output_path}\")"
      },
      "session_id": "qwen2.5-coder:14b_5d09_tpusg_batch",
      "metadata": {
        "num_run": 6
      },
      "tags": [
        "benchmark",
        "qwen2.5-coder:14b",
        "tpu_sketch_generator"
      ],
      "latency": 13.298,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-22-09-25-289762_chatcmpl-0758760f-ed07-4dc2-841c-e1b1c3a7f850",
          "traceId": "58f2b702",
          "type": "GENERATION",
          "name": "58f2_tpusg_gen_attempt#1",
          "startTime": "2025-08-19T19:09:25.289000+00:00",
          "endTime": "2025-08-19T19:09:38.587000+00:00",
          "model": "qwen2.5-coder:14b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 13298.0,
          "promptTokens": 1833,
          "environment": "default",
          "completionTokens": 782,
          "costDetails": {},
          "totalTokens": 2615,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "updatedAt": "2025-08-19T19:10:36.026Z",
      "environment": "default",
      "createdAt": "2025-08-19T19:09:25.810Z"
    },
    {
      "id": "1373e76a",
      "timestamp": "2025-08-19T19:08:10.167000+00:00",
      "name": "1373_tpu_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import os\nfrom tflite_runtime.interpreter import Interpreter, load_delegate\nimport numpy as np\nimport cv2\n\n# Phase 1: Setup\n\n# Paths/Parameters\nmodel_path  = \"/home/mendel/tinyml_autopilot/models/edgetpu_detect.tflite\"\nlabel_path  = \"/home/mendel/tinyml_autopilot/models/labelmap.txt\"\ninput_path  = \"/home/mendel/tinyml_autopilot/data//sheeps.mp4\"\noutput_path  = \"/home/mendel/tinyml_autopilot/results/sheeps_detections.mp4\"\nconfidence_threshold  = 0.5\n\n# Load Labels\nwith open(label_path, 'r') as f:\n    labels = [line.strip() for line in f.readlines()]\n\n# Load Interpreter with EdgeTPU\ntry:\n    interpreter = Interpreter(model_path=model_path,\n                              experimental_delegates=[load_delegate('libedgetpu.so.1.0')])\nexcept Exception as e:\n    try:\n        interpreter = Interpreter(model_path=model_path,\n                                  experimental_delegates=[load_delegate('/usr/lib/aarch64-linux-gnu/libedgetpu.so.1.0')])\n    except Exception as e:\n        raise Exception(\"Failed to load EdgeTPU delegate: \" + str(e))\n\ninterpreter.allocate_tensors()\n\n# Get Model Details\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\nheight = input_details[0]['shape'][1]\nwidth = input_details[0]['shape'][2]\n\n# Phase 2: Input Acquisition & Preprocessing Loop\n\ncap = cv2.VideoCapture(input_path)\nfourcc = cv2.VideoWriter_fourcc(*'XVID')\nout = cv2.VideoWriter(output_path, fourcc, 30.0, (int(cap.get(3)), int(cap.get(4))))\n\nwhile cap.isOpened():\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    # Preprocess Data\n    input_data = cv2.resize(frame, (width, height))\n    input_data = np.expand_dims(input_data, axis=0)\n    input_data = input_data.astype(np.uint8)\n\n    # Quantization Handling\n    floating_model = (input_details[0]['dtype'] == np.float32)\n    if floating_model:\n        input_data = (np.float32(input_data) - 127.5) / 127.5\n\n    # Phase 3: Inference\n\n    interpreter.set_tensor(input_details[0]['index'], input_data)\n    interpreter.invoke()\n\n    # Phase 4: Output Interpretation & Handling Loop\n\n    # Get Output Tensor(s)\n    boxes = interpreter.get_tensor(output_details[0]['index'])[0]\n    classes = interpreter.get_tensor(output_details[1]['index'])[0]\n    scores = interpreter.get_tensor(output_details[2]['index'])[0]\n\n    for i in range(len(scores)):\n        if scores[i] > confidence_threshold:\n            y_min, x_min, y_max, x_max = boxes[i]\n            label = labels[int(classes[i])]\n            cv2.rectangle(frame, (int(x_min * frame.shape[1]), int(y_min * frame.shape[0])),\n                          (int(x_max * frame.shape[1]), int(y_max * frame.shape[0])), (0, 255, 0), 2)\n            cv2.putText(frame, f'{label}: {scores[i]:.2f}', (int(x_min * frame.shape[1]), int(y_min * frame.shape[0]) - 10),\n                        cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n\n    out.write(frame)\n\ncap.release()\nout.release()\ncv2.destroyAllWindows()\n\n# Phase 5: Cleanup\n\ncap.release()\nout.release()"
      },
      "session_id": "qwen2.5-coder:14b_5d09_tpusg_batch",
      "metadata": {
        "num_run": 5
      },
      "tags": [
        "benchmark",
        "qwen2.5-coder:14b",
        "tpu_sketch_generator"
      ],
      "latency": 14.743,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-22-08-10-169362_chatcmpl-ccd485db-262f-410c-9bf4-358c412a0728",
          "traceId": "1373e76a",
          "type": "GENERATION",
          "name": "1373_tpusg_gen_attempt#1",
          "startTime": "2025-08-19T19:08:10.169000+00:00",
          "endTime": "2025-08-19T19:08:24.912000+00:00",
          "model": "qwen2.5-coder:14b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 14743.0,
          "promptTokens": 1833,
          "environment": "default",
          "completionTokens": 874,
          "costDetails": {},
          "totalTokens": 2707,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "updatedAt": "2025-08-19T19:09:22.787Z",
      "environment": "default",
      "createdAt": "2025-08-19T19:08:10.689Z"
    },
    {
      "id": "fb6d1e0c",
      "timestamp": "2025-08-19T19:06:59.359000+00:00",
      "name": "fb6d_tpu_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import os\nimport time\nimport numpy as np\nfrom tflite_runtime.interpreter import Interpreter, load_delegate\n\n# Configuration parameters\nmodel_path  = \"/home/mendel/tinyml_autopilot/models/edgetpu_detect.tflite\"\nlabel_path  = \"/home/mendel/tinyml_autopilot/models/labelmap.txt\"\ninput_path  = \"/home/mendel/tinyml_autopilot/data//sheeps.mp4\"\noutput_path  = \"/home/mendel/tinyml_autopilot/results/sheeps_detections.mp4\"\nconfidence_threshold  = 0.5\n\n# Load labels\nwith open(label_path, 'r') as f:\n    labels = [line.strip() for line in f.readlines()]\n\n# Load interpreter with EdgeTPU delegate\ntry:\n    interpreter = Interpreter(model_path=model_path, \n                              experimental_delegates=[load_delegate('libedgetpu.so.1.0')])\nexcept Exception as e:\n    print(f\"Failed to load EdgeTPU delegate: {e}\")\n    try:\n        interpreter = Interpreter(model_path=model_path, \n                                  experimental_delegates=[load_delegate('/usr/lib/aarch64-linux-gnu/libedgetpu.so.1.0')])\n    except Exception as e:\n        print(f\"Failed to load alternative EdgeTPU delegate: {e}\")\n        exit(1)\n\ninterpreter.allocate_tensors()\n\n# Get model details\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\ninput_shape = input_details[0]['shape']\nfloating_model = (input_details[0]['dtype'] == np.float32)\n\n# Import OpenCV for video processing\nimport cv2\n\n# Initialize video capture and writer\ncap = cv2.VideoCapture(input_path)\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\nout = cv2.VideoWriter(output_path, fourcc, 30.0, (int(cap.get(3)), int(cap.get(4))))\n\nwhile cap.isOpened():\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    # Preprocess input\n    resized_frame = cv2.resize(frame, (input_shape[1], input_shape[2]))\n    input_data = np.expand_dims(resized_frame, axis=0)\n    if floating_model:\n        input_data = (np.float32(input_data) - 127.5) / 127.5\n\n    # Set input tensor\n    interpreter.set_tensor(input_details[0]['index'], input_data)\n\n    # Run inference\n    start_time = time.time()\n    interpreter.invoke()\n    end_time = time.time()\n\n    print(f\"Inference time: {end_time - start_time} seconds\")\n\n    # Get output tensors\n    boxes = interpreter.get_tensor(output_details[0]['index'])[0]\n    classes = interpreter.get_tensor(output_details[1]['index'])[0]\n    scores = interpreter.get_tensor(output_details[2]['index'])[0]\n\n    # Post-process results\n    for i in range(len(scores)):\n        if scores[i] > confidence_threshold:\n            label_id = int(classes[i])\n            label_name = labels[label_id]\n            box = boxes[i]\n            y1, x1, y2, x2 = map(int, box)\n            cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n            cv2.putText(frame, f'{label_name}: {scores[i]:.2f}', (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n\n    # Write the frame with detections\n    out.write(frame)\n\n# Release resources\ncap.release()\nout.release()\ncv2.destroyAllWindows()"
      },
      "session_id": "qwen2.5-coder:14b_5d09_tpusg_batch",
      "metadata": {
        "num_run": 4
      },
      "tags": [
        "benchmark",
        "qwen2.5-coder:14b",
        "tpu_sketch_generator"
      ],
      "latency": 13.354,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-22-06-59-361344_chatcmpl-51ab9ea4-117f-4f60-85f7-eda4992d4b64",
          "traceId": "fb6d1e0c",
          "type": "GENERATION",
          "name": "fb6d_tpusg_gen_attempt#1",
          "startTime": "2025-08-19T19:06:59.361000+00:00",
          "endTime": "2025-08-19T19:07:12.715000+00:00",
          "model": "qwen2.5-coder:14b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 13354.0,
          "promptTokens": 1833,
          "environment": "default",
          "completionTokens": 786,
          "costDetails": {},
          "totalTokens": 2619,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "updatedAt": "2025-08-19T19:08:09.745Z",
      "environment": "default",
      "createdAt": "2025-08-19T19:06:59.883Z"
    },
    {
      "id": "e729077b",
      "timestamp": "2025-08-19T19:05:25.218000+00:00",
      "name": "e729_tpu_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import numpy as np\nfrom tflite_runtime.interpreter import Interpreter, load_delegate\n\n# Configuration parameters\nmodel_path   = \"/home/mendel/tinyml_autopilot/models/edgetpu_detect.tflite\"\nlabel_path   = \"/home/mendel/tinyml_autopilot/models/labelmap.txt\"\ninput_path   = \"/home/mendel/tinyml_autopilot/data//sheeps.mp4\"\noutput_path   = \"/home/mendel/tinyml_autopilot/results/sheeps_detections.mp4\"\nconfidence_threshold   = 0.5\n\n# Load labels\nwith open(label_path, 'r') as f:\n    labels = [line.strip() for line in f.readlines()]\n\n# Load interpreter with EdgeTPU acceleration\ntry:\n    interpreter = Interpreter(model_path=model_path, \n                            experimental_delegates=[load_delegate('libedgetpu.so.1.0')])\nexcept Exception as e:\n    print(f\"Failed to load delegate: {e}\")\n    try:\n        interpreter = Interpreter(model_path=model_path, \n                                experimental_delegates=[load_delegate('/usr/lib/aarch64-linux-gnu/libedgetpu.so.1.0')])\n    except Exception as e:\n        print(f\"Failed to load delegate from alternative path: {e}\")\n        raise\n\ninterpreter.allocate_tensors()\n\n# Get model details\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\n# Input acquisition & preprocessing\nimport cv2\ncap = cv2.VideoCapture(input_path)\nframe_width = int(cap.get(3))\nframe_height = int(cap.get(4))\nfourcc = cv2.VideoWriter_fourcc(*'XVID')\nout = cv2.VideoWriter(output_path, fourcc, 20.0, (frame_width, frame_height))\n\nwhile cap.isOpened():\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    # Preprocess data\n    input_data = cv2.resize(frame, (input_details[0]['shape'][1], input_details[0]['shape'][2]))\n    input_data = np.expand_dims(input_data, axis=0)\n    input_data = input_data.astype(np.float32) / 255.0\n\n    # Quantization handling\n    if input_details[0]['dtype'] == np.uint8:\n        input_data = (input_data * 255).astype(np.uint8)\n\n    # Set input tensor and run inference\n    interpreter.set_tensor(input_details[0]['index'], input_data)\n    interpreter.invoke()\n\n    # Get output tensors\n    boxes = interpreter.get_tensor(output_details[0]['index'])\n    classes = interpreter.get_tensor(output_details[1]['index'])\n    scores = interpreter.get_tensor(output_details[2]['index'])\n\n    # Interpret results\n    for i in range(len(scores[0])):\n        if scores[0][i] > confidence_threshold:\n            box = boxes[0][i]\n            class_id = int(classes[0][i])\n            label = labels[class_id]\n\n            # Draw bounding box and label\n            y1, x1, y2, x2 = map(int, box)\n            cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n            cv2.putText(frame, f'{label}: {scores[0][i]:.2f}', (x1, y1 - 10),\n                        cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n\n    # Handle output\n    out.write(frame)\n\n# Cleanup\ncap.release()\nout.release()\ncv2.destroyAllWindows()"
      },
      "session_id": "qwen2.5-coder:14b_5d09_tpusg_batch",
      "metadata": {
        "num_run": 3
      },
      "tags": [
        "benchmark",
        "qwen2.5-coder:14b",
        "tpu_sketch_generator"
      ],
      "latency": 35.584,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-22-05-25-221863_chatcmpl-a37de7a3-483c-4108-812b-39925dd56fec",
          "traceId": "e729077b",
          "type": "GENERATION",
          "name": "e729_tpusg_gen_attempt#1",
          "startTime": "2025-08-19T19:05:25.221000+00:00",
          "endTime": "2025-08-19T19:05:38.245000+00:00",
          "model": "qwen2.5-coder:14b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 13024.0,
          "promptTokens": 1833,
          "environment": "default",
          "completionTokens": 765,
          "costDetails": {},
          "totalTokens": 2598,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-22-05-46-144558_chatcmpl-9fa4bc78-7e1b-43ab-8cb5-0f05701674ec",
          "traceId": "e729077b",
          "type": "GENERATION",
          "name": "e729_tpusg_gen_attempt#2",
          "startTime": "2025-08-19T19:05:46.144000+00:00",
          "endTime": "2025-08-19T19:06:00.805000+00:00",
          "model": "qwen2.5-coder:14b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 14661.0,
          "promptTokens": 2048,
          "environment": "default",
          "completionTokens": 870,
          "costDetails": {},
          "totalTokens": 2918,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "updatedAt": "2025-08-19T19:06:58.403Z",
      "environment": "default",
      "createdAt": "2025-08-19T19:05:25.750Z"
    },
    {
      "id": "5505bc06",
      "timestamp": "2025-08-19T19:04:11.976000+00:00",
      "name": "5505_tpu_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import os\nimport numpy as np\nimport cv2\nfrom tflite_runtime.interpreter import Interpreter, load_delegate\n\n# Configuration parameters\nmodel_path  = \"/home/mendel/tinyml_autopilot/models/edgetpu_detect.tflite\"\nlabel_path  = \"/home/mendel/tinyml_autopilot/models/labelmap.txt\"\ninput_path  = \"/home/mendel/tinyml_autopilot/data//sheeps.mp4\"\noutput_path  = \"/home/mendel/tinyml_autopilot/results/sheeps_detections.mp4\"\nconfidence_threshold  = 0.5\n\n# Phase 1: Setup\ntry:\n    interpreter = Interpreter(model_path=model_path, experimental_delegates=[load_delegate('libedgetpu.so.1')])\nexcept Exception as e:\n    print(f\"Failed to load EdgeTPU delegate with default path. Trying alternative: {e}\")\n    try:\n        interpreter = Interpreter(model_path=model_path, experimental_delegates=[load_delegate('/usr/lib/aarch64-linux-gnu/libedgetpu.so.1')])\n    except Exception as e2:\n        raise RuntimeError(f\"Failed to load EdgeTPU delegate with both paths: {e2}\")\n\ninterpreter.allocate_tensors()\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\n# Load labels\nwith open(label_path, 'r') as f:\n    labels = [line.strip() for line in f.readlines()]\n\n# Phase 2: Input Acquisition & Preprocessing Loop\ncap = cv2.VideoCapture(input_path)\nif not cap.isOpened():\n    raise RuntimeError(\"Failed to open video file\")\n\nframe_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\nframe_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\nout = cv2.VideoWriter(output_path, fourcc, 30.0, (frame_width, frame_height))\n\nwhile cap.isOpened():\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    # Preprocess data\n    input_data = cv2.resize(frame, (input_details[0]['shape'][1], input_details[0]['shape'][2]))\n    input_data = cv2.cvtColor(input_data, cv2.COLOR_BGR2RGB)\n    input_data = np.expand_dims(input_data, axis=0)\n    \n    if input_details[0]['dtype'] == np.float32:\n        input_data = (np.float32(input_data) - 127.5) / 127.5\n\n    # Phase 3: Inference\n    interpreter.set_tensor(input_details[0]['index'], input_data)\n    interpreter.invoke()\n\n    # Phase 4: Output Interpretation & Handling Loop\n    boxes = interpreter.get_tensor(output_details[0]['index'])[0]\n    classes = interpreter.get_tensor(output_details[1]['index'])[0] \n    scores = interpreter.get_tensor(output_details[2]['index'])[0]\n\n    for i in range(len(scores)):\n        if scores[i] > confidence_threshold:\n            class_id = int(classes[i])\n            box = boxes[i]\n            ymin, xmin, ymax, xmax = map(int, [box[0] * frame_height, box[1] * frame_width, box[2] * frame_height, box[3] * frame_width])\n            cv2.rectangle(frame, (xmin, ymin), (xmax, ymax), (255, 0, 0), 2)\n            label = f\"{labels[class_id]}: {scores[i]:.2f}\"\n            cv2.putText(frame, label, (xmin, ymin - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (36,255,12), 2)\n\n    # Handle Output\n    out.write(frame)\n\n# Phase 5: Cleanup\ncap.release()\nout.release()\ncv2.destroyAllWindows()"
      },
      "session_id": "qwen2.5-coder:14b_5d09_tpusg_batch",
      "metadata": {
        "num_run": 2
      },
      "tags": [
        "benchmark",
        "qwen2.5-coder:14b",
        "tpu_sketch_generator"
      ],
      "latency": 13.727,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-22-04-11-978341_chatcmpl-751ceae7-2986-4a6e-b392-ef33389aad3f",
          "traceId": "5505bc06",
          "type": "GENERATION",
          "name": "5505_tpusg_gen_attempt#1",
          "startTime": "2025-08-19T19:04:11.978000+00:00",
          "endTime": "2025-08-19T19:04:25.705000+00:00",
          "model": "qwen2.5-coder:14b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 13727.0,
          "promptTokens": 1833,
          "environment": "default",
          "completionTokens": 810,
          "costDetails": {},
          "totalTokens": 2643,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "updatedAt": "2025-08-19T19:05:23.190Z",
      "environment": "default",
      "createdAt": "2025-08-19T19:04:12.498Z"
    },
    {
      "id": "5e4b14e6",
      "timestamp": "2025-08-19T19:02:59.335000+00:00",
      "name": "5e4b_tpu_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import os\nimport time\nimport numpy as np\nfrom tflite_runtime.interpreter import Interpreter, load_delegate\n\n# Configuration Parameters\nmodel_path  = \"/home/mendel/tinyml_autopilot/models/edgetpu_detect.tflite\"\nlabel_path  = \"/home/mendel/tinyml_autopilot/models/labelmap.txt\"\ninput_path  = \"/home/mendel/tinyml_autopilot/data//sheeps.mp4\"\noutput_path  = \"/home/mendel/tinyml_autopilot/results/sheeps_detections.mp4\"\nconfidence_threshold  = 0.5\n\n# Phase 1: Setup\n# 1.3 Load Labels\nwith open(label_path, 'r') as f:\n    labels = [line.strip() for line in f.readlines()]\n\n# 1.4 Load Interpreter with EdgeTPU\ntry:\n    interpreter = Interpreter(model_path=model_path,\n                              experimental_delegates=[load_delegate('libedgetpu.so.1.0')])\nexcept Exception as e:\n    try:\n        interpreter = Interpreter(model_path=model_path,\n                                  experimental_delegates=[load_delegate('/usr/lib/aarch64-linux-gnu/libedgetpu.so.1.0')])\n    except Exception as e:\n        print(f\"Failed to load EdgeTPU delegate: {e}\")\n        exit(1)\n\ninterpreter.allocate_tensors()\n\n# 1.5 Get Model Details\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\ninput_shape = input_details[0]['shape']\ninput_dtype = input_details[0]['dtype']\n\n# Phase 2: Input Acquisition & Preprocessing Loop\nimport cv2\n\ncap = cv2.VideoCapture(input_path)\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\nout = cv2.VideoWriter(output_path, fourcc, cap.get(cv2.CAP_PROP_FPS), (int(cap.get(3)), int(cap.get(4))))\n\nwhile cap.isOpened():\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    # 2.2 Preprocess Data\n    resized_frame = cv2.resize(frame, (input_shape[1], input_shape[2]))\n    input_data = np.expand_dims(resized_frame, axis=0).astype(input_dtype)\n\n    # 2.3 Quantization Handling\n    floating_model = (input_details[0]['dtype'] == np.float32)\n    if floating_model:\n        input_data = (np.float32(input_data) - 127.5) / 127.5\n\n    # Phase 3: Inference\n    interpreter.set_tensor(input_details[0]['index'], input_data)\n    interpreter.invoke()\n\n    # Phase 4: Output Interpretation & Handling Loop\n    # 4.1 Get Output Tensor(s)\n    boxes = interpreter.get_tensor(output_details[0]['index'])[0]\n    classes = interpreter.get_tensor(output_details[1]['index'])[0]\n    scores = interpreter.get_tensor(output_details[2]['index'])[0]\n\n    # 4.2 Interpret Results\n    for i in range(len(scores)):\n        if scores[i] > confidence_threshold:\n            class_id = int(classes[i])\n            box = boxes[i]\n            ymin, xmin, ymax, xmax = map(int, box * [frame.shape[0], frame.shape[1], frame.shape[0], frame.shape[1]])\n            cv2.rectangle(frame, (xmin, ymin), (xmax, ymax), (0, 255, 0), 2)\n            label_text = f'{labels[class_id]}: {scores[i]:.2f}'\n            cv2.putText(frame, label_text, (xmin, ymin - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n\n    # 4.3 Post-processing\n    # Already handled in the loop above\n\n    # 4.4 Handle Output\n    out.write(frame)\n\n# Phase 5: Cleanup\ncap.release()\nout.release()\ncv2.destroyAllWindows()"
      },
      "session_id": "qwen2.5-coder:14b_5d09_tpusg_batch",
      "metadata": {
        "num_run": 1
      },
      "tags": [
        "benchmark",
        "qwen2.5-coder:14b",
        "tpu_sketch_generator"
      ],
      "latency": 14.348,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-22-02-59-337143_chatcmpl-7fb9ef63-8afa-496d-bf47-e9349167d30d",
          "traceId": "5e4b14e6",
          "type": "GENERATION",
          "name": "5e4b_tpusg_gen_attempt#1",
          "startTime": "2025-08-19T19:02:59.337000+00:00",
          "endTime": "2025-08-19T19:03:13.685000+00:00",
          "model": "qwen2.5-coder:14b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 14348.0,
          "promptTokens": 1833,
          "environment": "default",
          "completionTokens": 839,
          "costDetails": {},
          "totalTokens": 2672,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "updatedAt": "2025-08-19T19:04:11.554Z",
      "environment": "default",
      "createdAt": "2025-08-19T19:02:59.857Z"
    }
  ],
  "meta": {
    "total_items": 30
  }
}
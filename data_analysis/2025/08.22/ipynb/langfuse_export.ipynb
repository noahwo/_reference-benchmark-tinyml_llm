{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setting up\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ref: https://langfuse.com/docs/query-traces\n",
    "import os\n",
    "import json\n",
    "from langfuse import Langfuse\n",
    "import json\n",
    "import os\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "LOCAL_HOST = True\n",
    "\n",
    "\"\"\"Define session_id\"\"\"\n",
    "# session_id=\"qwen2.5-coder_f4d4_dp_batch\"\n",
    "session_id_list = [\n",
    "    # \"qwen2.5-coder:32b_4e11_tpsg_batch\",\n",
    "    # \"qwen2.5-coder:32b_ae24_tpsg_batch\"\n",
    "    \"qwen2.5-coder:14b_c83f_tpusg_batch\",\n",
    "    \"qwen2.5-coder:14b_c83f_psg_batch\"\n",
    "]\n",
    "\n",
    "\n",
    "\"\"\"Define paths\"\"\"\n",
    " \n",
    "parent_dir = os.path.dirname(os.getcwd())\n",
    "\n",
    " \n",
    "# date = os.path.basename(parent_dir)\n",
    "tex_dir = os.path.join(parent_dir, \"tex\")\n",
    "processed_data_dir = os.path.join(parent_dir, \"processed_data\")\n",
    "raw_export_dir = os.path.join(parent_dir, \"raw_export\")\n",
    "ipynb_dir = os.path.join(parent_dir, \"ipynb\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Export raw data\n",
    "\n",
    "Langfuse added a limit of 20 API invocations per minute. https://langfuse.com/faq/all/api-limits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching traces for session qwen2.5-coder:14b_c83f_tpusg_batch...\n",
      "Fetching observation data for time-03-26-36-529470_chatcmpl-945c5f9f-cf3c-4c00-8e47-92e5c86705a3...\n",
      "Fetching observation data for time-03-25-28-963478_chatcmpl-d66d74cf-5da8-4236-9026-634ba753a2c3...\n",
      "Fetching observation data for time-03-24-16-173487_chatcmpl-9d39e85d-d152-4a1b-8373-2208f078cfb8...\n",
      "Fetching observation data for time-03-23-08-606344_chatcmpl-2c5b1d3f-4e82-4790-be26-078db2b93be1...\n",
      "Fetching observation data for time-03-21-11-028718_chatcmpl-ac3aab4a-80d7-46e3-bb0b-a7471d1d3edf...\n",
      "Fetching observation data for time-03-21-32-275878_chatcmpl-52e16218-1bbc-4bf0-9caf-414fe9693373...\n",
      "Fetching observation data for time-03-21-56-365651_chatcmpl-9ad29c36-875f-43ef-b71f-f96339959137...\n",
      "Fetching observation data for time-03-19-58-261013_chatcmpl-89aa28c8-3a6e-4feb-a58e-c07bc163fca2...\n",
      "Fetching observation data for time-03-14-59-718492_chatcmpl-c8074626-76dc-42e9-a75a-534ddd96fa90...\n",
      "Fetching observation data for time-03-15-18-284037_chatcmpl-f0bda787-17f0-486d-a6e0-a538037a351e...\n",
      "Fetching observation data for time-03-16-58-527586_chatcmpl-c062341e-202d-4c69-9268-2846ba0a45c2...\n",
      "Fetching observation data for time-03-18-40-979754_chatcmpl-33aaf672-78a1-4415-921a-03a0f0eb17b0...\n",
      "Fetching observation data for time-03-13-21-779736_chatcmpl-4b4ba657-a9d0-4d60-8402-c00185b15630...\n",
      "Fetching observation data for time-03-13-43-716336_chatcmpl-c0e26943-98f0-4cda-8766-2959c4fd9353...\n",
      "Fetching observation data for time-03-12-12-597800_chatcmpl-7ef7e80b-dd20-411d-8005-0e77c1b37c2a...\n",
      "Fetching observation data for time-03-10-59-774746_chatcmpl-bc026644-6601-459d-acbe-6f19c7f22dad...\n",
      "Fetching observation data for time-03-09-20-806822_chatcmpl-95392048-4314-4388-a4be-efce6363d1a5...\n",
      "Fetching observation data for time-03-09-42-966606_chatcmpl-5b768505-df43-42e0-b5d2-5f3495262602...\n",
      "Fetching observation data for time-03-08-07-934594_chatcmpl-2c9eb45b-34bd-4a5d-8d08-69a54c023546...\n",
      "Fetching observation data for time-03-06-55-102001_chatcmpl-78da3643-274c-4d50-82a2-f97e43d1411f...\n",
      "Fetching observation data for time-03-05-47-572586_chatcmpl-f20666e9-c256-4809-964f-36a765373a17...\n",
      "Fetching observation data for time-03-04-39-997619_chatcmpl-d65192c8-d160-4630-9190-c29faeea941e...\n",
      "Fetching observation data for time-03-03-29-492018_chatcmpl-511bb183-7cf6-45bd-b6e2-146b5a6cdc46...\n",
      "Fetching observation data for time-03-01-23-923277_chatcmpl-2959968f-ef20-479a-8aaf-bb616d5e9d8f...\n",
      "Fetching observation data for time-03-01-47-061473_chatcmpl-85208c69-1c12-41f6-bb68-ebec53455d2c...\n",
      "Fetching observation data for time-03-02-12-996233_chatcmpl-425d9e15-a076-41f0-a350-e359f3f3e352...\n",
      "Fetching observation data for time-03-00-10-358042_chatcmpl-246a259a-ebbd-4d39-807b-3e891b49502f...\n",
      "Fetching observation data for time-02-58-12-791188_chatcmpl-bcc60e47-e940-48da-a433-ec408e12846d...\n",
      "Fetching observation data for time-02-58-35-168496_chatcmpl-38314a5b-a06c-49e0-b8d9-43e9cbd70e70...\n",
      "Fetching observation data for time-02-58-56-509940_chatcmpl-c474cfe3-edd2-4d5e-aea3-60b3b03b1455...\n",
      "Fetching observation data for time-02-56-22-279300_chatcmpl-51a5c019-7dfc-4158-a52e-52ffd3352358...\n",
      "Fetching observation data for time-02-56-41-653972_chatcmpl-632b04c8-e712-4153-af2f-45006e8229ae...\n",
      "Fetching observation data for time-02-57-02-246241_chatcmpl-d21da406-979e-4482-93f3-adf1d68a718d...\n",
      "Fetching observation data for time-02-57-23-375508_chatcmpl-294a7128-2e3a-44ea-b7c3-184ee255d27b...\n",
      "Fetching observation data for time-02-57-44-753425_chatcmpl-f47b495b-2a7f-44fa-adbf-7468a205e000...\n",
      "Fetching observation data for 717febd7-bf95-4e2e-9212-34fdc1eb3ed7...\n",
      "Fetching observation data for time-02-55-09-692859_chatcmpl-4b63a8f6-059f-408f-af2e-c28793a54266...\n",
      "Fetching observation data for time-02-53-35-167321_chatcmpl-4b86e39e-3907-481c-959b-9724e5678cb1...\n",
      "Fetching observation data for time-02-53-55-300898_chatcmpl-0bbeff58-ff62-496c-97ce-fc57c3e5060c...\n",
      "Fetching observation data for time-02-51-34-650057_chatcmpl-6d6d4d8d-a0cc-499a-9d73-fece4a6dc432...\n",
      "Fetching observation data for time-02-51-56-369953_chatcmpl-33481b00-6c00-45e5-a6c1-b44fe29fcc53...\n",
      "Fetching observation data for time-02-52-20-125831_chatcmpl-5ebeb068-6aac-4c9c-9ebe-88ea56463c3c...\n",
      "Fetching observation data for time-02-50-22-667169_chatcmpl-79f2b9b9-8643-4cee-adea-4d9d6da6b2c1...\n",
      "Fetching observation data for time-02-48-50-817118_chatcmpl-e936f3b7-11b3-4e49-8065-837a66a466ff...\n",
      "Fetching observation data for time-02-49-11-181508_chatcmpl-f8615840-04aa-4da3-8f1f-2f22f6b0aff4...\n",
      "Fetching observation data for time-02-47-42-317873_chatcmpl-52fe673a-9e80-4a79-995e-49911fa58a50...\n",
      "Fetching observation data for time-02-46-29-810766_chatcmpl-33f57d2c-ede3-4639-906d-7d87e1ad27d8...\n",
      "Fetching observation data for time-02-44-30-272068_chatcmpl-08e4da75-b13d-4575-af0f-fbc5c9a99501...\n",
      "Fetching observation data for time-02-44-51-567084_chatcmpl-7527aa3b-fae7-4197-adec-774dda884257...\n",
      "Fetching observation data for time-02-45-14-862704_chatcmpl-74efe536-a149-4ff9-922f-1f634842eacc...\n",
      "Fetching observation data for time-02-43-22-769606_chatcmpl-e57d6444-7502-40ea-a743-9528f85c3433...\n",
      "Fetching observation data for time-02-40-53-816882_chatcmpl-e517515e-c819-4ddc-a45b-7e9cb8d7f7b8...\n",
      "Fetching observation data for time-02-41-37-466183_chatcmpl-95bfecc6-884c-47bc-82c2-b13ac04abb75...\n",
      "Fetching observation data for time-02-42-02-262588_chatcmpl-9af059d0-2ac7-4917-9fcf-d9fefa55d9bf...\n",
      "Fetching observation data for time-02-42-26-786742_chatcmpl-792ce005-2fce-423d-a08a-79992590e9d9...\n",
      "Fetching observation data for time-02-42-51-964019_chatcmpl-008c817f-2529-457e-b11d-0954908e3ee3...\n",
      "Fetching observation data for 6799d62b-bbe5-4f1e-b8a0-d42dcca968c7...\n",
      "Raw JSON saved to: /home/han/Projects/reference-benchmark-tinyml_llm/data_analysis/2025/08.22/raw_export/raw_qwen2.5-coder:14b_c83f_tpusg_batch.json\n",
      "Fetching traces for session qwen2.5-coder:14b_c83f_psg_batch...\n",
      "Fetching observation data for time-03-59-06-496300_chatcmpl-7cc62aea-7e28-46de-a8f9-a4c02eed1647...\n",
      "Fetching observation data for time-03-59-18-568055_chatcmpl-3775336e-d245-4674-9177-c6db52bf1a5d...\n",
      "Fetching observation data for time-03-57-58-904479_chatcmpl-cdbec683-3cd2-4631-8b9e-c1550e95d65e...\n",
      "Fetching observation data for time-03-58-10-015278_chatcmpl-deb455e2-3b86-4a30-baf1-9cd9df5feafd...\n",
      "Fetching observation data for time-03-58-22-748129_chatcmpl-e0988d3c-a959-49b2-b6d6-682a777fec8f...\n",
      "Fetching observation data for time-03-58-38-257254_chatcmpl-c50c0110-18e3-4a41-905a-098bbad26bf8...\n",
      "Fetching observation data for time-03-57-11-287441_chatcmpl-acf90efa-7c2f-49bf-b105-e429aac7e1bd...\n",
      "Fetching observation data for time-03-56-08-594609_chatcmpl-6c94a098-0eec-48b8-b3e5-f1b71bc2423f...\n",
      "Fetching observation data for time-03-56-19-361071_chatcmpl-19d1668e-cdb2-40ce-a4da-2d89f02d4a14...\n",
      "Fetching observation data for time-03-55-30-928398_chatcmpl-3bfded65-0989-4c45-8b1d-8442f005df4c...\n",
      "Fetching observation data for time-03-55-41-937472_chatcmpl-99cbc777-4ca9-4283-a744-82288069f152...\n",
      "Fetching observation data for time-03-54-41-064916_chatcmpl-80e4946c-792c-4061-beea-e041b5034ba8...\n",
      "Fetching observation data for time-03-54-51-635292_chatcmpl-abbd3026-5999-4f1f-9184-246f09d43394...\n",
      "Fetching observation data for time-03-55-03-717996_chatcmpl-f6a1f046-d7bc-423e-b12c-e19a3a66f439...\n",
      "Fetching observation data for time-03-53-52-409395_chatcmpl-1b104fc7-aa4b-43df-9c7d-2e3208e82232...\n",
      "Fetching observation data for time-03-52-49-695674_chatcmpl-84dc664b-2707-4742-ac54-b99384d629e7...\n",
      "Fetching observation data for time-03-53-00-426408_chatcmpl-13a977f6-c11f-46df-aae6-15e5355b4e4d...\n",
      "Fetching observation data for time-03-52-00-886521_chatcmpl-a2e50948-96de-488f-8dee-850e4c71eb32...\n",
      "Fetching observation data for time-03-50-45-925314_chatcmpl-b3bc1181-292a-4300-8b88-63e95a026e1b...\n",
      "Fetching observation data for time-03-50-56-673227_chatcmpl-57d8a28f-74ce-4abe-9b48-20d3a4ef82af...\n",
      "Fetching observation data for time-03-51-11-035948_chatcmpl-62115b93-bc7a-4f7c-aede-dc94f2a21137...\n",
      "Fetching observation data for time-03-51-25-007418_chatcmpl-a8cfa8f5-18bc-4d2a-a451-155938bd0dbc...\n",
      "Fetching observation data for time-03-51-39-989463_chatcmpl-306cf026-60db-44ec-9aa9-23249d5f8f63...\n",
      "Fetching observation data for 29082b66-8752-4a0f-9606-f5865162c33c...\n",
      "Fetching observation data for time-03-49-41-999282_chatcmpl-a1beea7c-b52f-4d38-9e66-5957b89b25af...\n",
      "Fetching observation data for time-03-49-53-627788_chatcmpl-7a65f99f-3c47-414e-b1fc-ac13a95d9646...\n",
      "Fetching observation data for time-03-48-28-243518_chatcmpl-748eabf9-3dc8-4784-be6d-6f843f5f4e02...\n",
      "Fetching observation data for time-03-48-38-812977_chatcmpl-c6627de2-2243-44ae-8661-03d947805d4d...\n",
      "Fetching observation data for time-03-48-52-181081_chatcmpl-0419a2b3-e2dc-4c14-a8ae-f6ebbe07c562...\n",
      "Fetching observation data for time-03-47-16-405501_chatcmpl-41cbd217-37d9-43bb-b420-bc06470ecc78...\n",
      "Fetching observation data for time-03-47-27-837979_chatcmpl-b638680d-4122-45ea-86cb-107c5ffb995f...\n",
      "Fetching observation data for time-03-47-42-193154_chatcmpl-d8d44aae-47ad-4b85-b925-27ab3b0efe57...\n",
      "Fetching observation data for time-03-47-55-421168_chatcmpl-6ba905dc-2d3c-4584-a688-afd2ec67612b...\n",
      "Fetching observation data for time-03-48-09-136949_chatcmpl-1bc405d6-6241-4669-a811-4c413ce026ac...\n",
      "Fetching observation data for 591a6e0b-1f2f-4e48-a528-85d7e203e352...\n",
      "Fetching observation data for time-03-46-00-686921_chatcmpl-f15c09b5-b494-4c4b-a844-365aef57a29e...\n",
      "Fetching observation data for time-03-46-10-541245_chatcmpl-d7b9ed39-4f85-4a3f-9972-84a54ff1f8fb...\n",
      "Fetching observation data for time-03-46-21-985186_chatcmpl-6e7c0ebf-435f-4195-a6fd-35cb6b429340...\n",
      "Fetching observation data for time-03-46-33-440185_chatcmpl-b69f65c6-e97c-422a-845a-67ab8f271c5c...\n",
      "Fetching observation data for time-03-46-47-939346_chatcmpl-5df3ccc6-50b5-431c-be61-5d4eba56bad2...\n",
      "Fetching observation data for time-03-44-58-959213_chatcmpl-1f85f719-3c38-438c-aa12-fa9963ba5292...\n",
      "Fetching observation data for time-03-45-09-967041_chatcmpl-66617d12-164d-4378-a5e1-2544957e079f...\n",
      "Fetching observation data for time-03-44-01-063511_chatcmpl-a2fc76f6-f563-45ab-b034-0fb12758b936...\n",
      "Fetching observation data for time-03-44-12-410981_chatcmpl-95351b57-4b20-4a8d-9c74-2ba5131f8ead...\n",
      "Fetching observation data for time-03-44-28-953725_chatcmpl-8e6ed267-1b73-4854-8a65-2640c5f6dde4...\n",
      "Fetching observation data for time-03-42-52-999848_chatcmpl-9123e2c0-7704-4fb8-b952-e8f203736dcd...\n",
      "Fetching observation data for time-03-43-04-168360_chatcmpl-cb6f5fb3-15ea-4c6a-a724-558d9a45b79a...\n",
      "Fetching observation data for time-03-43-16-194699_chatcmpl-a588c8b8-1d6f-499f-a00b-5d25666e9a04...\n",
      "Fetching observation data for time-03-43-28-912544_chatcmpl-c074a347-907d-4d0a-808b-4713c937f93c...\n",
      "Fetching observation data for time-03-43-41-845271_chatcmpl-a62c48db-7c7d-49ae-be6a-72287ffae431...\n",
      "Fetching observation data for 17d14e93-7309-4611-b670-f3458e1af6cf...\n",
      "Fetching observation data for time-03-41-40-716664_chatcmpl-dbd11a45-e9b6-42ed-85bf-256668b07302...\n",
      "Fetching observation data for time-03-41-51-416317_chatcmpl-4bdfa126-5036-48c9-84b0-3a786462ffae...\n",
      "Fetching observation data for time-03-42-04-701984_chatcmpl-029a6023-cdf0-4a0d-a6c2-563e6fcaa179...\n",
      "Fetching observation data for time-03-42-18-374514_chatcmpl-66f7a424-ebf6-494d-9a43-2c2f9038cfc2...\n",
      "Fetching observation data for time-03-42-32-135051_chatcmpl-3b2a3eaa-84c1-4489-9f45-93eb3275806f...\n",
      "Fetching observation data for d4570b9b-5421-48cd-8e5b-31c6dc4e449d...\n",
      "Fetching observation data for time-03-40-36-028235_chatcmpl-8ec73979-c8f8-410f-a9c8-146d87f21c65...\n",
      "Fetching observation data for time-03-40-47-491942_chatcmpl-8652104e-3f7b-4f24-9ab3-10363ef71cd8...\n",
      "Fetching observation data for time-03-39-16-195335_chatcmpl-a3b1ddc6-3337-46e9-938a-96a785ad3883...\n",
      "Fetching observation data for time-03-39-26-895538_chatcmpl-10ac105e-406d-433e-a548-378f18d4de71...\n",
      "Fetching observation data for time-03-39-43-052490_chatcmpl-3fdb1934-7f6f-4e0e-ab42-9c851b64f416...\n",
      "Fetching observation data for time-03-39-59-883486_chatcmpl-bc1b61c6-5555-43e9-b6e5-198467c33b50...\n",
      "Fetching observation data for time-03-40-17-842106_chatcmpl-ddd07671-074c-4d88-8e83-17523e8a2fe2...\n",
      "Fetching observation data for 51ba08ab-6d38-43d7-b325-9f36daec50b5...\n",
      "Fetching observation data for time-03-38-04-671325_chatcmpl-0cb67384-2d8c-46c2-bee4-96fdb167d5c8...\n",
      "Fetching observation data for time-03-38-15-554716_chatcmpl-d943fab1-bf1c-4542-807f-1a3e46631b56...\n",
      "Fetching observation data for time-03-38-28-065056_chatcmpl-89df0d1d-dafc-484b-bf9b-4811783a00bf...\n",
      "Fetching observation data for time-03-38-40-288252_chatcmpl-763ec514-94a1-48db-81bf-7d7bcba308e2...\n",
      "Fetching observation data for time-03-38-56-408879_chatcmpl-c9152459-70f4-49ad-ac31-41706120f1c6...\n",
      "Fetching observation data for 33314a86-29a2-41ba-8fb4-b4a0806a8847...\n",
      "Fetching observation data for time-03-36-35-137244_chatcmpl-1ba0862e-d3e6-460b-95fc-44cb227bed92...\n",
      "Fetching observation data for time-03-36-45-611084_chatcmpl-6e46fd44-43d8-4510-880d-72437d513ae7...\n",
      "Fetching observation data for time-03-37-00-113492_chatcmpl-bd94cc5f-eceb-4b0d-a523-68605ad468be...\n",
      "Fetching observation data for time-03-37-13-307311_chatcmpl-8d5b6e55-9edd-4d9f-80d2-7547ae2b0a76...\n",
      "Fetching observation data for time-03-35-15-612009_chatcmpl-5d18c950-3fab-467d-9374-1f3615c87a19...\n",
      "Fetching observation data for time-03-35-26-304284_chatcmpl-d021af17-506a-443a-9a6e-f2d03a9b6243...\n",
      "Fetching observation data for time-03-35-41-522229_chatcmpl-6f761b8b-0d72-4977-82d4-990350c17075...\n",
      "Fetching observation data for time-03-33-45-087115_chatcmpl-5cc37c76-3c0d-4a62-9f4b-211c8b587e43...\n",
      "Fetching observation data for time-03-33-55-348056_chatcmpl-d223c4db-58d4-4ad4-ab05-0eba6f03ae92...\n",
      "Fetching observation data for time-03-34-09-440320_chatcmpl-f7af7310-4900-4cc6-9c3f-fbaf4c458f88...\n",
      "Fetching observation data for time-03-34-24-900072_chatcmpl-83db0686-fcd2-4dcb-a828-54c89bacad31...\n",
      "Fetching observation data for time-03-32-19-936209_chatcmpl-ef4041bf-bb3d-48a5-aa01-0f1ab6309232...\n",
      "Fetching observation data for time-03-32-30-641060_chatcmpl-f24c7afd-4f22-4b7b-8b83-ee0d0d7d0afb...\n",
      "Fetching observation data for time-03-32-45-352246_chatcmpl-90190ada-379a-4bd2-9ecc-3164920acc3e...\n",
      "Fetching observation data for time-03-33-03-757622_chatcmpl-a0769538-ba17-45ef-996c-c44c3fd3c18d...\n",
      "Fetching observation data for time-03-33-20-620439_chatcmpl-797107fa-1447-4a9a-be8e-f1cf4ed49ac4...\n",
      "Fetching observation data for b3e74950-8e9b-4a1a-b70c-bc2189356ee6...\n",
      "Fetching observation data for time-03-31-38-415846_chatcmpl-640c619f-a56c-441d-8963-4cc66bb1b701...\n",
      "Fetching observation data for time-03-31-51-993354_chatcmpl-09747a47-1c4c-46b0-a003-cc4b355d9dea...\n",
      "Fetching observation data for time-03-30-42-916004_chatcmpl-b680a2f6-4dc6-4e1e-8247-2687e3129d3c...\n",
      "Fetching observation data for time-03-30-53-632066_chatcmpl-473dc724-f237-45ae-9e84-959e98b5ba4d...\n",
      "Fetching observation data for time-03-31-09-282506_chatcmpl-336952db-5c8e-443a-991f-1d81fd35ad36...\n",
      "Fetching observation data for time-03-29-42-414713_chatcmpl-29d47163-da3c-45dc-b015-4d075f8e406c...\n",
      "Fetching observation data for time-03-29-53-070118_chatcmpl-28d8dceb-6420-4743-9fb7-08e1e91baacf...\n",
      "Fetching observation data for time-03-30-13-173324_chatcmpl-fb9f3b6e-3371-4b53-a085-a01db0aca0fc...\n",
      "Fetching observation data for time-03-28-53-898884_chatcmpl-e6328028-c91a-4b7f-8726-02c469747ece...\n",
      "Fetching observation data for time-03-27-46-294306_chatcmpl-eb8b2131-76e0-4d4c-af38-12acf256e1aa...\n",
      "Fetching observation data for time-03-27-59-161744_chatcmpl-e842a86d-7322-4979-a481-bf48aeac15fe...\n",
      "Raw JSON saved to: /home/han/Projects/reference-benchmark-tinyml_llm/data_analysis/2025/08.22/raw_export/raw_qwen2.5-coder:14b_c83f_psg_batch.json\n"
     ]
    }
   ],
   "source": [
    "# ALTERNATIVE TO 2.\n",
    "import os\n",
    "import json\n",
    "from time import sleep\n",
    "from langfuse import Langfuse\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "\n",
    "# LANGFUSE_SERVICE_PUBLIC_KEY = \"pk-lf-559a2c0f-ee29-4c32-944c-bf73b5f0ce28\"\n",
    "# LANGFUSE_SERVICE_SECRET_KEY = \"sk-lf-75f8bf7f-a5db-4756-b0dd-d758a2a292c8\"\n",
    "# LANGFUSE_SERVICE_HOST = \"https://langfuse.hann.fi\"\n",
    "\n",
    "\n",
    "if LOCAL_HOST:\n",
    "    langfuse = Langfuse(\n",
    "        secret_key=\"sk-lf-75f8bf7f-a5db-4756-b0dd-d758a2a292c8\",\n",
    "        public_key=\"pk-lf-559a2c0f-ee29-4c32-944c-bf73b5f0ce28\",\n",
    "        host=\"https://langfuse.hann.fi\",\n",
    "    )\n",
    "else:\n",
    "    langfuse = Langfuse(\n",
    "        secret_key=LANGFUSE_SERVICE_SECRET_KEY,\n",
    "        public_key=LANGFUSE_SERVICE_PUBLIC_KEY,\n",
    "        host=LANGFUSE_SERVICE_HOST,\n",
    "    )\n",
    "\n",
    "API_invok_count = 0\n",
    "query_range_num_run = {\"start\": 0, \"end\": 1}\n",
    "\n",
    "\n",
    "class CustomJSONEncoder(json.JSONEncoder):\n",
    "    def __init__(self, *args, LOCAL_HOST=True, **kwargs):\n",
    "        self.LOCAL_HOST = LOCAL_HOST\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "    def default(self, obj):\n",
    "        if isinstance(obj, datetime):\n",
    "            return obj.isoformat()\n",
    "        if hasattr(obj, \"__dict__\"):\n",
    "            data = obj.__dict__.copy()\n",
    "            if \"observations\" in data:\n",
    "                data[\"observations\"] = [\n",
    "                    fetch_observation_data(obs, self.LOCAL_HOST)\n",
    "                    for obs in data[\"observations\"]\n",
    "                ]\n",
    "\n",
    "            return data\n",
    "        return super().default(obj)\n",
    "\n",
    "\n",
    "def fetch_observation_data(observation_id, LOCAL_HOST):\n",
    "    \"\"\"\n",
    "    Fetches observation data from Langfuse and returns its dictionary representation.\n",
    "    \"\"\"\n",
    "    print(f\"Fetching observation data for {observation_id}...\")\n",
    "    global API_invok_count\n",
    "    if API_invok_count >= 0 and not LOCAL_HOST:\n",
    "        print(\"Waiting for 3 seconds to fetch observation data...\")\n",
    "        for _ in tqdm(range(3), desc=\"Progress\", unit=\"s\"):\n",
    "            sleep(1)\n",
    "        API_invok_count = 0\n",
    "\n",
    "    observation_response = langfuse.fetch_observation(observation_id)\n",
    "    API_invok_count += 1\n",
    "\n",
    "    return observation_response.data.dict()\n",
    "\n",
    "\n",
    "def fetch_and_save_complete_data(session_id_list, raw_export_dir, LOCAL_HOST):\n",
    "    \"\"\"\n",
    "    Fetches complete trace data for each session ID and saves it to JSON files.\n",
    "\n",
    "    Parameters:\n",
    "        session_id_list (list): List of session IDs to process.\n",
    "        raw_export_dir (str): Directory path to save raw JSON files.\n",
    "    \"\"\"\n",
    "\n",
    "    def save_complete_data(session_id):\n",
    "        global API_invok_count\n",
    "        if API_invok_count >= 0 and not LOCAL_HOST:\n",
    "            print(\"Waiting for 4 seconds to fetch traces...\")\n",
    "            for _ in tqdm(range(4), desc=\"Progress\", unit=\"s\"):\n",
    "                sleep(1)\n",
    "            API_invok_count = 0\n",
    "\n",
    "        fetch_traces_response = langfuse.fetch_traces(session_id=session_id)\n",
    "        API_invok_count += 1\n",
    "\n",
    "        print(f\"Fetching traces for session {session_id}...\")\n",
    "        # Create directories if they don't exist\n",
    "        os.makedirs(raw_export_dir, exist_ok=True)\n",
    "\n",
    "        # Save complete data to JSON file\n",
    "        # if session_id.startswith(\"da0a\"):\n",
    "        #     session_id = \"phi4_\" + session_id\n",
    "        if \"tpsg\" in session_id:\n",
    "            session_id_ = session_id.replace(\"tpsg\", \"tpusg\")\n",
    "        else:\n",
    "            session_id_ = session_id\n",
    "            \n",
    "        raw_path = os.path.join(raw_export_dir, f\"raw_{session_id_}.json\")\n",
    "        with open(raw_path, \"w\") as f:\n",
    "            json.dump(fetch_traces_response, f, cls=CustomJSONEncoder, indent=2)\n",
    "\n",
    "        print(f\"Raw JSON saved to: {raw_path}\")\n",
    "\n",
    "    for session_id in session_id_list:\n",
    "        save_complete_data(session_id)\n",
    "\n",
    "\n",
    "fetch_and_save_complete_data(session_id_list, raw_export_dir, LOCAL_HOST)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Trim data\n",
    "\n",
    "Here also intercept the runs with fatal errors that need to be excluded from the analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPAN error_c8_tpusg_failure_signal_tpu_sketch_generator: Failed: Max retries reached with error. Last error: Traceback (most recent call last):\n",
      "  File \"script_a5bf77e9_1755647879.py\", line 35, in <module>\n",
      "    input_shape = input_details[0]['shape']\n",
      "TypeError: string indices must be integers.\n",
      "SPAN error_b3_tpusg_failure_signal_tpu_sketch_generator: Failed: Max retries reached with error. Last error: Traceback (most recent call last):\n",
      "  File \"script_77017dc4_1755646988.py\", line 73, in <module>\n",
      "    if scores[i] > confidence_threshold:\n",
      "ValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all().\n",
      "Successfully processed and saved trimmed data for session qwen2.5-coder:14b_c83f_tpusg_batch\n",
      "SPAN error_0c_psg_failure_signal_py_sketch_generator: Failed. Last error: Max retries reached with failure. Last error from execution: INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/wuguangh/Projects/tinyml-autopilot/results/object_detection/sketches/tmp_20250820035139_psg_qwen2.5-coder:14b/tmp_20250820035139_psg_qwen2.5-coder:14b.py\", line 61, in <module>\n",
      "    if scores[i] > confidence_threshold:\n",
      "ValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\n",
      "\n",
      "SPAN error_c7_psg_failure_signal_py_sketch_generator: Failed. Last error: Max retries reached with failure. Last error from execution: INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/wuguangh/Projects/tinyml-autopilot/results/object_detection/sketches/tmp_20250820034821_psg_qwen2.5-coder:14b/tmp_20250820034821_psg_qwen2.5-coder:14b.py\", line 54, in <module>\n",
      "    if scores[i] > confidence_threshold:\n",
      "ValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\n",
      "\n",
      "SPAN error_1f_psg_failure_signal_py_sketch_generator: Failed. Last error: Max retries reached with failure. Last error from execution: INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/wuguangh/Projects/tinyml-autopilot/results/object_detection/sketches/tmp_20250820034354_psg_qwen2.5-coder:14b/tmp_20250820034354_psg_qwen2.5-coder:14b.py\", line 53, in <module>\n",
      "    if scores[i] > confidence_threshold:\n",
      "ValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\n",
      "\n",
      "SPAN error_89_psg_failure_signal_py_sketch_generator: Failed. Last error: Max retries reached with failure. Last error from execution: INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/wuguangh/Projects/tinyml-autopilot/results/object_detection/sketches/tmp_20250820034245_psg_qwen2.5-coder:14b/tmp_20250820034245_psg_qwen2.5-coder:14b.py\", line 52, in <module>\n",
      "    if scores[i] > confidence_threshold:\n",
      "ValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\n",
      "\n",
      "SPAN error_8f_psg_failure_signal_py_sketch_generator: Failed. Last error: Max retries reached with failure. Last error from execution:   File \"/home/wuguangh/Projects/tinyml-autopilot/results/object_detection/sketches/tmp_20250820034028_psg_qwen2.5-coder:14b/tmp_20250820034028_psg_qwen2.5-coder:14b.py\", line 69\n",
      "    <REPORT THE FIX OF THE LAST ERROR>\n",
      "    ^\n",
      "SyntaxError: invalid syntax\n",
      "\n",
      "SPAN error_08_psg_failure_signal_py_sketch_generator: Failed. Last error: Max retries reached with failure. Last error from execution: INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/wuguangh/Projects/tinyml-autopilot/results/object_detection/sketches/tmp_20250820033909_psg_qwen2.5-coder:14b/tmp_20250820033909_psg_qwen2.5-coder:14b.py\", line 41, in <module>\n",
      "    interpreter.set_tensor(input_details[0]['index'], input_data)\n",
      "  File \"/home/wuguangh/.conda/envs/tinyml/lib/python3.10/site-packages/tflite_runtime/interpreter.py\", line 720, in set_tensor\n",
      "    self._interpreter.SetTensor(tensor_index, value)\n",
      "ValueError: Cannot set tensor: Got value of type FLOAT32 but expected type UINT8 for input 175, name: normalized_input_image_tensor \n",
      "\n",
      "SPAN error_c5_psg_failure_signal_py_sketch_generator: Failed. Last error: Max retries reached with failure. Last error from execution: 2025-08-20 03:33:36.045715: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-08-20 03:33:36.050204: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-08-20 03:33:36.064201: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-08-20 03:33:36.085681: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-08-20 03:33:36.091991: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-08-20 03:33:36.107768: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-08-20 03:33:36.966208: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/wuguangh/Projects/tinyml-autopilot/results/object_detection/sketches/tmp_20250820033335_psg_qwen2.5-coder:14b/tmp_20250820033335_psg_qwen2.5-coder:14b.py\", line 48, in <module>\n",
      "    num_detections = int(interpreter.get_tensor(output_details[0]['index'])[0])  # Access the first element\n",
      "TypeError: only length-1 arrays can be converted to Python scalars\n",
      "\n",
      "Successfully processed and saved trimmed data for session qwen2.5-coder:14b_c83f_psg_batch\n",
      "Total 0 traces skipped. They are []\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "skipped_traces = []\n",
    "\n",
    "\n",
    "def process_existing_observation(observation):\n",
    "    \"\"\"\n",
    "    Processes an existing observation dictionary by trimming unwanted keys.\n",
    "    \"\"\"\n",
    "    unwanted_observation_keys = [\n",
    "        \"completionStartTime\",\n",
    "        \"metadata\",\n",
    "        \"timeToFirstToken\",\n",
    "        \"createdAt\",\n",
    "        \"usageDetails\",\n",
    "        \"usage\",\n",
    "        \"projectId\",\n",
    "        \"unit\",\n",
    "        \"updatedAt\",\n",
    "        \"version\",\n",
    "        \"parentObservationId\",\n",
    "        \"promptId\",\n",
    "        \"promptName\",\n",
    "        \"promptVersion\",\n",
    "        \"modelId\",\n",
    "        \"inputPrice\",\n",
    "        \"outputPrice\",\n",
    "        \"totalPrice\",\n",
    "        # \"modelParameters\",\n",
    "        \"input\",\n",
    "        \"output\",\n",
    "    ]\n",
    "\n",
    "    # If observation is a dictionary containing observation data\n",
    "    if isinstance(observation, dict):\n",
    "        trimmed_observation = {\n",
    "            k: v for k, v in observation.items() if k not in unwanted_observation_keys\n",
    "        }\n",
    "        return trimmed_observation\n",
    "    return observation\n",
    "\n",
    "\n",
    "def trim_data(data):\n",
    "    \"\"\"\n",
    "    Recursively trims the data structure.\n",
    "    \"\"\"\n",
    "\n",
    "    if isinstance(data, dict):\n",
    "        # Process the current dictionary\n",
    "        unwanted_trace_keys = [\n",
    "            \"release\",\n",
    "            \"version\",\n",
    "            \"user_id\",\n",
    "            \"public\",\n",
    "            \"html_path\",\n",
    "            \"scores\",\n",
    "            \"bookmarked\",\n",
    "            \"projectId\",\n",
    "            \"externalId\",\n",
    "            \"page\",\n",
    "            \"limit\",\n",
    "            \"total_pages\",\n",
    "        ]\n",
    "\n",
    "        # If this is a trace that contains observations, check for fatal errors\n",
    "        if \"observations\" in data:\n",
    "            # Check for SPAN observations with fatal errors before processing\n",
    "            skip_trace = False\n",
    "            for obs in data[\"observations\"]:\n",
    "                if isinstance(obs, dict) and obs.get(\"name\").startswith(\"error\"):\n",
    "                    status_message = obs.get(\"statusMessage\", \"\")\n",
    "                    ob_name = obs.get(\"name\")\n",
    "                    print(f\"SPAN {ob_name}: {status_message}\")\n",
    "\n",
    "                    if \"Fatal error\" in status_message:\n",
    "                        print(f\"Found Fatal error in SPAN observation, skipping trace\")\n",
    "                        skip_trace = True\n",
    "                        skipped_traces.append(data[\"name\"])\n",
    "                        break\n",
    "\n",
    "            if skip_trace:\n",
    "                return None  # Signal to skip this trace\n",
    "\n",
    "        # Create a new dictionary with wanted keys and recursively process values\n",
    "        trimmed_data = {}\n",
    "        for key, value in data.items():\n",
    "            if key not in unwanted_trace_keys:\n",
    "                if key == \"observations\":\n",
    "                    # Special handling for observations\n",
    "                    trimmed_data[key] = [\n",
    "                        process_existing_observation(obs) for obs in value\n",
    "                    ]\n",
    "                elif isinstance(value, (dict, list)):\n",
    "                    # Recursively process nested structures\n",
    "                    trimmed_data[key] = trim_data(value)\n",
    "                else:\n",
    "                    trimmed_data[key] = value\n",
    "\n",
    "        return trimmed_data\n",
    "\n",
    "    elif isinstance(data, list):\n",
    "        # Recursively process each item in the list\n",
    "        processed_items = []\n",
    "        for item in data:\n",
    "            processed_item = trim_data(item)\n",
    "            if processed_item is not None:  # Only add items that weren't filtered out\n",
    "                processed_items.append(processed_item)\n",
    "        return processed_items\n",
    "\n",
    "    else:\n",
    "        # Return non-dict, non-list values as is\n",
    "        return data\n",
    "\n",
    "\n",
    "def read_and_trim_data(session_id_list, raw_export_dir, trimmed_export_dir):\n",
    "    \"\"\"\n",
    "    Reads complete data from JSON files, trims the data, and saves the trimmed data to new JSON files.\n",
    "    \"\"\"\n",
    "    os.makedirs(trimmed_export_dir, exist_ok=True)\n",
    "\n",
    "    for session_id in session_id_list:\n",
    "        try:\n",
    "            if session_id.startswith(\"da0a\"):\n",
    "                session_id = \"phi4_\" + session_id\n",
    "            # Read raw data\n",
    "            if \"tpsg\" in session_id:\n",
    "                session_id_ = session_id.replace(\"tpsg\", \"tpusg\")\n",
    "            else:\n",
    "                session_id_ = session_id\n",
    "            raw_path = os.path.join(raw_export_dir, f\"raw_{session_id_}.json\")\n",
    "            with open(raw_path, \"r\") as f:\n",
    "                data = json.load(f)\n",
    "\n",
    "            # Process and trim the data\n",
    "            trimmed_data = trim_data(data)\n",
    "\n",
    "            # If the entire data was filtered out (unlikely but possible)\n",
    "            if trimmed_data is None:\n",
    "                print(\n",
    "                    f\"All traces in session {session_id} were filtered due to fatal errors\"\n",
    "                )\n",
    "                continue\n",
    "\n",
    "            # Save trimmed data\n",
    "            if \"tpsg\" in session_id:\n",
    "                session_id_ = session_id.replace(\"tpsg\", \"tpusg\")\n",
    "            else:\n",
    "                session_id_ = session_id\n",
    "            trimmed_path = os.path.join(\n",
    "                trimmed_export_dir, f\"trimmed_{session_id_}.json\"\n",
    "            )\n",
    "            with open(trimmed_path, \"w\") as f:\n",
    "                json.dump(trimmed_data, f, indent=2)\n",
    "\n",
    "            print(\n",
    "                f\"Successfully processed and saved trimmed data for session {session_id}\"\n",
    "            )\n",
    "\n",
    "            # Optional: Verify trimming worked\n",
    "            # print(f\"Verifying trimmed data for session {session_id}...\")\n",
    "            # verify_trimming(trimmed_path)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing session {session_id}: {str(e)}\")\n",
    "\n",
    "\n",
    "def verify_trimming(trimmed_path):\n",
    "    \"\"\"\n",
    "    Verifies that the trimmed data doesn't contain unwanted keys.\n",
    "    \"\"\"\n",
    "    with open(trimmed_path, \"r\") as f:\n",
    "        trimmed_data = json.load(f)\n",
    "\n",
    "    unwanted_keys = [\n",
    "        \"release\",\n",
    "        \"version\",\n",
    "        \"user_id\",\n",
    "        \"public\",\n",
    "        \"html_path\",\n",
    "        \"scores\",\n",
    "        \"bookmarked\",\n",
    "        \"projectId\",\n",
    "        \"externalId\",\n",
    "        \"page\",\n",
    "        \"limit\",\n",
    "        \"total_pages\",\n",
    "        \"completionStartTime\",\n",
    "        \"metadata\",\n",
    "        \"usageDetails\",\n",
    "        \"timeToFirstToken\",\n",
    "        \"createdAt\",\n",
    "        \"completionTokens\",\n",
    "        \"promptTokens\",\n",
    "        \"projectId\",\n",
    "        \"unit\",\n",
    "        \"updatedAt\",\n",
    "        \"version\",\n",
    "        # \"statusMessage\",\n",
    "        \"parentObservationId\",\n",
    "        \"promptId\",\n",
    "        \"promptName\",\n",
    "        \"promptVersion\",\n",
    "        \"modelId\",\n",
    "        \"inputPrice\",\n",
    "        \"outputPrice\",\n",
    "        \"totalPrice\",\n",
    "        \"calculatedInputCost\",\n",
    "        \"calculatedOutputCost\",\n",
    "        \"calculatedTotalCost\",\n",
    "    ]\n",
    "\n",
    "    def check_keys(obj):\n",
    "        if isinstance(obj, dict):\n",
    "            for key in obj.keys():\n",
    "                if key in unwanted_keys:\n",
    "                    print(f\"Warning: Found unwanted key '{key}' in trimmed data\")\n",
    "            for value in obj.values():\n",
    "                check_keys(value)\n",
    "        elif isinstance(obj, list):\n",
    "            for item in obj:\n",
    "                check_keys(item)\n",
    "\n",
    "    check_keys(trimmed_data)\n",
    "    print(\"Verification complete\")\n",
    "\n",
    "\n",
    "# Usage example:\n",
    "read_and_trim_data(session_id_list, raw_export_dir, raw_export_dir)\n",
    "print(f\"Total {len(skipped_traces)} traces skipped. They are {skipped_traces}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Generate CSV files from JSON\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing session qwen2.5-coder:14b_c83f_tpusg_batch, simple id qwen2.5-coder:14b_c83f. Look for /home/han/Projects/reference-benchmark-tinyml_llm/data_analysis/2025/08.22/raw_export/trimmed_qwen2.5-coder:14b_c83f_tpusg_batch.json\n",
      "/home/han/Projects/reference-benchmark-tinyml_llm/data_analysis/2025/08.22/processed_data/qwen2.5-coder:14b_c83f/clean_qwen2.5-coder:14b_c83f_tpusg_batch.csv\n",
      "Successfully saved CSV to: /home/han/Projects/reference-benchmark-tinyml_llm/data_analysis/2025/08.22/processed_data/qwen2.5-coder:14b_c83f/clean_qwen2.5-coder:14b_c83f_tpusg_batch.csv\n",
      "Processing session qwen2.5-coder:14b_c83f_psg_batch, simple id qwen2.5-coder:14b_c83f. Look for /home/han/Projects/reference-benchmark-tinyml_llm/data_analysis/2025/08.22/raw_export/trimmed_qwen2.5-coder:14b_c83f_psg_batch.json\n",
      "/home/han/Projects/reference-benchmark-tinyml_llm/data_analysis/2025/08.22/processed_data/qwen2.5-coder:14b_c83f/clean_qwen2.5-coder:14b_c83f_psg_batch.csv\n",
      "Successfully saved CSV to: /home/han/Projects/reference-benchmark-tinyml_llm/data_analysis/2025/08.22/processed_data/qwen2.5-coder:14b_c83f/clean_qwen2.5-coder:14b_c83f_psg_batch.csv\n"
     ]
    }
   ],
   "source": [
    "import traceback\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from dateutil import parser\n",
    "\n",
    "\n",
    "def json_to_csv(session_id):\n",
    "    \"\"\"\n",
    "    Convert JSON trace data to CSV format with aggregated metrics.\n",
    "\n",
    "    Args:\n",
    "        session_id (str): Identifier for the session to process\n",
    "    \"\"\"\n",
    "\n",
    "    def extract_observation_details(observations, trace_id):\n",
    "        \"\"\"Extract and aggregate metrics from observations\"\"\"\n",
    "        metrics = {\n",
    "            \"status\": None,\n",
    "            \"latency\": 0,\n",
    "            \"total_tokens\": 0,\n",
    "            \"prompt_tokens\": 0,\n",
    "            \"completion_tokens\": 0,\n",
    "            \"total_cost\": 0,\n",
    "            \"input_cost\": 0,\n",
    "            \"output_cost\": 0,\n",
    "            \"parameters\": set(),\n",
    "        }\n",
    "\n",
    "        # Process GENERATION observations\n",
    "        for obs in (o for o in observations if o[\"type\"] == \"GENERATION\"):\n",
    "            metrics[\"total_tokens\"] += obs[\"totalTokens\"]\n",
    "            metrics[\"prompt_tokens\"] += obs[\"promptTokens\"]\n",
    "            metrics[\"completion_tokens\"] += obs[\"completionTokens\"]\n",
    "            metrics[\"latency\"] += obs[\"latency\"]\n",
    "            for key, value in obs[\"modelParameters\"].items():\n",
    "                metrics[\"parameters\"].add(key + \":\" + value)\n",
    "\n",
    "            # Add costs if present\n",
    "            for cost_type in [\"Total\", \"Input\", \"Output\"]:\n",
    "                key = f\"calculated{cost_type}Cost\"\n",
    "                metric_key = cost_type.lower() + \"_cost\"\n",
    "                if obs.get(key) is not None:\n",
    "                    metrics[metric_key] += obs[key]\n",
    "        if len(metrics[\"parameters\"]) == 0:\n",
    "            metrics[\"parameters\"] = \"N/A\"\n",
    "        # Process SPAN observations for status\n",
    "        status_indicators = [\n",
    "            obs[\"name\"]\n",
    "            for obs in observations\n",
    "            if obs[\"type\"] == \"SPAN\" and \"start_\" not in obs[\"name\"]\n",
    "        ]\n",
    "\n",
    "        #  if later than 2025-05-19, use status_signal_from_output\n",
    "        if datetime.now() > datetime(2025, 5, 19):\n",
    "            pass\n",
    "        else:\n",
    "            # Determine status\n",
    "            success_signals = sum(\"end_\" in name for name in status_indicators)\n",
    "            failure_signals = sum(\n",
    "                \"failure_signal\" in name for name in status_indicators\n",
    "            )\n",
    "\n",
    "            if success_signals + failure_signals > 1:\n",
    "                raise ValueError(\n",
    "                    f\"Multiple status indicators found in trace {trace_id}\"\n",
    "                )\n",
    "\n",
    "            metrics[\"status\"] = (\n",
    "                \"success\"\n",
    "                if success_signals\n",
    "                else \"failure\" if failure_signals else \"unknown\"\n",
    "            )\n",
    "\n",
    "        metrics[\"prompt_cost\"] = metrics.pop(\"input_cost\")\n",
    "        metrics[\"completion_cost\"] = metrics.pop(\"output_cost\")\n",
    "        metrics[\"latency\"] = round(metrics[\"latency\"] / 1000, 2)\n",
    "        return metrics\n",
    "\n",
    "    def cal_time(trace):\n",
    "        time_diff = datetime.fromisoformat(\n",
    "            trace[\"updatedAt\"].replace(\"Z\", \"+00:00\")\n",
    "        ) - datetime.fromisoformat(trace[\"createdAt\"].replace(\"Z\", \"+00:00\"))\n",
    "        seconds_diff = time_diff.total_seconds()\n",
    "        return seconds_diff\n",
    "\n",
    "    try:\n",
    "\n",
    "        if session_id.startswith(\"da0a\"):\n",
    "            session_id = \"phi4_\" + session_id\n",
    "        simple_session_id = session_id.rsplit(\"_\", 2)[0]\n",
    "\n",
    "        \n",
    "        # Load JSON data\n",
    "        if \"tpsg\" in session_id:\n",
    "                session_id_ = session_id.replace(\"tpsg\", \"tpusg\")\n",
    "        else:\n",
    "                session_id_ = session_id\n",
    "        trimmed_path = os.path.join(raw_export_dir, f\"trimmed_{session_id_}.json\")\n",
    "        print(\n",
    "            f\"Processing session {session_id}, simple id {simple_session_id}. Look for {trimmed_path}\"\n",
    "        )\n",
    "        with open(trimmed_path, \"r\") as file:\n",
    "            traces = json.load(file)[\"data\"]\n",
    "\n",
    "        # Process traces\n",
    "        rows = [\n",
    "            {\n",
    "                \"num_run\": trace[\"metadata\"][\"num_run\"],\n",
    "                \"name\": trace[\"name\"],\n",
    "                \"trace_id\": trace[\"id\"],\n",
    "                \"batch_id\": trace[\"session_id\"],\n",
    "                # \"latency\": cal_time(trace),\n",
    "                # \"latency\": round(trace[\"latency\"], 2),\n",
    "                **extract_observation_details(\n",
    "                    trace[\"observations\"],\n",
    "                    trace[\"id\"],\n",
    "                ),\n",
    "                \"status\": (\n",
    "                    \"failure\"\n",
    "                    if trace[\"output\"][\"status\"].lower() == \"failed\"\n",
    "                    else \"success\"\n",
    "                ),\n",
    "                \"tags\": trace[\"tags\"],\n",
    "                \"timestamp\": int(parser.isoparse(trace[\"timestamp\"]).timestamp()),\n",
    "            }\n",
    "            for trace in traces\n",
    "        ]\n",
    "        # print(rows)\n",
    "        # print(rows)\n",
    "        # Create and save DataFrame\n",
    "        df = pd.DataFrame(rows).sort_values(\"num_run\")\n",
    "\n",
    "        output_dir = os.path.join(processed_data_dir, f\"{simple_session_id}\")\n",
    "        if not os.path.exists(output_dir):\n",
    "            os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "        output_path = os.path.join(output_dir, f\"clean_{session_id_}.csv\")\n",
    "\n",
    "        print(output_path)\n",
    "        df.to_csv(output_path, index=False)\n",
    "        print(f\"Successfully saved CSV to: {output_path}\")\n",
    "\n",
    "    except FileNotFoundError as e:\n",
    "        print(\n",
    "            f\"FileNotFoundError: For session {session_id} not found. Looked for {trimmed_path}\\nError info: \\n{e}\\n\\nTraceback: {traceback.format_exc()}\"\n",
    "        )\n",
    "    except json.JSONDecodeError:\n",
    "        print(f\"Error: Invalid JSON format in input file for session {session_id}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing session {session_id}: {str(e)}\")\n",
    "\n",
    "\n",
    "# Example usage\n",
    "for session_id in session_id_list:\n",
    "    json_to_csv(session_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CSV Generation with Generation Counts\n",
    "\n",
    "This section creates CSV files similar to the langfuse_export section 3, but adds a column for the number of generation attempts used for each trace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing sessions: ['qwen2.5-coder:14b_c83f_tpusg_batch', 'qwen2.5-coder:14b_c83f_psg_batch']\n",
      "Looking for raw files in: /home/han/Projects/reference-benchmark-tinyml_llm/data_analysis/2025/08.22/raw_export\n",
      "Will save CSV files to: /home/han/Projects/reference-benchmark-tinyml_llm/data_analysis/2025/08.22/processed_data\n",
      "Processing session qwen2.5-coder:14b_c83f_tpusg_batch, simple id qwen2.5-coder:14b_c83f. Look for /home/han/Projects/reference-benchmark-tinyml_llm/data_analysis/2025/08.22/raw_export/trimmed_qwen2.5-coder:14b_c83f_tpusg_batch.json\n",
      "/home/han/Projects/reference-benchmark-tinyml_llm/data_analysis/2025/08.22/processed_data/qwen2.5-coder:14b_c83f/clean_qwen2.5-coder:14b_c83f_tpusg_batch.csv\n",
      "Successfully saved CSV to: /home/han/Projects/reference-benchmark-tinyml_llm/data_analysis/2025/08.22/processed_data/qwen2.5-coder:14b_c83f/clean_qwen2.5-coder:14b_c83f_tpusg_batch.csv\n",
      "Processing session qwen2.5-coder:14b_c83f_psg_batch, simple id qwen2.5-coder:14b_c83f. Look for /home/han/Projects/reference-benchmark-tinyml_llm/data_analysis/2025/08.22/raw_export/trimmed_qwen2.5-coder:14b_c83f_psg_batch.json\n",
      "/home/han/Projects/reference-benchmark-tinyml_llm/data_analysis/2025/08.22/processed_data/qwen2.5-coder:14b_c83f/clean_qwen2.5-coder:14b_c83f_psg_batch.csv\n",
      "Successfully saved CSV to: /home/han/Projects/reference-benchmark-tinyml_llm/data_analysis/2025/08.22/processed_data/qwen2.5-coder:14b_c83f/clean_qwen2.5-coder:14b_c83f_psg_batch.csv\n"
     ]
    }
   ],
   "source": [
    "import traceback\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "from datetime import datetime\n",
    "from dateutil import parser\n",
    "\n",
    "# Setup paths - same as langfuse_export\n",
    "parent_dir = os.path.dirname(os.getcwd())\n",
    "raw_export_dir = os.path.join(parent_dir, \"raw_export\")\n",
    "processed_data_dir = os.path.join(parent_dir, \"processed_data\")\n",
    "\n",
    "\n",
    "# Get session id list from data directory\n",
    "session_id_list = []\n",
    "\n",
    "for root, dirs, files in os.walk(raw_export_dir):\n",
    "    for file in files:\n",
    "        file_path = os.path.join(root, file)\n",
    "        if \"trimmed_\" in file_path:\n",
    "            session_id = file_path.split('trimmed_')[1].rstrip('.json')\n",
    "            session_id_list.append(session_id)\n",
    "\n",
    "print(f\"Processing sessions: {session_id_list}\")\n",
    "print(f\"Looking for raw files in: {raw_export_dir}\")\n",
    "print(f\"Will save CSV files to: {processed_data_dir}\")\n",
    "\n",
    "\n",
    "def json_to_csv_weighted(session_id):\n",
    "    \"\"\"\n",
    "    Convert JSON trace data to CSV format with aggregated metrics.\n",
    "    Upgraded version that includes generation_count column.\n",
    "\n",
    "    Args:\n",
    "        session_id (str): Identifier for the session to process\n",
    "    \"\"\"\n",
    "\n",
    "    def extract_observation_details(observations, trace_id):\n",
    "        \"\"\"Extract and aggregate metrics from observations\"\"\"\n",
    "        metrics = {\n",
    "            \"status\": None,\n",
    "            \"latency\": 0,\n",
    "            \"total_tokens\": 0,\n",
    "            \"prompt_tokens\": 0,\n",
    "            \"completion_tokens\": 0,\n",
    "            \"total_cost\": 0,\n",
    "            \"input_cost\": 0,\n",
    "            \"output_cost\": 0,\n",
    "            \"parameters\": set(),\n",
    "            \"generation_count\": 0,  # New field for generation count\n",
    "        }\n",
    "\n",
    "        # Count generations and process GENERATION observations\n",
    "        for obs in (o for o in observations if o[\"type\"] == \"GENERATION\"):\n",
    "            metrics[\"generation_count\"] += 1\n",
    "            metrics[\"total_tokens\"] += obs[\"totalTokens\"]\n",
    "            metrics[\"prompt_tokens\"] += obs[\"promptTokens\"]\n",
    "            metrics[\"completion_tokens\"] += obs[\"completionTokens\"]\n",
    "            metrics[\"latency\"] += obs[\"latency\"]\n",
    "            for key, value in obs[\"modelParameters\"].items():\n",
    "                metrics[\"parameters\"].add(key + \":\" + value)\n",
    "\n",
    "            # Add costs if present\n",
    "            for cost_type in [\"Total\", \"Input\", \"Output\"]:\n",
    "                key = f\"calculated{cost_type}Cost\"\n",
    "                metric_key = cost_type.lower() + \"_cost\"\n",
    "                if obs.get(key) is not None:\n",
    "                    metrics[metric_key] += obs[key]\n",
    "                    \n",
    "        if len(metrics[\"parameters\"]) == 0:\n",
    "            metrics[\"parameters\"] = \"N/A\"\n",
    "            \n",
    "        # Process SPAN observations for status\n",
    "        status_indicators = [\n",
    "            obs[\"name\"]\n",
    "            for obs in observations\n",
    "            if obs[\"type\"] == \"SPAN\" and \"start_\" not in obs[\"name\"]\n",
    "        ]\n",
    "\n",
    "        #  if later than 2025-05-19, use status_signal_from_output\n",
    "        if datetime.now() > datetime(2025, 5, 19):\n",
    "            pass\n",
    "        else:\n",
    "            # Determine status\n",
    "            success_signals = sum(\"end_\" in name for name in status_indicators)\n",
    "            failure_signals = sum(\n",
    "                \"failure_signal\" in name for name in status_indicators\n",
    "            )\n",
    "\n",
    "            if success_signals + failure_signals > 1:\n",
    "                raise ValueError(\n",
    "                    f\"Multiple status indicators found in trace {trace_id}\"\n",
    "                )\n",
    "\n",
    "            metrics[\"status\"] = (\n",
    "                \"success\"\n",
    "                if success_signals\n",
    "                else \"failure\" if failure_signals else \"unknown\"\n",
    "            )\n",
    "\n",
    "        metrics[\"prompt_cost\"] = metrics.pop(\"input_cost\")\n",
    "        metrics[\"completion_cost\"] = metrics.pop(\"output_cost\")\n",
    "        metrics[\"latency\"] = round(metrics[\"latency\"] / 1000, 2)\n",
    "        return metrics\n",
    "\n",
    "    def cal_time(trace):\n",
    "        time_diff = datetime.fromisoformat(\n",
    "            trace[\"updatedAt\"].replace(\"Z\", \"+00:00\")\n",
    "        ) - datetime.fromisoformat(trace[\"createdAt\"].replace(\"Z\", \"+00:00\"))\n",
    "        seconds_diff = time_diff.total_seconds()\n",
    "        return seconds_diff\n",
    "\n",
    "    try:\n",
    "        if session_id.startswith(\"da0a\"):\n",
    "            session_id = \"phi4_\" + session_id\n",
    "        simple_session_id = session_id.rsplit(\"_\", 2)[0]\n",
    "\n",
    "        # Load JSON data\n",
    "        if \"tpsg\" in session_id:\n",
    "            session_id_ = session_id.replace(\"tpsg\", \"tpusg\")\n",
    "        else:\n",
    "            session_id_ = session_id\n",
    "        trimmed_path = os.path.join(raw_export_dir, f\"trimmed_{session_id_}.json\")\n",
    "        print(\n",
    "            f\"Processing session {session_id}, simple id {simple_session_id}. Look for {trimmed_path}\"\n",
    "        )\n",
    "        with open(trimmed_path, \"r\") as file:\n",
    "            traces = json.load(file)[\"data\"]\n",
    "\n",
    "        # Process traces\n",
    "        rows = [\n",
    "            {\n",
    "                \"num_run\": trace[\"metadata\"][\"num_run\"],\n",
    "                \"name\": trace[\"name\"],\n",
    "                \"trace_id\": trace[\"id\"],\n",
    "                \"batch_id\": trace[\"session_id\"],\n",
    "                # \"latency\": cal_time(trace),\n",
    "                # \"latency\": round(trace[\"latency\"], 2),\n",
    "                **extract_observation_details(\n",
    "                    trace[\"observations\"],\n",
    "                    trace[\"id\"],\n",
    "                ),\n",
    "                \"status\": (\n",
    "                    \"failure\"\n",
    "                    if trace[\"output\"][\"status\"].lower() == \"failed\"\n",
    "                    else \"success\"\n",
    "                ),\n",
    "                \"tags\": trace[\"tags\"],\n",
    "                \"timestamp\": int(parser.isoparse(trace[\"timestamp\"]).timestamp()),\n",
    "            }\n",
    "            for trace in traces\n",
    "        ]\n",
    "        \n",
    "        # Create and save DataFrame\n",
    "        df = pd.DataFrame(rows).sort_values(\"num_run\")\n",
    "\n",
    "        output_dir = os.path.join(processed_data_dir, f\"{simple_session_id}\")\n",
    "        if not os.path.exists(output_dir):\n",
    "            os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "        output_path = os.path.join(output_dir, f\"clean_{session_id_}.csv\")\n",
    "\n",
    "        print(output_path)\n",
    "        df.to_csv(output_path, index=False)\n",
    "        print(f\"Successfully saved CSV to: {output_path}\")\n",
    "\n",
    "    except FileNotFoundError as e:\n",
    "        print(\n",
    "            f\"FileNotFoundError: For session {session_id} not found. Looked for {trimmed_path}\\nError info: \\n{e}\\n\\nTraceback: {traceback.format_exc()}\"\n",
    "        )\n",
    "    except json.JSONDecodeError:\n",
    "        print(f\"Error: Invalid JSON format in input file for session {session_id}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing session {session_id}: {str(e)}\")\n",
    "\n",
    "\n",
    "\n",
    "# Example usage\n",
    "for session_id in session_id_list:\n",
    "    json_to_csv_weighted(session_id)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llmdev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

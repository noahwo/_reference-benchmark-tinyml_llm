{
  "data": [
    {
      "id": "f788c78d",
      "timestamp": "2025-09-24T10:20:01.114000+00:00",
      "name": "f788_model_converter",
      "input": null,
      "output": {
        "status": "success",
        "code": "import tensorflow as tf\n\n# Load the original Keras model\noriginal_model = tf.keras.models.load_model(\"models/fruit_to_emoji/og_model/model.keras\")\n\n# Define conversion parameters for quantization\nconverter = tf.lite.TFLiteConverter.from_keras_model(original_model)\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\nconverter.target_spec.supported_types = [tf.int8]\n\n# Define the input tensor details\ndef representative_dataset_gen():\n    # Create a generator that provides representative dataset examples\n    for _ in range(100):  # Use at least one example per class of your model\n        # Get sample input data as a numpy array in a method of your choosing.\n        # Here, we assume you have a function `get_sample_input_data()` that returns a float32 tensor.\n        yield [tf.random.normal([1, *original_model.input_shape[1:]], dtype=tf.float32)]\n\nconverter.representative_dataset = representative_dataset_gen\n\n# Ensure that if any ops can't be quantized, the converter throws an error\nconverter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\nconverter.inference_input_type = tf.float32  # or tf.int8\nconverter.inference_output_type = tf.uint8\n\n# Convert the model to the TensorFlow Lite format with quantization\ntflite_model_quant = converter.convert()\n\n# Save the converted TFLite model\nwith open(\"models/fruit_to_emoji/tflite_model/model_quant.tflite\", \"wb\") as f:\n    f.write(tflite_model_quant)"
      },
      "session_id": "qwen2.5-coder:32b_7e6b_mc_batch",
      "metadata": {
        "num_run": 30
      },
      "tags": [
        "benchmark",
        "model_converter",
        "qwen2.5-coder:32b"
      ],
      "latency": 9.177,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-13-20-01-115901_chatcmpl-338776b5-6230-42eb-abdb-0bf03c9da6c3",
          "traceId": "f788c78d",
          "type": "GENERATION",
          "name": "f7_mc_conversion_code_gen",
          "startTime": "2025-09-24T10:20:01.115000+00:00",
          "endTime": "2025-09-24T10:20:10.291000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 9176.0,
          "promptTokens": 451,
          "totalTokens": 793,
          "completionTokens": 342,
          "environment": "default",
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "dc4d7c4a-b687-47fa-939b-cf40ddc561ea",
          "traceId": "f788c78d",
          "type": "SPAN",
          "name": "start_model_converter",
          "startTime": "2025-09-24T10:20:01.114000+00:00",
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "promptTokens": 0,
          "totalTokens": 0,
          "completionTokens": 0,
          "environment": "default",
          "costDetails": {},
          "endTime": null,
          "model": null,
          "modelParameters": null,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null,
          "latency": null
        }
      ],
      "createdAt": "2025-09-24T10:20:01.633Z",
      "environment": "default",
      "updatedAt": "2025-09-24T10:20:19.227Z"
    },
    {
      "id": "39ad1e32",
      "timestamp": "2025-09-24T10:18:45.777000+00:00",
      "name": "39ad_model_converter",
      "input": null,
      "output": {
        "status": "success",
        "code": "import tensorflow as tf\nimport numpy as np\n\n# Load the original Keras model\noriginal_model = tf.keras.models.load_model(\"models/fruit_to_emoji/og_model/model.keras\")\n\n# Define the representative dataset generator function\ndef representative_dataset_gen():\n    for _ in range(100):\n        # Get sample input data as a numpy array in a method of your choosing.\n        data = np.random.rand(1, 224, 224, 3).astype(np.float32)  # Example shape and random data\n        yield [data]\n\n# Define converter\nconverter = tf.lite.TFLiteConverter.from_keras_model(original_model)\n\n# Set the optimization flag to optimize for size.\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\n\n# Define representative dataset for quantization\nconverter.representative_dataset = representative_dataset_gen\n\n# Ensure that if any ops can't be quantized, the converter throws an error\nconverter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n\n# Set the input and output tensors to int8 (APIs added in r2.3)\nconverter.inference_input_type = tf.float32  # Keep input as float32 for this example\nconverter.inference_output_type = tf.uint8\n\n# Convert the model\ntflite_model_quant = converter.convert()\n\n# Save the quantized TFLite model to disk\nwith open(\"models/fruit_to_emoji/tflite_model/model_quant.tflite\", \"wb\") as f:\n    f.write(tflite_model_quant)"
      },
      "session_id": "qwen2.5-coder:32b_7e6b_mc_batch",
      "metadata": {
        "num_run": 29
      },
      "tags": [
        "benchmark",
        "model_converter",
        "qwen2.5-coder:32b"
      ],
      "latency": 45.488,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-13-18-45-779243_chatcmpl-26f9973a-c592-4ed0-bf77-14ea23e1579d",
          "traceId": "39ad1e32",
          "type": "GENERATION",
          "name": "39_mc_conversion_code_gen",
          "startTime": "2025-09-24T10:18:45.779000+00:00",
          "endTime": "2025-09-24T10:18:58.176000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 12397.0,
          "promptTokens": 451,
          "totalTokens": 788,
          "completionTokens": 337,
          "environment": "default",
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-13-19-01-500351_chatcmpl-e551052d-8cde-4fe6-b9cb-8fe524536d77",
          "traceId": "39ad1e32",
          "type": "GENERATION",
          "name": "39_mc_error_handling_attempt#2",
          "startTime": "2025-09-24T10:19:01.500000+00:00",
          "endTime": "2025-09-24T10:19:21.514000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 20014.0,
          "promptTokens": 2048,
          "totalTokens": 2513,
          "completionTokens": 465,
          "environment": "default",
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-13-19-21-515969_chatcmpl-fc1fc2ae-041a-49ec-bb7a-f80cc4bf51f3",
          "traceId": "39ad1e32",
          "type": "GENERATION",
          "name": "39_mc_error_handling_attempt#3",
          "startTime": "2025-09-24T10:19:21.515000+00:00",
          "endTime": "2025-09-24T10:19:31.265000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 9750.0,
          "promptTokens": 858,
          "totalTokens": 1197,
          "completionTokens": 339,
          "environment": "default",
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "95d5f761-8a1d-4900-bf1a-e2faea4840c3",
          "traceId": "39ad1e32",
          "type": "SPAN",
          "name": "start_model_converter",
          "startTime": "2025-09-24T10:18:45.777000+00:00",
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "promptTokens": 0,
          "totalTokens": 0,
          "completionTokens": 0,
          "environment": "default",
          "costDetails": {},
          "endTime": null,
          "model": null,
          "modelParameters": null,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null,
          "latency": null
        }
      ],
      "createdAt": "2025-09-24T10:18:46.297Z",
      "environment": "default",
      "updatedAt": "2025-09-24T10:19:40.989Z"
    },
    {
      "id": "e026acf2",
      "timestamp": "2025-09-24T10:17:36.442000+00:00",
      "name": "e026_model_converter",
      "input": null,
      "output": {
        "status": "success",
        "code": "import tensorflow as tf\nimport numpy as np\n\n# Load the original model\noriginal_model = tf.keras.models.load_model(\"models/fruit_to_emoji/og_model/model.keras\")\n\n# Define quantization parameters\ninference_input_type = tf.float32\ninference_output_type = tf.uint8\nsupported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n\n# Convert the model to TFLite with 8-bit integer quantization\nconverter = tf.lite.TFLiteConverter.from_keras_model(original_model)\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\nconverter.target_spec.supported_ops = supported_ops\nconverter.inference_input_type = inference_input_type\nconverter.inference_output_type = inference_output_type\n\n# Define a generator function to provide representative dataset for calibration\ndef representative_data_gen():\n    # Load and preprocess your training data here\n    # For demonstration, let's assume we have a dataset `train_images` with shape (num_samples, height, width, channels)\n    train_images = np.random.rand(100, 64, 64, 3).astype(np.float32)  # Example random data\n    for input_value in tf.data.Dataset.from_tensor_slices(train_images).batch(1):\n        yield [input_value]\n\nconverter.representative_dataset = representative_data_gen\n\n# Ensure that if any ops can't be quantized, the converter throws an error\nconverter.target_spec.supported_types = [tf.int8]\n\n# Convert the model and save it to a file\ntflite_model_quant = converter.convert()\nwith open(\"models/fruit_to_emoji/tflite_model/model_quant.tflite\", \"wb\") as f:\n    f.write(tflite_model_quant)"
      },
      "session_id": "qwen2.5-coder:32b_7e6b_mc_batch",
      "metadata": {
        "num_run": 28
      },
      "tags": [
        "benchmark",
        "model_converter",
        "qwen2.5-coder:32b"
      ],
      "latency": 39.606,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-13-17-36-444970_chatcmpl-e90c3cdf-aab0-45b4-acb4-7ef9c523912b",
          "traceId": "e026acf2",
          "type": "GENERATION",
          "name": "e0_mc_conversion_code_gen",
          "startTime": "2025-09-24T10:17:36.444000+00:00",
          "endTime": "2025-09-24T10:17:47.743000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 11299.0,
          "promptTokens": 451,
          "totalTokens": 815,
          "completionTokens": 364,
          "environment": "default",
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-13-17-51-189266_chatcmpl-128cf37e-4f7e-470f-afdd-42e7b7fb686d",
          "traceId": "e026acf2",
          "type": "GENERATION",
          "name": "e0_mc_error_handling_attempt#2",
          "startTime": "2025-09-24T10:17:51.189000+00:00",
          "endTime": "2025-09-24T10:18:05.399000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 14210.0,
          "promptTokens": 2048,
          "totalTokens": 2495,
          "completionTokens": 447,
          "environment": "default",
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-13-18-05-403970_chatcmpl-c281d332-a79c-4b45-8ffc-7025502a72a7",
          "traceId": "e026acf2",
          "type": "GENERATION",
          "name": "e0_mc_error_handling_attempt#3",
          "startTime": "2025-09-24T10:18:05.403000+00:00",
          "endTime": "2025-09-24T10:18:16.049000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 10646.0,
          "promptTokens": 885,
          "totalTokens": 1254,
          "completionTokens": 369,
          "environment": "default",
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "0ec46d90-c8f8-4d1a-9c28-7caff8cdd070",
          "traceId": "e026acf2",
          "type": "SPAN",
          "name": "start_model_converter",
          "startTime": "2025-09-24T10:17:36.443000+00:00",
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "promptTokens": 0,
          "totalTokens": 0,
          "completionTokens": 0,
          "environment": "default",
          "costDetails": {},
          "endTime": null,
          "model": null,
          "modelParameters": null,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null,
          "latency": null
        }
      ],
      "createdAt": "2025-09-24T10:17:36.962Z",
      "environment": "default",
      "updatedAt": "2025-09-24T10:18:25.117Z"
    },
    {
      "id": "4fdfa8bd",
      "timestamp": "2025-09-24T10:16:56.078000+00:00",
      "name": "4fdf_model_converter",
      "input": null,
      "output": {
        "status": "success",
        "code": "import tensorflow as tf\n\n# Load the original Keras model\noriginal_model = tf.keras.models.load_model(\"models/fruit_to_emoji/og_model/model.keras\")\n\n# Define the representative dataset generator for quantization\ndef representative_dataset():\n    # Replace with your actual data loading and preprocessing logic\n    for _ in range(100):  # Use a representative number of samples\n        # Example: Load and preprocess data here\n        data = tf.random.normal([1, 224, 224, 3])  # Adjust shape as per your model's input\n        yield [tf.dtypes.cast(data, tf.float32)]\n\n# Define conversion parameters for quantization\nconverter = tf.lite.TFLiteConverter.from_keras_model(original_model)\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\nconverter.representative_dataset = representative_dataset\nconverter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\nconverter.inference_input_type = tf.float32  # Set the input type to float32\nconverter.inference_output_type = tf.uint8   # Set the output type to uint8\n\n# Convert the model to TFLite with quantization\ntflite_quant_model = converter.convert()\n\n# Save the converted TFLite model\nwith open(\"models/fruit_to_emoji/tflite_model/model_quant.tflite\", \"wb\") as f:\n    f.write(tflite_quant_model)"
      },
      "session_id": "qwen2.5-coder:32b_7e6b_mc_batch",
      "metadata": {
        "num_run": 27
      },
      "tags": [
        "benchmark",
        "model_converter",
        "qwen2.5-coder:32b"
      ],
      "latency": 10.62,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-13-16-56-080562_chatcmpl-e82d6c81-ab05-4d09-9f77-88e8c1bf9802",
          "traceId": "4fdfa8bd",
          "type": "GENERATION",
          "name": "4f_mc_conversion_code_gen",
          "startTime": "2025-09-24T10:16:56.080000+00:00",
          "endTime": "2025-09-24T10:17:06.699000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 10619.0,
          "promptTokens": 451,
          "totalTokens": 765,
          "completionTokens": 314,
          "environment": "default",
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "85ca6137-cfb5-479b-82c5-2fcf252b9962",
          "traceId": "4fdfa8bd",
          "type": "SPAN",
          "name": "start_model_converter",
          "startTime": "2025-09-24T10:16:56.079000+00:00",
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "promptTokens": 0,
          "totalTokens": 0,
          "completionTokens": 0,
          "environment": "default",
          "costDetails": {},
          "endTime": null,
          "model": null,
          "modelParameters": null,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null,
          "latency": null
        }
      ],
      "createdAt": "2025-09-24T10:16:56.599Z",
      "environment": "default",
      "updatedAt": "2025-09-24T10:17:16.254Z"
    },
    {
      "id": "7c475ddd",
      "timestamp": "2025-09-24T10:14:53.302000+00:00",
      "name": "7c47_model_converter",
      "input": null,
      "output": {
        "status": "success",
        "code": "import tensorflow as tf\n\n# Load your dataset (example using CIFAR-10)\n(x_train, _), (_, _) = tf.keras.datasets.cifar10.load_data()\n\n# Preprocess your data if necessary (e.g., normalize pixel values)\nx_train = x_train.astype('float32') / 255.0\n\n# Define a representative dataset generator\ndef representative_data_gen():\n    for input_value in tf.data.Dataset.from_tensor_slices((x_train[:100])).batch(1).take(100):\n        yield [input_value]\n\n# Load your Keras model from the specified path\nmodel = tf.keras.models.load_model('models/fruit_to_emoji/og_model/model.keras')\n\n# Convert the model to TensorFlow Lite format for quantization\nconverter = tf.lite.TFLiteConverter.from_keras_model(model)\n\n# Set the optimization flag to optimize for size and specify integer-only quantization\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\nconverter.representative_dataset = representative_data_gen\nconverter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\nconverter.inference_input_type = tf.int8  # or tf.uint8 if preferred\nconverter.inference_output_type = tf.int8  # or tf.uint8 if preferred\n\n# Convert the model to the TensorFlow Lite format with quantization applied\ntflite_model = converter.convert()\n\n# Save the quantized model to a file\nwith open('models/fruit_to_emoji/tflite_model/model_quant.tflite', 'wb') as f:\n    f.write(tflite_model)"
      },
      "session_id": "qwen2.5-coder:32b_7e6b_mc_batch",
      "metadata": {
        "num_run": 26
      },
      "tags": [
        "benchmark",
        "model_converter",
        "qwen2.5-coder:32b"
      ],
      "latency": 86.674,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-13-14-53-303872_chatcmpl-cfb9eaf5-540a-4a5b-8a14-3c0734c4696b",
          "traceId": "7c475ddd",
          "type": "GENERATION",
          "name": "7c_mc_conversion_code_gen",
          "startTime": "2025-09-24T10:14:53.303000+00:00",
          "endTime": "2025-09-24T10:15:00.026000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 6723.0,
          "promptTokens": 451,
          "totalTokens": 701,
          "completionTokens": 250,
          "environment": "default",
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-13-15-03-452492_chatcmpl-cd8715d7-4a86-4be0-95a1-f419be480f0e",
          "traceId": "7c475ddd",
          "type": "GENERATION",
          "name": "7c_mc_error_handling_attempt#2",
          "startTime": "2025-09-24T10:15:03.452000+00:00",
          "endTime": "2025-09-24T10:15:18.920000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 15468.0,
          "promptTokens": 2048,
          "totalTokens": 2544,
          "completionTokens": 496,
          "environment": "default",
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-13-15-22-585034_chatcmpl-f14adcc4-40fe-410a-808e-356a1e043d3e",
          "traceId": "7c475ddd",
          "type": "GENERATION",
          "name": "7c_mc_error_handling_attempt#3",
          "startTime": "2025-09-24T10:15:22.585000+00:00",
          "endTime": "2025-09-24T10:15:34.473000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 11888.0,
          "promptTokens": 1356,
          "totalTokens": 1750,
          "completionTokens": 394,
          "environment": "default",
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-13-15-37-877752_chatcmpl-8ec9d255-b533-4ebb-9af6-ccb7a45cc870",
          "traceId": "7c475ddd",
          "type": "GENERATION",
          "name": "7c_mc_error_handling_attempt#4",
          "startTime": "2025-09-24T10:15:37.877000+00:00",
          "endTime": "2025-09-24T10:15:55.712000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 17835.0,
          "promptTokens": 2048,
          "totalTokens": 2626,
          "completionTokens": 578,
          "environment": "default",
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-13-16-03-835523_chatcmpl-63b989d6-d1b4-443e-87bc-810ec94d4d83",
          "traceId": "7c475ddd",
          "type": "GENERATION",
          "name": "7c_mc_error_handling_attempt#5",
          "startTime": "2025-09-24T10:16:03.835000+00:00",
          "endTime": "2025-09-24T10:16:19.976000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 16141.0,
          "promptTokens": 1852,
          "totalTokens": 2378,
          "completionTokens": 526,
          "environment": "default",
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "837137c9-08e8-452e-a125-e234f61a17bb",
          "traceId": "7c475ddd",
          "type": "SPAN",
          "name": "start_model_converter",
          "startTime": "2025-09-24T10:14:53.302000+00:00",
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "promptTokens": 0,
          "totalTokens": 0,
          "completionTokens": 0,
          "environment": "default",
          "costDetails": {},
          "endTime": null,
          "model": null,
          "modelParameters": null,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null,
          "latency": null
        }
      ],
      "createdAt": "2025-09-24T10:14:53.823Z",
      "environment": "default",
      "updatedAt": "2025-09-24T10:16:35.515Z"
    },
    {
      "id": "7e56a5d4",
      "timestamp": "2025-09-24T10:13:31.939000+00:00",
      "name": "7e56_model_converter",
      "input": null,
      "output": {
        "status": "success",
        "code": "import tensorflow as tf\nimport numpy as np\n\n# Define input shape based on your model's expected input shape\n# For demonstration, let's assume input_shape is (224, 224, 3) which is common for image models\ninput_shape = (1, 224, 224, 3)\n\n# Load the original model\noriginal_model = tf.keras.models.load_model(\"models/fruit_to_emoji/og_model/model.keras\")\n\n# Define conversion parameters for 8-bit integer quantization\nconverter = tf.lite.TFLiteConverter.from_keras_model(original_model)\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\nconverter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n\n# Define input and output types for quantization\ninference_input_type = tf.float32\ninference_output_type = tf.uint8\n\n# Set the representative dataset for calibration (required for integer quantization)\ndef representative_dataset_gen():\n    # Replace this with your actual data generator function that yields input data as a list of numpy arrays.\n    for _ in range(100):\n        yield [np.array(np.random.random_sample(input_shape), dtype=np.float32)]\n\nconverter.representative_dataset = representative_dataset_gen\nconverter.inference_input_type = inference_input_type  # or tf.uint8 if your model inputs are uint8\nconverter.inference_output_type = inference_output_type\n\n# Convert the model to TFLite format with quantization\ntflite_model_quant = converter.convert()\n\n# Save the quantized TFLite model\nwith open(\"models/fruit_to_emoji/tflite_model/model_quant.tflite\", \"wb\") as f:\n    f.write(tflite_model_quant)"
      },
      "session_id": "qwen2.5-coder:32b_7e6b_mc_batch",
      "metadata": {
        "num_run": 25
      },
      "tags": [
        "benchmark",
        "model_converter",
        "qwen2.5-coder:32b"
      ],
      "latency": 51.055,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-13-13-31-941292_chatcmpl-95e59b99-f19c-4288-8fcf-63a5c56cf52f",
          "traceId": "7e56a5d4",
          "type": "GENERATION",
          "name": "7e_mc_conversion_code_gen",
          "startTime": "2025-09-24T10:13:31.941000+00:00",
          "endTime": "2025-09-24T10:13:46.149000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 14208.0,
          "promptTokens": 451,
          "totalTokens": 758,
          "completionTokens": 307,
          "environment": "default",
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-13-13-49-503731_chatcmpl-c1b4a854-eb94-4fce-8696-4cfa8ac69ab5",
          "traceId": "7e56a5d4",
          "type": "GENERATION",
          "name": "7e_mc_error_handling_attempt#2",
          "startTime": "2025-09-24T10:13:49.503000+00:00",
          "endTime": "2025-09-24T10:14:02.146000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 12643.0,
          "promptTokens": 2048,
          "totalTokens": 2441,
          "completionTokens": 393,
          "environment": "default",
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-13-14-02-149622_chatcmpl-2dd15f07-dfa2-4d6c-9e87-6c4006883a0e",
          "traceId": "7e56a5d4",
          "type": "GENERATION",
          "name": "7e_mc_error_handling_attempt#3",
          "startTime": "2025-09-24T10:14:02.149000+00:00",
          "endTime": "2025-09-24T10:14:22.994000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 20845.0,
          "promptTokens": 828,
          "totalTokens": 1362,
          "completionTokens": 534,
          "environment": "default",
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "16fb4472-9d47-4efc-88a0-a0867dfd05c9",
          "traceId": "7e56a5d4",
          "type": "SPAN",
          "name": "start_model_converter",
          "startTime": "2025-09-24T10:13:31.939000+00:00",
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "promptTokens": 0,
          "totalTokens": 0,
          "completionTokens": 0,
          "environment": "default",
          "costDetails": {},
          "endTime": null,
          "model": null,
          "modelParameters": null,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null,
          "latency": null
        }
      ],
      "createdAt": "2025-09-24T10:13:32.461Z",
      "environment": "default",
      "updatedAt": "2025-09-24T10:14:32.572Z"
    },
    {
      "id": "f39d4249",
      "timestamp": "2025-09-24T10:12:36.446000+00:00",
      "name": "f39d_model_converter",
      "input": null,
      "output": {
        "status": "success",
        "code": "import tensorflow as tf\n\n# Load the original model\noriginal_model = tf.keras.models.load_model(\"models/fruit_to_emoji/og_model/model.keras\")\n\n# Define conversion parameters for quantization\nconverter = tf.lite.TFLiteConverter.from_keras_model(original_model)\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\n\n# Define quantization specifications\ninference_input_type = tf.float32  # Input type float32\ninference_output_type = tf.uint8   # Output type uint8\n\n# Ensure that if any ops can't be quantized, the converter throws an error\nconverter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\nconverter.inference_input_type = inference_input_type\nconverter.inference_output_type = inference_output_type\n\n# Define a representative dataset generator\ndef representative_dataset_gen():\n    for _ in range(100):  # Generate a few samples from the input data\n        # Replace this with actual data loading code\n        yield [tf.random.uniform(shape=(1, 28, 28, 3), minval=0, maxval=255, dtype=tf.float32)]\n\n# Set the representative dataset for quantization\nconverter.representative_dataset = representative_dataset_gen\n\n# Convert the model to TFLite format with full integer quantization\ntflite_quant_model = converter.convert()\n\n# Save the converted model to a file\nwith open(\"models/fruit_to_emoji/tflite_model_int8.tflite\", \"wb\") as f:\n    f.write(tflite_quant_model)"
      },
      "session_id": "qwen2.5-coder:32b_7e6b_mc_batch",
      "metadata": {
        "num_run": 24
      },
      "tags": [
        "benchmark",
        "model_converter",
        "qwen2.5-coder:32b"
      ],
      "latency": 26.431,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-13-12-36-447551_chatcmpl-b651bc96-59d0-44e0-8403-a68654c2906c",
          "traceId": "f39d4249",
          "type": "GENERATION",
          "name": "f3_mc_conversion_code_gen",
          "startTime": "2025-09-24T10:12:36.447000+00:00",
          "endTime": "2025-09-24T10:12:44.326000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 7879.0,
          "promptTokens": 451,
          "totalTokens": 690,
          "completionTokens": 239,
          "environment": "default",
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-13-12-47-350244_chatcmpl-f5b6d90c-21aa-4c11-b5ca-a4c803aa4d9d",
          "traceId": "f39d4249",
          "type": "GENERATION",
          "name": "f3_mc_error_handling_attempt#2",
          "startTime": "2025-09-24T10:12:47.350000+00:00",
          "endTime": "2025-09-24T10:13:02.877000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 15527.0,
          "promptTokens": 1708,
          "totalTokens": 2204,
          "completionTokens": 496,
          "environment": "default",
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "0dffc412-ce0b-4f90-99e3-307d7c0d8112",
          "traceId": "f39d4249",
          "type": "SPAN",
          "name": "start_model_converter",
          "startTime": "2025-09-24T10:12:36.446000+00:00",
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "promptTokens": 0,
          "totalTokens": 0,
          "completionTokens": 0,
          "environment": "default",
          "costDetails": {},
          "endTime": null,
          "model": null,
          "modelParameters": null,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null,
          "latency": null
        }
      ],
      "createdAt": "2025-09-24T10:12:36.968Z",
      "environment": "default",
      "updatedAt": "2025-09-24T10:13:11.578Z"
    },
    {
      "id": "3a4281ca",
      "timestamp": "2025-09-24T10:11:29.109000+00:00",
      "name": "3a42_model_converter",
      "input": null,
      "output": {
        "status": "success",
        "code": "import tensorflow as tf\nimport numpy as np\n\n# Load the original Keras model\noriginal_model = tf.keras.models.load_model(\"models/fruit_to_emoji/og_model/model.keras\")\n\n# Define conversion parameters for 8-bit integer quantization\nconverter = tf.lite.TFLiteConverter.from_keras_model(original_model)\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\n\n# Define the data types for quantization\ninference_input_type = tf.float32\ninference_output_type = tf.uint8\n\n# Ensure that if any ops can't be quantized, the converter throws an error\nconverter.target_spec.supported_ops = [\n    tf.lite.OpsSet.TFLITE_BUILTINS_INT8,\n    tf.lite.OpsSet.TFLITE_BUILTINS\n]\n\n# Define a representative dataset generator function\ndef representative_dataset_gen():\n    # Replace `input_data` with actual data from your dataset.\n    for _ in range(100):\n        # Get sample input data as a numpy array in a method of your choosing.\n        input_data = np.random.rand(1, 224, 224, 3).astype(np.float32)  # Example shape and type\n        yield [input_data]\n\n# Set the representative dataset for quantization\nconverter.representative_dataset = representative_dataset_gen\n\n# Set the input and output tensors to float32 (input) and uint8 (output)\nconverter.inference_input_type = inference_input_type\nconverter.inference_output_type = inference_output_type\n\n# Convert the model\ntflite_model_quant = converter.convert()\n\n# Save the quantized TFLite model\nwith open(\"models/fruit_to_emoji/tflite_model/model_quant.tflite\", \"wb\") as f:\n    f.write(tflite_model_quant)"
      },
      "session_id": "qwen2.5-coder:32b_7e6b_mc_batch",
      "metadata": {
        "num_run": 23
      },
      "tags": [
        "benchmark",
        "model_converter",
        "qwen2.5-coder:32b"
      ],
      "latency": 37.165,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-13-11-29-111289_chatcmpl-05e26508-dab2-45a6-b3c8-02b27d91fbe4",
          "traceId": "3a4281ca",
          "type": "GENERATION",
          "name": "3a_mc_conversion_code_gen",
          "startTime": "2025-09-24T10:11:29.111000+00:00",
          "endTime": "2025-09-24T10:11:40.890000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 11779.0,
          "promptTokens": 451,
          "totalTokens": 844,
          "completionTokens": 393,
          "environment": "default",
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-13-11-44-243539_chatcmpl-21fec2ad-fb60-4561-a513-60a5e72e8c76",
          "traceId": "3a4281ca",
          "type": "GENERATION",
          "name": "3a_mc_error_handling_attempt#2",
          "startTime": "2025-09-24T10:11:44.243000+00:00",
          "endTime": "2025-09-24T10:11:55.275000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 11032.0,
          "promptTokens": 2048,
          "totalTokens": 2382,
          "completionTokens": 334,
          "environment": "default",
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-13-11-55-278519_chatcmpl-9299ba8c-9307-4586-97b0-9ea503b16f2e",
          "traceId": "3a4281ca",
          "type": "GENERATION",
          "name": "3a_mc_error_handling_attempt#3",
          "startTime": "2025-09-24T10:11:55.278000+00:00",
          "endTime": "2025-09-24T10:12:06.274000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 10996.0,
          "promptTokens": 914,
          "totalTokens": 1297,
          "completionTokens": 383,
          "environment": "default",
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "d41706ad-d1f8-4828-b082-32c4685d8f8c",
          "traceId": "3a4281ca",
          "type": "SPAN",
          "name": "start_model_converter",
          "startTime": "2025-09-24T10:11:29.109000+00:00",
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "promptTokens": 0,
          "totalTokens": 0,
          "completionTokens": 0,
          "environment": "default",
          "costDetails": {},
          "endTime": null,
          "model": null,
          "modelParameters": null,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null,
          "latency": null
        }
      ],
      "createdAt": "2025-09-24T10:11:29.633Z",
      "environment": "default",
      "updatedAt": "2025-09-24T10:12:15.780Z"
    },
    {
      "id": "bec57a71",
      "timestamp": "2025-09-24T10:10:32.554000+00:00",
      "name": "bec5_model_converter",
      "input": null,
      "output": {
        "status": "success",
        "code": "import tensorflow as tf\n\n# Load the original Keras model\noriginal_model = tf.keras.models.load_model(\"models/fruit_to_emoji/og_model/model.keras\")\n\n# Define conversion parameters for quantization\nconverter = tf.lite.TFLiteConverter.from_keras_model(original_model)\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\n\n# Define representative dataset generator function\ndef representative_dataset_gen():\n    # Replace this with your actual data generation logic\n    for _ in range(100):  # Number of calibration steps\n        input_data = tf.random.normal([1, *original_model.input_shape[1:]], dtype=tf.float32)\n        yield [input_data]\n\n# Define quantization parameters\ninference_input_type = tf.float32  # Input type as float32\ninference_output_type = tf.uint8   # Output type as uint8\n\n# Ensure that if any ops can't be quantized, the converter throws an error\nconverter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\nconverter.inference_input_type = inference_input_type\nconverter.inference_output_type = inference_output_type\nconverter.representative_dataset = representative_dataset_gen\n\n# Convert the model to TFLite with 8-bit integer quantization\nquantized_tflite_model = converter.convert()\n\n# Save the quantized model to a file\nwith open(\"models/fruit_to_emoji/tflite_model/model_quantized.tflite\", \"wb\") as f:\n    f.write(quantized_tflite_model)"
      },
      "session_id": "qwen2.5-coder:32b_7e6b_mc_batch",
      "metadata": {
        "num_run": 22
      },
      "tags": [
        "benchmark",
        "model_converter",
        "qwen2.5-coder:32b"
      ],
      "latency": 26.805,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-13-10-32-556246_chatcmpl-6ff815ef-61b4-4d9b-8b5c-b61eac89f33d",
          "traceId": "bec57a71",
          "type": "GENERATION",
          "name": "be_mc_conversion_code_gen",
          "startTime": "2025-09-24T10:10:32.556000+00:00",
          "endTime": "2025-09-24T10:10:41.309000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 8753.0,
          "promptTokens": 451,
          "totalTokens": 696,
          "completionTokens": 245,
          "environment": "default",
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-13-10-44-411089_chatcmpl-6c04b363-4e56-48db-8b45-6453eb9daa54",
          "traceId": "bec57a71",
          "type": "GENERATION",
          "name": "be_mc_error_handling_attempt#2",
          "startTime": "2025-09-24T10:10:44.411000+00:00",
          "endTime": "2025-09-24T10:10:59.359000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 14948.0,
          "promptTokens": 1715,
          "totalTokens": 2149,
          "completionTokens": 434,
          "environment": "default",
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "0e74c663-78d5-406e-b424-34b550a335c2",
          "traceId": "bec57a71",
          "type": "SPAN",
          "name": "start_model_converter",
          "startTime": "2025-09-24T10:10:32.554000+00:00",
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "promptTokens": 0,
          "totalTokens": 0,
          "completionTokens": 0,
          "environment": "default",
          "costDetails": {},
          "endTime": null,
          "model": null,
          "modelParameters": null,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null,
          "latency": null
        }
      ],
      "createdAt": "2025-09-24T10:10:33.079Z",
      "environment": "default",
      "updatedAt": "2025-09-24T10:11:08.241Z"
    },
    {
      "id": "d9aa6d4a",
      "timestamp": "2025-09-24T10:08:35.181000+00:00",
      "name": "d9aa_model_converter",
      "input": null,
      "output": {
        "status": "failed",
        "last_code": "import tensorflow as tf\nimport numpy as np\n\n# Load your original Keras model\noriginal_model = tf.keras.models.load_model(\"models/fruit_to_emoji/og_model/model.keras\")\n\n# Save the Keras model in SavedModel format\nsaved_model_path = \"models/fruit_to_emoji/og_model/saved_model\"\noriginal_model.save(saved_model_path)\n\n# Assume you have already loaded or created a dataset for calibration\ndef representative_data_gen():\n    # Generate some sample input data here\n    for _ in range(100):\n        yield [np.random.rand(1, 28, 28).astype(np.float32)]\n\n# Load the SavedModel\nsaved_model_dir = saved_model_path\n\n# Convert the model to TFLite with quantization\nconverter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir)\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\nconverter.representative_dataset = representative_data_gen\nconverter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\nconverter.inference_input_type = tf.uint8  # Corrected as per your requirement\nconverter.inference_output_type = tf.uint8  # Corrected as per your requirement\n\ntflite_model_quantized = converter.convert()\n\n# Save the TFLite model\nwith open(\"models/fruit_to_emoji/tflite_model/model_quant.tflite\", \"wb\") as f:\n    f.write(tflite_model_quantized)",
        "last_error": "Failed to generate valid code after the max 5 attempts. Last error from code execution: 2025-09-24 13:10:04.153602: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n2025-09-24 13:10:04.156893: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n2025-09-24 13:10:04.167674: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2025-09-24 13:10:04.184868: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2025-09-24 13:10:04.190102: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n2025-09-24 13:10:04.202314: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\nTo enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n2025-09-24 13:10:05.062085: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n/home/wuguangh/.conda/envs/tinyml/lib/python3.10/site-packages/keras/src/saving/saving_lib.py:757: UserWarning: Skipping variable loading for optimizer 'rmsprop', because it has 8 variables whereas the saved optimizer has 7 variables. \n  saveable.load_own_variables(weights_store.get(inner_path))\nTraceback (most recent call last):\n  File \"/home/wuguangh/Projects/tinyml-autopilot/models/fruit_to_emoji/tflite_model/tmp_converter20250924131003.py/tmp_20250924131003_mc_qwen2.5-coder:32b/tmp_20250924131003_mc_qwen2.5-coder:32b.py\", line 9, in <module>\n    original_model.save(saved_model_path)\n  File \"/home/wuguangh/.conda/envs/tinyml/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\", line 122, in error_handler\n    raise e.with_traceback(filtered_tb) from None\n  File \"/home/wuguangh/.conda/envs/tinyml/lib/python3.10/site-packages/keras/src/saving/saving_api.py\", line 114, in save_model\n    raise ValueError(\nValueError: Invalid filepath extension for saving. Please add either a `.keras` extension for the native Keras format (recommended) or a `.h5` extension. Use `model.export(filepath)` if you want to export a SavedModel for use with TFLite/TFServing/etc. Received: filepath=models/fruit_to_emoji/og_model/saved_model.\n"
      },
      "session_id": "qwen2.5-coder:32b_7e6b_mc_batch",
      "metadata": {
        "num_run": 21
      },
      "tags": [
        "benchmark",
        "model_converter",
        "qwen2.5-coder:32b"
      ],
      "latency": 91.745,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-13-08-35-183411_chatcmpl-7cfcefb4-1dd4-4608-bea8-cfcca2730c98",
          "traceId": "d9aa6d4a",
          "type": "GENERATION",
          "name": "d9_mc_conversion_code_gen",
          "startTime": "2025-09-24T10:08:35.183000+00:00",
          "endTime": "2025-09-24T10:08:44.143000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 8960.0,
          "promptTokens": 451,
          "totalTokens": 778,
          "completionTokens": 327,
          "environment": "default",
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-13-08-47-455103_chatcmpl-b6b06987-a8f8-422d-a8ce-00e1a4ec0d8d",
          "traceId": "d9aa6d4a",
          "type": "GENERATION",
          "name": "d9_mc_error_handling_attempt#2",
          "startTime": "2025-09-24T10:08:47.455000+00:00",
          "endTime": "2025-09-24T10:09:07.352000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 19897.0,
          "promptTokens": 2048,
          "totalTokens": 2690,
          "completionTokens": 642,
          "environment": "default",
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-13-09-07-370269_chatcmpl-f0efb226-aaee-4663-ba21-9fe18011e7ec",
          "traceId": "d9aa6d4a",
          "type": "GENERATION",
          "name": "d9_mc_error_handling_attempt#3",
          "startTime": "2025-09-24T10:09:07.370000+00:00",
          "endTime": "2025-09-24T10:09:18.627000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 11257.0,
          "promptTokens": 1009,
          "totalTokens": 1399,
          "completionTokens": 390,
          "environment": "default",
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-13-09-23-969638_chatcmpl-bc531b0c-2d3a-4376-a0c8-9a611e2bd6e3",
          "traceId": "d9aa6d4a",
          "type": "GENERATION",
          "name": "d9_mc_error_handling_attempt#4",
          "startTime": "2025-09-24T10:09:23.969000+00:00",
          "endTime": "2025-09-24T10:09:39.915000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 15946.0,
          "promptTokens": 1823,
          "totalTokens": 2356,
          "completionTokens": 533,
          "environment": "default",
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-13-09-42-918381_chatcmpl-0ebaa3fa-bab1-46b4-a4a8-565398b9862f",
          "traceId": "d9aa6d4a",
          "type": "GENERATION",
          "name": "d9_mc_error_handling_attempt#5",
          "startTime": "2025-09-24T10:09:42.918000+00:00",
          "endTime": "2025-09-24T10:10:03.897000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 20979.0,
          "promptTokens": 1673,
          "totalTokens": 2146,
          "completionTokens": 473,
          "environment": "default",
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "27c6c914-91ea-444c-801c-0753e8c2a949",
          "traceId": "d9aa6d4a",
          "type": "SPAN",
          "name": "start_model_converter",
          "startTime": "2025-09-24T10:08:35.181000+00:00",
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "promptTokens": 0,
          "totalTokens": 0,
          "completionTokens": 0,
          "environment": "default",
          "costDetails": {},
          "endTime": null,
          "model": null,
          "modelParameters": null,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null,
          "latency": null
        },
        {
          "id": "e4aedc38-e8d4-4df4-8ce0-1c20a61272a4",
          "traceId": "d9aa6d4a",
          "type": "SPAN",
          "name": "error_d9_mc_failure_signal_model_converter",
          "startTime": "2025-09-24T10:10:06.926000+00:00",
          "level": "ERROR",
          "statusMessage": "Failed. Last error: Failed to generate valid code after the max 5 attempts. Last error from code execution: 2025-09-24 13:10:04.153602: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n2025-09-24 13:10:04.156893: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n2025-09-24 13:10:04.167674: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2025-09-24 13:10:04.184868: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2025-09-24 13:10:04.190102: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n2025-09-24 13:10:04.202314: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\nTo enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n2025-09-24 13:10:05.062085: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n/home/wuguangh/.conda/envs/tinyml/lib/python3.10/site-packages/keras/src/saving/saving_lib.py:757: UserWarning: Skipping variable loading for optimizer 'rmsprop', because it has 8 variables whereas the saved optimizer has 7 variables. \n  saveable.load_own_variables(weights_store.get(inner_path))\nTraceback (most recent call last):\n  File \"/home/wuguangh/Projects/tinyml-autopilot/models/fruit_to_emoji/tflite_model/tmp_converter20250924131003.py/tmp_20250924131003_mc_qwen2.5-coder:32b/tmp_20250924131003_mc_qwen2.5-coder:32b.py\", line 9, in <module>\n    original_model.save(saved_model_path)\n  File \"/home/wuguangh/.conda/envs/tinyml/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\", line 122, in error_handler\n    raise e.with_traceback(filtered_tb) from None\n  File \"/home/wuguangh/.conda/envs/tinyml/lib/python3.10/site-packages/keras/src/saving/saving_api.py\", line 114, in save_model\n    raise ValueError(\nValueError: Invalid filepath extension for saving. Please add either a `.keras` extension for the native Keras format (recommended) or a `.h5` extension. Use `model.export(filepath)` if you want to export a SavedModel for use with TFLite/TFServing/etc. Received: filepath=models/fruit_to_emoji/og_model/saved_model.\n",
          "calculatedTotalCost": 0.0,
          "promptTokens": 0,
          "totalTokens": 0,
          "completionTokens": 0,
          "environment": "default",
          "costDetails": {},
          "endTime": null,
          "model": null,
          "modelParameters": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null,
          "latency": null
        }
      ],
      "createdAt": "2025-09-24T10:08:35.704Z",
      "environment": "default",
      "updatedAt": "2025-09-24T10:10:12.339Z"
    },
    {
      "id": "5274e394",
      "timestamp": "2025-09-24T10:07:08.853000+00:00",
      "name": "5274_model_converter",
      "input": null,
      "output": {
        "status": "success",
        "code": "import tensorflow as tf\n\n# Load your model from the correct path\nmodel = tf.keras.models.load_model('models/fruit_to_emoji/og_model/model.keras')\n\n# Define a generator function that provides a representative dataset\ndef representative_data_gen():\n    # Replace with your actual data loading logic\n    # Example: for input_value in dataset:\n    #           yield [input_value.astype(np.float32)]\n    for _ in range(100):  # Dummy example, replace with real data\n        yield [tf.random.uniform(shape=(1, 64, 64, 3), minval=0.0, maxval=1.0, dtype=tf.float32)]\n\n# Convert the model to TensorFlow Lite format with integer-only quantization\nconverter = tf.lite.TFLiteConverter.from_keras_model(model)\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\nconverter.representative_dataset = representative_data_gen\nconverter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8, tf.lite.OpsSet.TFLITE_BUILTINS]\nconverter.inference_input_type = tf.float32  # Input is float32 as per your requirement\nconverter.inference_output_type = tf.float32  # Output is float32 as per your requirement\nconverter.quantized_input_stats = [(0.0, 1.0)] * model.input_shape[-1]  # Example stats, replace with actual if needed\n\n# Convert the model and save it to the specified path\ntflite_model = converter.convert()\nwith open('models/fruit_to_emoji/tflite_model/model.tflite', 'wb') as f:\n    f.write(tflite_model)"
      },
      "session_id": "qwen2.5-coder:32b_7e6b_mc_batch",
      "metadata": {
        "num_run": 20
      },
      "tags": [
        "benchmark",
        "model_converter",
        "qwen2.5-coder:32b"
      ],
      "latency": 57.438,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-13-07-08-854733_chatcmpl-e3b3951b-8e20-4e51-8701-8861fcd17116",
          "traceId": "5274e394",
          "type": "GENERATION",
          "name": "52_mc_conversion_code_gen",
          "startTime": "2025-09-24T10:07:08.854000+00:00",
          "endTime": "2025-09-24T10:07:18.641000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 9787.0,
          "promptTokens": 451,
          "totalTokens": 818,
          "completionTokens": 367,
          "environment": "default",
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-13-07-23-408498_chatcmpl-320ab1fe-831c-421f-b4c9-052fbaf8bdd7",
          "traceId": "5274e394",
          "type": "GENERATION",
          "name": "52_mc_error_handling_attempt#2",
          "startTime": "2025-09-24T10:07:23.408000+00:00",
          "endTime": "2025-09-24T10:07:41.089000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 17681.0,
          "promptTokens": 2048,
          "totalTokens": 2622,
          "completionTokens": 574,
          "environment": "default",
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-13-07-44-503429_chatcmpl-57534a28-820f-4606-8773-cac615a016d0",
          "traceId": "5274e394",
          "type": "GENERATION",
          "name": "52_mc_error_handling_attempt#3",
          "startTime": "2025-09-24T10:07:44.503000+00:00",
          "endTime": "2025-09-24T10:08:06.291000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 21788.0,
          "promptTokens": 1723,
          "totalTokens": 2289,
          "completionTokens": 566,
          "environment": "default",
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "b54814e3-1d5b-450e-8fcc-fd4727118b1a",
          "traceId": "5274e394",
          "type": "SPAN",
          "name": "start_model_converter",
          "startTime": "2025-09-24T10:07:08.853000+00:00",
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "promptTokens": 0,
          "totalTokens": 0,
          "completionTokens": 0,
          "environment": "default",
          "costDetails": {},
          "endTime": null,
          "model": null,
          "modelParameters": null,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null,
          "latency": null
        }
      ],
      "createdAt": "2025-09-24T10:07:09.376Z",
      "environment": "default",
      "updatedAt": "2025-09-24T10:08:15.009Z"
    },
    {
      "id": "9476dcf6",
      "timestamp": "2025-09-24T10:05:45.494000+00:00",
      "name": "9476_model_converter",
      "input": null,
      "output": {
        "status": "success",
        "code": "import numpy as np\nimport tensorflow as tf\n\n# Load the original model\nmodel = tf.keras.models.load_model('models/fruit_to_emoji/og_model/model.keras')\n\n# Define the converter\nconverter = tf.lite.TFLiteConverter.from_keras_model(model)\n\n# Set the optimization flag to optimize for size (quantization)\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\n\ndef representative_dataset_gen():\n    # Generate a representative dataset for calibration\n    # Here we are using random data, but in practice you should use your actual dataset\n    for _ in range(100):\n        yield [np.array([np.random.rand(224, 224, 3)], dtype=np.float32)]\n\n# Set the representative dataset for quantization\nconverter.representative_dataset = representative_dataset_gen\n\n# Ensure that if any ops can't be quantized, the converter throws an error\nconverter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\nconverter.inference_input_type = tf.uint8  # Corrected to uint8 as per your requirement\nconverter.inference_output_type = tf.uint8  # Corrected to uint8 as per your requirement\n\n# Convert the model\nquantized_tflite_model = converter.convert()\n\n# Save the quantized TFLite model to a file\nwith open('models/fruit_to_emoji/tflite_model/model_quant.tflite', 'wb') as f:\n    f.write(quantized_tflite_model)"
      },
      "session_id": "qwen2.5-coder:32b_7e6b_mc_batch",
      "metadata": {
        "num_run": 19
      },
      "tags": [
        "benchmark",
        "model_converter",
        "qwen2.5-coder:32b"
      ],
      "latency": 53.789,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-13-05-45-495406_chatcmpl-540dd3ea-56bf-4e8e-972e-d7657b83baa2",
          "traceId": "9476dcf6",
          "type": "GENERATION",
          "name": "94_mc_conversion_code_gen",
          "startTime": "2025-09-24T10:05:45.495000+00:00",
          "endTime": "2025-09-24T10:05:59.003000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 13508.0,
          "promptTokens": 451,
          "totalTokens": 749,
          "completionTokens": 298,
          "environment": "default",
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-13-06-02-431313_chatcmpl-af80e78a-92ab-4e44-9644-8db37114c54f",
          "traceId": "9476dcf6",
          "type": "GENERATION",
          "name": "94_mc_error_handling_attempt#2",
          "startTime": "2025-09-24T10:06:02.431000+00:00",
          "endTime": "2025-09-24T10:06:22.560000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 20129.0,
          "promptTokens": 2048,
          "totalTokens": 2495,
          "completionTokens": 447,
          "environment": "default",
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-13-06-25-484699_chatcmpl-6c64f0be-b68c-4bf8-9ed7-e64125b8d4c9",
          "traceId": "9476dcf6",
          "type": "GENERATION",
          "name": "94_mc_error_handling_attempt#3",
          "startTime": "2025-09-24T10:06:25.484000+00:00",
          "endTime": "2025-09-24T10:06:39.283000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 13799.0,
          "promptTokens": 1420,
          "totalTokens": 1817,
          "completionTokens": 397,
          "environment": "default",
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "5eabe802-8fad-49ab-b753-7cf18b6f875e",
          "traceId": "9476dcf6",
          "type": "SPAN",
          "name": "start_model_converter",
          "startTime": "2025-09-24T10:05:45.494000+00:00",
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "promptTokens": 0,
          "totalTokens": 0,
          "completionTokens": 0,
          "environment": "default",
          "costDetails": {},
          "endTime": null,
          "model": null,
          "modelParameters": null,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null,
          "latency": null
        }
      ],
      "createdAt": "2025-09-24T10:05:46.016Z",
      "environment": "default",
      "updatedAt": "2025-09-24T10:06:48.694Z"
    },
    {
      "id": "f71f85be",
      "timestamp": "2025-09-24T10:04:59.141000+00:00",
      "name": "f71f_model_converter",
      "input": null,
      "output": {
        "status": "success",
        "code": "import tensorflow as tf\n\n# Load the original Keras model\noriginal_model = tf.keras.models.load_model(\"models/fruit_to_emoji/og_model/model.keras\")\n\n# Define conversion parameters for quantization\nconverter = tf.lite.TFLiteConverter.from_keras_model(original_model)\n\n# Set the optimization flag to optimize for size (which includes 8-bit integer quantization)\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\n\n# Define representative dataset generator function\ndef representative_data_gen():\n    # Load and preprocess a few sample data points as input tensors\n    # This should be replaced with actual representative samples from your dataset\n    for _ in range(100):\n        # Replace `input_data` with the appropriate input data for your model\n        input_data = tf.random.uniform(shape=(1, 224, 224, 3), dtype=tf.float32)\n        yield [input_data]\n\n# Set the representative dataset for quantization\nconverter.representative_dataset = representative_data_gen\n\n# Ensure that if any ops can't be quantized, an error is thrown\nconverter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\nconverter.inference_input_type = tf.float32  # or tf.uint8 if your model inputs are uint8\nconverter.inference_output_type = tf.uint8\n\n# Convert the model to TFLite with quantization\ntflite_quant_model = converter.convert()\n\n# Save the converted model to a file\nwith open(\"models/fruit_to_emoji/tflite_model/model_quant.tflite\", \"wb\") as f:\n    f.write(tflite_quant_model)"
      },
      "session_id": "qwen2.5-coder:32b_7e6b_mc_batch",
      "metadata": {
        "num_run": 18
      },
      "tags": [
        "benchmark",
        "model_converter",
        "qwen2.5-coder:32b"
      ],
      "latency": 16.351,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-13-04-59-143337_chatcmpl-631d3f74-47ae-4b81-9d72-9dd5dd009cfb",
          "traceId": "f71f85be",
          "type": "GENERATION",
          "name": "f7_mc_conversion_code_gen",
          "startTime": "2025-09-24T10:04:59.143000+00:00",
          "endTime": "2025-09-24T10:05:15.493000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 16350.0,
          "promptTokens": 451,
          "totalTokens": 805,
          "completionTokens": 354,
          "environment": "default",
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "ed9d8169-20cb-4f0b-b418-fe0e57de6ccd",
          "traceId": "f71f85be",
          "type": "SPAN",
          "name": "start_model_converter",
          "startTime": "2025-09-24T10:04:59.142000+00:00",
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "promptTokens": 0,
          "totalTokens": 0,
          "completionTokens": 0,
          "environment": "default",
          "costDetails": {},
          "endTime": null,
          "model": null,
          "modelParameters": null,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null,
          "latency": null
        }
      ],
      "createdAt": "2025-09-24T10:04:59.663Z",
      "environment": "default",
      "updatedAt": "2025-09-24T10:05:24.789Z"
    },
    {
      "id": "82d01214",
      "timestamp": "2025-09-24T10:04:19.781000+00:00",
      "name": "82d0_model_converter",
      "input": null,
      "output": {
        "status": "success",
        "code": "import tensorflow as tf\n\n# Load the original Keras model\nmodel = tf.keras.models.load_model(\"models/fruit_to_emoji/og_model/model.keras\")\n\n# Define conversion parameters for quantization\nconverter = tf.lite.TFLiteConverter.from_keras_model(model)\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\n\n# Define calibration dataset generator function\ndef representative_dataset_gen():\n    # Replace this with your actual data loading and preprocessing logic\n    # For demonstration, we are using dummy data\n    import numpy as np\n    for _ in range(100):\n        yield [np.random.uniform(low=0.0, high=1.0, size=(1, 224, 224, 3)).astype(np.float32)]\n\n# Set the representative dataset for quantization\nconverter.representative_dataset = representative_dataset_gen\n\n# Ensure that if any ops can't be quantized, the converter throws an error\nconverter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\nconverter.inference_input_type = tf.float32  # or tf.int8\nconverter.inference_output_type = tf.uint8\n\n# Convert the model to TFLite with quantization\ntflite_model_quant = converter.convert()\n\n# Save the converted model to a file\nwith open(\"models/fruit_to_emoji/tflite_model/model_quant.tflite\", \"wb\") as f:\n    f.write(tflite_model_quant)"
      },
      "session_id": "qwen2.5-coder:32b_7e6b_mc_batch",
      "metadata": {
        "num_run": 17
      },
      "tags": [
        "benchmark",
        "model_converter",
        "qwen2.5-coder:32b"
      ],
      "latency": 9.693,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-13-04-19-783125_chatcmpl-e5c5c433-5182-4d4f-8f95-11322c3aae30",
          "traceId": "82d01214",
          "type": "GENERATION",
          "name": "82_mc_conversion_code_gen",
          "startTime": "2025-09-24T10:04:19.783000+00:00",
          "endTime": "2025-09-24T10:04:29.474000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 9691.0,
          "promptTokens": 451,
          "totalTokens": 769,
          "completionTokens": 318,
          "environment": "default",
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "bf417d00-83e1-4278-8578-a372b994a617",
          "traceId": "82d01214",
          "type": "SPAN",
          "name": "start_model_converter",
          "startTime": "2025-09-24T10:04:19.781000+00:00",
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "promptTokens": 0,
          "totalTokens": 0,
          "completionTokens": 0,
          "environment": "default",
          "costDetails": {},
          "endTime": null,
          "model": null,
          "modelParameters": null,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null,
          "latency": null
        }
      ],
      "createdAt": "2025-09-24T10:04:20.303Z",
      "environment": "default",
      "updatedAt": "2025-09-24T10:04:38.922Z"
    },
    {
      "id": "634bce9a",
      "timestamp": "2025-09-24T10:03:42.439000+00:00",
      "name": "634b_model_converter",
      "input": null,
      "output": {
        "status": "success",
        "code": "import tensorflow as tf\n\n# Load the original model\noriginal_model = tf.keras.models.load_model(\"models/fruit_to_emoji/og_model/model.keras\")\n\n# Define conversion parameters for quantization\nconverter = tf.lite.TFLiteConverter.from_keras_model(original_model)\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\n\n# Define representative dataset generator\ndef representative_dataset_gen():\n    # Replace this with your actual data loading logic\n    # This is a dummy implementation\n    import numpy as np\n    for _ in range(100):\n        yield [np.random.rand(1, 224, 224, 3).astype(np.float32)]\n\nconverter.representative_dataset = representative_dataset_gen\n\n# Ensure that if any ops can't be quantized, the converter throws an error\nconverter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\nconverter.inference_input_type = tf.float32  # or tf.int8\nconverter.inference_output_type = tf.uint8\n\n# Convert the model\nquantized_tflite_model = converter.convert()\n\n# Save the converted model\nwith open(\"models/fruit_to_emoji/tflite_model/model_quant.tflite\", \"wb\") as f:\n    f.write(quantized_tflite_model)"
      },
      "session_id": "qwen2.5-coder:32b_7e6b_mc_batch",
      "metadata": {
        "num_run": 16
      },
      "tags": [
        "benchmark",
        "model_converter",
        "qwen2.5-coder:32b"
      ],
      "latency": 7.639,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-13-03-42-441097_chatcmpl-e474ad99-59a9-4f1f-80a3-e7f6e76e121f",
          "traceId": "634bce9a",
          "type": "GENERATION",
          "name": "63_mc_conversion_code_gen",
          "startTime": "2025-09-24T10:03:42.441000+00:00",
          "endTime": "2025-09-24T10:03:50.078000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 7637.0,
          "promptTokens": 451,
          "totalTokens": 734,
          "completionTokens": 283,
          "environment": "default",
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "876e6689-7e05-4b8f-be94-4ad82eced452",
          "traceId": "634bce9a",
          "type": "SPAN",
          "name": "start_model_converter",
          "startTime": "2025-09-24T10:03:42.439000+00:00",
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "promptTokens": 0,
          "totalTokens": 0,
          "completionTokens": 0,
          "environment": "default",
          "costDetails": {},
          "endTime": null,
          "model": null,
          "modelParameters": null,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null,
          "latency": null
        }
      ],
      "createdAt": "2025-09-24T10:03:42.961Z",
      "environment": "default",
      "updatedAt": "2025-09-24T10:03:59.577Z"
    },
    {
      "id": "f8bbeb36",
      "timestamp": "2025-09-24T10:01:57.016000+00:00",
      "name": "f8bb_model_converter",
      "input": null,
      "output": {
        "status": "success",
        "code": "import tensorflow as tf\n\n# Load the original model\noriginal_model_path = \"models/fruit_to_emoji/og_model/model.keras\"\nmodel = tf.keras.models.load_model(original_model_path)\n\ndef representative_data_gen():\n    # Convert the input shape tuple to a list\n    input_shape = [1] + list(model.input_shape[1:])\n    \n    for _ in range(100):  # Generate 100 samples\n        input_data = tf.random.normal(input_shape, dtype=tf.float32)\n        yield [input_data]\n\n# Initialize the TFLite converter\nconverter = tf.lite.TFLiteConverter.from_keras_model(model)\n\n# Set up quantization\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\nconverter.representative_dataset = representative_data_gen\n\n# Ensure that if any ops can't be quantized, the converter throws an error\nconverter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n\n# Set the input and output tensors to int8 (only the input and output need to be specified)\nconverter.inference_input_type = tf.int8\nconverter.inference_output_type = tf.uint8  # Change to uint8 as per your requirement\n\n# Convert the model\ntflite_quant_model = converter.convert()\n\n# Save the quantized model to a file\nconverted_model_path = \"models/fruit_to_emoji/tflite_model/model_quant.tflite\"\nwith open(converted_model_path, 'wb') as f:\n    f.write(tflite_quant_model)"
      },
      "session_id": "qwen2.5-coder:32b_7e6b_mc_batch",
      "metadata": {
        "num_run": 15
      },
      "tags": [
        "benchmark",
        "model_converter",
        "qwen2.5-coder:32b"
      ],
      "latency": 76.14,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-13-01-57-019096_chatcmpl-2b7e9f06-5f6a-4fb4-bfb6-bbe99c9858eb",
          "traceId": "f8bbeb36",
          "type": "GENERATION",
          "name": "f8_mc_conversion_code_gen",
          "startTime": "2025-09-24T10:01:57.019000+00:00",
          "endTime": "2025-09-24T10:02:09.223000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 12204.0,
          "promptTokens": 451,
          "totalTokens": 787,
          "completionTokens": 336,
          "environment": "default",
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-13-02-12-801383_chatcmpl-abc1d9bf-a300-440e-a3c0-c72b569d5aaf",
          "traceId": "f8bbeb36",
          "type": "GENERATION",
          "name": "f8_mc_error_handling_attempt#2",
          "startTime": "2025-09-24T10:02:12.801000+00:00",
          "endTime": "2025-09-24T10:02:28.951000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 16150.0,
          "promptTokens": 2048,
          "totalTokens": 2568,
          "completionTokens": 520,
          "environment": "default",
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-13-02-28-958435_chatcmpl-f8a148bf-1301-4bcf-9584-27aa8bdb5f59",
          "traceId": "f8bbeb36",
          "type": "GENERATION",
          "name": "f8_mc_error_handling_attempt#3",
          "startTime": "2025-09-24T10:02:28.958000+00:00",
          "endTime": "2025-09-24T10:02:38.616000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 9658.0,
          "promptTokens": 857,
          "totalTokens": 1193,
          "completionTokens": 336,
          "environment": "default",
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-13-02-42-133472_chatcmpl-327efbf6-9fce-4e4f-a8a3-83150a5a2767",
          "traceId": "f8bbeb36",
          "type": "GENERATION",
          "name": "f8_mc_error_handling_attempt#4",
          "startTime": "2025-09-24T10:02:42.133000+00:00",
          "endTime": "2025-09-24T10:02:55.737000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 13604.0,
          "promptTokens": 2048,
          "totalTokens": 2473,
          "completionTokens": 425,
          "environment": "default",
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-13-02-58-683585_chatcmpl-bdcc4f57-4eb6-4c5f-85ab-7b710005ca63",
          "traceId": "f8bbeb36",
          "type": "GENERATION",
          "name": "f8_mc_error_handling_attempt#5",
          "startTime": "2025-09-24T10:02:58.683000+00:00",
          "endTime": "2025-09-24T10:03:13.157000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 14474.0,
          "promptTokens": 1400,
          "totalTokens": 1887,
          "completionTokens": 487,
          "environment": "default",
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "648ea393-55ab-454d-8dab-84261b3c4182",
          "traceId": "f8bbeb36",
          "type": "SPAN",
          "name": "start_model_converter",
          "startTime": "2025-09-24T10:01:57.017000+00:00",
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "promptTokens": 0,
          "totalTokens": 0,
          "completionTokens": 0,
          "environment": "default",
          "costDetails": {},
          "endTime": null,
          "model": null,
          "modelParameters": null,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null,
          "latency": null
        }
      ],
      "createdAt": "2025-09-24T10:01:57.539Z",
      "environment": "default",
      "updatedAt": "2025-09-24T10:03:21.646Z"
    },
    {
      "id": "9369465f",
      "timestamp": "2025-09-24T10:00:36.418000+00:00",
      "name": "9369_model_converter",
      "input": null,
      "output": {
        "status": "success",
        "code": "import tensorflow as tf\nimport numpy as np\n\n# Define input shape based on your model's expected input dimensions\n# Example: input_shape = (height, width, channels)\ninput_shape = (224, 224, 3)  # Replace with the actual input shape of your model\n\n# Load the original Keras model\nmodel = tf.keras.models.load_model(\"models/fruit_to_emoji/og_model/model.keras\")\n\n# Define conversion parameters for quantization\nconverter = tf.lite.TFLiteConverter.from_keras_model(model)\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\n\n# Define tensor types and supported operations for quantization\ninference_input_type = tf.float32\ninference_output_type = tf.uint8\nsupported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8, tf.lite.OpsSet.SELECT_TF_OPS]\n\n# Set the input and output tensors to float32 and uint8 respectively\nconverter.inference_input_type = inference_input_type\nconverter.inference_output_type = inference_output_type\n\n# Define a representative dataset generator function for calibration\ndef representative_data_gen():\n    # Replace this with your actual data loading logic\n    for _ in range(100):\n        yield [np.random.uniform(low=0.0, high=1.0, size=(1, *input_shape)).astype(np.float32)]\n\n# Set the representative dataset for calibration\nconverter.representative_dataset = representative_data_gen\n\n# Ensure that if any ops can't be quantized, they are still converted to TFLite\nconverter.target_spec.supported_ops = supported_ops\n\n# Convert the model\ntflite_quant_model = converter.convert()\n\n# Save the converted and quantized model\nwith open(\"models/fruit_to_emoji/tflite_model/model_quant.tflite\", \"wb\") as f:\n    f.write(tflite_quant_model)"
      },
      "session_id": "qwen2.5-coder:32b_7e6b_mc_batch",
      "metadata": {
        "num_run": 14
      },
      "tags": [
        "benchmark",
        "model_converter",
        "qwen2.5-coder:32b"
      ],
      "latency": 49.326,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-13-00-36-419926_chatcmpl-589d6c52-0cd8-4442-af8f-96b2b7c8b1de",
          "traceId": "9369465f",
          "type": "GENERATION",
          "name": "93_mc_conversion_code_gen",
          "startTime": "2025-09-24T10:00:36.419000+00:00",
          "endTime": "2025-09-24T10:00:47.360000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 10941.0,
          "promptTokens": 451,
          "totalTokens": 808,
          "completionTokens": 357,
          "environment": "default",
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-13-00-51-475795_chatcmpl-89c28328-14fa-4da9-8a9e-819615be70d9",
          "traceId": "9369465f",
          "type": "GENERATION",
          "name": "93_mc_error_handling_attempt#2",
          "startTime": "2025-09-24T10:00:51.475000+00:00",
          "endTime": "2025-09-24T10:01:10.320000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 18845.0,
          "promptTokens": 2048,
          "totalTokens": 2568,
          "completionTokens": 520,
          "environment": "default",
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-13-01-10-323020_chatcmpl-5113fb29-3eb5-43d8-8f78-b0267424a684",
          "traceId": "9369465f",
          "type": "GENERATION",
          "name": "93_mc_error_handling_attempt#3",
          "startTime": "2025-09-24T10:01:10.323000+00:00",
          "endTime": "2025-09-24T10:01:25.744000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 15421.0,
          "promptTokens": 878,
          "totalTokens": 1379,
          "completionTokens": 501,
          "environment": "default",
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "cebfc457-dfca-456e-a250-d5f5955f5ff6",
          "traceId": "9369465f",
          "type": "SPAN",
          "name": "start_model_converter",
          "startTime": "2025-09-24T10:00:36.418000+00:00",
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "promptTokens": 0,
          "totalTokens": 0,
          "completionTokens": 0,
          "environment": "default",
          "costDetails": {},
          "endTime": null,
          "model": null,
          "modelParameters": null,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null,
          "latency": null
        }
      ],
      "createdAt": "2025-09-24T10:00:36.943Z",
      "environment": "default",
      "updatedAt": "2025-09-24T10:01:36.060Z"
    },
    {
      "id": "92159950",
      "timestamp": "2025-09-24T09:59:27+00:00",
      "name": "9215_model_converter",
      "input": null,
      "output": {
        "status": "success",
        "code": "import tensorflow as tf\nimport numpy as np\n\n# Load the original Keras model\noriginal_model = tf.keras.models.load_model(\"models/fruit_to_emoji/og_model/model.keras\")\n\n# Define conversion parameters for 8-bit integer quantization\nconverter = tf.lite.TFLiteConverter.from_keras_model(original_model)\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\n\n# Define the data types for quantization\ninference_input_type = tf.float32\ninference_output_type = tf.uint8\n\n# Ensure that if any ops can't be quantized, they fall back to float\nconverter.target_spec.supported_ops = [\n    tf.lite.OpsSet.TFLITE_BUILTINS_INT8,\n    tf.lite.OpsSet.TFLITE_BUILTINS\n]\n\n# Define a representative dataset generator function\ndef representative_data_gen():\n    # Replace 'input_shape' with the actual shape of your model's input\n    input_shape = (1, 224, 224, 3)  # Example shape for an image input\n    for _ in range(100):\n        # Get sample input data as a numpy array in a method of your choosing.\n        yield [np.array(np.random.random_sample(input_shape), dtype=np.float32)]\n\nconverter.representative_dataset = representative_data_gen\n\n# Set the input and output tensors to int8 (APIs added in r2.3)\nconverter.inference_input_type = inference_input_type\nconverter.inference_output_type = inference_output_type\n\n# Convert the model\ntflite_quant_model = converter.convert()\n\n# Save the quantized TFLite model to disk\nwith open(\"models/fruit_to_emoji/tflite_model/model_quant.tflite\", 'wb') as f:\n    f.write(tflite_quant_model)"
      },
      "session_id": "qwen2.5-coder:32b_7e6b_mc_batch",
      "metadata": {
        "num_run": 13
      },
      "tags": [
        "benchmark",
        "model_converter",
        "qwen2.5-coder:32b"
      ],
      "latency": 38.841,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-12-59-27-002325_chatcmpl-6b070cda-900e-444e-98d9-f799e76ae21d",
          "traceId": "92159950",
          "type": "GENERATION",
          "name": "92_mc_conversion_code_gen",
          "startTime": "2025-09-24T09:59:27.002000+00:00",
          "endTime": "2025-09-24T09:59:38.683000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 11681.0,
          "promptTokens": 451,
          "totalTokens": 784,
          "completionTokens": 333,
          "environment": "default",
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-12-59-42-235017_chatcmpl-81392e34-9caa-4a43-9992-0e767d0884f2",
          "traceId": "92159950",
          "type": "GENERATION",
          "name": "92_mc_error_handling_attempt#2",
          "startTime": "2025-09-24T09:59:42.235000+00:00",
          "endTime": "2025-09-24T09:59:53.870000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 11635.0,
          "promptTokens": 2048,
          "totalTokens": 2311,
          "completionTokens": 263,
          "environment": "default",
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-12-59-53-876636_chatcmpl-fa60eb61-ffb7-4906-bbb6-9b26735b10e7",
          "traceId": "92159950",
          "type": "GENERATION",
          "name": "92_mc_error_handling_attempt#3",
          "startTime": "2025-09-24T09:59:53.876000+00:00",
          "endTime": "2025-09-24T10:00:05.841000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 11965.0,
          "promptTokens": 854,
          "totalTokens": 1276,
          "completionTokens": 422,
          "environment": "default",
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "78de46d5-6e60-401c-9fe0-95ed6cb9163f",
          "traceId": "92159950",
          "type": "SPAN",
          "name": "start_model_converter",
          "startTime": "2025-09-24T09:59:27+00:00",
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "promptTokens": 0,
          "totalTokens": 0,
          "completionTokens": 0,
          "environment": "default",
          "costDetails": {},
          "endTime": null,
          "model": null,
          "modelParameters": null,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null,
          "latency": null
        }
      ],
      "createdAt": "2025-09-24T09:59:27.521Z",
      "environment": "default",
      "updatedAt": "2025-09-24T10:00:15.670Z"
    },
    {
      "id": "379dd1c4",
      "timestamp": "2025-09-24T09:58:16.451000+00:00",
      "name": "379d_model_converter",
      "input": null,
      "output": {
        "status": "success",
        "code": "import tensorflow as tf\nimport numpy as np\n\n# Load the original Keras model\noriginal_model = tf.keras.models.load_model(\"models/fruit_to_emoji/og_model/model.keras\")\n\n# Define conversion parameters for quantization\nconverter = tf.lite.TFLiteConverter.from_keras_model(original_model)\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\n\n# Define the data type for input and output tensors\ninference_input_type = tf.float32\ninference_output_type = tf.uint8\n\n# Ensure that if any ops can't be quantized, the converter throws an error\nconverter.target_spec.supported_ops = [\n    tf.lite.OpsSet.TFLITE_BUILTINS_INT8,\n    tf.lite.OpsSet.TFLITE_BUILTINS\n]\n\n# Define a generator function to provide calibration data\ndef representative_data_gen():\n    for _ in range(100):\n        # Replace this with your actual input data generator logic\n        yield [np.random.uniform(low=0, high=255, size=(1, 64, 64, 3)).astype(np.float32)]\n\n# Set the representative dataset for quantization\nconverter.representative_dataset = representative_data_gen\n\n# Ensure that if any ops can't be quantized, the converter throws an error\nconverter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n\n# Set the input and output tensors to int8 (except the first layer which stays float32)\nconverter.inference_input_type = inference_input_type\nconverter.inference_output_type = inference_output_type\n\n# Convert the model\ntflite_quant_model = converter.convert()\n\n# Save the quantized TFLite model\nwith open(\"models/fruit_to_emoji/tflite_model/model_quant.tflite\", \"wb\") as f:\n    f.write(tflite_quant_model)"
      },
      "session_id": "qwen2.5-coder:32b_7e6b_mc_batch",
      "metadata": {
        "num_run": 12
      },
      "tags": [
        "benchmark",
        "model_converter",
        "qwen2.5-coder:32b"
      ],
      "latency": 41.024,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-12-58-16-452545_chatcmpl-c38ca2b8-9e8e-49ed-82e8-2e2dc134599e",
          "traceId": "379dd1c4",
          "type": "GENERATION",
          "name": "37_mc_conversion_code_gen",
          "startTime": "2025-09-24T09:58:16.452000+00:00",
          "endTime": "2025-09-24T09:58:26.962000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 10510.0,
          "promptTokens": 451,
          "totalTokens": 847,
          "completionTokens": 396,
          "environment": "default",
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-12-58-30-418484_chatcmpl-20f7e535-3b3a-490b-9676-116577feeef2",
          "traceId": "379dd1c4",
          "type": "GENERATION",
          "name": "37_mc_error_handling_attempt#2",
          "startTime": "2025-09-24T09:58:30.418000+00:00",
          "endTime": "2025-09-24T09:58:40.636000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 10218.0,
          "promptTokens": 2048,
          "totalTokens": 2346,
          "completionTokens": 298,
          "environment": "default",
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-12-58-40-639553_chatcmpl-a7ee7ef2-4c08-4eab-a099-1b94ce5e3cae",
          "traceId": "379dd1c4",
          "type": "GENERATION",
          "name": "37_mc_error_handling_attempt#3",
          "startTime": "2025-09-24T09:58:40.639000+00:00",
          "endTime": "2025-09-24T09:58:57.475000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 16836.0,
          "promptTokens": 917,
          "totalTokens": 1495,
          "completionTokens": 578,
          "environment": "default",
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "b0b401b5-7b35-4955-85d2-ff5e887d54fd",
          "traceId": "379dd1c4",
          "type": "SPAN",
          "name": "start_model_converter",
          "startTime": "2025-09-24T09:58:16.451000+00:00",
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "promptTokens": 0,
          "totalTokens": 0,
          "completionTokens": 0,
          "environment": "default",
          "costDetails": {},
          "endTime": null,
          "model": null,
          "modelParameters": null,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null,
          "latency": null
        }
      ],
      "createdAt": "2025-09-24T09:58:16.975Z",
      "environment": "default",
      "updatedAt": "2025-09-24T09:59:06.627Z"
    },
    {
      "id": "f519b02b",
      "timestamp": "2025-09-24T09:57:14.041000+00:00",
      "name": "f519_model_converter",
      "input": null,
      "output": {
        "status": "success",
        "code": "import tensorflow as tf\nimport numpy as np\n\n# Load the original Keras model\noriginal_model = tf.keras.models.load_model(\"models/fruit_to_emoji/og_model/model.keras\")\n\n# Define conversion parameters for quantization\nconverter = tf.lite.TFLiteConverter.from_keras_model(original_model)\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\n\n# Define representative dataset generator function\ndef representative_dataset_gen():\n    for _ in range(100):\n        # Get sample input data as a numpy array in a method of your choosing.\n        # Here, we assume the input shape is (1, 224, 224, 3) and dtype is float32\n        yield [np.array(np.random.random_sample((1, 224, 224, 3)), dtype=np.float32)]\n\n# Set the representative dataset for quantization\nconverter.representative_dataset = representative_dataset_gen\n\n# Ensure that if any ops can't be quantized, the converter throws an error\nconverter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\nconverter.inference_input_type = tf.float32  # Input type is float32\nconverter.inference_output_type = tf.uint8    # Output type is uint8\n\n# Convert the model to the TensorFlow Lite format with quantization\ntflite_model_quant = converter.convert()\n\n# Save the converted model\nwith open(\"models/fruit_to_emoji/tflite_model/model_quant.tflite\", 'wb') as f:\n    f.write(tflite_model_quant)"
      },
      "session_id": "qwen2.5-coder:32b_7e6b_mc_batch",
      "metadata": {
        "num_run": 11
      },
      "tags": [
        "benchmark",
        "model_converter",
        "qwen2.5-coder:32b"
      ],
      "latency": 32.522,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-12-57-14-042049_chatcmpl-d172c68f-a6e9-489e-bb40-97d46286c3fd",
          "traceId": "f519b02b",
          "type": "GENERATION",
          "name": "f5_mc_conversion_code_gen",
          "startTime": "2025-09-24T09:57:14.042000+00:00",
          "endTime": "2025-09-24T09:57:22.997000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 8955.0,
          "promptTokens": 451,
          "totalTokens": 779,
          "completionTokens": 328,
          "environment": "default",
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-12-57-26-586138_chatcmpl-847cbdf3-af6f-4ff0-960d-a9c05dc20638",
          "traceId": "f519b02b",
          "type": "GENERATION",
          "name": "f5_mc_error_handling_attempt#2",
          "startTime": "2025-09-24T09:57:26.586000+00:00",
          "endTime": "2025-09-24T09:57:35.175000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 8589.0,
          "promptTokens": 2048,
          "totalTokens": 2290,
          "completionTokens": 242,
          "environment": "default",
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-12-57-35-178264_chatcmpl-abb17a3e-9412-4ebf-8226-e31ebcf67875",
          "traceId": "f519b02b",
          "type": "GENERATION",
          "name": "f5_mc_error_handling_attempt#3",
          "startTime": "2025-09-24T09:57:35.178000+00:00",
          "endTime": "2025-09-24T09:57:46.563000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 11385.0,
          "promptTokens": 849,
          "totalTokens": 1191,
          "completionTokens": 342,
          "environment": "default",
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "453db211-25bc-44cd-bd27-b92fd8fd3ce0",
          "traceId": "f519b02b",
          "type": "SPAN",
          "name": "start_model_converter",
          "startTime": "2025-09-24T09:57:14.041000+00:00",
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "promptTokens": 0,
          "totalTokens": 0,
          "completionTokens": 0,
          "environment": "default",
          "costDetails": {},
          "endTime": null,
          "model": null,
          "modelParameters": null,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null,
          "latency": null
        }
      ],
      "createdAt": "2025-09-24T09:57:14.565Z",
      "environment": "default",
      "updatedAt": "2025-09-24T09:57:56.169Z"
    },
    {
      "id": "fb6daf08",
      "timestamp": "2025-09-24T09:56:03.512000+00:00",
      "name": "fb6d_model_converter",
      "input": null,
      "output": {
        "status": "success",
        "code": "import tensorflow as tf\n\n# Load the original Keras model\noriginal_model = tf.keras.models.load_model(\"models/fruit_to_emoji/og_model/model.keras\")\n\n# Define converter\nconverter = tf.lite.TFLiteConverter.from_keras_model(original_model)\n\n# Set quantization parameters\ninference_input_type = tf.float32\ninference_output_type = tf.uint8\n\n# Define representative dataset generator\ndef representative_data_gen():\n    for _ in range(100):\n        # Generate a random input data of the same shape as your model's expected input\n        data = tf.random.normal([1, 128, 128, 3], dtype=tf.float32)\n        yield [data]\n\n# Set the optimization flag to optimize for size (which enables quantization)\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\nconverter.representative_dataset = representative_data_gen\nconverter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\nconverter.inference_input_type = inference_input_type  # or tf.float32\nconverter.inference_output_type = inference_output_type  # or tf.uint8\n\n# Convert the model to the TensorFlow Lite format with quantization\ntflite_model_quant = converter.convert()\n\n# Save the quantized model to a file\nwith open(\"models/fruit_to_emoji/tflite_model/model_quant.tflite\", \"wb\") as f:\n    f.write(tflite_model_quant)"
      },
      "session_id": "qwen2.5-coder:32b_7e6b_mc_batch",
      "metadata": {
        "num_run": 10
      },
      "tags": [
        "benchmark",
        "model_converter",
        "qwen2.5-coder:32b"
      ],
      "latency": 40.933,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-12-56-03-513768_chatcmpl-11fcaeb4-7311-4343-9129-aeb793cca845",
          "traceId": "fb6daf08",
          "type": "GENERATION",
          "name": "fb_mc_conversion_code_gen",
          "startTime": "2025-09-24T09:56:03.513000+00:00",
          "endTime": "2025-09-24T09:56:12.434000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 8921.0,
          "promptTokens": 451,
          "totalTokens": 791,
          "completionTokens": 340,
          "environment": "default",
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-12-56-16-183033_chatcmpl-4c591cd7-9c59-49b9-adba-023e0b3962c6",
          "traceId": "fb6daf08",
          "type": "GENERATION",
          "name": "fb_mc_error_handling_attempt#2",
          "startTime": "2025-09-24T09:56:16.183000+00:00",
          "endTime": "2025-09-24T09:56:35.347000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 19164.0,
          "promptTokens": 2048,
          "totalTokens": 2680,
          "completionTokens": 632,
          "environment": "default",
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-12-56-35-354586_chatcmpl-8a1d0ad3-3a83-4357-9068-33a9726890a5",
          "traceId": "fb6daf08",
          "type": "GENERATION",
          "name": "fb_mc_error_handling_attempt#3",
          "startTime": "2025-09-24T09:56:35.354000+00:00",
          "endTime": "2025-09-24T09:56:44.445000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 9091.0,
          "promptTokens": 861,
          "totalTokens": 1177,
          "completionTokens": 316,
          "environment": "default",
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "fdcfb723-c47a-4d49-b713-4e77695bc8de",
          "traceId": "fb6daf08",
          "type": "SPAN",
          "name": "start_model_converter",
          "startTime": "2025-09-24T09:56:03.512000+00:00",
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "promptTokens": 0,
          "totalTokens": 0,
          "completionTokens": 0,
          "environment": "default",
          "costDetails": {},
          "endTime": null,
          "model": null,
          "modelParameters": null,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null,
          "latency": null
        }
      ],
      "createdAt": "2025-09-24T09:56:04.038Z",
      "environment": "default",
      "updatedAt": "2025-09-24T09:56:53.743Z"
    },
    {
      "id": "f7942cb2",
      "timestamp": "2025-09-24T09:55:21.056000+00:00",
      "name": "f794_model_converter",
      "input": null,
      "output": {
        "status": "success",
        "code": "import tensorflow as tf\n\n# Load the original Keras model\noriginal_model = tf.keras.models.load_model(\"models/fruit_to_emoji/og_model/model.keras\")\n\n# Define conversion parameters for quantization\nconverter = tf.lite.TFLiteConverter.from_keras_model(original_model)\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\n\n# Define the data type requirements for quantization\ninference_input_type = tf.float32\ninference_output_type = tf.uint8\n\n# Ensure that if any ops can't be quantized, the converter throws an error\nconverter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\nconverter.inference_input_type = inference_input_type\nconverter.inference_output_type = inference_output_type\n\n# Function to provide a representative dataset for calibration\ndef representative_data_gen():\n    # Assuming you have a function `load_calibration_data()` that returns a generator of input data\n    # This is a placeholder and should be replaced with actual data loading logic\n    for _ in range(100):\n        yield [tf.random.normal([1, 299, 299, 3], dtype=tf.float32)]\n\n# Set the representative dataset for quantization calibration\nconverter.representative_dataset = representative_data_gen\n\n# Convert the model to TFLite format with quantization\ntflite_quant_model = converter.convert()\n\n# Save the converted model\nwith open(\"models/fruit_to_emoji/tflite_model/model_quant.tflite\", \"wb\") as f:\n    f.write(tflite_quant_model)"
      },
      "session_id": "qwen2.5-coder:32b_7e6b_mc_batch",
      "metadata": {
        "num_run": 9
      },
      "tags": [
        "benchmark",
        "model_converter",
        "qwen2.5-coder:32b"
      ],
      "latency": 11.791,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-12-55-21-058993_chatcmpl-e40e8304-f0cd-4c3c-bf6d-dd769be9852b",
          "traceId": "f7942cb2",
          "type": "GENERATION",
          "name": "f7_mc_conversion_code_gen",
          "startTime": "2025-09-24T09:55:21.058000+00:00",
          "endTime": "2025-09-24T09:55:32.848000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 11790.0,
          "promptTokens": 451,
          "totalTokens": 788,
          "completionTokens": 337,
          "environment": "default",
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "05ce0f68-d3da-41f8-8ff9-2336f4edb694",
          "traceId": "f7942cb2",
          "type": "SPAN",
          "name": "start_model_converter",
          "startTime": "2025-09-24T09:55:21.057000+00:00",
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "promptTokens": 0,
          "totalTokens": 0,
          "completionTokens": 0,
          "environment": "default",
          "costDetails": {},
          "endTime": null,
          "model": null,
          "modelParameters": null,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null,
          "latency": null
        }
      ],
      "createdAt": "2025-09-24T09:55:21.582Z",
      "environment": "default",
      "updatedAt": "2025-09-24T09:55:43.186Z"
    },
    {
      "id": "3c0895c2",
      "timestamp": "2025-09-24T09:53:56.327000+00:00",
      "name": "3c08_model_converter",
      "input": null,
      "output": {
        "status": "success",
        "code": "import tensorflow as tf\n\n# Load the original Keras model\noriginal_model_path = \"models/fruit_to_emoji/og_model/model.keras\"\nyour_keras_model = tf.keras.models.load_model(original_model_path)\n\n# Load or define your calibration dataset\n# For demonstration, let's assume `calibration_dataset` is a TensorFlow Dataset object.\n# Replace this with your actual data loading code.\ncalibration_data = tf.data.Dataset.from_tensor_slices([\n    # Add representative samples here. For example:\n    tf.random.normal((1, 224, 224, 3)),  # Assuming input shape is (None, 224, 224, 3)\n    tf.random.normal((1, 224, 224, 3)),\n    tf.random.normal((1, 224, 224, 3))\n])\n\ndef representative_dataset_gen():\n    # Get one sample at a time from the calibration dataset\n    for input_value in calibration_data:\n        # The model expects data in a specific format and scale.\n        # Here we assume the model takes inputs of type `float32` with values in [0, 1].\n        yield [input_value]\n\n# Convert your Keras model to TFLite format\nconverter = tf.lite.TFLiteConverter.from_keras_model(your_keras_model)\n\n# Set optimization flag\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\n\n# Ensure that if any ops can't be quantized, the converter throws an error\nconverter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\nconverter.inference_input_type = tf.uint8  # As per requirement\nconverter.inference_output_type = tf.uint8  # As per requirement\n\n# Set the representative dataset generator function\nconverter.representative_dataset = representative_dataset_gen\n\n# Convert to TFLite model and save it to a file\ntflite_quant_model = converter.convert()\nconverted_model_path = \"models/fruit_to_emoji/tflite_model/model_quant.tflite\"\nwith open(converted_model_path, 'wb') as f:\n    f.write(tflite_quant_model)"
      },
      "session_id": "qwen2.5-coder:32b_7e6b_mc_batch",
      "metadata": {
        "num_run": 8
      },
      "tags": [
        "benchmark",
        "model_converter",
        "qwen2.5-coder:32b"
      ],
      "latency": 55.288,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-12-53-56-329180_chatcmpl-e5ada262-1d5e-4ac5-88f4-b5ec3d676ec3",
          "traceId": "3c0895c2",
          "type": "GENERATION",
          "name": "3c_mc_conversion_code_gen",
          "startTime": "2025-09-24T09:53:56.329000+00:00",
          "endTime": "2025-09-24T09:54:07.374000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 11045.0,
          "promptTokens": 451,
          "totalTokens": 722,
          "completionTokens": 271,
          "environment": "default",
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-12-54-11-187785_chatcmpl-29e04422-39d3-4e96-80f6-bff4d97b95b2",
          "traceId": "3c0895c2",
          "type": "GENERATION",
          "name": "3c_mc_error_handling_attempt#2",
          "startTime": "2025-09-24T09:54:11.187000+00:00",
          "endTime": "2025-09-24T09:54:34.304000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 23117.0,
          "promptTokens": 2048,
          "totalTokens": 2705,
          "completionTokens": 657,
          "environment": "default",
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-12-54-37-321828_chatcmpl-8e269eb0-ce4e-4a19-9dc6-cbe1d2f318e5",
          "traceId": "3c0895c2",
          "type": "GENERATION",
          "name": "3c_mc_error_handling_attempt#3",
          "startTime": "2025-09-24T09:54:37.321000+00:00",
          "endTime": "2025-09-24T09:54:51.615000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 14294.0,
          "promptTokens": 1540,
          "totalTokens": 2009,
          "completionTokens": 469,
          "environment": "default",
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "5aaac072-251a-4959-a5ea-4c2854e016d7",
          "traceId": "3c0895c2",
          "type": "SPAN",
          "name": "start_model_converter",
          "startTime": "2025-09-24T09:53:56.327000+00:00",
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "promptTokens": 0,
          "totalTokens": 0,
          "completionTokens": 0,
          "environment": "default",
          "costDetails": {},
          "endTime": null,
          "model": null,
          "modelParameters": null,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null,
          "latency": null
        }
      ],
      "createdAt": "2025-09-24T09:53:56.851Z",
      "environment": "default",
      "updatedAt": "2025-09-24T09:55:00.536Z"
    },
    {
      "id": "82256b4e",
      "timestamp": "2025-09-24T09:52:41.313000+00:00",
      "name": "8225_model_converter",
      "input": null,
      "output": {
        "status": "success",
        "code": "import tensorflow as tf\nimport numpy as np\n\n# Load the original Keras model\nmodel = tf.keras.models.load_model(\"models/fruit_to_emoji/og_model/model.keras\")\n\n# Define conversion parameters for quantization\nconverter = tf.lite.TFLiteConverter.from_keras_model(model)\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\n\n# Define data types for quantization\ninference_input_type = tf.float32\ninference_output_type = tf.uint8\n\n# Ensure that if any ops can't be quantized, the converter throws an error\nconverter.target_spec.supported_ops = [\n    tf.lite.OpsSet.TFLITE_BUILTINS_INT8,\n    tf.lite.OpsSet.TFLITE_BUILTINS\n]\n\n# Define a representative dataset generator function\ndef representative_dataset_gen():\n    for _ in range(100):\n        # Get sample input data as a numpy array in a method of your choosing.\n        # Here, we assume the model takes an input of shape (1, 224, 224, 3)\n        input_data = np.random.rand(1, 224, 224, 3).astype(np.float32)\n        yield [input_data]\n\n# Set the representative dataset for quantization\nconverter.representative_dataset = representative_dataset_gen\n\n# Ensure that if any ops can't be quantized, the converter throws an error\nconverter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\nconverter.inference_input_type = inference_input_type  # or tf.float32\nconverter.inference_output_type = inference_output_type  # or tf.uint8\n\n# Convert the model to the TensorFlow Lite format with quantization\ntflite_model_quant = converter.convert()\n\n# Save the converted model to a file\nwith open(\"models/fruit_to_emoji/tflite_model/model_quant.tflite\", 'wb') as f:\n    f.write(tflite_model_quant)"
      },
      "session_id": "qwen2.5-coder:32b_7e6b_mc_batch",
      "metadata": {
        "num_run": 7
      },
      "tags": [
        "benchmark",
        "model_converter",
        "qwen2.5-coder:32b"
      ],
      "latency": 43.52,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-12-52-41-315267_chatcmpl-6495ab23-48da-4996-9f63-83c3582fd527",
          "traceId": "82256b4e",
          "type": "GENERATION",
          "name": "82_mc_conversion_code_gen",
          "startTime": "2025-09-24T09:52:41.315000+00:00",
          "endTime": "2025-09-24T09:52:51.673000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 10358.0,
          "promptTokens": 451,
          "totalTokens": 821,
          "completionTokens": 370,
          "environment": "default",
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-12-52-55-106113_chatcmpl-a2877162-b523-4b50-8687-af477cda042e",
          "traceId": "82256b4e",
          "type": "GENERATION",
          "name": "82_mc_error_handling_attempt#2",
          "startTime": "2025-09-24T09:52:55.106000+00:00",
          "endTime": "2025-09-24T09:53:12.787000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 17681.0,
          "promptTokens": 2048,
          "totalTokens": 2610,
          "completionTokens": 562,
          "environment": "default",
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-12-53-12-796463_chatcmpl-504c290b-9e4a-4f6e-8b7e-2aebf50f7c15",
          "traceId": "82256b4e",
          "type": "GENERATION",
          "name": "82_mc_error_handling_attempt#3",
          "startTime": "2025-09-24T09:53:12.796000+00:00",
          "endTime": "2025-09-24T09:53:24.833000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 12037.0,
          "promptTokens": 891,
          "totalTokens": 1316,
          "completionTokens": 425,
          "environment": "default",
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "141fc6a0-e429-4f5a-9e77-e8c7bd61a59e",
          "traceId": "82256b4e",
          "type": "SPAN",
          "name": "start_model_converter",
          "startTime": "2025-09-24T09:52:41.313000+00:00",
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "promptTokens": 0,
          "totalTokens": 0,
          "completionTokens": 0,
          "environment": "default",
          "costDetails": {},
          "endTime": null,
          "model": null,
          "modelParameters": null,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null,
          "latency": null
        }
      ],
      "createdAt": "2025-09-24T09:52:41.836Z",
      "environment": "default",
      "updatedAt": "2025-09-24T09:53:35.524Z"
    },
    {
      "id": "addb8e4f",
      "timestamp": "2025-09-24T09:51:14.949000+00:00",
      "name": "addb_model_converter",
      "input": null,
      "output": {
        "status": "success",
        "code": "import tensorflow as tf\nimport numpy as np\n\n# Load the original Keras model\noriginal_model = tf.keras.models.load_model(\"models/fruit_to_emoji/og_model/model.keras\")\n\n# Define conversion parameters for quantization\nconverter = tf.lite.TFLiteConverter.from_keras_model(original_model)\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\n\n# Define the input and output tensor types for quantization\ninference_input_type = tf.float32\ninference_output_type = tf.uint8\n\n# Ensure that if any ops can't be quantized, they are still included as fp32\nconverter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8, tf.lite.OpsSet.TFLITE_BUILTINS]\n\n# Define a generator function for calibration\ndef representative_data_gen():\n    # Replace with your actual data loading and preprocessing logic\n    for _ in range(100):\n        input_data = np.array(np.random.random_sample((1, 224, 224, 3)), dtype=np.float32)\n        yield [input_data]\n\n# Set the representative dataset for calibration\nconverter.representative_dataset = representative_data_gen\n\n# Set the input and output types for quantization\nconverter.inference_input_type = inference_input_type  # Set the input tensor type to float32\nconverter.inference_output_type = inference_output_type  # Set the output tensor type to uint8\n\n# Convert the model\ntflite_quant_model = converter.convert()\n\n# Save the quantized TFLite model\nwith open(\"models/fruit_to_emoji/tflite_model/model_quant.tflite\", \"wb\") as f:\n    f.write(tflite_quant_model)"
      },
      "session_id": "qwen2.5-coder:32b_7e6b_mc_batch",
      "metadata": {
        "num_run": 6
      },
      "tags": [
        "benchmark",
        "model_converter",
        "qwen2.5-coder:32b"
      ],
      "latency": 56.278,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-12-51-14-951427_chatcmpl-e221291d-dabf-41c3-a2a9-159220c908c4",
          "traceId": "addb8e4f",
          "type": "GENERATION",
          "name": "ad_mc_conversion_code_gen",
          "startTime": "2025-09-24T09:51:14.951000+00:00",
          "endTime": "2025-09-24T09:51:30.275000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 15324.0,
          "promptTokens": 451,
          "totalTokens": 849,
          "completionTokens": 398,
          "environment": "default",
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-12-51-33-654167_chatcmpl-8281bc23-2c9e-4697-899c-985a6c2a91c4",
          "traceId": "addb8e4f",
          "type": "GENERATION",
          "name": "ad_mc_error_handling_attempt#2",
          "startTime": "2025-09-24T09:51:33.654000+00:00",
          "endTime": "2025-09-24T09:51:57.418000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 23764.0,
          "promptTokens": 2048,
          "totalTokens": 2620,
          "completionTokens": 572,
          "environment": "default",
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-12-51-57-420545_chatcmpl-16bd255e-8c11-4072-a144-74f8f31f7142",
          "traceId": "addb8e4f",
          "type": "GENERATION",
          "name": "ad_mc_error_handling_attempt#3",
          "startTime": "2025-09-24T09:51:57.420000+00:00",
          "endTime": "2025-09-24T09:52:11.227000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 13807.0,
          "promptTokens": 919,
          "totalTokens": 1289,
          "completionTokens": 370,
          "environment": "default",
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "7f7ea9de-e6cc-48b9-addc-fd67f0264437",
          "traceId": "addb8e4f",
          "type": "SPAN",
          "name": "start_model_converter",
          "startTime": "2025-09-24T09:51:14.949000+00:00",
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "promptTokens": 0,
          "totalTokens": 0,
          "completionTokens": 0,
          "environment": "default",
          "costDetails": {},
          "endTime": null,
          "model": null,
          "modelParameters": null,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null,
          "latency": null
        }
      ],
      "createdAt": "2025-09-24T09:51:15.473Z",
      "environment": "default",
      "updatedAt": "2025-09-24T09:52:20.860Z"
    },
    {
      "id": "e50d8f2b",
      "timestamp": "2025-09-24T09:50:00.321000+00:00",
      "name": "e50d_model_converter",
      "input": null,
      "output": {
        "status": "success",
        "code": "import tensorflow as tf\n\n# Load the original Keras model\noriginal_model = tf.keras.models.load_model(\"models/fruit_to_emoji/og_model/model.keras\")\n\n# Define representative dataset generator function for quantization\ndef representative_dataset_gen():\n    # This is a placeholder. Replace with actual data loading logic.\n    # Assuming you have a test dataset, let's create a dummy one for demonstration purposes.\n    import numpy as np\n    \n    # Example: Generate random data similar to your input shape and type\n    # Replace (100, 28, 28, 3) with the actual shape of your input data\n    for _ in range(100):\n        data = np.random.rand(1, 28, 28, 3).astype(np.float32)\n        yield [data]\n\n# Convert the model to TensorFlow Lite format with 8-bit integer quantization\nconverter = tf.lite.TFLiteConverter.from_keras_model(original_model)\n\n# Define quantization parameters\ninference_input_type = tf.float32\ninference_output_type = tf.uint8\n\n# Set the optimization flag and specify the target spec for dynamic range quantization\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\nconverter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n\n# Ensure that if any ops can't be quantized, the converter throws an error\nconverter.target_spec.supported_types = [tf.int8]\nconverter.inference_input_type = inference_input_type  # or tf.float32\nconverter.inference_output_type = inference_output_type  # or tf.uint8\n\n# Provide a representative dataset to ensure we quantize correctly\nconverter.representative_dataset = representative_dataset_gen\n\n# Convert the model and save it to the specified path\ntflite_model = converter.convert()\nwith open(\"models/fruit_to_emoji/tflite_model/model_quant.tflite\", \"wb\") as f:\n    f.write(tflite_model)"
      },
      "session_id": "qwen2.5-coder:32b_7e6b_mc_batch",
      "metadata": {
        "num_run": 5
      },
      "tags": [
        "benchmark",
        "model_converter",
        "qwen2.5-coder:32b"
      ],
      "latency": 44.572,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-12-50-00-322812_chatcmpl-387f276f-39c4-4dba-aec8-3ac11b22eea0",
          "traceId": "e50d8f2b",
          "type": "GENERATION",
          "name": "e5_mc_conversion_code_gen",
          "startTime": "2025-09-24T09:50:00.322000+00:00",
          "endTime": "2025-09-24T09:50:09.969000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 9647.0,
          "promptTokens": 451,
          "totalTokens": 814,
          "completionTokens": 363,
          "environment": "default",
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-12-50-13-567822_chatcmpl-4a1c40b6-03c7-4687-ad85-c998b8685260",
          "traceId": "e50d8f2b",
          "type": "GENERATION",
          "name": "e5_mc_error_handling_attempt#2",
          "startTime": "2025-09-24T09:50:13.567000+00:00",
          "endTime": "2025-09-24T09:50:29.442000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 15875.0,
          "promptTokens": 2048,
          "totalTokens": 2560,
          "completionTokens": 512,
          "environment": "default",
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-12-50-29-448008_chatcmpl-bf071a8c-66fe-4a19-bf1d-3f522bbfb123",
          "traceId": "e50d8f2b",
          "type": "GENERATION",
          "name": "e5_mc_error_handling_attempt#3",
          "startTime": "2025-09-24T09:50:29.448000+00:00",
          "endTime": "2025-09-24T09:50:44.894000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 15446.0,
          "promptTokens": 884,
          "totalTokens": 1437,
          "completionTokens": 553,
          "environment": "default",
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "f673a25e-e385-4bdf-ac6d-52016cafaa76",
          "traceId": "e50d8f2b",
          "type": "SPAN",
          "name": "start_model_converter",
          "startTime": "2025-09-24T09:50:00.322000+00:00",
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "promptTokens": 0,
          "totalTokens": 0,
          "completionTokens": 0,
          "environment": "default",
          "costDetails": {},
          "endTime": null,
          "model": null,
          "modelParameters": null,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null,
          "latency": null
        }
      ],
      "createdAt": "2025-09-24T09:50:00.844Z",
      "environment": "default",
      "updatedAt": "2025-09-24T09:50:53.948Z"
    },
    {
      "id": "95bd9bb7",
      "timestamp": "2025-09-24T09:48:43.915000+00:00",
      "name": "95bd_model_converter",
      "input": null,
      "output": {
        "status": "success",
        "code": "import tensorflow as tf\nimport numpy as np\n\n# Load the original Keras model\noriginal_model = tf.keras.models.load_model(\"models/fruit_to_emoji/og_model/model.keras\")\n\n# Define quantization parameters\ninference_input_type = tf.float32\ninference_output_type = tf.uint8\nsupported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n\n# Convert the model to a TFLite model with quantization\nconverter = tf.lite.TFLiteConverter.from_keras_model(original_model)\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\nconverter.target_spec.supported_types = [tf.int8]\nconverter.inference_input_type = inference_input_type\nconverter.inference_output_type = inference_output_type\nconverter.target_spec.supported_ops = supported_ops\n\n# Define a representative dataset generator function\ndef representative_dataset_gen():\n    # Replace `sample_input_data` with actual input data from your dataset.\n    for _ in range(100):\n        sample_input_data = np.random.rand(1, 224, 224, 3)  # Example shape (batch_size, height, width, channels)\n        yield [np.array(sample_input_data, dtype=np.float32)]\n\n# Set the representative dataset for quantization\nconverter.representative_dataset = representative_dataset_gen\n\n# Convert the model to TFLite\nquantized_tflite_model = converter.convert()\n\n# Save the converted TFLite model\nwith open(\"models/fruit_to_emoji/tflite_model/model_quant.tflite\", \"wb\") as f:\n    f.write(quantized_tflite_model)"
      },
      "session_id": "qwen2.5-coder:32b_7e6b_mc_batch",
      "metadata": {
        "num_run": 4
      },
      "tags": [
        "benchmark",
        "model_converter",
        "qwen2.5-coder:32b"
      ],
      "latency": 45.81,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-12-48-43-917472_chatcmpl-b772a542-b8ba-4731-9bf4-8d2bed44f629",
          "traceId": "95bd9bb7",
          "type": "GENERATION",
          "name": "95_mc_conversion_code_gen",
          "startTime": "2025-09-24T09:48:43.917000+00:00",
          "endTime": "2025-09-24T09:48:53.884000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 9967.0,
          "promptTokens": 451,
          "totalTokens": 819,
          "completionTokens": 368,
          "environment": "default",
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-12-48-57-732044_chatcmpl-f36a8907-f599-43ff-b093-e77e9b6dcf7d",
          "traceId": "95bd9bb7",
          "type": "GENERATION",
          "name": "95_mc_error_handling_attempt#2",
          "startTime": "2025-09-24T09:48:57.732000+00:00",
          "endTime": "2025-09-24T09:49:19.749000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 22017.0,
          "promptTokens": 2048,
          "totalTokens": 2608,
          "completionTokens": 560,
          "environment": "default",
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-12-49-19-752020_chatcmpl-c014b400-200d-4eff-814a-b8f9762066c0",
          "traceId": "95bd9bb7",
          "type": "GENERATION",
          "name": "95_mc_error_handling_attempt#3",
          "startTime": "2025-09-24T09:49:19.752000+00:00",
          "endTime": "2025-09-24T09:49:29.725000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 9973.0,
          "promptTokens": 889,
          "totalTokens": 1237,
          "completionTokens": 348,
          "environment": "default",
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "8057d172-bbd3-4873-9ae9-7136b5065cac",
          "traceId": "95bd9bb7",
          "type": "SPAN",
          "name": "start_model_converter",
          "startTime": "2025-09-24T09:48:43.915000+00:00",
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "promptTokens": 0,
          "totalTokens": 0,
          "completionTokens": 0,
          "environment": "default",
          "costDetails": {},
          "endTime": null,
          "model": null,
          "modelParameters": null,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null,
          "latency": null
        }
      ],
      "createdAt": "2025-09-24T09:48:44.439Z",
      "environment": "default",
      "updatedAt": "2025-09-24T09:49:39.581Z"
    },
    {
      "id": "babe78c1",
      "timestamp": "2025-09-24T09:47:29.448000+00:00",
      "name": "babe_model_converter",
      "input": null,
      "output": {
        "status": "success",
        "code": "import tensorflow as tf\nimport numpy as np\n\n# Load the original Keras model\noriginal_model = tf.keras.models.load_model(\"models/fruit_to_emoji/og_model/model.keras\")\n\n# Define conversion parameters for 8-bit integer quantization\nconverter = tf.lite.TFLiteConverter.from_keras_model(original_model)\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\n\n# Define representative dataset generator function\ndef representative_dataset_gen():\n    # Dummy input data with the same shape as your model's input\n    for _ in range(100):\n        yield [np.random.rand(1, *original_model.input_shape[1:]).astype(np.float32)]\n\n# Set the representative dataset for quantization\nconverter.representative_dataset = representative_dataset_gen\n\n# Ensure that if any ops can't be quantized, the converter throws an error\nconverter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\nconverter.inference_input_type = tf.float32  # Use float32 for input to maintain compatibility with model input type\nconverter.inference_output_type = tf.uint8\n\n# Convert the model\ntflite_quant_model = converter.convert()\n\n# Save the converted TFLite model\nwith open(\"models/fruit_to_emoji/tflite_model/model_quant.tflite\", \"wb\") as f:\n    f.write(tflite_quant_model)"
      },
      "session_id": "qwen2.5-coder:32b_7e6b_mc_batch",
      "metadata": {
        "num_run": 3
      },
      "tags": [
        "benchmark",
        "model_converter",
        "qwen2.5-coder:32b"
      ],
      "latency": 44.504,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-12-47-29-450206_chatcmpl-b29e5a09-ebaf-4845-a537-dd8a95435194",
          "traceId": "babe78c1",
          "type": "GENERATION",
          "name": "ba_mc_conversion_code_gen",
          "startTime": "2025-09-24T09:47:29.450000+00:00",
          "endTime": "2025-09-24T09:47:38.632000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 9182.0,
          "promptTokens": 451,
          "totalTokens": 744,
          "completionTokens": 293,
          "environment": "default",
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-12-47-42-686010_chatcmpl-353214ee-2f5c-4901-acd0-938df29299e4",
          "traceId": "babe78c1",
          "type": "GENERATION",
          "name": "ba_mc_error_handling_attempt#2",
          "startTime": "2025-09-24T09:47:42.686000+00:00",
          "endTime": "2025-09-24T09:48:05.301000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 22615.0,
          "promptTokens": 2048,
          "totalTokens": 2485,
          "completionTokens": 437,
          "environment": "default",
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-12-48-05-305346_chatcmpl-f3b3329d-7194-4b8f-9225-274c1d368c64",
          "traceId": "babe78c1",
          "type": "GENERATION",
          "name": "ba_mc_error_handling_attempt#3",
          "startTime": "2025-09-24T09:48:05.305000+00:00",
          "endTime": "2025-09-24T09:48:13.952000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 8647.0,
          "promptTokens": 814,
          "totalTokens": 1113,
          "completionTokens": 299,
          "environment": "default",
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "09344387-37d9-4ba8-a17a-0a1d252acdb9",
          "traceId": "babe78c1",
          "type": "SPAN",
          "name": "start_model_converter",
          "startTime": "2025-09-24T09:47:29.448000+00:00",
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "promptTokens": 0,
          "totalTokens": 0,
          "completionTokens": 0,
          "environment": "default",
          "costDetails": {},
          "endTime": null,
          "model": null,
          "modelParameters": null,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null,
          "latency": null
        }
      ],
      "createdAt": "2025-09-24T09:47:29.972Z",
      "environment": "default",
      "updatedAt": "2025-09-24T09:48:23.112Z"
    },
    {
      "id": "31148cc3",
      "timestamp": "2025-09-24T09:45:50.861000+00:00",
      "name": "3114_model_converter",
      "input": null,
      "output": {
        "status": "success",
        "code": "import numpy as np\nimport tensorflow as tf\n\n# Define input shape\ninput_shape = (299, 299, 3)  # Example input shape for an image model\n\ndef representative_data_gen():\n    yield [np.random.rand(1, *input_shape).astype(np.float32)]  # Replace `input_shape` with the actual shape\n\n# Load your Keras model\nmodel = tf.keras.models.load_model('models/fruit_to_emoji/og_model/model.keras')\n\n# Convert the Keras model to a TensorFlow Lite model\nconverter = tf.lite.TFLiteConverter.from_keras_model(model)\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\nconverter.representative_dataset = representative_data_gen\n\n# Define quantization parameters based on your requirements\ninference_input_type = tf.float32  # or tf.uint8 if you want integer-only quantization\ninference_output_type = tf.float32  # or tf.uint8 if you want integer-only quantization\n\nconverter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\nconverter.inference_input_type = inference_input_type\nconverter.inference_output_type = inference_output_type\n\n# Define the input and output data types for quantization\nif converter.inference_input_type == tf.uint8 or converter.inference_output_type == tf.uint8:\n    converter.quantized_input_stats = {'input_1': (0., 255.)}  # Replace 'input_1' with your actual input layer name\n\n# Convert the model to the TensorFlow Lite format\ntflite_model = converter.convert()\n\n# Save the converted model to a file\nwith open('models/fruit_to_emoji/tflite_model/model.tflite', 'wb') as f:\n    f.write(tflite_model)"
      },
      "session_id": "qwen2.5-coder:32b_7e6b_mc_batch",
      "metadata": {
        "num_run": 2
      },
      "tags": [
        "benchmark",
        "model_converter",
        "qwen2.5-coder:32b"
      ],
      "latency": 68.789,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-12-45-50-863255_chatcmpl-b73aa04e-aa0e-4893-8299-12651584706c",
          "traceId": "31148cc3",
          "type": "GENERATION",
          "name": "31_mc_conversion_code_gen",
          "startTime": "2025-09-24T09:45:50.863000+00:00",
          "endTime": "2025-09-24T09:46:00.262000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 9399.0,
          "promptTokens": 451,
          "totalTokens": 794,
          "completionTokens": 343,
          "environment": "default",
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-12-46-04-392142_chatcmpl-895fecea-1542-44d1-b97e-a64afab6b125",
          "traceId": "31148cc3",
          "type": "GENERATION",
          "name": "31_mc_error_handling_attempt#2",
          "startTime": "2025-09-24T09:46:04.392000+00:00",
          "endTime": "2025-09-24T09:46:20.611000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 16219.0,
          "promptTokens": 2048,
          "totalTokens": 2571,
          "completionTokens": 523,
          "environment": "default",
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-12-46-24-745133_chatcmpl-47995cc4-76a2-4530-a0ac-658df5c05bdb",
          "traceId": "31148cc3",
          "type": "GENERATION",
          "name": "31_mc_error_handling_attempt#3",
          "startTime": "2025-09-24T09:46:24.745000+00:00",
          "endTime": "2025-09-24T09:46:59.650000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 34905.0,
          "promptTokens": 1828,
          "totalTokens": 2409,
          "completionTokens": 581,
          "environment": "default",
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "9e2b8e45-bc9e-4850-b74d-729d5c0ec9e5",
          "traceId": "31148cc3",
          "type": "SPAN",
          "name": "start_model_converter",
          "startTime": "2025-09-24T09:45:50.861000+00:00",
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "promptTokens": 0,
          "totalTokens": 0,
          "completionTokens": 0,
          "environment": "default",
          "costDetails": {},
          "endTime": null,
          "model": null,
          "modelParameters": null,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null,
          "latency": null
        }
      ],
      "createdAt": "2025-09-24T09:45:51.386Z",
      "environment": "default",
      "updatedAt": "2025-09-24T09:47:08.551Z"
    },
    {
      "id": "23f7cf98",
      "timestamp": "2025-09-24T09:43:28.081000+00:00",
      "name": "23f7_model_converter",
      "input": null,
      "output": {
        "status": "success",
        "code": "import tensorflow as tf\n\n# Load the original Keras model\noriginal_model = tf.keras.models.load_model(\"models/fruit_to_emoji/og_model/model.keras\")\n\n# Define a generator function for the representative dataset\ndef representative_dataset():\n    # Replace this with your actual data loading and preprocessing logic\n    for _ in range(100):  # Use a sufficient number of samples\n        input_data = tf.random.normal([1, 224, 224, 3], dtype=tf.float32)  # Example shape and type\n        yield [input_data]\n\n# Define conversion parameters for quantization\nconverter = tf.lite.TFLiteConverter.from_keras_model(original_model)\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\nconverter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n\n# Define the input and output types for quantization\ninference_input_type = tf.float32\ninference_output_type = tf.uint8\nconverter.inference_input_type = inference_input_type\nconverter.inference_output_type = inference_output_type\n\n# Ensure that if any ops can't be quantized, they will fail at conversion time\nconverter.target_spec.supported_types = [tf.int8]\n\n# Set the representative dataset for full integer quantization\nconverter.representative_dataset = representative_dataset\n\n# Convert the model to TFLite format\nquantized_tflite_model = converter.convert()\n\n# Save the quantized model to a file\nwith open(\"models/fruit_to_emoji/tflite_model/quantized_model.tflite\", \"wb\") as f:\n    f.write(quantized_tflite_model)"
      },
      "session_id": "qwen2.5-coder:32b_7e6b_mc_batch",
      "metadata": {
        "num_run": 1
      },
      "tags": [
        "benchmark",
        "model_converter",
        "qwen2.5-coder:32b"
      ],
      "latency": 108.566,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-12-43-28-093737_chatcmpl-aedbe13c-7b26-46c3-9c51-f13be1dfeb4b",
          "traceId": "23f7cf98",
          "type": "GENERATION",
          "name": "23_mc_conversion_code_gen",
          "startTime": "2025-09-24T09:43:28.093000+00:00",
          "endTime": "2025-09-24T09:44:58.166000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 90073.0,
          "promptTokens": 451,
          "totalTokens": 699,
          "completionTokens": 248,
          "environment": "default",
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-12-45-01-393982_chatcmpl-1c2be079-f718-4b53-a924-b2bea3605887",
          "traceId": "23f7cf98",
          "type": "GENERATION",
          "name": "23_mc_error_handling_attempt#2",
          "startTime": "2025-09-24T09:45:01.393000+00:00",
          "endTime": "2025-09-24T09:45:16.647000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 15254.0,
          "promptTokens": 1718,
          "totalTokens": 2227,
          "completionTokens": 509,
          "environment": "default",
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "29e6cf62-d7a2-4915-a3e3-47028a223c69",
          "traceId": "23f7cf98",
          "type": "SPAN",
          "name": "start_model_converter",
          "startTime": "2025-09-24T09:43:28.081000+00:00",
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "promptTokens": 0,
          "totalTokens": 0,
          "completionTokens": 0,
          "environment": "default",
          "costDetails": {},
          "endTime": null,
          "model": null,
          "modelParameters": null,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null,
          "latency": null
        }
      ],
      "createdAt": "2025-09-24T09:43:28.808Z",
      "environment": "default",
      "updatedAt": "2025-09-24T09:45:29.503Z"
    }
  ],
  "meta": {
    "total_items": 30
  }
}
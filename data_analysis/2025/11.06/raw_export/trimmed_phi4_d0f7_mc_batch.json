{
  "data": [
    {
      "id": "dc5c96a9",
      "timestamp": "2025-10-23T07:35:08.349000+00:00",
      "name": "dc5c_model_converter",
      "input": null,
      "output": null,
      "session_id": "phi4_d0f7_mc_batch",
      "metadata": {
        "num_run": 28
      },
      "tags": [
        "benchmark",
        "model_converter",
        "phi4:latest"
      ],
      "latency": 5.258,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-10-35-08-351095_chatcmpl-ee78558f-7a73-4cc2-beea-64c8158e60c2",
          "traceId": "dc5c96a9",
          "type": "GENERATION",
          "name": "dc_mc_conversion_code_gen",
          "startTime": "2025-10-23T07:35:08.351000+00:00",
          "endTime": "2025-10-23T07:35:13.607000+00:00",
          "model": "phi4:latest",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 5256.0,
          "environment": "default",
          "promptTokens": 416,
          "completionTokens": 391,
          "totalTokens": 807,
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "47dd0bbd-579e-473b-81a6-f9ef203fdf9c",
          "traceId": "dc5c96a9",
          "type": "SPAN",
          "name": "start_model_converter",
          "startTime": "2025-10-23T07:35:08.349000+00:00",
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "environment": "default",
          "promptTokens": 0,
          "completionTokens": 0,
          "totalTokens": 0,
          "costDetails": {},
          "endTime": null,
          "model": null,
          "modelParameters": null,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null,
          "latency": null
        }
      ],
      "environment": "default",
      "updatedAt": "2025-10-23T07:35:19.506Z",
      "createdAt": "2025-10-23T07:35:08.866Z"
    },
    {
      "id": "e50c8589",
      "timestamp": "2025-10-23T07:34:33.824000+00:00",
      "name": "e50c_model_converter",
      "input": null,
      "output": {
        "status": "success",
        "code": "import tensorflow as tf\n\ndef convert_keras_to_tflite_quantized(original_model_path, converted_model_path):\n    # Load the original Keras model\n    keras_model = tf.keras.models.load_model(original_model_path)\n    \n    # Convert the model to TensorFlow Lite format with quantization\n    converter = tf.lite.TFLiteConverter.from_keras_model(keras_model)\n    \n    # Set the optimization strategy for quantized operations\n    converter.optimizations = [tf.lite.Optimize.DEFAULT]\n    \n    # Define representative dataset for calibration of the quantized model\n    def representative_dataset_gen():\n        import numpy as np\n        # Generate dummy data with a shape matching the input layer\n        for _ in range(100):\n            yield [np.random.uniform(-1, 1, (1, *keras_model.input_shape[1:])).astype(np.float32)]\n    \n    converter.representative_dataset = representative_dataset_gen\n    \n    # Define input and output data types\n    converter.target_spec.supported_ops = [\n        tf.lite.OpsSet.TFLITE_BUILTINS_INT8,\n        tf.lite.OpsSet.TFLITE_BUILTINS  # Enable TensorFlow Lite ops.\n    ]\n    \n    # Set the input and output tensors to be quantized\n    converter.inference_input_type = tf.uint8\n    converter.inference_output_type = tf.uint8\n    \n    # Perform the conversion and save the model\n    tflite_quant_model = converter.convert()\n    with open(converted_model_path, 'wb') as f:\n        f.write(tflite_quant_model)\n\n# Execute the conversion function with predefined paths\nconvert_keras_to_tflite_quantized(\"models/fruit_to_emoji/og_model/model.keras\", \"models/fruit_to_emoji/tflite_model/model_quant.tflite\")"
      },
      "session_id": "phi4_d0f7_mc_batch",
      "metadata": {
        "num_run": 27
      },
      "tags": [
        "benchmark",
        "model_converter",
        "phi4:latest"
      ],
      "latency": 5.116,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-10-34-33-826040_chatcmpl-83f43265-9896-40ec-af27-de42acebc562",
          "traceId": "e50c8589",
          "type": "GENERATION",
          "name": "e5_mc_conversion_code_gen",
          "startTime": "2025-10-23T07:34:33.826000+00:00",
          "endTime": "2025-10-23T07:34:38.940000+00:00",
          "model": "phi4:latest",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 5114.0,
          "environment": "default",
          "promptTokens": 416,
          "completionTokens": 380,
          "totalTokens": 796,
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "e4d9505a-5587-4984-8e01-75df2ad8928b",
          "traceId": "e50c8589",
          "type": "SPAN",
          "name": "start_model_converter",
          "startTime": "2025-10-23T07:34:33.824000+00:00",
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "environment": "default",
          "promptTokens": 0,
          "completionTokens": 0,
          "totalTokens": 0,
          "costDetails": {},
          "endTime": null,
          "model": null,
          "modelParameters": null,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null,
          "latency": null
        }
      ],
      "environment": "default",
      "updatedAt": "2025-10-23T07:34:47.987Z",
      "createdAt": "2025-10-23T07:34:34.342Z"
    },
    {
      "id": "ecb6333d",
      "timestamp": "2025-10-23T07:33:59.325000+00:00",
      "name": "ecb6_model_converter",
      "input": null,
      "output": {
        "status": "success",
        "code": "import tensorflow as tf\n\n# Define the input data types and supported operations for quantization\ninference_input_type = tf.float32\ninference_output_type = tf.uint8\nsupported_ops = [\n    tf.lite.OpsSet.TFLITE_BUILTINS,  # Enable TensorFlow Lite ops.\n    tf.lite.OpsSet.TFLITE_BUILTINS_INT8  # Enable INT8 operations for quantization.\n]\n\n# Convert the original Keras model to TFLite with 8-bit integer quantization\noriginal_model = tf.keras.models.load_model(\"models/fruit_to_emoji/og_model/model.keras\")\n\n# Set up a representative dataset generator for calibration during quantization\ndef representative_dataset_gen():\n    # This should be replaced with actual data samples in the same format as input to your model\n    for _ in range(100):\n        yield [tf.random.uniform((1, 224, 224, 3), dtype=tf.float32)]  # Example shape\n\n# Convert and quantize the model using TFLiteConverter\nconverter = tf.lite.TFLiteConverter.from_keras_model(original_model)\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\nconverter.representative_dataset = representative_dataset_gen\nconverter.target_spec.supported_ops = supported_ops\nconverter.inference_input_type = inference_input_type\nconverter.inference_output_type = inference_output_type\n\n# Convert the model and save it to a file\ntflite_quant_model = converter.convert()\nwith open(\"models/fruit_to_emoji/tflite_model/model_quant.tflite\", \"wb\") as f:\n    f.write(tflite_quant_model)"
      },
      "session_id": "phi4_d0f7_mc_batch",
      "metadata": {
        "num_run": 26
      },
      "tags": [
        "benchmark",
        "model_converter",
        "phi4:latest"
      ],
      "latency": 4.606,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-10-33-59-327118_chatcmpl-61963b71-1c2d-4fda-8445-5e860b6254ba",
          "traceId": "ecb6333d",
          "type": "GENERATION",
          "name": "ec_mc_conversion_code_gen",
          "startTime": "2025-10-23T07:33:59.327000+00:00",
          "endTime": "2025-10-23T07:34:03.931000+00:00",
          "model": "phi4:latest",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 4604.0,
          "environment": "default",
          "promptTokens": 416,
          "completionTokens": 342,
          "totalTokens": 758,
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "af3df0eb-fe70-4db7-932b-adf6a4e90ede",
          "traceId": "ecb6333d",
          "type": "SPAN",
          "name": "start_model_converter",
          "startTime": "2025-10-23T07:33:59.325000+00:00",
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "environment": "default",
          "promptTokens": 0,
          "completionTokens": 0,
          "totalTokens": 0,
          "costDetails": {},
          "endTime": null,
          "model": null,
          "modelParameters": null,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null,
          "latency": null
        }
      ],
      "environment": "default",
      "updatedAt": "2025-10-23T07:34:13.463Z",
      "createdAt": "2025-10-23T07:33:59.841Z"
    },
    {
      "id": "3cbc10fd",
      "timestamp": "2025-10-23T07:33:23.820000+00:00",
      "name": "3cbc_model_converter",
      "input": null,
      "output": {
        "status": "success",
        "code": "import tensorflow as tf\n\n# Load the original Keras model\noriginal_model = tf.keras.models.load_model(\"models/fruit_to_emoji/og_model/model.keras\")\n\n# Set up TFLite converter with 8-bit integer quantization\nconverter = tf.lite.TFLiteConverter.from_keras_model(original_model)\n\n# Define representative dataset function for calibration\ndef representative_dataset_gen():\n    # Provide a sample of data to calibrate the model\n    for _ in range(100):\n        yield [tf.random.uniform([1, *input_shape], dtype=tf.float32)]\n\n# Set input and output types for quantization\nconverter.inference_input_type = tf.uint8  # As per configuration for input datatype\nconverter.inference_output_type = tf.uint8  # As per configuration for output datatype\n\n# Configure supported operations\nsupported_ops = [\n    tf.lite.OpsSet.TFLITE_BUILTINS,  # Use the TFLite built-in ops.\n    tf.lite.OpsSet.TFLITE_BUILTINS_INT8  # Enable INT8 quantization.\n]\n\nconverter.target_spec.supported_ops = supported_ops\n\n# Set the representative dataset for calibration\ninput_shape = original_model.input_shape[1:]  # Exclude batch size\nconverter.representative_dataset = representative_dataset_gen\n\n# Perform optimization and set the converter to use INT8 quantization\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\n\n# Convert the model\ntflite_quant_model = converter.convert()\n\n# Save the converted TFLite model with 8-bit integer quantization\nwith open(\"models/fruit_to_emoji/tflite_model/model_quant.tflite\", \"wb\") as f:\n    f.write(tflite_quant_model)"
      },
      "session_id": "phi4_d0f7_mc_batch",
      "metadata": {
        "num_run": 25
      },
      "tags": [
        "benchmark",
        "model_converter",
        "phi4:latest"
      ],
      "latency": 5.085,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-10-33-23-822187_chatcmpl-ec928c7e-7534-4eab-922b-380c5137686c",
          "traceId": "3cbc10fd",
          "type": "GENERATION",
          "name": "3c_mc_conversion_code_gen",
          "startTime": "2025-10-23T07:33:23.822000+00:00",
          "endTime": "2025-10-23T07:33:28.905000+00:00",
          "model": "phi4:latest",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 5083.0,
          "environment": "default",
          "promptTokens": 416,
          "completionTokens": 361,
          "totalTokens": 777,
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "b0fc0f08-3f57-4720-9dbf-b6f6c363776b",
          "traceId": "3cbc10fd",
          "type": "SPAN",
          "name": "start_model_converter",
          "startTime": "2025-10-23T07:33:23.820000+00:00",
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "environment": "default",
          "promptTokens": 0,
          "completionTokens": 0,
          "totalTokens": 0,
          "costDetails": {},
          "endTime": null,
          "model": null,
          "modelParameters": null,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null,
          "latency": null
        }
      ],
      "environment": "default",
      "updatedAt": "2025-10-23T07:33:37.934Z",
      "createdAt": "2025-10-23T07:33:24.337Z"
    },
    {
      "id": "8e16feb0",
      "timestamp": "2025-10-23T07:32:03.309000+00:00",
      "name": "8e16_model_converter",
      "input": null,
      "output": {
        "status": "success",
        "code": "import numpy as np\nimport tensorflow as tf\n\ndef representative_data_gen():\n    for _ in range(100):\n        yield [np.random.rand(1, 224, 224, 3).astype(np.float32)]\n\ndef convert_model(original_model_path, converted_model_path):\n    model = tf.keras.models.load_model(original_model_path)\n\n    converter = tf.lite.TFLiteConverter.from_keras_model(model)\n    \n    converter.optimizations = [tf.lite.Optimize.DEFAULT]\n    converter.representative_dataset = representative_data_gen\n    converter.target_spec.supported_ops = [\n        tf.lite.OpsSet.TFLITE_BUILTINS_INT8\n    ]\n    converter.inference_input_type = tf.uint8\n    converter.inference_output_type = tf.uint8\n    \n    tflite_quant_model = converter.convert()\n    \n    with open(converted_model_path, 'wb') as f:\n        f.write(tflite_quant_model)\n\noriginal_model_path = \"models/fruit_to_emoji/og_model/model.keras\"\nconverted_model_path = \"models/fruit_to_emoji/tflite_model/model_quant.tflite\"\n\nconvert_model(original_model_path, converted_model_path)"
      },
      "session_id": "phi4_d0f7_mc_batch",
      "metadata": {
        "num_run": 24
      },
      "tags": [
        "benchmark",
        "model_converter",
        "phi4:latest"
      ],
      "latency": 50.555,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-10-32-03-311815_chatcmpl-f201e97e-d3ee-4d97-8927-bc50cc273966",
          "traceId": "8e16feb0",
          "type": "GENERATION",
          "name": "8e_mc_conversion_code_gen",
          "startTime": "2025-10-23T07:32:03.311000+00:00",
          "endTime": "2025-10-23T07:32:08.473000+00:00",
          "model": "phi4:latest",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 5162.0,
          "environment": "default",
          "promptTokens": 416,
          "completionTokens": 381,
          "totalTokens": 797,
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-10-32-12-028450_chatcmpl-29d5cf9b-9caf-446a-be86-a1c5099cec9f",
          "traceId": "8e16feb0",
          "type": "GENERATION",
          "name": "8e_mc_error_handling_attempt#2",
          "startTime": "2025-10-23T07:32:12.028000+00:00",
          "endTime": "2025-10-23T07:32:21.830000+00:00",
          "model": "phi4:latest",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 9802.0,
          "environment": "default",
          "promptTokens": 2048,
          "completionTokens": 620,
          "totalTokens": 2668,
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-10-32-25-019472_chatcmpl-01c78495-0d6e-4b2c-90f6-d8c31b8d19b0",
          "traceId": "8e16feb0",
          "type": "GENERATION",
          "name": "8e_mc_error_handling_attempt#3",
          "startTime": "2025-10-23T07:32:25.019000+00:00",
          "endTime": "2025-10-23T07:32:31.366000+00:00",
          "model": "phi4:latest",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 6347.0,
          "environment": "default",
          "promptTokens": 1459,
          "completionTokens": 407,
          "totalTokens": 1866,
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-10-32-35-084299_chatcmpl-45cdfc80-ed8d-40c8-b283-1241cecc3a88",
          "traceId": "8e16feb0",
          "type": "GENERATION",
          "name": "8e_mc_error_handling_attempt#4",
          "startTime": "2025-10-23T07:32:35.084000+00:00",
          "endTime": "2025-10-23T07:32:46.409000+00:00",
          "model": "phi4:latest",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 11325.0,
          "environment": "default",
          "promptTokens": 2048,
          "completionTokens": 731,
          "totalTokens": 2779,
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-10-32-49-566951_chatcmpl-93aa800b-9b7e-45be-a2d2-7c62f76e314b",
          "traceId": "8e16feb0",
          "type": "GENERATION",
          "name": "8e_mc_error_handling_attempt#5",
          "startTime": "2025-10-23T07:32:49.566000+00:00",
          "endTime": "2025-10-23T07:32:53.865000+00:00",
          "model": "phi4:latest",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 4299.0,
          "environment": "default",
          "promptTokens": 1780,
          "completionTokens": 247,
          "totalTokens": 2027,
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "e2b4b2c9-7948-4bda-a764-2c13b3c765e0",
          "traceId": "8e16feb0",
          "type": "SPAN",
          "name": "start_model_converter",
          "startTime": "2025-10-23T07:32:03.310000+00:00",
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "environment": "default",
          "promptTokens": 0,
          "completionTokens": 0,
          "totalTokens": 0,
          "costDetails": {},
          "endTime": null,
          "model": null,
          "modelParameters": null,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null,
          "latency": null
        }
      ],
      "environment": "default",
      "updatedAt": "2025-10-23T07:33:03.517Z",
      "createdAt": "2025-10-23T07:32:03.829Z"
    },
    {
      "id": "56480ee2",
      "timestamp": "2025-10-23T07:31:27.816000+00:00",
      "name": "5648_model_converter",
      "input": null,
      "output": {
        "status": "success",
        "code": "import tensorflow as tf\n\n# Load the original Keras model\nkeras_model = tf.keras.models.load_model(\"models/fruit_to_emoji/og_model/model.keras\")\n\n# Define representative dataset for calibration\ndef representative_data_gen():\n    # Generate data to calibrate quantization. Adjust based on your actual data.\n    for _ in range(100):\n        yield [tf.random.uniform([1, 224, 224, 3], minval=0, maxval=255, dtype=tf.float32)]\n\n# Convert the model to TensorFlow Lite with int8 quantization\nconverter = tf.lite.TFLiteConverter.from_keras_model(keras_model)\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\nconverter.representative_dataset = representative_data_gen\n\n# Set input and output data types for quantization\nconverter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\nconverter.inference_input_type = tf.uint8  # Corresponds to uint8 as per output_datatype\nconverter.inference_output_type = tf.uint8\n\n# Convert the model\ntflite_quant_model = converter.convert()\n\n# Save the quantized TFLite model\nwith open(\"models/fruit_to_emoji/tflite_model/model_quant.tflite\", \"wb\") as f:\n    f.write(tflite_quant_model)"
      },
      "session_id": "phi4_d0f7_mc_batch",
      "metadata": {
        "num_run": 23
      },
      "tags": [
        "benchmark",
        "model_converter",
        "phi4:latest"
      ],
      "latency": 4.394,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-10-31-27-818115_chatcmpl-1401dece-a5a7-4847-b371-c69c8610797e",
          "traceId": "56480ee2",
          "type": "GENERATION",
          "name": "56_mc_conversion_code_gen",
          "startTime": "2025-10-23T07:31:27.818000+00:00",
          "endTime": "2025-10-23T07:31:32.210000+00:00",
          "model": "phi4:latest",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 4392.0,
          "environment": "default",
          "promptTokens": 416,
          "completionTokens": 300,
          "totalTokens": 716,
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "a28903bb-dabb-478c-9cf7-00188238c7b0",
          "traceId": "56480ee2",
          "type": "SPAN",
          "name": "start_model_converter",
          "startTime": "2025-10-23T07:31:27.816000+00:00",
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "environment": "default",
          "promptTokens": 0,
          "completionTokens": 0,
          "totalTokens": 0,
          "costDetails": {},
          "endTime": null,
          "model": null,
          "modelParameters": null,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null,
          "latency": null
        }
      ],
      "environment": "default",
      "updatedAt": "2025-10-23T07:31:41.923Z",
      "createdAt": "2025-10-23T07:31:28.337Z"
    },
    {
      "id": "ba2a1999",
      "timestamp": "2025-10-23T07:30:45.320000+00:00",
      "name": "ba2a_model_converter",
      "input": null,
      "output": {
        "status": "success",
        "code": "import tensorflow as tf\n\n# Define the input and output data types for quantization\ninference_input_type = tf.float32  # Original model input type\ninference_output_type = tf.uint8   # Quantized model output type\n\n# Specify supported operations for quantization\nsupported_ops = [\n    tf.lite.OpsSet.TFLITE_BUILTINS,      # Built-in ops for TFLite\n    tf.lite.OpsSet.TFLITE_BUILTINS_INT8  # Int8 ops for quantization\n]\n\n# Load the original Keras model\noriginal_model = tf.keras.models.load_model(\"models/fruit_to_emoji/og_model/model.keras\")\n\n# Convert the Keras model to a TensorFlow Lite model with quantization\nconverter = tf.lite.TFLiteConverter.from_keras_model(original_model)\n\n# Set up quantization parameters\ndef representative_data_gen():\n    for _ in range(100):\n        yield [tf.random.uniform(original_model.input_shape[1:], dtype=tf.float32)]\n\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\nconverter.representative_dataset = representative_data_gen\nconverter.target_spec.supported_ops = supported_ops\nconverter.inference_input_type = inference_input_type\nconverter.inference_output_type = inference_output_type\n\n# Perform the conversion and save the quantized model to the specified path\ntflite_quant_model = converter.convert()\nwith open(\"models/fruit_to_emoji/tflite_model/model_quant.tflite\", \"wb\") as f:\n    f.write(tflite_quant_model)"
      },
      "session_id": "phi4_d0f7_mc_batch",
      "metadata": {
        "num_run": 22
      },
      "tags": [
        "benchmark",
        "model_converter",
        "phi4:latest"
      ],
      "latency": 13.044,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-10-30-45-322725_chatcmpl-76eb4941-88c6-446e-b299-c20328eb8413",
          "traceId": "ba2a1999",
          "type": "GENERATION",
          "name": "ba_mc_conversion_code_gen",
          "startTime": "2025-10-23T07:30:45.322000+00:00",
          "endTime": "2025-10-23T07:30:50.140000+00:00",
          "model": "phi4:latest",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 4818.0,
          "environment": "default",
          "promptTokens": 416,
          "completionTokens": 344,
          "totalTokens": 760,
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-10-30-53-414859_chatcmpl-f8d3a135-8f81-47db-a032-8340a321021c",
          "traceId": "ba2a1999",
          "type": "GENERATION",
          "name": "ba_mc_error_handling_attempt#2",
          "startTime": "2025-10-23T07:30:53.414000+00:00",
          "endTime": "2025-10-23T07:30:58.365000+00:00",
          "model": "phi4:latest",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 4951.0,
          "environment": "default",
          "promptTokens": 1317,
          "completionTokens": 323,
          "totalTokens": 1640,
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "0cb3b594-6a48-4528-ada7-d653ea146635",
          "traceId": "ba2a1999",
          "type": "SPAN",
          "name": "start_model_converter",
          "startTime": "2025-10-23T07:30:45.321000+00:00",
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "environment": "default",
          "promptTokens": 0,
          "completionTokens": 0,
          "totalTokens": 0,
          "costDetails": {},
          "endTime": null,
          "model": null,
          "modelParameters": null,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null,
          "latency": null
        }
      ],
      "environment": "default",
      "updatedAt": "2025-10-23T07:31:07.503Z",
      "createdAt": "2025-10-23T07:30:45.836Z"
    },
    {
      "id": "5476b6d2",
      "timestamp": "2025-10-23T07:29:25.635000+00:00",
      "name": "5476_model_converter",
      "input": null,
      "output": {
        "status": "failed",
        "last_code": "import tensorflow as tf\nimport numpy as np\n\n# Load the Keras model from a .h5 file\nmodel = tf.keras.models.load_model('saved_model/my_fruit_model.h5')\n\n# Create a converter object for converting the model to TFLite format\nconverter = tf.lite.TFLiteConverter.from_keras_model(model)\n\n# Specify the representative dataset function\ndef representative_data_gen():\n    # Ideally, use actual data samples here. For demonstration purposes,\n    # we'll continue using random data as in your example.\n    for _ in range(100):\n        yield [np.random.rand(224, 224, 3).astype(np.float32)]\n\n# Assign the function to the converter's representative dataset property\nconverter.representative_dataset = representative_data_gen\n\n# Set optimization flag to optimize for size and latency\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\n\n# Specify the supported types for quantization (int8 in this case)\nconverter.target_spec.supported_types = [tf.int8]\n\n# Define the input and output type as float32 (required for calibration)\ninput_type = tf.float32\noutput_type = tf.float32\n\n# Set the input and output arrays to have the specified types\nconverter.inference_input_type = input_type\nconverter.inference_output_type = output_type\n\n# Convert the model to TFLite format with quantization\ntflite_quant_model = converter.convert()\n\n# Save the converted model to a .tflite file\nwith open('converted_model.tflite', 'wb') as f:\n    f.write(tflite_quant_model)\n\nprint(\"Model conversion and quantization completed. Saved as 'converted_model.tflite'.\")",
        "last_error": "Failed to generate valid code after the max 5 attempts. Last error from code execution: 2025-10-23 10:30:16.351108: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n2025-10-23 10:30:16.354479: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n2025-10-23 10:30:16.365492: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2025-10-23 10:30:16.383566: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2025-10-23 10:30:16.388817: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n2025-10-23 10:30:16.401977: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\nTo enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n2025-10-23 10:30:17.333487: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\nTraceback (most recent call last):\n  File \"/home/wuguangh/Projects/tinyml-autopilot/models/fruit_to_emoji/tflite_model/tmp_converter20251023103016.py/tmp_20251023103016_mc_phi4:latest/tmp_20251023103016_mc_phi4:latest.py\", line 5, in <module>\n    model = tf.keras.models.load_model('saved_model/my_fruit_model.h5')\n  File \"/home/wuguangh/.conda/envs/tinyml/lib/python3.10/site-packages/keras/src/saving/saving_api.py\", line 196, in load_model\n    return legacy_h5_format.load_model_from_hdf5(\n  File \"/home/wuguangh/.conda/envs/tinyml/lib/python3.10/site-packages/keras/src/legacy/saving/legacy_h5_format.py\", line 116, in load_model_from_hdf5\n    f = h5py.File(filepath, mode=\"r\")\n  File \"/home/wuguangh/.conda/envs/tinyml/lib/python3.10/site-packages/h5py/_hl/files.py\", line 561, in __init__\n    fid = make_fid(name, mode, userblock_size, fapl, fcpl, swmr=swmr)\n  File \"/home/wuguangh/.conda/envs/tinyml/lib/python3.10/site-packages/h5py/_hl/files.py\", line 235, in make_fid\n    fid = h5f.open(name, flags, fapl=fapl)\n  File \"h5py/_objects.pyx\", line 54, in h5py._objects.with_phil.wrapper\n  File \"h5py/_objects.pyx\", line 55, in h5py._objects.with_phil.wrapper\n  File \"h5py/h5f.pyx\", line 102, in h5py.h5f.open\nFileNotFoundError: [Errno 2] Unable to synchronously open file (unable to open file: name = 'saved_model/my_fruit_model.h5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)\n"
      },
      "session_id": "phi4_d0f7_mc_batch",
      "metadata": {
        "num_run": 21
      },
      "tags": [
        "benchmark",
        "model_converter",
        "phi4:latest"
      ],
      "latency": 53.967,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-10-29-25-638491_chatcmpl-87f541ce-49e7-422c-af48-f1fd1d9295e9",
          "traceId": "5476b6d2",
          "type": "GENERATION",
          "name": "54_mc_conversion_code_gen",
          "startTime": "2025-10-23T07:29:25.638000+00:00",
          "endTime": "2025-10-23T07:29:30.632000+00:00",
          "model": "phi4:latest",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 4994.0,
          "environment": "default",
          "promptTokens": 416,
          "completionTokens": 372,
          "totalTokens": 788,
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-10-29-34-186357_chatcmpl-e7ffb627-1794-4586-b841-efbc6222c3a4",
          "traceId": "5476b6d2",
          "type": "GENERATION",
          "name": "54_mc_error_handling_attempt#2",
          "startTime": "2025-10-23T07:29:34.186000+00:00",
          "endTime": "2025-10-23T07:29:41.904000+00:00",
          "model": "phi4:latest",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 7718.0,
          "environment": "default",
          "promptTokens": 1863,
          "completionTokens": 490,
          "totalTokens": 2353,
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-10-29-45-254523_chatcmpl-3c8f28e7-0ae6-4c03-8bac-0b0541345fad",
          "traceId": "5476b6d2",
          "type": "GENERATION",
          "name": "54_mc_error_handling_attempt#3",
          "startTime": "2025-10-23T07:29:45.254000+00:00",
          "endTime": "2025-10-23T07:29:51.462000+00:00",
          "model": "phi4:latest",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 6208.0,
          "environment": "default",
          "promptTokens": 1508,
          "completionTokens": 394,
          "totalTokens": 1902,
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-10-29-55-991294_chatcmpl-61dc83af-1aaf-4fa5-86e1-0aa4c9e746a9",
          "traceId": "5476b6d2",
          "type": "GENERATION",
          "name": "54_mc_error_handling_attempt#4",
          "startTime": "2025-10-23T07:29:55.991000+00:00",
          "endTime": "2025-10-23T07:30:02.071000+00:00",
          "model": "phi4:latest",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 6080.0,
          "environment": "default",
          "promptTokens": 1958,
          "completionTokens": 386,
          "totalTokens": 2344,
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-10-30-05-936136_chatcmpl-51cc76f4-5b25-42e3-923e-2ca0625f3d43",
          "traceId": "5476b6d2",
          "type": "GENERATION",
          "name": "54_mc_error_handling_attempt#5",
          "startTime": "2025-10-23T07:30:05.936000+00:00",
          "endTime": "2025-10-23T07:30:16.064000+00:00",
          "model": "phi4:latest",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 10128.0,
          "environment": "default",
          "promptTokens": 2048,
          "completionTokens": 647,
          "totalTokens": 2695,
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "0ddb0f78-e7f0-4c9e-b4cd-7f52cc10555b",
          "traceId": "5476b6d2",
          "type": "SPAN",
          "name": "error_54_mc_failure_signal_model_converter",
          "startTime": "2025-10-23T07:30:19.603000+00:00",
          "level": "ERROR",
          "statusMessage": "Failed. Last error: Failed to generate valid code after the max 5 attempts. Last error from code execution: 2025-10-23 10:30:16.351108: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n2025-10-23 10:30:16.354479: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n2025-10-23 10:30:16.365492: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2025-10-23 10:30:16.383566: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2025-10-23 10:30:16.388817: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n2025-10-23 10:30:16.401977: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\nTo enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n2025-10-23 10:30:17.333487: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\nTraceback (most recent call last):\n  File \"/home/wuguangh/Projects/tinyml-autopilot/models/fruit_to_emoji/tflite_model/tmp_converter20251023103016.py/tmp_20251023103016_mc_phi4:latest/tmp_20251023103016_mc_phi4:latest.py\", line 5, in <module>\n    model = tf.keras.models.load_model('saved_model/my_fruit_model.h5')\n  File \"/home/wuguangh/.conda/envs/tinyml/lib/python3.10/site-packages/keras/src/saving/saving_api.py\", line 196, in load_model\n    return legacy_h5_format.load_model_from_hdf5(\n  File \"/home/wuguangh/.conda/envs/tinyml/lib/python3.10/site-packages/keras/src/legacy/saving/legacy_h5_format.py\", line 116, in load_model_from_hdf5\n    f = h5py.File(filepath, mode=\"r\")\n  File \"/home/wuguangh/.conda/envs/tinyml/lib/python3.10/site-packages/h5py/_hl/files.py\", line 561, in __init__\n    fid = make_fid(name, mode, userblock_size, fapl, fcpl, swmr=swmr)\n  File \"/home/wuguangh/.conda/envs/tinyml/lib/python3.10/site-packages/h5py/_hl/files.py\", line 235, in make_fid\n    fid = h5f.open(name, flags, fapl=fapl)\n  File \"h5py/_objects.pyx\", line 54, in h5py._objects.with_phil.wrapper\n  File \"h5py/_objects.pyx\", line 55, in h5py._objects.with_phil.wrapper\n  File \"h5py/h5f.pyx\", line 102, in h5py.h5f.open\nFileNotFoundError: [Errno 2] Unable to synchronously open file (unable to open file: name = 'saved_model/my_fruit_model.h5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)\n",
          "calculatedTotalCost": 0.0,
          "environment": "default",
          "promptTokens": 0,
          "completionTokens": 0,
          "totalTokens": 0,
          "costDetails": {},
          "endTime": null,
          "model": null,
          "modelParameters": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null,
          "latency": null
        },
        {
          "id": "fece70c5-dabf-45ca-8075-1b7191fb4fe0",
          "traceId": "5476b6d2",
          "type": "SPAN",
          "name": "start_model_converter",
          "startTime": "2025-10-23T07:29:25.636000+00:00",
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "environment": "default",
          "promptTokens": 0,
          "completionTokens": 0,
          "totalTokens": 0,
          "costDetails": {},
          "endTime": null,
          "model": null,
          "modelParameters": null,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null,
          "latency": null
        }
      ],
      "environment": "default",
      "updatedAt": "2025-10-23T07:30:24.859Z",
      "createdAt": "2025-10-23T07:29:26.152Z"
    },
    {
      "id": "d2de57b5",
      "timestamp": "2025-10-23T07:28:48.705000+00:00",
      "name": "d2de_model_converter",
      "input": null,
      "output": {
        "status": "success",
        "code": "import tensorflow as tf\n\n# Define paths for the original and converted models\noriginal_model_path = \"models/fruit_to_emoji/og_model/model.keras\"\nconverted_model_path = \"models/fruit_to_emoji/tflite_model/model_quant.tflite\"\n\n# Load the original Keras model\nmodel = tf.keras.models.load_model(original_model_path)\n\n# Convert the model to TensorFlow Lite format with quantization\nconverter = tf.lite.TFLiteConverter.from_keras_model(model)\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\n\n# Define the input and output data types for inference\ninference_input_type = tf.uint8\ninference_output_type = tf.uint8\n\n# Specify supported operations, including INT8 quantization\nsupported_ops = [\n    tf.lite.OpsSet.TFLITE_BUILTINS,\n    tf.lite.OpsSet.TFLITE_BUILTINS_INT8\n]\n\nconverter.target_spec.supported_ops = supported_ops\nconverter.inference_input_type = inference_input_type\nconverter.inference_output_type = inference_output_type\n\n# Set the representative dataset for calibration\ndef representative_dataset_gen():\n    import numpy as np\n    # Assuming we have some input data to use for calibration\n    for _ in range(100):\n        yield [np.random.uniform(-1, 1, size=(1, 224, 224, 3)).astype(np.float32)]\n\nconverter.representative_dataset = representative_dataset_gen\n\n# Convert the model and save it to a file\ntflite_model = converter.convert()\nwith open(converted_model_path, 'wb') as f:\n    f.write(tflite_model)"
      },
      "session_id": "phi4_d0f7_mc_batch",
      "metadata": {
        "num_run": 20
      },
      "tags": [
        "benchmark",
        "model_converter",
        "phi4:latest"
      ],
      "latency": 4.584,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-10-28-48-707700_chatcmpl-fe8e6531-616b-4dbd-b107-90e3bb300776",
          "traceId": "d2de57b5",
          "type": "GENERATION",
          "name": "d2_mc_conversion_code_gen",
          "startTime": "2025-10-23T07:28:48.707000+00:00",
          "endTime": "2025-10-23T07:28:53.289000+00:00",
          "model": "phi4:latest",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 4582.0,
          "environment": "default",
          "promptTokens": 416,
          "completionTokens": 342,
          "totalTokens": 758,
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "9a0e6a79-76d6-4b7f-aa8f-c86db7857493",
          "traceId": "d2de57b5",
          "type": "SPAN",
          "name": "start_model_converter",
          "startTime": "2025-10-23T07:28:48.705000+00:00",
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "environment": "default",
          "promptTokens": 0,
          "completionTokens": 0,
          "totalTokens": 0,
          "costDetails": {},
          "endTime": null,
          "model": null,
          "modelParameters": null,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null,
          "latency": null
        }
      ],
      "environment": "default",
      "updatedAt": "2025-10-23T07:29:04.360Z",
      "createdAt": "2025-10-23T07:28:49.222Z"
    },
    {
      "id": "9a82fa7a",
      "timestamp": "2025-10-23T07:28:14.095000+00:00",
      "name": "9a82_model_converter",
      "input": null,
      "output": {
        "status": "success",
        "code": "import tensorflow as tf\n\n# Define the input and output data types for quantization\ninference_input_type = tf.float32\ninference_output_type = tf.uint8\n\n# Define supported operations for integer quantization\nsupported_ops = [\n    tf.lite.OpsSet.TFLITE_BUILTINS,\n    tf.lite.OpsSet.TFLITE_BUILTINS_INT8\n]\n\n# Load the original model\nmodel = tf.keras.models.load_model(\"models/fruit_to_emoji/og_model/model.keras\")\n\n# Convert the Keras model to a TensorFlow Lite model with 8-bit integer quantization\nconverter = tf.lite.TFLiteConverter.from_keras_model(model)\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\nconverter.representative_dataset = lambda: [\n    (tf.cast(tf.random.normal([1, *input_shape]), inference_input_type),) \n    for _ in range(100)  # Generate 100 sample inputs\n]\n\n# Set the supported operations to include INT8 quantization\nconverter.target_spec.supported_ops = supported_ops\n\n# Define input and output type constraints\ndef representative_dataset_gen():\n    for _ in range(100):\n        yield [tf.cast(tf.random.normal([1, *model.input_shape[1:]]), inference_input_type)]\n        \nconverter.representative_dataset = representative_dataset_gen\n\n# Set the inference input and output types\nconverter.inference_input_type = inference_input_type\nconverter.inference_output_type = inference_output_type\n\n# Convert the model to TFLite format\ntflite_quant_model = converter.convert()\n\n# Save the quantized model to a file\nwith open(\"models/fruit_to_emoji/tflite_model/model_quant.tflite\", \"wb\") as f:\n    f.write(tflite_quant_model)"
      },
      "session_id": "phi4_d0f7_mc_batch",
      "metadata": {
        "num_run": 19
      },
      "tags": [
        "benchmark",
        "model_converter",
        "phi4:latest"
      ],
      "latency": 5.145,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-10-28-14-097074_chatcmpl-c04a2390-0810-41a7-9759-eb4c9d782acc",
          "traceId": "9a82fa7a",
          "type": "GENERATION",
          "name": "9a_mc_conversion_code_gen",
          "startTime": "2025-10-23T07:28:14.097000+00:00",
          "endTime": "2025-10-23T07:28:19.240000+00:00",
          "model": "phi4:latest",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 5143.0,
          "environment": "default",
          "promptTokens": 416,
          "completionTokens": 373,
          "totalTokens": 789,
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "d2556dff-e9a7-4124-a7b4-0e9a7f82e5f4",
          "traceId": "9a82fa7a",
          "type": "SPAN",
          "name": "start_model_converter",
          "startTime": "2025-10-23T07:28:14.095000+00:00",
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "environment": "default",
          "promptTokens": 0,
          "completionTokens": 0,
          "totalTokens": 0,
          "costDetails": {},
          "endTime": null,
          "model": null,
          "modelParameters": null,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null,
          "latency": null
        }
      ],
      "environment": "default",
      "updatedAt": "2025-10-23T07:28:27.725Z",
      "createdAt": "2025-10-23T07:28:14.613Z"
    },
    {
      "id": "594307da",
      "timestamp": "2025-10-23T07:27:40.738000+00:00",
      "name": "5943_model_converter",
      "input": null,
      "output": {
        "status": "success",
        "code": "import tensorflow as tf\n\n# Load the original Keras model\noriginal_model = tf.keras.models.load_model('models/fruit_to_emoji/og_model/model.keras')\n\n# Set up a representative dataset for quantization calibration\ndef representative_data_gen():\n    # Assuming you have some sample data to use for calibration, replace with actual dataset\n    sample_input = [tf.random.uniform([1, 224, 224, 3], minval=0, maxval=255, dtype=tf.float32)]\n    for input_value in sample_input:\n        yield [input_value]\n\n# Convert the model to TensorFlow Lite format with quantization\nconverter = tf.lite.TFLiteConverter.from_keras_model(original_model)\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\nconverter.representative_dataset = representative_data_gen\n\n# Set up the inference input and output types for quantized models\nconverter.inference_input_type = tf.uint8  # Since output datatype is uint8, we use uint8 for input as well\nconverter.inference_output_type = tf.uint8\n\n# Specify supported operations for quantization to int8\nsupported_ops = [\n    tf.lite.OpsSet.TFLITE_BUILTINS_INT8,\n    tf.lite.OpsSet.TFLITE_BUILTINS  # Ensure compatibility with float models\n]\n\nconverter.target_spec.supported_ops = supported_ops\n\n# Perform the conversion and save the quantized model\ntflite_quant_model = converter.convert()\nwith open('models/fruit_to_emoji/tflite_model/model_quant.tflite', 'wb') as f:\n    f.write(tflite_quant_model)"
      },
      "session_id": "phi4_d0f7_mc_batch",
      "metadata": {
        "num_run": 18
      },
      "tags": [
        "benchmark",
        "model_converter",
        "phi4:latest"
      ],
      "latency": 4.675,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-10-27-40-739816_chatcmpl-28b5bbc5-ce4a-4a9a-9f2b-71f8daccb5c5",
          "traceId": "594307da",
          "type": "GENERATION",
          "name": "59_mc_conversion_code_gen",
          "startTime": "2025-10-23T07:27:40.739000+00:00",
          "endTime": "2025-10-23T07:27:45.413000+00:00",
          "model": "phi4:latest",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 4674.0,
          "environment": "default",
          "promptTokens": 416,
          "completionTokens": 341,
          "totalTokens": 757,
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "ef8e9e04-c856-4566-9a79-73d0483b55fa",
          "traceId": "594307da",
          "type": "SPAN",
          "name": "start_model_converter",
          "startTime": "2025-10-23T07:27:40.738000+00:00",
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "environment": "default",
          "promptTokens": 0,
          "completionTokens": 0,
          "totalTokens": 0,
          "costDetails": {},
          "endTime": null,
          "model": null,
          "modelParameters": null,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null,
          "latency": null
        }
      ],
      "environment": "default",
      "updatedAt": "2025-10-23T07:27:53.900Z",
      "createdAt": "2025-10-23T07:27:41.254Z"
    },
    {
      "id": "1f9c4d9a",
      "timestamp": "2025-10-23T07:26:45.345000+00:00",
      "name": "1f9c_model_converter",
      "input": null,
      "output": {
        "status": "success",
        "code": "import tensorflow as tf\nimport os\n\n# Load the original Keras model from the specified path\noriginal_model_path = \"models/fruit_to_emoji/og_model/model.keras\"\nconverted_model_path = \"models/fruit_to_emoji/tflite_model/model_quant.tflite\"\n\nmodel = tf.keras.models.load_model(original_model_path)\n\ninput_size = (224, 224)  # Example input size, adjust as necessary\n\ndef representative_data_gen():\n    \"\"\"Generates data for quantization calibration.\"\"\"\n    for _ in range(100):\n        yield [tf.random.uniform((1, *input_size, 3), maxval=1.0, dtype=tf.float32)]\n\n# Convert the model to TensorFlow Lite format with quantization\nconverter = tf.lite.TFLiteConverter.from_keras_model(model)\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\nconverter.representative_dataset = representative_data_gen\n\n# Specify supported types for integer quantization\nconverter.target_spec.supported_ops = [\n    tf.lite.OpsSet.TFLITE_BUILTINS,\n    tf.lite.OpsSet.TFLITE_BUILTINS_INT8\n]\n\n# Set the input and output data types to match the specified configuration parameters\ninput_signature = [tf.TensorSpec(shape=[1, *input_size, 3], dtype=tf.float32)]\nconverter.inference_input_type = tf.uint8\nconverter.inference_output_type = tf.uint8\n\ntflite_quant_model = converter.convert()\n\n# Save the quantized model to a file at the specified path\nwith open(converted_model_path, 'wb') as f:\n    f.write(tflite_quant_model)"
      },
      "session_id": "phi4_d0f7_mc_batch",
      "metadata": {
        "num_run": 17
      },
      "tags": [
        "benchmark",
        "model_converter",
        "phi4:latest"
      ],
      "latency": 25.202,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-10-26-45-347603_chatcmpl-8ba12327-1bdc-43ea-ba99-dc486ddd0f87",
          "traceId": "1f9c4d9a",
          "type": "GENERATION",
          "name": "1f_mc_conversion_code_gen",
          "startTime": "2025-10-23T07:26:45.347000+00:00",
          "endTime": "2025-10-23T07:26:50.408000+00:00",
          "model": "phi4:latest",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 5061.0,
          "environment": "default",
          "promptTokens": 416,
          "completionTokens": 378,
          "totalTokens": 794,
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-10-26-53-735571_chatcmpl-3b140e63-7690-4fd1-b1de-a082c4238e29",
          "traceId": "1f9c4d9a",
          "type": "GENERATION",
          "name": "1f_mc_error_handling_attempt#2",
          "startTime": "2025-10-23T07:26:53.735000+00:00",
          "endTime": "2025-10-23T07:27:02.326000+00:00",
          "model": "phi4:latest",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 8591.0,
          "environment": "default",
          "promptTokens": 2048,
          "completionTokens": 535,
          "totalTokens": 2583,
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-10-27-05-258883_chatcmpl-484198b9-4c2c-43d1-ad41-95e0821cd236",
          "traceId": "1f9c4d9a",
          "type": "GENERATION",
          "name": "1f_mc_error_handling_attempt#3",
          "startTime": "2025-10-23T07:27:05.258000+00:00",
          "endTime": "2025-10-23T07:27:10.547000+00:00",
          "model": "phi4:latest",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 5289.0,
          "environment": "default",
          "promptTokens": 1188,
          "completionTokens": 345,
          "totalTokens": 1533,
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "40467fa4-cda3-47f7-8911-641b059695fb",
          "traceId": "1f9c4d9a",
          "type": "SPAN",
          "name": "start_model_converter",
          "startTime": "2025-10-23T07:26:45.345000+00:00",
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "environment": "default",
          "promptTokens": 0,
          "completionTokens": 0,
          "totalTokens": 0,
          "costDetails": {},
          "endTime": null,
          "model": null,
          "modelParameters": null,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null,
          "latency": null
        }
      ],
      "environment": "default",
      "updatedAt": "2025-10-23T07:27:19.980Z",
      "createdAt": "2025-10-23T07:26:45.864Z"
    },
    {
      "id": "d41337f4",
      "timestamp": "2025-10-23T07:26:10.989000+00:00",
      "name": "d413_model_converter",
      "input": null,
      "output": {
        "status": "success",
        "code": "import tensorflow as tf\n\n# Load the original Keras model\noriginal_model = tf.keras.models.load_model(\"models/fruit_to_emoji/og_model/model.keras\")\n\n# Convert the model to a TensorFlow Lite format with 8-bit integer quantization\nconverter = tf.lite.TFLiteConverter.from_keras_model(original_model)\n\n# Define the input and output data types for inference\ninference_input_type = tf.uint8\ninference_output_type = tf.uint8\n\n# Set the optimization strategy and representative dataset function\ndef representative_dataset_gen():\n    # Assuming a small sample of input data, e.g., numpy array with shape (num_samples, *input_shape)\n    import numpy as np\n    for _ in range(100):\n        yield [np.random.uniform(-1.0, 1.0, size=original_model.input_shape[1:]).astype(np.float32)]\n\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\nconverter.representative_dataset = representative_dataset_gen\n\n# Set supported operations for integer quantization\nsupported_ops = [\n    tf.lite.OpsSet.TFLITE_BUILTINS,  # Enable TensorFlow Lite ops.\n    tf.lite.OpsSet.TFLITE_BUILTINS_INT8  # Enable TensorFlow Lite ops with INT8 support.\n]\n\nconverter.target_spec.supported_ops = supported_ops\n# Set the input and output types to UINT8 for quantization\nconverter.inference_input_type = inference_input_type\nconverter.inference_output_type = inference_output_type\n\n# Convert the model\nquantized_tflite_model = converter.convert()\n\n# Save the converted model to a file\nwith open(\"models/fruit_to_emoji/tflite_model/model_quant.tflite\", \"wb\") as f:\n    f.write(quantized_tflite_model)"
      },
      "session_id": "phi4_d0f7_mc_batch",
      "metadata": {
        "num_run": 16
      },
      "tags": [
        "benchmark",
        "model_converter",
        "phi4:latest"
      ],
      "latency": 5.089,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-10-26-10-991275_chatcmpl-8caaa843-fc71-42ab-83d8-3e226303d60a",
          "traceId": "d41337f4",
          "type": "GENERATION",
          "name": "d4_mc_conversion_code_gen",
          "startTime": "2025-10-23T07:26:10.991000+00:00",
          "endTime": "2025-10-23T07:26:16.078000+00:00",
          "model": "phi4:latest",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 5087.0,
          "environment": "default",
          "promptTokens": 416,
          "completionTokens": 374,
          "totalTokens": 790,
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "f3ab8f5f-cb0b-4699-8362-d59d062b6758",
          "traceId": "d41337f4",
          "type": "SPAN",
          "name": "start_model_converter",
          "startTime": "2025-10-23T07:26:10.989000+00:00",
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "environment": "default",
          "promptTokens": 0,
          "completionTokens": 0,
          "totalTokens": 0,
          "costDetails": {},
          "endTime": null,
          "model": null,
          "modelParameters": null,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null,
          "latency": null
        }
      ],
      "environment": "default",
      "updatedAt": "2025-10-23T07:26:24.678Z",
      "createdAt": "2025-10-23T07:26:11.505Z"
    },
    {
      "id": "47a66ad6",
      "timestamp": "2025-10-23T07:25:15.688000+00:00",
      "name": "47a6_model_converter",
      "input": null,
      "output": {
        "status": "success",
        "code": "import tensorflow as tf\nimport numpy as np\n\n# Load the original Keras model from the specified path\noriginal_model = tf.keras.models.load_model(\"models/fruit_to_emoji/og_model/model.keras\")\n\n# Convert a Keras model to TFLite format with INT8 quantization.\nconverter = tf.lite.TFLiteConverter.from_keras_model(original_model)\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\n\n# Define representative dataset for calibration\ndef representative_dataset_gen():\n    for _ in range(100):\n        yield [np.random.rand(*original_model.input_shape[1:]).astype(np.float32)]\n\nconverter.representative_dataset = representative_dataset_gen\n\n# Set the supported operations to include INT8 quantization\nconverter.target_spec.supported_ops = [\n    tf.lite.OpsSet.TFLITE_BUILTINS_INT8,\n    tf.lite.OpsSet.TFLITE_BUILTINS,\n]\n\nconverter.inference_input_type = tf.uint8  # Use uint8 for quantized input.\nconverter.inference_output_type = tf.uint8  # Use uint8 for quantized output.\n\n# Convert the model\ntflite_quant_model = converter.convert()\n\n# Save the converted model to a file with predefined path\nwith open(\"models/fruit_to_emoji/tflite_model/model_quant.tflite\", \"wb\") as f:\n    f.write(tflite_quant_model)"
      },
      "session_id": "phi4_d0f7_mc_batch",
      "metadata": {
        "num_run": 15
      },
      "tags": [
        "benchmark",
        "model_converter",
        "phi4:latest"
      ],
      "latency": 23.756,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-10-25-15-689782_chatcmpl-b9c768a1-9e2d-4947-bea6-0cb0c020378d",
          "traceId": "47a66ad6",
          "type": "GENERATION",
          "name": "47_mc_conversion_code_gen",
          "startTime": "2025-10-23T07:25:15.689000+00:00",
          "endTime": "2025-10-23T07:25:19.696000+00:00",
          "model": "phi4:latest",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 4007.0,
          "environment": "default",
          "promptTokens": 416,
          "completionTokens": 296,
          "totalTokens": 712,
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-10-25-23-175477_chatcmpl-00eaab04-4670-49fc-81f0-df8d9535b64d",
          "traceId": "47a66ad6",
          "type": "GENERATION",
          "name": "47_mc_error_handling_attempt#2",
          "startTime": "2025-10-23T07:25:23.175000+00:00",
          "endTime": "2025-10-23T07:25:31.896000+00:00",
          "model": "phi4:latest",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 8721.0,
          "environment": "default",
          "promptTokens": 2048,
          "completionTokens": 545,
          "totalTokens": 2593,
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-10-25-34-862828_chatcmpl-04de47b6-b99b-41bd-ad46-64482007c71f",
          "traceId": "47a66ad6",
          "type": "GENERATION",
          "name": "47_mc_error_handling_attempt#3",
          "startTime": "2025-10-23T07:25:34.862000+00:00",
          "endTime": "2025-10-23T07:25:39.444000+00:00",
          "model": "phi4:latest",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 4582.0,
          "environment": "default",
          "promptTokens": 1232,
          "completionTokens": 293,
          "totalTokens": 1525,
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "67fc8bb9-0ae4-4124-ada4-204a9d15457c",
          "traceId": "47a66ad6",
          "type": "SPAN",
          "name": "start_model_converter",
          "startTime": "2025-10-23T07:25:15.688000+00:00",
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "environment": "default",
          "promptTokens": 0,
          "completionTokens": 0,
          "totalTokens": 0,
          "costDetails": {},
          "endTime": null,
          "model": null,
          "modelParameters": null,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null,
          "latency": null
        }
      ],
      "environment": "default",
      "updatedAt": "2025-10-23T07:25:50.356Z",
      "createdAt": "2025-10-23T07:25:16.205Z"
    },
    {
      "id": "b0865829",
      "timestamp": "2025-10-23T07:24:41.324000+00:00",
      "name": "b086_model_converter",
      "input": null,
      "output": {
        "status": "success",
        "code": "import tensorflow as tf\n\n# Load the original Keras model\noriginal_model = tf.keras.models.load_model(\"models/fruit_to_emoji/og_model/model.keras\")\n\n# Define representative dataset function for quantization\ndef representative_data_gen():\n    # Assuming a dummy dataset generator that yields float32 input data\n    for _ in range(100):\n        yield [tf.random.uniform((1, 224, 224, 3), dtype=tf.float32)]\n\n# Set up the converter with optimization and representative dataset\nconverter = tf.lite.TFLiteConverter.from_keras_model(original_model)\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\nconverter.representative_dataset = representative_data_gen\n\n# Define input/output data types for quantization\nconverter.inference_input_type = tf.uint8  # uint8 because of the specified output datatype\nconverter.inference_output_type = tf.uint8\n\n# Specify supported operations for int8 quantization\nconverter.target_spec.supported_ops = [\n    tf.lite.OpsSet.TFLITE_BUILTINS, \n    tf.lite.OpsSet.TFLITE_BUILTINS_INT8\n]\n\n# Convert the model to TFLite with 8-bit integer quantization\nquantized_model = converter.convert()\n\n# Save the converted model\nwith open(\"models/fruit_to_emoji/tflite_model/model_quant.tflite\", \"wb\") as f:\n    f.write(quantized_model)"
      },
      "session_id": "phi4_d0f7_mc_batch",
      "metadata": {
        "num_run": 14
      },
      "tags": [
        "benchmark",
        "model_converter",
        "phi4:latest"
      ],
      "latency": 4.232,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-10-24-41-327458_chatcmpl-fab15e77-a107-4f0b-9a6e-aa34acf60330",
          "traceId": "b0865829",
          "type": "GENERATION",
          "name": "b0_mc_conversion_code_gen",
          "startTime": "2025-10-23T07:24:41.327000+00:00",
          "endTime": "2025-10-23T07:24:45.558000+00:00",
          "model": "phi4:latest",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 4231.0,
          "environment": "default",
          "promptTokens": 416,
          "completionTokens": 300,
          "totalTokens": 716,
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "23b50462-7b0d-4695-8f6d-b2d8e5a5e4f3",
          "traceId": "b0865829",
          "type": "SPAN",
          "name": "start_model_converter",
          "startTime": "2025-10-23T07:24:41.326000+00:00",
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "environment": "default",
          "promptTokens": 0,
          "completionTokens": 0,
          "totalTokens": 0,
          "costDetails": {},
          "endTime": null,
          "model": null,
          "modelParameters": null,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null,
          "latency": null
        }
      ],
      "environment": "default",
      "updatedAt": "2025-10-23T07:24:54.954Z",
      "createdAt": "2025-10-23T07:24:41.843Z"
    },
    {
      "id": "8ae13cf9",
      "timestamp": "2025-10-23T07:23:56.889000+00:00",
      "name": "8ae1_model_converter",
      "input": null,
      "output": {
        "status": "success",
        "code": "import tensorflow as tf\nimport numpy as np\n\n# Define the input data types for quantization\ninference_input_type = [tf.float32]\ninference_output_type = [tf.uint8]\n\n# Specify the supported operations for quantized inference\nsupported_ops = [\n    tf.lite.OpsSet.TFLITE_BUILTINS_INT8,  # Enable int8 ops\n    tf.lite.OpsSet.TFLITE_BUILTINS        # Include standard built-in ops\n]\n\n# Load the original Keras model\noriginal_model = tf.keras.models.load_model(\"models/fruit_to_emoji/og_model/model.keras\")\n\n# Convert the model to TFLite with 8-bit integer quantization\nconverter = tf.lite.TFLiteConverter.from_keras_model(original_model)\n\n# Set the optimization flag for faster inference and smaller model size\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\n\n# Define representative dataset for calibration\ndef representative_data_gen():\n    for _ in range(100):\n        # Generate or fetch some sample data with the correct input shape\n        yield [np.random.normal(size=(1, *original_model.input_shape[1:])).astype(np.float32)]\n\nconverter.representative_dataset = representative_data_gen\n\n# Set the supported types to enable quantization\nconverter.supported_ops = supported_ops\n\n# Specify the type for each input and output tensor\nconverter.inference_input_type = tf.uint8  # Use uint8 for input data\nconverter.inference_output_type = tf.uint8 # Output is already set as uint8\n\n# Convert the model\ntflite_quant_model = converter.convert()\n\n# Save the converted model to a file\nwith open(\"models/fruit_to_emoji/tflite_model_quantized.tflite\", \"wb\") as f:\n    f.write(tflite_quant_model)"
      },
      "session_id": "phi4_d0f7_mc_batch",
      "metadata": {
        "num_run": 13
      },
      "tags": [
        "benchmark",
        "model_converter",
        "phi4:latest"
      ],
      "latency": 14.669,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-10-23-56-890938_chatcmpl-fe1bf6a9-6c4f-4fe0-b5fc-597ed6afc03a",
          "traceId": "8ae13cf9",
          "type": "GENERATION",
          "name": "8a_mc_conversion_code_gen",
          "startTime": "2025-10-23T07:23:56.890000+00:00",
          "endTime": "2025-10-23T07:24:02.043000+00:00",
          "model": "phi4:latest",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 5153.0,
          "environment": "default",
          "promptTokens": 416,
          "completionTokens": 370,
          "totalTokens": 786,
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-10-24-05-371287_chatcmpl-a1abac61-1da0-4f00-b5d0-dd8f185f5b1e",
          "traceId": "8ae13cf9",
          "type": "GENERATION",
          "name": "8a_mc_error_handling_attempt#2",
          "startTime": "2025-10-23T07:24:05.371000+00:00",
          "endTime": "2025-10-23T07:24:11.558000+00:00",
          "model": "phi4:latest",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 6187.0,
          "environment": "default",
          "promptTokens": 1879,
          "completionTokens": 381,
          "totalTokens": 2260,
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "a165abbe-ff73-4aef-87ca-ecc2b2a07c2e",
          "traceId": "8ae13cf9",
          "type": "SPAN",
          "name": "start_model_converter",
          "startTime": "2025-10-23T07:23:56.889000+00:00",
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "environment": "default",
          "promptTokens": 0,
          "completionTokens": 0,
          "totalTokens": 0,
          "costDetails": {},
          "endTime": null,
          "model": null,
          "modelParameters": null,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null,
          "latency": null
        }
      ],
      "environment": "default",
      "updatedAt": "2025-10-23T07:24:20.536Z",
      "createdAt": "2025-10-23T07:23:57.410Z"
    },
    {
      "id": "3a1bbd3d",
      "timestamp": "2025-10-23T07:22:52.470000+00:00",
      "name": "3a1b_model_converter",
      "input": null,
      "output": {
        "status": "success",
        "code": "import tensorflow as tf\n\n# Load your pre-trained Keras model\nmodel = tf.keras.applications.MobileNetV2(weights='imagenet', input_shape=(224, 224, 3))\n\n# Convert the model to TFLite format with dynamic range quantization\nconverter = tf.lite.TFLiteConverter.from_keras_model(model)\n\n# Define a representative dataset generator function for calibration\ndef representative_data_gen():\n    # Generate random data that matches your model's input shape and dtype\n    for _ in range(100):\n        yield [tf.random.uniform([1, 224, 224, 3], maxval=256, dtype=tf.float32)]\n\n# Set the optimization flag to `DEFAULT` which includes dynamic range quantization\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\n\n# Provide the representative data for calibration\nconverter.representative_dataset = representative_data_gen\n\n# Convert and save the model as a TFLite file\ntflite_quant_model = converter.convert()\n\nwith open('model_quant.tflite', 'wb') as f:\n    f.write(tflite_quant_model)\n\nprint(\"Quantized TFLite model saved successfully.\")"
      },
      "session_id": "phi4_d0f7_mc_batch",
      "metadata": {
        "num_run": 12
      },
      "tags": [
        "benchmark",
        "model_converter",
        "phi4:latest"
      ],
      "latency": 18.611,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-10-22-52-471914_chatcmpl-98aaec5c-1030-4f9a-bb0b-563096733736",
          "traceId": "3a1bbd3d",
          "type": "GENERATION",
          "name": "3a_mc_conversion_code_gen",
          "startTime": "2025-10-23T07:22:52.471000+00:00",
          "endTime": "2025-10-23T07:22:57.592000+00:00",
          "model": "phi4:latest",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 5121.0,
          "environment": "default",
          "promptTokens": 416,
          "completionTokens": 368,
          "totalTokens": 784,
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-10-23-01-071127_chatcmpl-37f6ebd7-3910-4af6-8cbc-1270d738fa4e",
          "traceId": "3a1bbd3d",
          "type": "GENERATION",
          "name": "3a_mc_error_handling_attempt#2",
          "startTime": "2025-10-23T07:23:01.071000+00:00",
          "endTime": "2025-10-23T07:23:11.081000+00:00",
          "model": "phi4:latest",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 10010.0,
          "environment": "default",
          "promptTokens": 2048,
          "completionTokens": 634,
          "totalTokens": 2682,
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "80582fef-82d6-4d6e-be31-2a2a431eee33",
          "traceId": "3a1bbd3d",
          "type": "SPAN",
          "name": "start_model_converter",
          "startTime": "2025-10-23T07:22:52.470000+00:00",
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "environment": "default",
          "promptTokens": 0,
          "completionTokens": 0,
          "totalTokens": 0,
          "costDetails": {},
          "endTime": null,
          "model": null,
          "modelParameters": null,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null,
          "latency": null
        }
      ],
      "environment": "default",
      "updatedAt": "2025-10-23T07:23:36.679Z",
      "createdAt": "2025-10-23T07:22:52.987Z"
    },
    {
      "id": "1ff9d584",
      "timestamp": "2025-10-23T07:22:18.105000+00:00",
      "name": "1ff9_model_converter",
      "input": null,
      "output": {
        "status": "success",
        "code": "import tensorflow as tf\n\n# Load the original Keras model\noriginal_model = tf.keras.models.load_model(\"models/fruit_to_emoji/og_model/model.keras\")\n\n# Convert the model to TensorFlow Lite format with 8-bit integer quantization\nconverter = tf.lite.TFLiteConverter.from_keras_model(original_model)\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\n\n# Define representative dataset for calibration\ndef representative_dataset_gen():\n    # Assuming we have a function to load and preprocess data as the model expects\n    # This should yield batches of input samples, e.g., from validation set\n    # Replace this with actual data loading logic\n    for _ in range(100):\n        sample_input = tf.random.normal([1, original_model.input_shape[1]], dtype=tf.float32)\n        yield [sample_input]\n\nconverter.representative_dataset = representative_dataset_gen\n\n# Set the supported operations to include INT8 quantization\nconverter.target_spec.supported_ops = [\n    tf.lite.OpsSet.TFLITE_BUILTINS,\n    tf.lite.OpsSet.TFLITE_BUILTINS_INT8\n]\n\n# Define input and output types for quantization\nconverter.inference_input_type = tf.uint8  # uint8 as specified in the configuration\nconverter.inference_output_type = tf.uint8  # uint8 as specified in the configuration\n\n# Convert the model\ntflite_model_quant = converter.convert()\n\n# Save the converted TFLite model to file\nwith open(\"models/fruit_to_emoji/tflite_model/model_quant.tflite\", \"wb\") as f:\n    f.write(tflite_model_quant)"
      },
      "session_id": "phi4_d0f7_mc_batch",
      "metadata": {
        "num_run": 11
      },
      "tags": [
        "benchmark",
        "model_converter",
        "phi4:latest"
      ],
      "latency": 4.729,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-10-22-18-107706_chatcmpl-260aa2b3-04e3-4f2a-85de-ee6bcc5013f6",
          "traceId": "1ff9d584",
          "type": "GENERATION",
          "name": "1f_mc_conversion_code_gen",
          "startTime": "2025-10-23T07:22:18.107000+00:00",
          "endTime": "2025-10-23T07:22:22.835000+00:00",
          "model": "phi4:latest",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 4728.0,
          "environment": "default",
          "promptTokens": 416,
          "completionTokens": 344,
          "totalTokens": 760,
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "1e6d5c8c-1258-4d74-a065-a16c8513eade",
          "traceId": "1ff9d584",
          "type": "SPAN",
          "name": "start_model_converter",
          "startTime": "2025-10-23T07:22:18.106000+00:00",
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "environment": "default",
          "promptTokens": 0,
          "completionTokens": 0,
          "totalTokens": 0,
          "costDetails": {},
          "endTime": null,
          "model": null,
          "modelParameters": null,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null,
          "latency": null
        }
      ],
      "environment": "default",
      "updatedAt": "2025-10-23T07:22:31.745Z",
      "createdAt": "2025-10-23T07:22:18.627Z"
    },
    {
      "id": "18a36bbb",
      "timestamp": "2025-10-23T07:21:21.624000+00:00",
      "name": "18a3_model_converter",
      "input": null,
      "output": {
        "status": "success",
        "code": "import numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras.models import load_model\n\n# Load the original Keras model from file\noriginal_model_path = \"models/fruit_to_emoji/og_model/model.keras\"\nmodel = load_model(original_model_path)\n\ndef representative_data_gen():\n    # Use real or representative data instead of random data for better calibration\n    for _ in range(100):  # Adjust the number according to your needs\n        sample_image = np.random.rand(1, 224, 224, 3).astype(np.float32)\n        yield [sample_image]\n\n# Define the converter with post-training quantization parameters\nconverter = tf.lite.TFLiteConverter.from_keras_model(model)\n\n# Set the optimization flag to default optimizations including quantization\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\n\n# Specify that we want to use integer-only quantization (quantize all ops)\nconverter.target_spec.supported_ops = [\n    tf.lite.OpsSet.TFLITE_BUILTINS_INT8  # Enable TensorFlow Lite Ops only for integer operations.\n]\nconverter.inference_input_type = tf.uint8\nconverter.inference_output_type = tf.uint8\n\n# Assign the representative dataset generator function\nconverter.representative_dataset = representative_data_gen\n\n# Convert the model to a TensorFlow Lite format with quantization\ntflite_quant_model = converter.convert()\n\n# Save the converted model to a file\nconverted_model_path = \"models/fruit_to_emoji/tflite_model/model_quant.tflite\"\nwith open(converted_model_path, 'wb') as f:\n    f.write(tflite_quant_model)\n\nprint(\"Model converted and saved successfully.\")"
      },
      "session_id": "phi4_d0f7_mc_batch",
      "metadata": {
        "num_run": 10
      },
      "tags": [
        "benchmark",
        "model_converter",
        "phi4:latest"
      ],
      "latency": 26.442,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-10-21-21-626584_chatcmpl-bc7e0bc2-d950-4910-ba2d-c8d2a96be9e5",
          "traceId": "18a36bbb",
          "type": "GENERATION",
          "name": "18_mc_conversion_code_gen",
          "startTime": "2025-10-23T07:21:21.626000+00:00",
          "endTime": "2025-10-23T07:21:26.556000+00:00",
          "model": "phi4:latest",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 4930.0,
          "environment": "default",
          "promptTokens": 416,
          "completionTokens": 360,
          "totalTokens": 776,
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-10-21-29-941769_chatcmpl-71903fa8-830a-4349-9da5-225b3762a5a8",
          "traceId": "18a36bbb",
          "type": "GENERATION",
          "name": "18_mc_error_handling_attempt#2",
          "startTime": "2025-10-23T07:21:29.941000+00:00",
          "endTime": "2025-10-23T07:21:39.681000+00:00",
          "model": "phi4:latest",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 9740.0,
          "environment": "default",
          "promptTokens": 2048,
          "completionTokens": 619,
          "totalTokens": 2667,
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-10-21-42-704209_chatcmpl-3a50a748-e3db-4bc0-bfa5-fb54ef2aab7f",
          "traceId": "18a36bbb",
          "type": "GENERATION",
          "name": "18_mc_error_handling_attempt#3",
          "startTime": "2025-10-23T07:21:42.704000+00:00",
          "endTime": "2025-10-23T07:21:48.067000+00:00",
          "model": "phi4:latest",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 5363.0,
          "environment": "default",
          "promptTokens": 1241,
          "completionTokens": 349,
          "totalTokens": 1590,
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "f8233f68-dc39-445e-97b4-88d40462cfac",
          "traceId": "18a36bbb",
          "type": "SPAN",
          "name": "start_model_converter",
          "startTime": "2025-10-23T07:21:21.625000+00:00",
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "environment": "default",
          "promptTokens": 0,
          "completionTokens": 0,
          "totalTokens": 0,
          "costDetails": {},
          "endTime": null,
          "model": null,
          "modelParameters": null,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null,
          "latency": null
        }
      ],
      "environment": "default",
      "updatedAt": "2025-10-23T07:21:57.830Z",
      "createdAt": "2025-10-23T07:21:22.145Z"
    },
    {
      "id": "b973199b",
      "timestamp": "2025-10-23T07:20:32.271000+00:00",
      "name": "b973_model_converter",
      "input": null,
      "output": {
        "status": "success",
        "code": "import tensorflow as tf\n\ndef representative_data_gen():\n    # This function should provide a dataset that is representative of the input data for calibration.\n    # Replace this with your actual dataset loading and preprocessing logic.\n    for _ in range(100):  # Assuming we have 100 samples to use for calibration\n        yield [tf.random.normal([1, 224, 224, 3])]  # Adjust shape as per your model's input\n\n# Define the original and converted model paths\noriginal_model_path = \"models/fruit_to_emoji/og_model/model.keras\"\nconverted_model_path = \"models/fruit_to_emoji/tflite_model/model_quant.tflite\"\n\n# Load the Keras model from file\nkeras_model = tf.keras.models.load_model(original_model_path)\n\n# Set up the TFLite converter\nconverter = tf.lite.TFLiteConverter.from_keras_model(keras_model)\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\nconverter.representative_dataset = representative_data_gen\nconverter.target_spec.supported_ops = [\n    tf.lite.OpsSet.TFLITE_BUILTINS_INT8,\n    tf.lite.OpsSet.TFLITE_BUILTINS\n]\nconverter.inference_input_type = tf.uint8  # Using uint8 for input quantization as per requirements\nconverter.inference_output_type = tf.uint8  # Using uint8 for output quantization as per requirements\n\n# Convert the model to TFLite format with quantization\ntflite_model_quant = converter.convert()\n\n# Save the converted model to a file\nwith open(converted_model_path, 'wb') as f:\n    f.write(tflite_model_quant)"
      },
      "session_id": "phi4_d0f7_mc_batch",
      "metadata": {
        "num_run": 9
      },
      "tags": [
        "benchmark",
        "model_converter",
        "phi4:latest"
      ],
      "latency": 18.828,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-10-20-32-273142_chatcmpl-cc7e4db2-1f02-45b1-8839-5e76c3c924ef",
          "traceId": "b973199b",
          "type": "GENERATION",
          "name": "b9_mc_conversion_code_gen",
          "startTime": "2025-10-23T07:20:32.273000+00:00",
          "endTime": "2025-10-23T07:20:35.689000+00:00",
          "model": "phi4:latest",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 3416.0,
          "environment": "default",
          "promptTokens": 416,
          "completionTokens": 251,
          "totalTokens": 667,
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-10-20-38-722101_chatcmpl-73506ae8-46b6-4a89-a8a1-b15b6255982e",
          "traceId": "b973199b",
          "type": "GENERATION",
          "name": "b9_mc_error_handling_attempt#2",
          "startTime": "2025-10-23T07:20:38.722000+00:00",
          "endTime": "2025-10-23T07:20:42.894000+00:00",
          "model": "phi4:latest",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 4172.0,
          "environment": "default",
          "promptTokens": 1219,
          "completionTokens": 272,
          "totalTokens": 1491,
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-10-20-45-963240_chatcmpl-0872340f-3808-4840-b250-25272f858e63",
          "traceId": "b973199b",
          "type": "GENERATION",
          "name": "b9_mc_error_handling_attempt#3",
          "startTime": "2025-10-23T07:20:45.963000+00:00",
          "endTime": "2025-10-23T07:20:51.099000+00:00",
          "model": "phi4:latest",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 5136.0,
          "environment": "default",
          "promptTokens": 1213,
          "completionTokens": 350,
          "totalTokens": 1563,
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "aef0f51f-9e21-4d4c-aedb-18abed9b19f2",
          "traceId": "b973199b",
          "type": "SPAN",
          "name": "start_model_converter",
          "startTime": "2025-10-23T07:20:32.271000+00:00",
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "environment": "default",
          "promptTokens": 0,
          "completionTokens": 0,
          "totalTokens": 0,
          "costDetails": {},
          "endTime": null,
          "model": null,
          "modelParameters": null,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null,
          "latency": null
        }
      ],
      "environment": "default",
      "updatedAt": "2025-10-23T07:21:00.924Z",
      "createdAt": "2025-10-23T07:20:32.790Z"
    },
    {
      "id": "ad75ca02",
      "timestamp": "2025-10-23T07:19:55.887000+00:00",
      "name": "ad75_model_converter",
      "input": null,
      "output": {
        "status": "success",
        "code": "import tensorflow as tf\n\n# Define the data types for inference input and output\ninference_input_type = tf.uint8  # Quantized input type: uint8\ninference_output_type = tf.uint8  # Quantized output type: uint8\n\n# Specify supported operations for quantization\nsupported_ops = [\n    tf.lite.OpsSet.TFLITE_BUILTINS,  # Enable TensorFlow Lite built-in ops\n    tf.lite.OpsSet.TFLITE_BUILTINS_INT8  # Enable int8 operations for quantized models\n]\n\ndef convert_model(original_model_path, converted_model_path):\n    # Load the Keras model\n    keras_model = tf.keras.models.load_model(original_model_path)\n\n    # Define a representative dataset generator for calibration\n    def representative_dataset_gen():\n        # Assuming we have a function to load and preprocess data\n        for input_value in tf.data.Dataset.from_tensor_slices(load_and_preprocess_data()):\n            yield [input_value]\n\n    # Convert the model to TFLite with quantization\n    converter = tf.lite.TFLiteConverter.from_keras_model(keras_model)\n    \n    # Set up the converter options for quantization\n    converter.optimizations = [tf.lite.Optimize.DEFAULT]\n    converter.representative_dataset = representative_dataset_gen\n    converter.target_spec.supported_ops = supported_ops\n    converter.inference_input_type = inference_input_type\n    converter.inference_output_type = inference_output_type\n\n    # Perform the conversion\n    tflite_quant_model = converter.convert()\n\n    # Save the quantized model to a file\n    with open(converted_model_path, 'wb') as f:\n        f.write(tflite_quant_model)\n\ndef load_and_preprocess_data():\n    # This function should return preprocessed data for calibration\n    # For demonstration purposes, we use random data\n    # Replace this with actual data loading and preprocessing logic\n    import numpy as np\n    return np.random.rand(100, 224, 224, 3).astype(np.float32)  # Example shape\n\n# Execute the conversion\nconvert_model(original_model_path=\"models/fruit_to_emoji/og_model/model.keras\", \n              converted_model_path=\"models/fruit_to_emoji/tflite_model/model_quant.tflite\")"
      },
      "session_id": "phi4_d0f7_mc_batch",
      "metadata": {
        "num_run": 8
      },
      "tags": [
        "benchmark",
        "model_converter",
        "phi4:latest"
      ],
      "latency": 6.549,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-10-19-55-889767_chatcmpl-f46bfbd1-db8b-4c5f-91b7-e0a401b4afc5",
          "traceId": "ad75ca02",
          "type": "GENERATION",
          "name": "ad_mc_conversion_code_gen",
          "startTime": "2025-10-23T07:19:55.889000+00:00",
          "endTime": "2025-10-23T07:20:02.437000+00:00",
          "model": "phi4:latest",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 6548.0,
          "environment": "default",
          "promptTokens": 416,
          "completionTokens": 474,
          "totalTokens": 890,
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "d592c26f-299b-48ae-8220-889bad2a2579",
          "traceId": "ad75ca02",
          "type": "SPAN",
          "name": "start_model_converter",
          "startTime": "2025-10-23T07:19:55.888000+00:00",
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "environment": "default",
          "promptTokens": 0,
          "completionTokens": 0,
          "totalTokens": 0,
          "costDetails": {},
          "endTime": null,
          "model": null,
          "modelParameters": null,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null,
          "latency": null
        }
      ],
      "environment": "default",
      "updatedAt": "2025-10-23T07:20:12.046Z",
      "createdAt": "2025-10-23T07:19:56.406Z"
    },
    {
      "id": "969fdfba",
      "timestamp": "2025-10-23T07:18:35.472000+00:00",
      "name": "969f_model_converter",
      "input": null,
      "output": {
        "status": "failed",
        "last_code": "import tensorflow as tf\n\n# Assume input_shape is defined somewhere in your script, e.g., (224, 224, 3)\ninput_shape = (224, 224, 3)\n\ndef representative_dataset_gen():\n    for _ in range(num_calibration_steps):\n        # Generate a random image or use an actual dataset sample\n        # Here we generate a random float32 image with the correct input shape\n        yield [tf.random.uniform(input_shape, minval=0, maxval=1, dtype=tf.float32)]\n\n# Define num_calibration_steps based on your calibration needs.\nnum_calibration_steps = 100\n\ndef convert_model():\n    converter = tf.lite.TFLiteConverter.from_keras_model(model)\n    \n    # Set optimization strategy to default which includes quantization\n    converter.optimizations = [tf.lite.Optimize.DEFAULT]\n    \n    # Specify the representative dataset for calibration\n    converter.representative_dataset = representative_dataset_gen\n    \n    # Ensure all inputs and outputs are UINT8, since we want full integer quantization\n    converter.target_spec.supported_ops = [\n        tf.lite.OpsSet.TFLITE_BUILTINS_INT8\n    ]\n    \n    # Set the input and output tensors to uint8 with scale and zero-point for int8 quantization\n    converter.inference_input_type = tf.uint8  # or tf.int8 if you want symmetric quantization\n    converter.inference_output_type = tf.uint8  # or tf.int8\n\n    # Convert the model\n    tflite_quant_model = converter.convert()\n    \n    # Save the converted model to a file\n    with open('model.tflite', 'wb') as f:\n        f.write(tflite_quant_model)\n\n# Call this function to perform the conversion\nconvert_model()",
        "last_error": "Failed to generate valid code after the max 5 attempts. Last error from code execution: 2025-10-23 10:19:27.061551: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n2025-10-23 10:19:27.064588: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n2025-10-23 10:19:27.075291: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2025-10-23 10:19:27.092804: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2025-10-23 10:19:27.098156: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n2025-10-23 10:19:27.110651: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\nTo enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n2025-10-23 10:19:27.954862: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\nTraceback (most recent call last):\n  File \"/home/wuguangh/Projects/tinyml-autopilot/models/fruit_to_emoji/tflite_model/tmp_converter20251023101926.py/tmp_20251023101926_mc_phi4:latest/tmp_20251023101926_mc_phi4:latest.py\", line 41, in <module>\n    convert_model()\n  File \"/home/wuguangh/Projects/tinyml-autopilot/models/fruit_to_emoji/tflite_model/tmp_converter20251023101926.py/tmp_20251023101926_mc_phi4:latest/tmp_20251023101926_mc_phi4:latest.py\", line 16, in convert_model\n    converter = tf.lite.TFLiteConverter.from_keras_model(model)\nNameError: name 'model' is not defined\n"
      },
      "session_id": "phi4_d0f7_mc_batch",
      "metadata": {
        "num_run": 7
      },
      "tags": [
        "benchmark",
        "model_converter",
        "phi4:latest"
      ],
      "latency": 54.363,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-10-18-35-474413_chatcmpl-f7061d3a-cb1c-4a9c-91c3-5c5f1ebcf342",
          "traceId": "969fdfba",
          "type": "GENERATION",
          "name": "96_mc_conversion_code_gen",
          "startTime": "2025-10-23T07:18:35.474000+00:00",
          "endTime": "2025-10-23T07:18:40.216000+00:00",
          "model": "phi4:latest",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 4742.0,
          "environment": "default",
          "promptTokens": 416,
          "completionTokens": 352,
          "totalTokens": 768,
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-10-18-40-243770_chatcmpl-c3127585-5647-4889-8f1c-4e583de716e2",
          "traceId": "969fdfba",
          "type": "GENERATION",
          "name": "96_mc_error_handling_attempt#2",
          "startTime": "2025-10-23T07:18:40.243000+00:00",
          "endTime": "2025-10-23T07:18:45.159000+00:00",
          "model": "phi4:latest",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 4916.0,
          "environment": "default",
          "promptTokens": 892,
          "completionTokens": 342,
          "totalTokens": 1234,
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-10-18-48-646539_chatcmpl-024fcee4-8ddf-4d79-a8d9-a86159a4be99",
          "traceId": "969fdfba",
          "type": "GENERATION",
          "name": "96_mc_error_handling_attempt#3",
          "startTime": "2025-10-23T07:18:48.646000+00:00",
          "endTime": "2025-10-23T07:18:54.933000+00:00",
          "model": "phi4:latest",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 6287.0,
          "environment": "default",
          "promptTokens": 1833,
          "completionTokens": 396,
          "totalTokens": 2229,
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-10-18-58-368286_chatcmpl-748ff7f9-9866-437a-94e2-7a37a749af06",
          "traceId": "969fdfba",
          "type": "GENERATION",
          "name": "96_mc_error_handling_attempt#4",
          "startTime": "2025-10-23T07:18:58.368000+00:00",
          "endTime": "2025-10-23T07:19:11.603000+00:00",
          "model": "phi4:latest",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 13235.0,
          "environment": "default",
          "promptTokens": 2048,
          "completionTokens": 860,
          "totalTokens": 2908,
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-10-19-14-928955_chatcmpl-255b1ec4-afc1-45f9-ab88-e1b85b8b2e96",
          "traceId": "969fdfba",
          "type": "GENERATION",
          "name": "96_mc_error_handling_attempt#5",
          "startTime": "2025-10-23T07:19:14.928000+00:00",
          "endTime": "2025-10-23T07:19:26.803000+00:00",
          "model": "phi4:latest",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 11875.0,
          "environment": "default",
          "promptTokens": 2048,
          "completionTokens": 769,
          "totalTokens": 2817,
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "492ebcbb-61f4-40ae-b151-4cfb86775bbd",
          "traceId": "969fdfba",
          "type": "SPAN",
          "name": "error_96_mc_failure_signal_model_converter",
          "startTime": "2025-10-23T07:19:29.835000+00:00",
          "level": "ERROR",
          "statusMessage": "Failed. Last error: Failed to generate valid code after the max 5 attempts. Last error from code execution: 2025-10-23 10:19:27.061551: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n2025-10-23 10:19:27.064588: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n2025-10-23 10:19:27.075291: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2025-10-23 10:19:27.092804: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2025-10-23 10:19:27.098156: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n2025-10-23 10:19:27.110651: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\nTo enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n2025-10-23 10:19:27.954862: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\nTraceback (most recent call last):\n  File \"/home/wuguangh/Projects/tinyml-autopilot/models/fruit_to_emoji/tflite_model/tmp_converter20251023101926.py/tmp_20251023101926_mc_phi4:latest/tmp_20251023101926_mc_phi4:latest.py\", line 41, in <module>\n    convert_model()\n  File \"/home/wuguangh/Projects/tinyml-autopilot/models/fruit_to_emoji/tflite_model/tmp_converter20251023101926.py/tmp_20251023101926_mc_phi4:latest/tmp_20251023101926_mc_phi4:latest.py\", line 16, in convert_model\n    converter = tf.lite.TFLiteConverter.from_keras_model(model)\nNameError: name 'model' is not defined\n",
          "calculatedTotalCost": 0.0,
          "environment": "default",
          "promptTokens": 0,
          "completionTokens": 0,
          "totalTokens": 0,
          "costDetails": {},
          "endTime": null,
          "model": null,
          "modelParameters": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null,
          "latency": null
        },
        {
          "id": "a002ee12-eac9-44dd-8444-f00489315b82",
          "traceId": "969fdfba",
          "type": "SPAN",
          "name": "start_model_converter",
          "startTime": "2025-10-23T07:18:35.472000+00:00",
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "environment": "default",
          "promptTokens": 0,
          "completionTokens": 0,
          "totalTokens": 0,
          "costDetails": {},
          "endTime": null,
          "model": null,
          "modelParameters": null,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null,
          "latency": null
        }
      ],
      "environment": "default",
      "updatedAt": "2025-10-23T07:19:35.123Z",
      "createdAt": "2025-10-23T07:18:35.991Z"
    },
    {
      "id": "25406099",
      "timestamp": "2025-10-23T07:18:02.038000+00:00",
      "name": "2540_model_converter",
      "input": null,
      "output": {
        "status": "success",
        "code": "import tensorflow as tf\n\n# Load the Keras model\nmodel = tf.keras.models.load_model(\"models/fruit_to_emoji/og_model/model.keras\")\n\n# Convert the model to TensorFlow Lite format with quantization\nconverter = tf.lite.TFLiteConverter.from_keras_model(model)\n\n# Set up 8-bit integer quantization configuration\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\nconverter.representative_dataset = lambda: [\n    (input_data, input_data)\n    for _ in range(100)  # Use a sample generator function here\n]\n\n# Define the input and output types\ndef representative_data_gen():\n    for _ in range(100):\n        yield [tf.random.normal([1, *model.input_shape[1:]])]  # Adjust input shape as necessary\n\nconverter.representative_dataset = representative_data_gen\nconverter.target_spec.supported_ops = [\n    tf.lite.OpsSet.TFLITE_BUILTINS,\n    tf.lite.OpsSet.TFLITE_BUILTINS_INT8\n]\nconverter.inference_input_type = tf.uint8  # 8-bit unsigned integer input\nconverter.inference_output_type = tf.uint8  # 8-bit unsigned integer output\n\n# Convert the model and save it to a file\ntflite_quant_model = converter.convert()\nwith open(\"models/fruit_to_emoji/tflite_model/model_quant.tflite\", \"wb\") as f:\n    f.write(tflite_quant_model)"
      },
      "session_id": "phi4_d0f7_mc_batch",
      "metadata": {
        "num_run": 6
      },
      "tags": [
        "benchmark",
        "model_converter",
        "phi4:latest"
      ],
      "latency": 4.167,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-10-18-02-040885_chatcmpl-23100716-0be3-4e28-8545-804116b6c5ce",
          "traceId": "25406099",
          "type": "GENERATION",
          "name": "25_mc_conversion_code_gen",
          "startTime": "2025-10-23T07:18:02.040000+00:00",
          "endTime": "2025-10-23T07:18:06.206000+00:00",
          "model": "phi4:latest",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 4166.0,
          "environment": "default",
          "promptTokens": 416,
          "completionTokens": 308,
          "totalTokens": 724,
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "59c359ec-20b1-46ba-84c8-439abf899613",
          "traceId": "25406099",
          "type": "SPAN",
          "name": "start_model_converter",
          "startTime": "2025-10-23T07:18:02.039000+00:00",
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "environment": "default",
          "promptTokens": 0,
          "completionTokens": 0,
          "totalTokens": 0,
          "costDetails": {},
          "endTime": null,
          "model": null,
          "modelParameters": null,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null,
          "latency": null
        }
      ],
      "environment": "default",
      "updatedAt": "2025-10-23T07:18:15.225Z",
      "createdAt": "2025-10-23T07:18:02.557Z"
    },
    {
      "id": "14249bdd",
      "timestamp": "2025-10-23T07:17:26.551000+00:00",
      "name": "1424_model_converter",
      "input": null,
      "output": {
        "status": "success",
        "code": "import tensorflow as tf\n\ndef representative_data_gen():\n    # This function should yield a batch of input data that is representative \n    # of the dataset used for training. Adjust the path and method to load your specific data.\n    import numpy as np\n    \n    # Dummy data: replace with actual representative dataset\n    for _ in range(100):\n        yield [np.random.rand(1, 224, 224, 3).astype(np.float32)]\n\n# Load the original Keras model\noriginal_model = tf.keras.models.load_model(\"models/fruit_to_emoji/og_model/model.keras\")\n\n# Set up TFLite converter with quantization configurations\nconverter = tf.lite.TFLiteConverter.from_keras_model(original_model)\n\n# Specify input and output data types\ninference_input_type = tf.uint8  # uint8 for integer quantization\ninference_output_type = tf.uint8\n\n# Configure the supported operations for quantized inference\nsupported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, \n                 tf.lite.OpsSet.SELECT_TF_OPS,\n                 tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n\nconverter.inference_input_type = inference_input_type\nconverter.inference_output_type = inference_output_type\nconverter.supported_ops = supported_ops\n\n# Provide a representative dataset for quantization calibration\nconverter.representative_dataset = representative_data_gen\n\n# Enable integer quantization\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\nconverter.target_spec.supported_types = [tf.int8]\n\n# Convert the model to TFLite format with 8-bit integer quantization\ntflite_quant_model = converter.convert()\n\n# Save the converted model to a file\nwith open(\"models/fruit_to_emoji/tflite_model/model_quant.tflite\", \"wb\") as f:\n    f.write(tflite_quant_model)"
      },
      "session_id": "phi4_d0f7_mc_batch",
      "metadata": {
        "num_run": 5
      },
      "tags": [
        "benchmark",
        "model_converter",
        "phi4:latest"
      ],
      "latency": 5.304,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-10-17-26-553985_chatcmpl-5357ac52-249b-4f33-836d-613879d020cc",
          "traceId": "14249bdd",
          "type": "GENERATION",
          "name": "14_mc_conversion_code_gen",
          "startTime": "2025-10-23T07:17:26.553000+00:00",
          "endTime": "2025-10-23T07:17:31.856000+00:00",
          "model": "phi4:latest",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 5303.0,
          "environment": "default",
          "promptTokens": 416,
          "completionTokens": 394,
          "totalTokens": 810,
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "7cd66842-566c-4336-be08-e5542700c050",
          "traceId": "14249bdd",
          "type": "SPAN",
          "name": "start_model_converter",
          "startTime": "2025-10-23T07:17:26.552000+00:00",
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "environment": "default",
          "promptTokens": 0,
          "completionTokens": 0,
          "totalTokens": 0,
          "costDetails": {},
          "endTime": null,
          "model": null,
          "modelParameters": null,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null,
          "latency": null
        }
      ],
      "environment": "default",
      "updatedAt": "2025-10-23T07:17:41.406Z",
      "createdAt": "2025-10-23T07:17:27.070Z"
    },
    {
      "id": "90548589",
      "timestamp": "2025-10-23T07:16:52.029000+00:00",
      "name": "9054_model_converter",
      "input": null,
      "output": {
        "status": "success",
        "code": "import tensorflow as tf\n\ndef representative_data_gen():\n    # This function is a placeholder. In practice, it should yield batches of input data.\n    for _ in range(100):\n        yield [tf.random.normal([1, 224, 224, 3]).numpy()]\n\noriginal_model_path = \"models/fruit_to_emoji/og_model/model.keras\"\nconverted_model_path = \"models/fruit_to_emoji/tflite_model/model_quant.tflite\"\n\n# Load the Keras model\nkeras_model = tf.keras.models.load_model(original_model_path)\n\n# Convert the Keras model to a TensorFlow Lite model with 8-bit integer quantization\nconverter = tf.lite.TFLiteConverter.from_keras_model(keras_model)\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\nconverter.representative_dataset = representative_data_gen\n\n# Define input and output types for quantization\ninput_tensor_type = tf.float32\noutput_tensor_type = tf.uint8\n\n# Set supported operations\nsupported_ops = [\n    tf.lite.OpsSet.TFLITE_BUILTINS,  # enable TensorFlow Lite ops.\n    tf.lite.OpsSet.SELECT_TF_OPS     # enable TensorFlow ops.\n]\n\nconverter.target_spec.supported_ops = supported_ops\nconverter.inference_input_type = input_tensor_type\nconverter.inference_output_type = output_tensor_type\n\n# Perform the conversion\ntflite_quant_model = converter.convert()\n\n# Save the converted model to a file\nwith open(converted_model_path, \"wb\") as f:\n    f.write(tflite_quant_model)"
      },
      "session_id": "phi4_d0f7_mc_batch",
      "metadata": {
        "num_run": 4
      },
      "tags": [
        "benchmark",
        "model_converter",
        "phi4:latest"
      ],
      "latency": 4.536,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-10-16-52-031739_chatcmpl-b1e80d8a-b2f6-4df9-8b87-abbad8fe578f",
          "traceId": "90548589",
          "type": "GENERATION",
          "name": "90_mc_conversion_code_gen",
          "startTime": "2025-10-23T07:16:52.031000+00:00",
          "endTime": "2025-10-23T07:16:56.565000+00:00",
          "model": "phi4:latest",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 4534.0,
          "environment": "default",
          "promptTokens": 416,
          "completionTokens": 329,
          "totalTokens": 745,
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "100e82d7-92f1-40fd-ae1a-69fae01b7315",
          "traceId": "90548589",
          "type": "SPAN",
          "name": "start_model_converter",
          "startTime": "2025-10-23T07:16:52.029000+00:00",
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "environment": "default",
          "promptTokens": 0,
          "completionTokens": 0,
          "totalTokens": 0,
          "costDetails": {},
          "endTime": null,
          "model": null,
          "modelParameters": null,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null,
          "latency": null
        }
      ],
      "environment": "default",
      "updatedAt": "2025-10-23T07:17:06.185Z",
      "createdAt": "2025-10-23T07:16:52.547Z"
    },
    {
      "id": "a3a56d4b",
      "timestamp": "2025-10-23T07:15:53.647000+00:00",
      "name": "a3a5_model_converter",
      "input": null,
      "output": {
        "status": "success",
        "code": "import tensorflow as tf\n\n# Load the original Keras model\noriginal_model = tf.keras.models.load_model(\"models/fruit_to_emoji/og_model/model.keras\")\n\n# Convert the loaded Keras model to a TFLite model with full integer quantization\nconverter = tf.lite.TFLiteConverter.from_keras_model(original_model)\n\ndef representative_data_gen():\n    # Generate data that matches the expected input shape and dtype of the model\n    for _ in range(100):  # Number of samples can be adjusted\n        # Generate a random tensor with shape [1, 224, 224, 3] and values between 0 and 1\n        yield [tf.random.uniform([1, 224, 224, 3], dtype=tf.float32)]\n\n# Set the representative dataset for quantization\nconverter.representative_dataset = representative_data_gen\n\n# Specify that we want to use full integer quantization\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\nconverter.target_spec.supported_ops = [\n    tf.lite.OpsSet.TFLITE_BUILTINS_INT8,\n    tf.lite.OpsSet.SELECT_TF_OPS\n]\n\n# Set the input and output types for quantization compatibility\nconverter.inference_input_type = tf.uint8  # Use uint8 as per configuration\nconverter.inference_output_type = tf.uint8\n\n# Convert the model\ntflite_quant_model = converter.convert()\n\n# Save the converted model to a file\nwith open(\"models/fruit_to_emoji/tflite_model/model_quant.tflite\", \"wb\") as f:\n    f.write(tflite_quant_model)\n\nprint(\"Model successfully converted and saved.\")"
      },
      "session_id": "phi4_d0f7_mc_batch",
      "metadata": {
        "num_run": 3
      },
      "tags": [
        "benchmark",
        "model_converter",
        "phi4:latest"
      ],
      "latency": 28.098,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-10-15-53-648835_chatcmpl-658d7e6b-ca03-46c3-9928-961df69e4ed5",
          "traceId": "a3a56d4b",
          "type": "GENERATION",
          "name": "a3_mc_conversion_code_gen",
          "startTime": "2025-10-23T07:15:53.648000+00:00",
          "endTime": "2025-10-23T07:15:58.790000+00:00",
          "model": "phi4:latest",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 5142.0,
          "environment": "default",
          "promptTokens": 416,
          "completionTokens": 376,
          "totalTokens": 792,
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-10-16-02-203426_chatcmpl-6a867379-3e0a-4411-ac52-e3f5512c15b8",
          "traceId": "a3a56d4b",
          "type": "GENERATION",
          "name": "a3_mc_error_handling_attempt#2",
          "startTime": "2025-10-23T07:16:02.203000+00:00",
          "endTime": "2025-10-23T07:16:13.004000+00:00",
          "model": "phi4:latest",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 10801.0,
          "environment": "default",
          "promptTokens": 2048,
          "completionTokens": 668,
          "totalTokens": 2716,
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-10-16-16-059922_chatcmpl-f5c6d375-128f-4e4e-ac57-26bff5143829",
          "traceId": "a3a56d4b",
          "type": "GENERATION",
          "name": "a3_mc_error_handling_attempt#3",
          "startTime": "2025-10-23T07:16:16.059000+00:00",
          "endTime": "2025-10-23T07:16:21.745000+00:00",
          "model": "phi4:latest",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 5686.0,
          "environment": "default",
          "promptTokens": 1561,
          "completionTokens": 351,
          "totalTokens": 1912,
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "d3f9cec4-1d01-46d8-91cf-15742f53816b",
          "traceId": "a3a56d4b",
          "type": "SPAN",
          "name": "start_model_converter",
          "startTime": "2025-10-23T07:15:53.647000+00:00",
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "environment": "default",
          "promptTokens": 0,
          "completionTokens": 0,
          "totalTokens": 0,
          "costDetails": {},
          "endTime": null,
          "model": null,
          "modelParameters": null,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null,
          "latency": null
        }
      ],
      "environment": "default",
      "updatedAt": "2025-10-23T07:16:31.359Z",
      "createdAt": "2025-10-23T07:15:54.166Z"
    },
    {
      "id": "63449ff6",
      "timestamp": "2025-10-23T07:14:58.270000+00:00",
      "name": "6344_model_converter",
      "input": null,
      "output": {
        "status": "success",
        "code": "import tensorflow as tf\n\n# Load the Keras model from a .keras file\nmodel = tf.keras.models.load_model('models/fruit_to_emoji/og_model/model.keras')\n\n# Define the converter for TFLite with quantization\nconverter = tf.lite.TFLiteConverter.from_keras_model(model)\n\n# Set optimizations and enable default optimizations\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\n\n# Create a representative dataset generator function for quantization calibration\ndef representative_dataset_gen():\n    # Assuming input data shape is (1, 224, 224, 3) based on the model's expected input size\n    for _ in range(100):\n        yield [tf.random.uniform([1, 224, 224, 3], minval=0, maxval=255, dtype=tf.float32)]\n\n# Assign the representative dataset to the converter\nconverter.representative_dataset = representative_dataset_gen\n\n# Set supported operations and types for quantization\nconverter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\nconverter.inference_input_type = tf.uint8  # Input type as uint8 for quantized model\nconverter.inference_output_type = tf.uint8  # Output type as uint8\n\n# Convert the model to TFLite format with quantization\ntflite_quant_model = converter.convert()\n\n# Save the converted and quantized model to a file\nwith open('models/fruit_to_emoji/tflite_model/model_quant.tflite', 'wb') as f:\n    f.write(tflite_quant_model)"
      },
      "session_id": "phi4_d0f7_mc_batch",
      "metadata": {
        "num_run": 2
      },
      "tags": [
        "benchmark",
        "model_converter",
        "phi4:latest"
      ],
      "latency": 25.019,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-10-14-58-272850_chatcmpl-53baace0-bf93-4bbd-ba05-474d86686061",
          "traceId": "63449ff6",
          "type": "GENERATION",
          "name": "63_mc_conversion_code_gen",
          "startTime": "2025-10-23T07:14:58.272000+00:00",
          "endTime": "2025-10-23T07:15:03.198000+00:00",
          "model": "phi4:latest",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 4926.0,
          "environment": "default",
          "promptTokens": 416,
          "completionTokens": 347,
          "totalTokens": 763,
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-10-15-06-709239_chatcmpl-785d6f27-cd3d-403a-8d43-348173bb61e3",
          "traceId": "63449ff6",
          "type": "GENERATION",
          "name": "63_mc_error_handling_attempt#2",
          "startTime": "2025-10-23T07:15:06.709000+00:00",
          "endTime": "2025-10-23T07:15:14.736000+00:00",
          "model": "phi4:latest",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 8027.0,
          "environment": "default",
          "promptTokens": 2048,
          "completionTokens": 490,
          "totalTokens": 2538,
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-10-15-17-903170_chatcmpl-8b2b9da8-ce59-4abb-953d-b60434c8aa09",
          "traceId": "63449ff6",
          "type": "GENERATION",
          "name": "63_mc_error_handling_attempt#3",
          "startTime": "2025-10-23T07:15:17.903000+00:00",
          "endTime": "2025-10-23T07:15:23.290000+00:00",
          "model": "phi4:latest",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 5387.0,
          "environment": "default",
          "promptTokens": 1478,
          "completionTokens": 335,
          "totalTokens": 1813,
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "40bd704b-dc43-44e0-b52e-234e66a93cf5",
          "traceId": "63449ff6",
          "type": "SPAN",
          "name": "start_model_converter",
          "startTime": "2025-10-23T07:14:58.271000+00:00",
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "environment": "default",
          "promptTokens": 0,
          "completionTokens": 0,
          "totalTokens": 0,
          "costDetails": {},
          "endTime": null,
          "model": null,
          "modelParameters": null,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null,
          "latency": null
        }
      ],
      "environment": "default",
      "updatedAt": "2025-10-23T07:15:32.957Z",
      "createdAt": "2025-10-23T07:14:58.790Z"
    },
    {
      "id": "a98e49a3",
      "timestamp": "2025-10-23T07:12:32.877000+00:00",
      "name": "a98e_model_converter",
      "input": null,
      "output": {
        "status": "success",
        "code": "import tensorflow as tf\n\n# Load the original Keras model\noriginal_model = tf.keras.models.load_model(\"models/fruit_to_emoji/og_model/model.keras\")\n\n# Set up converter with 8-bit integer quantization configuration\nconverter = tf.lite.TFLiteConverter.from_keras_model(original_model)\n\n# Define input and output data types for quantization to float32 during calibration\ninference_input_type = tf.float32\ninference_output_type = tf.float32\n\n# Specify supported operations for TFLite with int8 quantized operators enabled\nsupported_ops = [\n    tf.lite.OpsSet.TFLITE_BUILTINS,  # Enable TensorFlow Lite ops.\n    tf.lite.OpsSet.TFLITE_BUILTINS_INT8  # Enable int8 quantized operators.\n]\n\nconverter.target_spec.supported_ops = supported_ops\n\n# Set the input and output tensors to float32 for conversion\nconverter.inference_input_type = inference_input_type\nconverter.inference_output_type = inference_output_type\n\n# Define a representative dataset function for calibration\ndef representative_data_gen():\n    # Assuming data is normalized, replace with actual data loading logic if necessary\n    for _ in range(100):\n        yield [tf.random.normal([1] + list(original_model.input_shape[1:]), dtype=tf.float32)]\n\nconverter.representative_dataset = representative_data_gen\n\n# Set the optimization strategy to optimize for size and quantization\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\n\n# Convert the model\ntflite_quant_model = converter.convert()\n\n# Save the converted model to a .tflite file\nwith open(\"models/fruit_to_emoji/tflite_model/model_quant.tflite\", \"wb\") as f:\n    f.write(tflite_quant_model)"
      },
      "session_id": "phi4_d0f7_mc_batch",
      "metadata": {
        "num_run": 1
      },
      "tags": [
        "benchmark",
        "model_converter",
        "phi4:latest"
      ],
      "latency": 116.452,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-10-12-32-887365_chatcmpl-13998755-cf66-461c-8b7e-78ef8abc4907",
          "traceId": "a98e49a3",
          "type": "GENERATION",
          "name": "a9_mc_conversion_code_gen",
          "startTime": "2025-10-23T07:12:32.887000+00:00",
          "endTime": "2025-10-23T07:13:36.725000+00:00",
          "model": "phi4:latest",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 63838.0,
          "environment": "default",
          "promptTokens": 416,
          "completionTokens": 369,
          "totalTokens": 785,
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-10-13-54-529496_chatcmpl-9f1914a5-a72e-4dd1-83b2-5004356f2aa2",
          "traceId": "a98e49a3",
          "type": "GENERATION",
          "name": "a9_mc_error_handling_attempt#2",
          "startTime": "2025-10-23T07:13:54.529000+00:00",
          "endTime": "2025-10-23T07:14:00.071000+00:00",
          "model": "phi4:latest",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 5542.0,
          "environment": "default",
          "promptTokens": 1861,
          "completionTokens": 326,
          "totalTokens": 2187,
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-10-14-03-455739_chatcmpl-c13e08e7-8d86-4cfa-9471-04e8d5a55bd4",
          "traceId": "a98e49a3",
          "type": "GENERATION",
          "name": "a9_mc_error_handling_attempt#3",
          "startTime": "2025-10-23T07:14:03.455000+00:00",
          "endTime": "2025-10-23T07:14:09.410000+00:00",
          "model": "phi4:latest",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 5955.0,
          "environment": "default",
          "promptTokens": 1817,
          "completionTokens": 359,
          "totalTokens": 2176,
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-10-14-12-850260_chatcmpl-07788919-442d-45ff-8743-97627a5089fd",
          "traceId": "a98e49a3",
          "type": "GENERATION",
          "name": "a9_mc_error_handling_attempt#4",
          "startTime": "2025-10-23T07:14:12.850000+00:00",
          "endTime": "2025-10-23T07:14:20.401000+00:00",
          "model": "phi4:latest",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 7551.0,
          "environment": "default",
          "promptTokens": 1850,
          "completionTokens": 470,
          "totalTokens": 2320,
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-10-14-23-483062_chatcmpl-7f8bb548-2284-48b6-bf2d-ec0bf897e283",
          "traceId": "a98e49a3",
          "type": "GENERATION",
          "name": "a9_mc_error_handling_attempt#5",
          "startTime": "2025-10-23T07:14:23.483000+00:00",
          "endTime": "2025-10-23T07:14:29.329000+00:00",
          "model": "phi4:latest",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 5846.0,
          "environment": "default",
          "promptTokens": 1488,
          "completionTokens": 370,
          "totalTokens": 1858,
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "ebbbed7d-0d00-4e65-b243-7cbecbad0d66",
          "traceId": "a98e49a3",
          "type": "SPAN",
          "name": "start_model_converter",
          "startTime": "2025-10-23T07:12:32.877000+00:00",
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "environment": "default",
          "promptTokens": 0,
          "completionTokens": 0,
          "totalTokens": 0,
          "costDetails": {},
          "endTime": null,
          "model": null,
          "modelParameters": null,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null,
          "latency": null
        }
      ],
      "environment": "default",
      "updatedAt": "2025-10-23T07:14:38.262Z",
      "createdAt": "2025-10-23T07:12:33.583Z"
    }
  ],
  "meta": {
    "total_items": 28
  }
}
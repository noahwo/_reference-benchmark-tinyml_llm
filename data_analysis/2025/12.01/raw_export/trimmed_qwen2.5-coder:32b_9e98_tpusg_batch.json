{
  "data": [
    {
      "id": "8be97509",
      "timestamp": "2025-12-10T16:54:38.330000+00:00",
      "name": "8be9_tpu_sketch_generator",
      "input": null,
      "output": {
        "status": "failed",
        "last_code": "import os\nimport cv2\nimport numpy as np\nfrom PIL import Image\nimport tflite_runtime.interpreter as tflite\nfrom pycoral.utils.edgetpu import make_interpreter\n\n# Configuration parameters\nmodel_path  = \"/home/mendel/tinyml_autopilot/models/edgetpu_detect.tflite\"\nlabel_path  = \"/home/mendel/tinyml_autopilot/models/labelmap.txt\"\ninput_path  = \"/home/mendel/tinyml_autopilot/data//sheeps.mp4\"\noutput_path  = \"/home/mendel/tinyml_autopilot/results/sheeps_detections.mp4\"\nconfidence_threshold  = 0.5\n\n# Load labels\ndef load_labels(filename):\n    with open(filename, 'r', encoding='utf-8') as f:\n        return {int(line.split(' ', maxsplit=1)[0]): line.strip() for line in f.readlines()}\n\nlabels = load_labels(label_path)\n\n# Initialize interpreter and allocate tensors\ninterpreter = make_interpreter(model_path, device=':0')\ninterpreter.allocate_tensors()\n\n# Get input and output details\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\n# Read video file\ncap = cv2.VideoCapture(input_path)\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\nout = cv2.VideoWriter(output_path, fourcc, cap.get(cv2.CAP_PROP_FPS), (int(cap.get(3)), int(cap.get(4))))\n\ndef preprocess(image):\n    image = Image.fromarray(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n    input_shape = input_details[0]['shape']\n    image = image.resize((input_shape[1], input_shape[2]), Image.ANTIALIAS)\n    return np.array(image)\n\ndef postprocess(output_data, frame_width, frame_height):\n    boxes = output_data['output_boxes'][0]\n    class_ids = output_data['output_classes'][0]\n    scores = output_data['output_scores'][0]\n\n    detections = []\n    for box, score, class_id in zip(boxes, scores, class_ids):\n        if score > confidence_threshold:\n            ymin, xmin, ymax, xmax = box\n            (left, right, top, bottom) = (xmin * frame_width, xmax * frame_width,\n                                          ymin * frame_height, ymax * frame_height)\n            detections.append((int(left), int(top), int(right - left), int(bottom - top),\n                               labels[int(class_id)], score))\n    return detections\n\ndef draw_detections(frame, detections):\n    for detection in detections:\n        (x, y, w, h, label, score) = detection\n        cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)\n        label_text = f'{label}: {score:.2f}'\n        cv2.putText(frame, label_text, (x, y - 10 if y > 30 else y + 30),\n                    cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n\nwhile cap.isOpened():\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    input_data = preprocess(frame)\n    interpreter.set_tensor(input_details[0]['index'], np.expand_dims(input_data, axis=0))\n    interpreter.invoke()\n\n    output_data = {name: np.squeeze(interpreter.get_tensor(output_detail['index']))\n                   for name, output_detail in zip(['output_boxes', 'output_classes', 'output_scores'],\n                                                  output_details)}\n\n    frame_height, frame_width, _ = frame.shape\n    detections = postprocess(output_data, frame_width, frame_height)\n    draw_detections(frame, detections)\n\n    out.write(frame)\n\ncap.release()\nout.release()\ncv2.destroyAllWindows()",
        "last_error": "Max retries reached with error. Last error: Traceback (most recent call last):\n  File \"script_a9058cf6_1765385842.py\", line 20, in <module>\n    labels = load_labels(label_path)\n  File \"script_a9058cf6_1765385842.py\", line 18, in load_labels\n    return {int(line.split(' ', maxsplit=1)[0]): line.strip() for line in f.readlines()}\n  File \"script_a9058cf6_1765385842.py\", line 18, in <dictcomp>\n    return {int(line.split(' ', maxsplit=1)[0]): line.strip() for line in f.readlines()}\nValueError: invalid literal for int() with base 10: 'person\\n'."
      },
      "session_id": "qwen2.5-coder:32b_9e98_tpusg_batch",
      "metadata": {
        "num_run": 30
      },
      "tags": [
        "ablation",
        "benchmark",
        "qwen2.5-coder:32b",
        "tpu_sketch_generator"
      ],
      "latency": 172.188,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-18-54-38-331977_chatcmpl-405e13d2-9644-4967-b77c-ef90bd41cc2f",
          "traceId": "8be97509",
          "type": "GENERATION",
          "name": "8be9_tpusg_gen_attempt#1",
          "startTime": "2025-12-10T16:54:38.331000+00:00",
          "endTime": "2025-12-10T16:55:04.339000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 26008.0,
          "promptTokens": 293,
          "costDetails": {},
          "environment": "default",
          "totalTokens": 994,
          "completionTokens": 701,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-18-55-12-326883_chatcmpl-d8a3662b-1b82-46e5-b791-94313e6f24ad",
          "traceId": "8be97509",
          "type": "GENERATION",
          "name": "8be9_tpusg_gen_attempt#2",
          "startTime": "2025-12-10T16:55:12.326000+00:00",
          "endTime": "2025-12-10T16:55:36.865000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 24539.0,
          "promptTokens": 293,
          "costDetails": {},
          "environment": "default",
          "totalTokens": 957,
          "completionTokens": 664,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-18-55-45-588422_chatcmpl-a09cbd2a-2364-4df6-9025-47a4f6765de9",
          "traceId": "8be97509",
          "type": "GENERATION",
          "name": "8be9_tpusg_gen_attempt#3",
          "startTime": "2025-12-10T16:55:45.588000+00:00",
          "endTime": "2025-12-10T16:56:10.700000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 25112.0,
          "promptTokens": 293,
          "costDetails": {},
          "environment": "default",
          "totalTokens": 973,
          "completionTokens": 680,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-18-56-18-365460_chatcmpl-5dcfa471-a717-413b-b0eb-ba90cf967183",
          "traceId": "8be97509",
          "type": "GENERATION",
          "name": "8be9_tpusg_gen_attempt#4",
          "startTime": "2025-12-10T16:56:18.365000+00:00",
          "endTime": "2025-12-10T16:56:43.092000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 24727.0,
          "promptTokens": 293,
          "costDetails": {},
          "environment": "default",
          "totalTokens": 962,
          "completionTokens": 669,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-18-56-50-794810_chatcmpl-b0f448b0-eba3-4b2f-baf4-d582a6c376ce",
          "traceId": "8be97509",
          "type": "GENERATION",
          "name": "8be9_tpusg_gen_attempt#5",
          "startTime": "2025-12-10T16:56:50.794000+00:00",
          "endTime": "2025-12-10T16:57:22.427000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 31633.0,
          "promptTokens": 293,
          "costDetails": {},
          "environment": "default",
          "totalTokens": 1148,
          "completionTokens": 855,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "fb41baef-24d7-4438-b9f5-ab3bd5881a1f",
          "traceId": "8be97509",
          "type": "SPAN",
          "name": "error_8b_tpusg_failure_signal_tpu_sketch_generator",
          "startTime": "2025-12-10T16:57:30.519000+00:00",
          "level": "ERROR",
          "statusMessage": "Failed: Max retries reached with error. Last error: Traceback (most recent call last):\n  File \"script_a9058cf6_1765385842.py\", line 20, in <module>\n    labels = load_labels(label_path)\n  File \"script_a9058cf6_1765385842.py\", line 18, in load_labels\n    return {int(line.split(' ', maxsplit=1)[0]): line.strip() for line in f.readlines()}\n  File \"script_a9058cf6_1765385842.py\", line 18, in <dictcomp>\n    return {int(line.split(' ', maxsplit=1)[0]): line.strip() for line in f.readlines()}\nValueError: invalid literal for int() with base 10: 'person\\n'.",
          "calculatedTotalCost": 0.0,
          "promptTokens": 0,
          "costDetails": {},
          "environment": "default",
          "totalTokens": 0,
          "completionTokens": 0,
          "endTime": null,
          "model": null,
          "modelParameters": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null,
          "latency": null
        }
      ],
      "environment": "default",
      "createdAt": "2025-12-10T16:54:38.851Z",
      "updatedAt": "2025-12-10T16:57:36.004Z"
    },
    {
      "id": "f6a00526",
      "timestamp": "2025-12-10T16:51:21.375000+00:00",
      "name": "f6a0_tpu_sketch_generator",
      "input": null,
      "output": {
        "status": "failed",
        "last_code": "import os\nimport cv2\nimport numpy as np\nfrom PIL import Image\nfrom pycoral.adapters import common\nfrom pycoral.adapters import detect\nfrom pycoral.utils.dataset import read_label_file\nfrom pycoral.utils.edgetpu import make_interpreter\n\n# Configuration parameters\nmodel_path  = \"/home/mendel/tinyml_autopilot/models/edgetpu_detect.tflite\"\nlabel_path  = \"/home/mendel/tinyml_autopilot/models/labelmap.txt\"\ninput_path  = \"/home/mendel/tinyml_autopilot/data//sheeps.mp4\"\noutput_path  = \"/home/mendel/tinyml_autopilot/results/sheeps_detections.mp4\"\nconfidence_threshold  = 0.5\n\n# Phase 1: Setup\ninterpreter = make_interpreter(model_path, {'device': '/usr/lib/aarch64-linux-gnu/libedgetpu.so.1.0'})\ninterpreter.allocate_tensors()\nlabels = read_label_file(label_path)\n\n# Load video and prepare writer\ncap = cv2.VideoCapture(input_path)\nwidth = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\nheight = int(cap.get(cv2.CAP_PROP_HEIGHT))\nfps = cap.get(cv2.CAP_PROP_FPS)\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\nout = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n\n# Phase 3: Inference and Output Handling\nwhile cap.isOpened():\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    # Phase 2: Preprocessing\n    img = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n    resized_img = img.resize(common.input_size(interpreter), Image.ANTIALIAS)\n    common.set_input(interpreter, resized_img)\n\n    interpreter.invoke()\n    objs = detect.get_objects(interpreter, score_threshold=confidence_threshold)\n\n    # Draw detection boxes with labels\n    for obj in objs:\n        bbox = obj.bbox\n        x0, y0 = int(bbox.xmin * width), int(bbox.ymin * height)\n        x1, y1 = int(bbox.xmax * width), int(bbox.ymax * height)\n        cv2.rectangle(frame, (x0, y0), (x1, y1), (255, 0, 0), 2)\n        label = '%s: %.2f' % (labels.get(obj.id, obj.id), obj.score)\n        cv2.putText(frame, label, (x0, y0 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (255, 0, 0), 2)\n\n    out.write(frame)\n\ncap.release()\nout.release()\ncv2.destroyAllWindows()",
        "last_error": "Max retries reached with error. Last error: Traceback (most recent call last):\n  File \"/home/mendel/tinyml_autopilot/tinyml-env/lib/python3.7/site-packages/tflite_runtime/interpreter.py\", line 160, in load_delegate\n    delegate = Delegate(library, options)\n  File \"/home/mendel/tinyml_autopilot/tinyml-env/lib/python3.7/site-packages/tflite_runtime/interpreter.py\", line 119, in __init__\n    raise ValueError(capture.message)\nValueError\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"script_a373af20_1765385644.py\", line 18, in <module>\n    interpreter = make_interpreter(model_path, {'device': '/usr/lib/aarch64-linux-gnu/libedgetpu.so.1.0'})\n  File \"/home/mendel/tinyml_autopilot/tinyml-env/lib/python3.7/site-packages/pycoral/utils/edgetpu.py\", line 87, in make_interpreter\n    delegates = [load_edgetpu_delegate({'device': device} if device else {})]\n  File \"/home/mendel/tinyml_autopilot/tinyml-env/lib/python3.7/site-packages/pycoral/utils/edgetpu.py\", line 52, in load_edgetpu_delegate\n    return tflite.load_delegate(_EDGETPU_SHARED_LIB, options or {})\n  File \"/home/mendel/tinyml_autopilot/tinyml-env/lib/python3.7/site-packages/tflite_runtime/interpreter.py\", line 163, in load_delegate\n    library, str(e)))\nValueError: Failed to load delegate from libedgetpu.so.1\n."
      },
      "session_id": "qwen2.5-coder:32b_9e98_tpusg_batch",
      "metadata": {
        "num_run": 29
      },
      "tags": [
        "ablation",
        "benchmark",
        "qwen2.5-coder:32b",
        "tpu_sketch_generator"
      ],
      "latency": 171.455,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-18-51-21-376576_chatcmpl-9c091b68-4e2d-4bf9-aeb8-c9deae29d3f7",
          "traceId": "f6a00526",
          "type": "GENERATION",
          "name": "f6a0_tpusg_gen_attempt#1",
          "startTime": "2025-12-10T16:51:21.376000+00:00",
          "endTime": "2025-12-10T16:51:46.667000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 25291.0,
          "promptTokens": 293,
          "costDetails": {},
          "environment": "default",
          "totalTokens": 976,
          "completionTokens": 683,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-18-51-54-360522_chatcmpl-3e4661ec-697c-4a77-9c3c-a830e0cf8c5e",
          "traceId": "f6a00526",
          "type": "GENERATION",
          "name": "f6a0_tpusg_gen_attempt#2",
          "startTime": "2025-12-10T16:51:54.360000+00:00",
          "endTime": "2025-12-10T16:52:25.954000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 31594.0,
          "promptTokens": 293,
          "costDetails": {},
          "environment": "default",
          "totalTokens": 1146,
          "completionTokens": 853,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-18-52-33-619131_chatcmpl-6e075d48-72fb-4811-8356-2917b180d640",
          "traceId": "f6a00526",
          "type": "GENERATION",
          "name": "f6a0_tpusg_gen_attempt#3",
          "startTime": "2025-12-10T16:52:33.619000+00:00",
          "endTime": "2025-12-10T16:53:02.626000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 29007.0,
          "promptTokens": 293,
          "costDetails": {},
          "environment": "default",
          "totalTokens": 1078,
          "completionTokens": 785,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-18-53-10-589739_chatcmpl-825324ad-79e5-49f2-a12d-71dc886a747c",
          "traceId": "f6a00526",
          "type": "GENERATION",
          "name": "f6a0_tpusg_gen_attempt#4",
          "startTime": "2025-12-10T16:53:10.589000+00:00",
          "endTime": "2025-12-10T16:53:33.968000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 23379.0,
          "promptTokens": 293,
          "costDetails": {},
          "environment": "default",
          "totalTokens": 926,
          "completionTokens": 633,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-18-53-42-103803_chatcmpl-2de69289-4e54-4462-bdaa-27c121485e7a",
          "traceId": "f6a00526",
          "type": "GENERATION",
          "name": "f6a0_tpusg_gen_attempt#5",
          "startTime": "2025-12-10T16:53:42.103000+00:00",
          "endTime": "2025-12-10T16:54:04.698000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 22595.0,
          "promptTokens": 293,
          "costDetails": {},
          "environment": "default",
          "totalTokens": 905,
          "completionTokens": 612,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "e7f0c57e-dabc-4571-a414-102efb53916c",
          "traceId": "f6a00526",
          "type": "SPAN",
          "name": "error_f6_tpusg_failure_signal_tpu_sketch_generator",
          "startTime": "2025-12-10T16:54:12.831000+00:00",
          "level": "ERROR",
          "statusMessage": "Failed: Max retries reached with error. Last error: Traceback (most recent call last):\n  File \"/home/mendel/tinyml_autopilot/tinyml-env/lib/python3.7/site-packages/tflite_runtime/interpreter.py\", line 160, in load_delegate\n    delegate = Delegate(library, options)\n  File \"/home/mendel/tinyml_autopilot/tinyml-env/lib/python3.7/site-packages/tflite_runtime/interpreter.py\", line 119, in __init__\n    raise ValueError(capture.message)\nValueError\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"script_a373af20_1765385644.py\", line 18, in <module>\n    interpreter = make_interpreter(model_path, {'device': '/usr/lib/aarch64-linux-gnu/libedgetpu.so.1.0'})\n  File \"/home/mendel/tinyml_autopilot/tinyml-env/lib/python3.7/site-packages/pycoral/utils/edgetpu.py\", line 87, in make_interpreter\n    delegates = [load_edgetpu_delegate({'device': device} if device else {})]\n  File \"/home/mendel/tinyml_autopilot/tinyml-env/lib/python3.7/site-packages/pycoral/utils/edgetpu.py\", line 52, in load_edgetpu_delegate\n    return tflite.load_delegate(_EDGETPU_SHARED_LIB, options or {})\n  File \"/home/mendel/tinyml_autopilot/tinyml-env/lib/python3.7/site-packages/tflite_runtime/interpreter.py\", line 163, in load_delegate\n    library, str(e)))\nValueError: Failed to load delegate from libedgetpu.so.1\n.",
          "calculatedTotalCost": 0.0,
          "promptTokens": 0,
          "costDetails": {},
          "environment": "default",
          "totalTokens": 0,
          "completionTokens": 0,
          "endTime": null,
          "model": null,
          "modelParameters": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null,
          "latency": null
        }
      ],
      "environment": "default",
      "createdAt": "2025-12-10T16:51:21.896Z",
      "updatedAt": "2025-12-10T16:54:18.279Z"
    },
    {
      "id": "3c0213e2",
      "timestamp": "2025-12-10T16:48:06.388000+00:00",
      "name": "3c02_tpu_sketch_generator",
      "input": null,
      "output": {
        "status": "failed",
        "last_code": "import cv2\nimport numpy as np\nfrom PIL import Image\nimport tflite_runtime.interpreter as tflite\nfrom pycoral.adapters.common import input_size\nfrom pycoral.adapters.detect import get_objects\nfrom pycoral.utils.edgetpu import make_interpreter\n\n# Configuration parameters\nmodel_path  = \"/home/mendel/tinyml_autopilot/models/edgetpu_detect.tflite\"\nlabel_path  = \"/home/mendel/tinyml_autopilot/models/labelmap.txt\"\ninput_path  = \"/home/mendel/tinyml_autopilot/data//sheeps.mp4\"\noutput_path  = \"/home/mendel/tinyml_autopilot/results/sheeps_detections.mp4\"\nconfidence_threshold  = 0.5\n\n# Load labels\ndef load_labels(filename):\n    with open(filename, 'r', encoding='utf-8') as f:\n        return {i: line.strip() for i, line in enumerate(f.readlines())}\n\nlabels = load_labels(label_path)\n\n# Setup TFLite interpreter for TPU\ninterpreter = make_interpreter(model_path)\ninterpreter.allocate_tensors()\n_, height, width, _ = input_size(interpreter)\n\n# Read video\ncap = cv2.VideoCapture(input_path)\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\nout = cv2.VideoWriter(output_path, fourcc, 30.0, (int(cap.get(3)), int(cap.get(4))))\n\ndef draw_detection_boxes(image, objs):\n    for obj in objs:\n        bbox = obj.bbox\n        label = f'{labels[obj.id]} {obj.score:.2f}'\n        cv2.rectangle(image, (bbox.xmin, bbox.ymin), (bbox.xmax, bbox.ymax), (0, 255, 0), 2)\n        cv2.putText(image, label, (bbox.xmin, bbox.ymin - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (36, 255, 12), 2)\n\n# Process video frame by frame\nwhile cap.isOpened():\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    # Preprocessing\n    image = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n    resized_image = image.resize((width, height))\n\n    # Inference\n    input_tensor = np.array(resized_image)\n    interpreter.set_tensor(interpreter.get_input_details()[0]['index'], np.expand_dims(input_tensor, axis=0))\n    interpreter.invoke()\n    objs = get_objects(interpreter, confidence_threshold)\n\n    # Postprocessing and drawing boxes\n    draw_detection_boxes(frame, objs)\n\n    # Output handling\n    out.write(frame)\n\ncap.release()\nout.release()\ncv2.destroyAllWindows()",
        "last_error": "Max retries reached with error. Last error: Traceback (most recent call last):\n  File \"script_c51e4dc6_1765385448.py\", line 26, in <module>\n    _, height, width, _ = input_size(interpreter)\nValueError: not enough values to unpack (expected 4, got 2)."
      },
      "session_id": "qwen2.5-coder:32b_9e98_tpusg_batch",
      "metadata": {
        "num_run": 28
      },
      "tags": [
        "ablation",
        "benchmark",
        "qwen2.5-coder:32b",
        "tpu_sketch_generator"
      ],
      "latency": 169.793,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-18-48-06-390063_chatcmpl-ac6ca11a-6e8d-4dee-b95a-f6c620a2705e",
          "traceId": "3c0213e2",
          "type": "GENERATION",
          "name": "3c02_tpusg_gen_attempt#1",
          "startTime": "2025-12-10T16:48:06.390000+00:00",
          "endTime": "2025-12-10T16:48:34.224000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 27834.0,
          "promptTokens": 293,
          "costDetails": {},
          "environment": "default",
          "totalTokens": 1043,
          "completionTokens": 750,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-18-48-41-724658_chatcmpl-4efb258f-1aa7-4a4f-866b-9e07605db439",
          "traceId": "3c0213e2",
          "type": "GENERATION",
          "name": "3c02_tpusg_gen_attempt#2",
          "startTime": "2025-12-10T16:48:41.724000+00:00",
          "endTime": "2025-12-10T16:49:08.297000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 26573.0,
          "promptTokens": 293,
          "costDetails": {},
          "environment": "default",
          "totalTokens": 1012,
          "completionTokens": 719,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-18-49-16-252666_chatcmpl-830ab659-ec50-44d8-9592-6d75f8721223",
          "traceId": "3c0213e2",
          "type": "GENERATION",
          "name": "3c02_tpusg_gen_attempt#3",
          "startTime": "2025-12-10T16:49:16.252000+00:00",
          "endTime": "2025-12-10T16:49:42.371000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 26119.0,
          "promptTokens": 293,
          "costDetails": {},
          "environment": "default",
          "totalTokens": 1000,
          "completionTokens": 707,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-18-49-50-306003_chatcmpl-13673ef7-1f1e-4317-af27-ebb79daccf58",
          "traceId": "3c0213e2",
          "type": "GENERATION",
          "name": "3c02_tpusg_gen_attempt#4",
          "startTime": "2025-12-10T16:49:50.306000+00:00",
          "endTime": "2025-12-10T16:50:19.768000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 29462.0,
          "promptTokens": 293,
          "costDetails": {},
          "environment": "default",
          "totalTokens": 1090,
          "completionTokens": 797,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-18-50-27-367952_chatcmpl-dd7dfe5f-dc17-46aa-9dd7-3a15b5ebdb00",
          "traceId": "3c0213e2",
          "type": "GENERATION",
          "name": "3c02_tpusg_gen_attempt#5",
          "startTime": "2025-12-10T16:50:27.367000+00:00",
          "endTime": "2025-12-10T16:50:48.045000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 20678.0,
          "promptTokens": 293,
          "costDetails": {},
          "environment": "default",
          "totalTokens": 853,
          "completionTokens": 560,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "4dc62efd-0983-45b8-bdd8-c93297a936f4",
          "traceId": "3c0213e2",
          "type": "SPAN",
          "name": "error_3c_tpusg_failure_signal_tpu_sketch_generator",
          "startTime": "2025-12-10T16:50:56.183000+00:00",
          "level": "ERROR",
          "statusMessage": "Failed: Max retries reached with error. Last error: Traceback (most recent call last):\n  File \"script_c51e4dc6_1765385448.py\", line 26, in <module>\n    _, height, width, _ = input_size(interpreter)\nValueError: not enough values to unpack (expected 4, got 2).",
          "calculatedTotalCost": 0.0,
          "promptTokens": 0,
          "costDetails": {},
          "environment": "default",
          "totalTokens": 0,
          "completionTokens": 0,
          "endTime": null,
          "model": null,
          "modelParameters": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null,
          "latency": null
        }
      ],
      "environment": "default",
      "createdAt": "2025-12-10T16:48:06.909Z",
      "updatedAt": "2025-12-10T16:51:01.554Z"
    },
    {
      "id": "84d90d6a",
      "timestamp": "2025-12-10T16:46:17.449000+00:00",
      "name": "84d9_tpu_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import numpy as np\nimport cv2\nfrom PIL import Image\nfrom tflite_runtime.interpreter import Interpreter\nfrom tflite_runtime.interpreter import load_delegate\n\n# Configuration parameters\nmodel_path  = \"/home/mendel/tinyml_autopilot/models/edgetpu_detect.tflite\"\nlabel_path  = \"/home/mendel/tinyml_autopilot/models/labelmap.txt\"\ninput_path  = \"/home/mendel/tinyml_autopilot/data//sheeps.mp4\"\noutput_path  = \"/home/mendel/tinyml_autopilot/results/sheeps_detections.mp4\"\nconfidence_threshold  = 0.5\n\n# Load labels\nwith open(label_path, 'r') as f:\n    labels = [line.strip() for line in f.readlines()]\n\n# Setup TFLite interpreter with Edge TPU delegate\ninterpreter = Interpreter(model_path=model_path,\n                          experimental_delegates=[load_delegate('/usr/lib/aarch64-linux-gnu/libedgetpu.so.1.0')])\ninterpreter.allocate_tensors()\n\n# Get input and output details\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\n# Read video from input path\ncap = cv2.VideoCapture(input_path)\nwidth = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\nheight = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\nfps = cap.get(cv2.CAP_PROP_FPS)\n\n# Define codec and create VideoWriter object\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\nout = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n\ndef preprocess_image(image):\n    image_resized = cv2.resize(image, (300, 300))\n    image_rgb = cv2.cvtColor(image_resized, cv2.COLOR_BGR2RGB)\n    input_data = np.expand_dims(image_rgb, axis=0)\n    return input_data\n\ndef draw_detection_boxes(image, boxes, classes, scores):\n    for i in range(len(scores)):\n        if scores[i] > confidence_threshold:\n            box = boxes[i]\n            class_id = int(classes[i])\n            label = labels[class_id]\n            score = scores[i]\n\n            # Scale the bounding box coordinates\n            ymin, xmin, ymax, xmax = box\n            ymin = int(ymin * height)\n            xmin = int(xmin * width)\n            ymax = int(ymax * height)\n            xmax = int(xmax * width)\n\n            # Draw rectangle and label on image\n            cv2.rectangle(image, (xmin, ymin), (xmax, ymax), (0, 255, 0), 2)\n            font_scale = 1\n            font = cv2.FONT_HERSHEY_SIMPLEX\n            text = f'{label}: {score:.2f}'\n            (text_width, text_height) = cv2.getTextSize(text, font, fontScale=font_scale, thickness=1)[0]\n            cv2.rectangle(image, (xmin, ymin - 30), (xmin + text_width, ymin), (0, 255, 0), -1)\n            cv2.putText(image, text, (xmin, ymin - 10), font, fontScale=font_scale, color=(0, 0, 0), thickness=1)\n\n# Process video frame by frame\nwhile cap.isOpened():\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    # Preprocess image\n    input_data = preprocess_image(frame)\n\n    # Run inference\n    interpreter.set_tensor(input_details[0]['index'], input_data)\n    interpreter.invoke()\n\n    # Extract results\n    boxes = interpreter.get_tensor(output_details[0]['index'])[0]\n    classes = interpreter.get_tensor(output_details[1]['index'])[0]\n    scores = interpreter.get_tensor(output_details[2]['index'])[0]\n\n    # Draw detection boxes and labels on frame\n    draw_detection_boxes(frame, boxes, classes, scores)\n\n    # Write the result to the output video file\n    out.write(frame)\n\n# Release everything when done\ncap.release()\nout.release()\ncv2.destroyAllWindows()"
      },
      "session_id": "qwen2.5-coder:32b_9e98_tpusg_batch",
      "metadata": {
        "num_run": 27
      },
      "tags": [
        "ablation",
        "benchmark",
        "qwen2.5-coder:32b",
        "tpu_sketch_generator"
      ],
      "latency": 30.78,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-18-46-17-450424_chatcmpl-bf694a2c-dc91-457c-942b-e1866f673c8f",
          "traceId": "84d90d6a",
          "type": "GENERATION",
          "name": "84d9_tpusg_gen_attempt#1",
          "startTime": "2025-12-10T16:46:17.450000+00:00",
          "endTime": "2025-12-10T16:46:48.230000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 30780.0,
          "promptTokens": 293,
          "costDetails": {},
          "environment": "default",
          "totalTokens": 1123,
          "completionTokens": 830,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "environment": "default",
      "createdAt": "2025-12-10T16:46:17.969Z",
      "updatedAt": "2025-12-10T16:47:46.135Z"
    },
    {
      "id": "58931382",
      "timestamp": "2025-12-10T16:44:24.438000+00:00",
      "name": "5893_tpu_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import cv2\nimport numpy as np\nfrom PIL import Image\nimport tflite_runtime.interpreter as tflite\nfrom pycoral.adapters.common import input_size\nfrom pycoral.utils.dataset import read_label_file\nfrom pycoral.utils.edgetpu import make_interpreter\n\n# Configuration parameters\nmodel_path  = \"/home/mendel/tinyml_autopilot/models/edgetpu_detect.tflite\"\nlabel_path  = \"/home/mendel/tinyml_autopilot/models/labelmap.txt\"\ninput_path  = \"/home/mendel/tinyml_autopilot/data//sheeps.mp4\"\noutput_path  = \"/home/mendel/tinyml_autopilot/results/sheeps_detections.mp4\"\nconfidence_threshold  = 0.5\n\n# Phase 1: Setup\ninterpreter = make_interpreter(model_path)\ninterpreter.allocate_tensors()\nlabels = read_label_file(label_path)\n\ndef set_input_tensor(interpreter, image):\n    tensor_index = interpreter.get_input_details()[0]['index']\n    input_tensor = interpreter.tensor(tensor_index)()[0]\n    input_tensor[:, :] = image\n\ndef get_output_tensor(interpreter, index):\n    output_details = interpreter.get_output_details()[index]\n    tensor = np.squeeze(interpreter.get_tensor(output_details['index']))\n    return tensor\n\n# Phase 2: Preprocessing\ncap = cv2.VideoCapture(input_path)\nwidth = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\nheight = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\nfps = cap.get(cv2.CAP_PROP_FPS)\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\nout = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n\nwhile cap.isOpened():\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    image = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n    resized_image = image.resize(input_size(interpreter), Image.ANTIALIAS)\n    set_input_tensor(interpreter, resized_image)\n\n    # Phase 3: Inference\n    interpreter.invoke()\n    \n    boxes = get_output_tensor(interpreter, 0)\n    class_ids = get_output_tensor(interpreter, 1)\n    scores = get_output_tensor(interpreter, 2)\n    count = int(get_output_tensor(interpreter, 3))\n\n    # Phase 4: Output Handling\n    for i in range(count):\n        if scores[i] >= confidence_threshold:\n            ymin, xmin, ymax, xmax = boxes[i]\n            height, width, _ = frame.shape\n            (xminn, xmaxx, yminn, ymaxx) = (int(xmin * width), int(xmax * width),\n                                            int(ymin * height), int(ymax * height))\n\n            cv2.rectangle(frame, (xminn, yminn), (xmaxx, ymaxx), (10, 255, 0), 2)\n            label = '%s: %d%%' % (labels.get(class_ids[i].astype(int)), int(scores[i]*100))\n            label_size, base_line = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.7, 2)\n            label_ymin = max(yminn, label_size[1] + 10)\n            cv2.rectangle(frame, (xminn, label_ymin-label_size[1]-10), (xminn+label_size[0], label_ymin+base_line-10), (255, 255, 255), cv2.FILLED)\n            cv2.putText(frame, label, (xminn, label_ymin-7), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 0), 2)\n\n    out.write(frame)\n\ncap.release()\nout.release()"
      },
      "session_id": "qwen2.5-coder:32b_9e98_tpusg_batch",
      "metadata": {
        "num_run": 26
      },
      "tags": [
        "ablation",
        "benchmark",
        "qwen2.5-coder:32b",
        "tpu_sketch_generator"
      ],
      "latency": 30.736,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-18-44-24-440106_chatcmpl-27c2bf63-f1c6-4164-a63f-312cc83d057a",
          "traceId": "58931382",
          "type": "GENERATION",
          "name": "5893_tpusg_gen_attempt#1",
          "startTime": "2025-12-10T16:44:24.440000+00:00",
          "endTime": "2025-12-10T16:44:55.176000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 30736.0,
          "promptTokens": 293,
          "costDetails": {},
          "environment": "default",
          "totalTokens": 1123,
          "completionTokens": 830,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "environment": "default",
      "createdAt": "2025-12-10T16:44:24.960Z",
      "updatedAt": "2025-12-10T16:45:57.139Z"
    },
    {
      "id": "b1434ab4",
      "timestamp": "2025-12-10T16:41:06.438000+00:00",
      "name": "b143_tpu_sketch_generator",
      "input": null,
      "output": {
        "status": "failed",
        "last_code": "import numpy as np\nimport cv2\nfrom PIL import Image\nfrom pycoral.adapters import common\nfrom pycoral.adapters import detect\nfrom pycoral.utils.dataset import read_label_file\nfrom pycoral.utils.edgetpu import make_interpreter\n\n# Configuration parameters\nmodel_path  = \"/home/mendel/tinyml_autopilot/models/edgetpu_detect.tflite\"\nlabel_path  = \"/home/mendel/tinyml_autopilot/models/labelmap.txt\"\ninput_path  = \"/home/mendel/tinyml_autopilot/data//sheeps.mp4\"\noutput_path  = \"/home/mendel/tinyml_autopilot/results/sheeps_detections.mp4\"\nconfidence_threshold  = 0.5\n\n# Setup\ninterpreter = make_interpreter(model_path, {'device': ':0'})\ninterpreter.allocate_tensors()\nlabels = read_label_file(label_path)\n\n# Load video\ncap = cv2.VideoCapture(input_path)\nif not cap.isOpened():\n    raise IOError(\"Cannot open input file\")\n\nframe_width = int(cap.get(3))\nframe_height = int(cap.get(4))\nfps = cap.get(cv2.CAP_PROP_FPS)\n\nout = cv2.VideoWriter(output_path, cv2.VideoWriter_fourcc(*'mp4v'), fps, (frame_width, frame_height))\n\n# Preprocessing and Inference\nwhile cap.isOpened():\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    img = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n    resized_img = img.resize(common.input_size(interpreter), Image.ANTIALIAS)\n    common.set_input(interpreter, resized_img)\n\n    interpreter.invoke()\n\n    # Postprocessing\n    detections = detect.get_objects(interpreter, confidence_threshold)[:5]\n    for obj in detections:\n        bbox = obj.bbox\n        x0, y0, x1, y1 = int(bbox.xmin * frame_width), int(bbox.ymin * frame_height), int(bbox.xmax * frame_width), int(bbox.ymax * frame_height)\n        label = f'{labels.get(obj.id, obj.id)}: {obj.score:.2f}'\n        cv2.rectangle(frame, (x0, y0), (x1, y1), (0, 255, 0), 2)\n        cv2.putText(frame, label, (x0, y0 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (36, 255, 12), 2)\n\n    out.write(frame)\n\ncap.release()\nout.release()\ncv2.destroyAllWindows()\n\n# mAP calculation is not typically done in real-time during inference and would require a separate script with ground truth annotations.",
        "last_error": "Max retries reached with error. Last error: Traceback (most recent call last):\n  File \"/home/mendel/tinyml_autopilot/tinyml-env/lib/python3.7/site-packages/tflite_runtime/interpreter.py\", line 160, in load_delegate\n    delegate = Delegate(library, options)\n  File \"/home/mendel/tinyml_autopilot/tinyml-env/lib/python3.7/site-packages/tflite_runtime/interpreter.py\", line 119, in __init__\n    raise ValueError(capture.message)\nValueError\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"script_d01da787_1765385031.py\", line 17, in <module>\n    interpreter = make_interpreter(model_path, {'device': ':0'})\n  File \"/home/mendel/tinyml_autopilot/tinyml-env/lib/python3.7/site-packages/pycoral/utils/edgetpu.py\", line 87, in make_interpreter\n    delegates = [load_edgetpu_delegate({'device': device} if device else {})]\n  File \"/home/mendel/tinyml_autopilot/tinyml-env/lib/python3.7/site-packages/pycoral/utils/edgetpu.py\", line 52, in load_edgetpu_delegate\n    return tflite.load_delegate(_EDGETPU_SHARED_LIB, options or {})\n  File \"/home/mendel/tinyml_autopilot/tinyml-env/lib/python3.7/site-packages/tflite_runtime/interpreter.py\", line 163, in load_delegate\n    library, str(e)))\nValueError: Failed to load delegate from libedgetpu.so.1\n."
      },
      "session_id": "qwen2.5-coder:32b_9e98_tpusg_batch",
      "metadata": {
        "num_run": 25
      },
      "tags": [
        "ablation",
        "benchmark",
        "qwen2.5-coder:32b",
        "tpu_sketch_generator"
      ],
      "latency": 172.591,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-18-41-06-439173_chatcmpl-db5a56f7-78a9-4ce9-8e71-f3930997c478",
          "traceId": "b1434ab4",
          "type": "GENERATION",
          "name": "b143_tpusg_gen_attempt#1",
          "startTime": "2025-12-10T16:41:06.439000+00:00",
          "endTime": "2025-12-10T16:41:32.484000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 26045.0,
          "promptTokens": 293,
          "costDetails": {},
          "environment": "default",
          "totalTokens": 995,
          "completionTokens": 702,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-18-41-40-203762_chatcmpl-60a99afc-01ec-48b3-bbb7-5e08536c5163",
          "traceId": "b1434ab4",
          "type": "GENERATION",
          "name": "b143_tpusg_gen_attempt#2",
          "startTime": "2025-12-10T16:41:40.203000+00:00",
          "endTime": "2025-12-10T16:42:05.293000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 25090.0,
          "promptTokens": 293,
          "costDetails": {},
          "environment": "default",
          "totalTokens": 972,
          "completionTokens": 679,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-18-42-13-347827_chatcmpl-8878c98d-ca9b-48ac-b7f0-9112d4568408",
          "traceId": "b1434ab4",
          "type": "GENERATION",
          "name": "b143_tpusg_gen_attempt#3",
          "startTime": "2025-12-10T16:42:13.347000+00:00",
          "endTime": "2025-12-10T16:42:40.624000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 27277.0,
          "promptTokens": 293,
          "costDetails": {},
          "environment": "default",
          "totalTokens": 1031,
          "completionTokens": 738,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-18-42-48-310904_chatcmpl-aee1dc91-c0bf-4257-ae26-b9032c8d520b",
          "traceId": "b1434ab4",
          "type": "GENERATION",
          "name": "b143_tpusg_gen_attempt#4",
          "startTime": "2025-12-10T16:42:48.310000+00:00",
          "endTime": "2025-12-10T16:43:19.892000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 31582.0,
          "promptTokens": 293,
          "costDetails": {},
          "environment": "default",
          "totalTokens": 1147,
          "completionTokens": 854,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-18-43-27-345755_chatcmpl-46680d5e-9a28-4251-92ce-be87d5e6dc68",
          "traceId": "b1434ab4",
          "type": "GENERATION",
          "name": "b143_tpusg_gen_attempt#5",
          "startTime": "2025-12-10T16:43:27.345000+00:00",
          "endTime": "2025-12-10T16:43:51.221000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 23876.0,
          "promptTokens": 293,
          "costDetails": {},
          "environment": "default",
          "totalTokens": 939,
          "completionTokens": 646,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "2af2f670-d0dc-4fdc-8048-abe1ffd6b33f",
          "traceId": "b1434ab4",
          "type": "SPAN",
          "name": "error_b1_tpusg_failure_signal_tpu_sketch_generator",
          "startTime": "2025-12-10T16:43:59.030000+00:00",
          "level": "ERROR",
          "statusMessage": "Failed: Max retries reached with error. Last error: Traceback (most recent call last):\n  File \"/home/mendel/tinyml_autopilot/tinyml-env/lib/python3.7/site-packages/tflite_runtime/interpreter.py\", line 160, in load_delegate\n    delegate = Delegate(library, options)\n  File \"/home/mendel/tinyml_autopilot/tinyml-env/lib/python3.7/site-packages/tflite_runtime/interpreter.py\", line 119, in __init__\n    raise ValueError(capture.message)\nValueError\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"script_d01da787_1765385031.py\", line 17, in <module>\n    interpreter = make_interpreter(model_path, {'device': ':0'})\n  File \"/home/mendel/tinyml_autopilot/tinyml-env/lib/python3.7/site-packages/pycoral/utils/edgetpu.py\", line 87, in make_interpreter\n    delegates = [load_edgetpu_delegate({'device': device} if device else {})]\n  File \"/home/mendel/tinyml_autopilot/tinyml-env/lib/python3.7/site-packages/pycoral/utils/edgetpu.py\", line 52, in load_edgetpu_delegate\n    return tflite.load_delegate(_EDGETPU_SHARED_LIB, options or {})\n  File \"/home/mendel/tinyml_autopilot/tinyml-env/lib/python3.7/site-packages/tflite_runtime/interpreter.py\", line 163, in load_delegate\n    library, str(e)))\nValueError: Failed to load delegate from libedgetpu.so.1\n.",
          "calculatedTotalCost": 0.0,
          "promptTokens": 0,
          "costDetails": {},
          "environment": "default",
          "totalTokens": 0,
          "completionTokens": 0,
          "endTime": null,
          "model": null,
          "modelParameters": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null,
          "latency": null
        }
      ],
      "environment": "default",
      "createdAt": "2025-12-10T16:41:06.960Z",
      "updatedAt": "2025-12-10T16:44:04.635Z"
    },
    {
      "id": "6c954f4c",
      "timestamp": "2025-12-10T16:37:07.451000+00:00",
      "name": "6c95_tpu_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import cv2\nimport numpy as np\nfrom tflite_runtime.interpreter import Interpreter\nfrom tflite_runtime.interpreter import load_delegate\n\n# Configuration parameters\nmodel_path  = \"/home/mendel/tinyml_autopilot/models/edgetpu_detect.tflite\"\nlabel_path  = \"/home/mendel/tinyml_autopilot/models/labelmap.txt\"\ninput_path  = \"/home/mendel/tinyml_autopilot/data//sheeps.mp4\"\noutput_path  = \"/home/mendel/tinyml_autopilot/results/sheeps_detections.mp4\"\nconfidence_threshold  = 0.5\n\n# Load the labels\nwith open(label_path, 'r') as f:\n    labels = [line.strip() for line in f.readlines()]\n\n# Setup: Load the TFLite interpreter with TPU delegate\ninterpreter = Interpreter(model_path=model_path,\n                          experimental_delegates=[load_delegate('/usr/lib/aarch64-linux-gnu/libedgetpu.so.1.0')])\ninterpreter.allocate_tensors()\n\n# Get model details\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\nheight = input_details[0]['shape'][1]\nwidth = input_details[0]['shape'][2]\n\n# Load video\ncap = cv2.VideoCapture(input_path)\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\nout = cv2.VideoWriter(output_path, fourcc, cap.get(cv2.CAP_PROP_FPS), (int(cap.get(3)), int(cap.get(4))))\n\ndef preprocess_image(image):\n    image_resized = cv2.resize(image, (width, height))\n    input_data = np.expand_dims(image_resized, axis=0)\n    return input_data.astype(np.uint8)\n\ndef draw_boxes(frame, boxes, classes, scores):\n    for i in range(len(scores)):\n        if scores[i] >= confidence_threshold:\n            ymin, xmin, ymax, xmax = boxes[i]\n            (left, right, top, bottom) = (xmin * frame.shape[1], xmax * frame.shape[1],\n                                          ymin * frame.shape[0], ymax * frame.shape[0])\n            cv2.rectangle(frame, (int(left), int(top)), (int(right), int(bottom)), (0, 255, 0), 2)\n            label = f'{labels[int(classes[i])]}: {scores[i]:.2f}'\n            cv2.putText(frame, label, (int(left), int(top) - 10),\n                        cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n\nwhile cap.isOpened():\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    # Preprocessing\n    input_data = preprocess_image(frame)\n    \n    # Inference\n    interpreter.set_tensor(input_details[0]['index'], input_data)\n    interpreter.invoke()\n    \n    boxes = interpreter.get_tensor(output_details[0]['index'])[0]\n    classes = interpreter.get_tensor(output_details[1]['index'])[0]\n    scores = interpreter.get_tensor(output_details[2]['index'])[0]\n\n    # Draw detection results\n    draw_boxes(frame, boxes, classes, scores)\n    \n    # Output handling\n    out.write(frame)\n\ncap.release()\nout.release()\ncv2.destroyAllWindows()"
      },
      "session_id": "qwen2.5-coder:32b_9e98_tpusg_batch",
      "metadata": {
        "num_run": 24
      },
      "tags": [
        "ablation",
        "benchmark",
        "qwen2.5-coder:32b",
        "tpu_sketch_generator"
      ],
      "latency": 161.365,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-18-37-07-452847_chatcmpl-cf6e9314-0a6a-4eef-8ed4-8f35b8350eb2",
          "traceId": "6c954f4c",
          "type": "GENERATION",
          "name": "6c95_tpusg_gen_attempt#1",
          "startTime": "2025-12-10T16:37:07.452000+00:00",
          "endTime": "2025-12-10T16:37:30.239000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 22787.0,
          "promptTokens": 293,
          "costDetails": {},
          "environment": "default",
          "totalTokens": 908,
          "completionTokens": 615,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-18-37-38-245076_chatcmpl-3136429b-d856-4c8f-838b-7242e4697fd2",
          "traceId": "6c954f4c",
          "type": "GENERATION",
          "name": "6c95_tpusg_gen_attempt#2",
          "startTime": "2025-12-10T16:37:38.245000+00:00",
          "endTime": "2025-12-10T16:38:05.882000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 27637.0,
          "promptTokens": 293,
          "costDetails": {},
          "environment": "default",
          "totalTokens": 1041,
          "completionTokens": 748,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-18-38-13-614903_chatcmpl-ebce9e3a-d4f0-4c38-9329-f329fb172254",
          "traceId": "6c954f4c",
          "type": "GENERATION",
          "name": "6c95_tpusg_gen_attempt#3",
          "startTime": "2025-12-10T16:38:13.614000+00:00",
          "endTime": "2025-12-10T16:38:38.858000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 25244.0,
          "promptTokens": 293,
          "costDetails": {},
          "environment": "default",
          "totalTokens": 976,
          "completionTokens": 683,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-18-38-46-863214_chatcmpl-df3e007e-022b-49eb-8322-f688a52058d4",
          "traceId": "6c954f4c",
          "type": "GENERATION",
          "name": "6c95_tpusg_gen_attempt#4",
          "startTime": "2025-12-10T16:38:46.863000+00:00",
          "endTime": "2025-12-10T16:39:16.775000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 29912.0,
          "promptTokens": 293,
          "costDetails": {},
          "environment": "default",
          "totalTokens": 1102,
          "completionTokens": 809,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-18-39-24-247340_chatcmpl-ab54c9aa-1614-445d-9471-9525a7b2654f",
          "traceId": "6c954f4c",
          "type": "GENERATION",
          "name": "6c95_tpusg_gen_attempt#5",
          "startTime": "2025-12-10T16:39:24.247000+00:00",
          "endTime": "2025-12-10T16:39:48.817000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 24570.0,
          "promptTokens": 293,
          "costDetails": {},
          "environment": "default",
          "totalTokens": 958,
          "completionTokens": 665,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "environment": "default",
      "createdAt": "2025-12-10T16:37:07.971Z",
      "updatedAt": "2025-12-10T16:40:46.718Z"
    },
    {
      "id": "eddbf399",
      "timestamp": "2025-12-10T16:33:46.259000+00:00",
      "name": "eddb_tpu_sketch_generator",
      "input": null,
      "output": {
        "status": "failed",
        "last_code": "import numpy as np\nimport cv2\nfrom PIL import Image\nfrom tflite_runtime.interpreter import Interpreter\nfrom tflite_runtime.interpreter import load_delegate\n\n# Configuration parameters\nmodel_path  = \"/home/mendel/tinyml_autopilot/models/edgetpu_detect.tflite\"\nlabel_path  = \"/home/mendel/tinyml_autopilot/models/labelmap.txt\"\ninput_path  = \"/home/mendel/tinyml_autopilot/data//sheeps.mp4\"\noutput_path  = \"/home/mendel/tinyml_autopilot/results/sheeps_detections.mp4\"\nconfidence_threshold  = 0.5\n\n# Load labels\nwith open(label_path, 'r') as f:\n    labels = {int(line.split(' ', 1)[0]): line.split(' ', 1)[1].strip() for line in f.readlines()}\n\n# Setup TFLite interpreter with Edge TPU delegate\ninterpreter = Interpreter(model_path=model_path,\n                          experimental_delegates=[load_delegate('/usr/lib/aarch64-linux-gnu/libedgetpu.so.1.0')])\ninterpreter.allocate_tensors()\n\n# Get input and output details\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\nheight, width, _ = input_details[0]['shape']\n\n# Read video from input path\ncap = cv2.VideoCapture(input_path)\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\nout = cv2.VideoWriter(output_path, fourcc, 30.0, (int(cap.get(3)), int(cap.get(4))))\n\ndef preprocess_image(image):\n    image_resized = image.resize((width, height))\n    input_data = np.array(image_resized)\n    input_data = np.expand_dims(input_data, axis=0)\n    return input_data\n\ndef draw_detection_boxes(frame, boxes, classes, scores):\n    for i in range(len(scores)):\n        if scores[i] > confidence_threshold:\n            ymin = int(boxes[i][0] * frame.shape[0])\n            xmin = int(boxes[i][1] * frame.shape[1])\n            ymax = int(boxes[i][2] * frame.shape[0])\n            xmax = int(boxes[i][3] * frame.shape[1])\n\n            cv2.rectangle(frame, (xmin, ymin), (xmax, ymax), (0, 255, 0), 2)\n            label = f'{labels[int(classes[i])]}: {scores[i]:.2f}'\n            cv2.putText(frame, label, (xmin, ymin - 10),\n                        cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n\nwhile cap.isOpened():\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    image_pil = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n    input_data = preprocess_image(image_pil)\n    interpreter.set_tensor(input_details[0]['index'], input_data)\n\n    # Run inference\n    interpreter.invoke()\n\n    # Extract results\n    boxes = interpreter.get_tensor(output_details[0]['index'])[0]\n    classes = interpreter.get_tensor(output_details[1]['index'])[0]\n    scores = interpreter.get_tensor(output_details[2]['index'])[0]\n\n    draw_detection_boxes(frame, boxes, classes, scores)\n\n    out.write(cv2.cvtColor(frame, cv2.COLOR_RGB2BGR))\n\ncap.release()\nout.release()",
        "last_error": "Max retries reached with error. Last error: Traceback (most recent call last):\n  File \"script_0590d1eb_1765384594.py\", line 16, in <module>\n    labels = {int(line.split(' ', 1)[0]): line.split(' ', 1)[1].strip() for line in f.readlines()}\n  File \"script_0590d1eb_1765384594.py\", line 16, in <dictcomp>\n    labels = {int(line.split(' ', 1)[0]): line.split(' ', 1)[1].strip() for line in f.readlines()}\nIndexError: list index out of range."
      },
      "session_id": "qwen2.5-coder:32b_9e98_tpusg_batch",
      "metadata": {
        "num_run": 23
      },
      "tags": [
        "ablation",
        "benchmark",
        "qwen2.5-coder:32b",
        "tpu_sketch_generator"
      ],
      "latency": 175.943,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-18-33-46-260624_chatcmpl-fe1c6573-081b-44eb-9e10-303be3ee2b51",
          "traceId": "eddbf399",
          "type": "GENERATION",
          "name": "eddb_tpusg_gen_attempt#1",
          "startTime": "2025-12-10T16:33:46.260000+00:00",
          "endTime": "2025-12-10T16:34:08.715000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 22455.0,
          "promptTokens": 293,
          "costDetails": {},
          "environment": "default",
          "totalTokens": 899,
          "completionTokens": 606,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-18-34-16-790445_chatcmpl-6075f98d-a3b2-4a59-80e6-f8945f2eea1a",
          "traceId": "eddbf399",
          "type": "GENERATION",
          "name": "eddb_tpusg_gen_attempt#2",
          "startTime": "2025-12-10T16:34:16.790000+00:00",
          "endTime": "2025-12-10T16:34:40.322000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 23532.0,
          "promptTokens": 293,
          "costDetails": {},
          "environment": "default",
          "totalTokens": 930,
          "completionTokens": 637,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-18-34-49-101172_chatcmpl-4a0dfd47-e403-4298-aa76-9108d25db227",
          "traceId": "eddbf399",
          "type": "GENERATION",
          "name": "eddb_tpusg_gen_attempt#3",
          "startTime": "2025-12-10T16:34:49.101000+00:00",
          "endTime": "2025-12-10T16:35:16.240000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 27139.0,
          "promptTokens": 293,
          "costDetails": {},
          "environment": "default",
          "totalTokens": 1027,
          "completionTokens": 734,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-18-35-23-898935_chatcmpl-5eda9d2b-9aff-4a44-a2f9-650420f04109",
          "traceId": "eddbf399",
          "type": "GENERATION",
          "name": "eddb_tpusg_gen_attempt#4",
          "startTime": "2025-12-10T16:35:23.898000+00:00",
          "endTime": "2025-12-10T16:35:57.695000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 33797.0,
          "promptTokens": 293,
          "costDetails": {},
          "environment": "default",
          "totalTokens": 1206,
          "completionTokens": 913,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-18-36-05-367858_chatcmpl-ba8ba562-9800-4f36-b6d9-facef1642808",
          "traceId": "eddbf399",
          "type": "GENERATION",
          "name": "eddb_tpusg_gen_attempt#5",
          "startTime": "2025-12-10T16:36:05.367000+00:00",
          "endTime": "2025-12-10T16:36:34.569000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 29202.0,
          "promptTokens": 293,
          "costDetails": {},
          "environment": "default",
          "totalTokens": 1083,
          "completionTokens": 790,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "de4143c2-9dfd-4365-8a5a-3d9d6a47ceed",
          "traceId": "eddbf399",
          "type": "SPAN",
          "name": "error_ed_tpusg_failure_signal_tpu_sketch_generator",
          "startTime": "2025-12-10T16:36:42.203000+00:00",
          "level": "ERROR",
          "statusMessage": "Failed: Max retries reached with error. Last error: Traceback (most recent call last):\n  File \"script_0590d1eb_1765384594.py\", line 16, in <module>\n    labels = {int(line.split(' ', 1)[0]): line.split(' ', 1)[1].strip() for line in f.readlines()}\n  File \"script_0590d1eb_1765384594.py\", line 16, in <dictcomp>\n    labels = {int(line.split(' ', 1)[0]): line.split(' ', 1)[1].strip() for line in f.readlines()}\nIndexError: list index out of range.",
          "calculatedTotalCost": 0.0,
          "promptTokens": 0,
          "costDetails": {},
          "environment": "default",
          "totalTokens": 0,
          "completionTokens": 0,
          "endTime": null,
          "model": null,
          "modelParameters": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null,
          "latency": null
        }
      ],
      "environment": "default",
      "createdAt": "2025-12-10T16:33:46.777Z",
      "updatedAt": "2025-12-10T16:36:47.449Z"
    },
    {
      "id": "95d891d2",
      "timestamp": "2025-12-10T16:32:01.287000+00:00",
      "name": "95d8_tpu_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import numpy as np\nimport cv2\nfrom PIL import Image\nfrom tflite_runtime.interpreter import Interpreter\nfrom tflite_runtime.interpreter import load_delegate\n\n# Configuration parameters\nmodel_path  = \"/home/mendel/tinyml_autopilot/models/edgetpu_detect.tflite\"\nlabel_path  = \"/home/mendel/tinyml_autopilot/models/labelmap.txt\"\ninput_path  = \"/home/mendel/tinyml_autopilot/data//sheeps.mp4\"\noutput_path  = \"/home/mendel/tinyml_autopilot/results/sheeps_detections.mp4\"\nconfidence_threshold  = 0.5\n\n# Load the label map\nwith open(label_path, 'r') as f:\n    labels = [line.strip() for line in f.readlines()]\n\n# Setup: Load the TFLite interpreter with TPU delegate\ninterpreter = Interpreter(model_path=model_path,\n                          experimental_delegates=[load_delegate('/usr/lib/aarch64-linux-gnu/libedgetpu.so.1.0')])\ninterpreter.allocate_tensors()\n\n# Get input and output details\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\n# Preprocessing function\ndef preprocess_image(image):\n    image_resized = cv2.resize(image, (300, 300))\n    image_rgb = cv2.cvtColor(image_resized, cv2.COLOR_BGR2RGB)\n    input_data = np.expand_dims(image_rgb, axis=0).astype(np.uint8)\n    return input_data\n\n# Function to draw detection boxes and labels\ndef draw_detection_boxes(image, boxes, classes, scores):\n    for i in range(len(scores)):\n        if scores[i] > confidence_threshold:\n            ymin, xmin, ymax, xmax = boxes[i]\n            label = labels[int(classes[i])]\n            score = scores[i]\n            \n            im_height, im_width, _ = image.shape\n            (left, right, top, bottom) = (xmin * im_width, xmax * im_width,\n                                          ymin * im_height, ymax * im_height)\n            \n            cv2.rectangle(image, (int(left), int(top)), (int(right), int(bottom)), (0, 255, 0), 2)\n            label_text = f'{label}: {score:.2f}'\n            cv2.putText(image, label_text, (int(left), int(top) - 10),\n                        cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n\n# Output handling: Read video, run inference, and save output\ncap = cv2.VideoCapture(input_path)\nfps = cap.get(cv2.CAP_PROP_FPS)\nwidth = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\nheight = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\nout = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n\nwhile cap.isOpened():\n    ret, frame = cap.read()\n    if not ret:\n        break\n    \n    input_data = preprocess_image(frame)\n    interpreter.set_tensor(input_details[0]['index'], input_data)\n    \n    interpreter.invoke()\n    \n    boxes = interpreter.get_tensor(output_details[0]['index'])[0]\n    classes = interpreter.get_tensor(output_details[1]['index'])[0]\n    scores = interpreter.get_tensor(output_details[2]['index'])[0]\n    \n    draw_detection_boxes(frame, boxes, classes, scores)\n    \n    out.write(frame)\n\ncap.release()\nout.release()\ncv2.destroyAllWindows()"
      },
      "session_id": "qwen2.5-coder:32b_9e98_tpusg_batch",
      "metadata": {
        "num_run": 22
      },
      "tags": [
        "ablation",
        "benchmark",
        "qwen2.5-coder:32b",
        "tpu_sketch_generator"
      ],
      "latency": 26.628,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-18-32-01-288511_chatcmpl-5854ae40-9a7e-42c7-8ad8-9455c64b3863",
          "traceId": "95d891d2",
          "type": "GENERATION",
          "name": "95d8_tpusg_gen_attempt#1",
          "startTime": "2025-12-10T16:32:01.288000+00:00",
          "endTime": "2025-12-10T16:32:27.916000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 26628.0,
          "promptTokens": 293,
          "costDetails": {},
          "environment": "default",
          "totalTokens": 1011,
          "completionTokens": 718,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "environment": "default",
      "createdAt": "2025-12-10T16:32:01.807Z",
      "updatedAt": "2025-12-10T16:33:26.108Z"
    },
    {
      "id": "a4a85d4b",
      "timestamp": "2025-12-10T16:27:57.338000+00:00",
      "name": "a4a8_tpu_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import numpy as np\nimport cv2\nfrom tflite_runtime.interpreter import Interpreter\nfrom tflite_runtime.interpreter import load_delegate\n\n# Configuration parameters\nmodel_path  = \"/home/mendel/tinyml_autopilot/models/edgetpu_detect.tflite\"\nlabel_path  = \"/home/mendel/tinyml_autopilot/models/labelmap.txt\"\ninput_path  = \"/home/mendel/tinyml_autopilot/data//sheeps.mp4\"\noutput_path  = \"/home/mendel/tinyml_autopilot/results/sheeps_detections.mp4\"\nconfidence_threshold  = 0.5\n\n# Load labels\ndef load_labels(filename):\n    with open(filename, 'r') as f:\n        return {i: line.strip() for i, line in enumerate(f.readlines())}\n\nlabels = load_labels(label_path)\n\n# Setup TFLite interpreter for TPU\ninterpreter = Interpreter(\n    model_path=model_path,\n    experimental_delegates=[load_delegate('/usr/lib/aarch64-linux-gnu/libedgetpu.so.1.0')]\n)\ninterpreter.allocate_tensors()\n\n# Get input and output details\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\n# Preprocessing function\ndef preprocess_image(image, target_size):\n    image_resized = cv2.resize(image, (target_size[1], target_size[0]))\n    image_rgb = cv2.cvtColor(image_resized, cv2.COLOR_BGR2RGB)\n    input_data = np.expand_dims(image_rgb, axis=0)\n    return input_data\n\n# Function to draw detection boxes\ndef draw_detection_boxes(frame, boxes, classes, scores):\n    height, width, _ = frame.shape\n    for box, cls, score in zip(boxes, classes, scores):\n        if score > confidence_threshold:\n            ymin, xmin, ymax, xmax = box\n            (left, right, top, bottom) = (\n                int(xmin * width), int(xmax * width),\n                int(ymin * height), int(ymax * height))\n            label = f'{labels[int(cls)]}: {score:.2f}'\n            cv2.rectangle(frame, (left, top), (right, bottom), (0, 255, 0), 2)\n            cv2.putText(frame, label, (left, top - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n\n# Read video input\ncap = cv2.VideoCapture(input_path)\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\nfps = cap.get(cv2.CAP_PROP_FPS)\nwidth = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\nheight = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n\nout = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n\n# Process video frames\nwhile cap.isOpened():\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    # Preprocess the frame\n    input_data = preprocess_image(frame, (input_details[0]['shape'][1], input_details[0]['shape'][2]))\n\n    # Run inference\n    interpreter.set_tensor(input_details[0]['index'], input_data)\n    interpreter.invoke()\n\n    # Extract results\n    boxes = interpreter.get_tensor(output_details[0]['index'])[0]\n    classes = interpreter.get_tensor(output_details[1]['index'])[0]\n    scores = interpreter.get_tensor(output_details[2]['index'])[0]\n\n    # Draw detection boxes\n    draw_detection_boxes(frame, boxes, classes, scores)\n\n    # Write the frame with detections to output video\n    out.write(frame)\n\n# Release resources\ncap.release()\nout.release()\ncv2.destroyAllWindows()"
      },
      "session_id": "qwen2.5-coder:32b_9e98_tpusg_batch",
      "metadata": {
        "num_run": 21
      },
      "tags": [
        "ablation",
        "benchmark",
        "qwen2.5-coder:32b",
        "tpu_sketch_generator"
      ],
      "latency": 165.788,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-18-27-57-339935_chatcmpl-0e03a501-0586-498d-85a7-e8fe395e8f97",
          "traceId": "a4a85d4b",
          "type": "GENERATION",
          "name": "a4a8_tpusg_gen_attempt#1",
          "startTime": "2025-12-10T16:27:57.339000+00:00",
          "endTime": "2025-12-10T16:28:22.889000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 25550.0,
          "promptTokens": 293,
          "costDetails": {},
          "environment": "default",
          "totalTokens": 982,
          "completionTokens": 689,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-18-28-30-975686_chatcmpl-87279646-626c-44bb-9b2c-686e489aecd9",
          "traceId": "a4a85d4b",
          "type": "GENERATION",
          "name": "a4a8_tpusg_gen_attempt#2",
          "startTime": "2025-12-10T16:28:30.975000+00:00",
          "endTime": "2025-12-10T16:28:58.174000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 27199.0,
          "promptTokens": 293,
          "costDetails": {},
          "environment": "default",
          "totalTokens": 1029,
          "completionTokens": 736,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-18-29-05-685786_chatcmpl-d6f97393-59b0-4c46-bf77-d5533d276d29",
          "traceId": "a4a85d4b",
          "type": "GENERATION",
          "name": "a4a8_tpusg_gen_attempt#3",
          "startTime": "2025-12-10T16:29:05.685000+00:00",
          "endTime": "2025-12-10T16:29:33.071000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 27386.0,
          "promptTokens": 293,
          "costDetails": {},
          "environment": "default",
          "totalTokens": 1034,
          "completionTokens": 741,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-18-29-41-117362_chatcmpl-859e68b4-0852-4030-9fcf-f1ac550121e6",
          "traceId": "a4a85d4b",
          "type": "GENERATION",
          "name": "a4a8_tpusg_gen_attempt#4",
          "startTime": "2025-12-10T16:29:41.117000+00:00",
          "endTime": "2025-12-10T16:30:07.372000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 26255.0,
          "promptTokens": 293,
          "costDetails": {},
          "environment": "default",
          "totalTokens": 1003,
          "completionTokens": 710,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-18-30-15-117411_chatcmpl-e6643a68-7f43-4e47-9f8b-7ece0055fdeb",
          "traceId": "a4a85d4b",
          "type": "GENERATION",
          "name": "a4a8_tpusg_gen_attempt#5",
          "startTime": "2025-12-10T16:30:15.117000+00:00",
          "endTime": "2025-12-10T16:30:43.127000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 28010.0,
          "promptTokens": 293,
          "costDetails": {},
          "environment": "default",
          "totalTokens": 1051,
          "completionTokens": 758,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "environment": "default",
      "createdAt": "2025-12-10T16:27:57.861Z",
      "updatedAt": "2025-12-10T16:31:41.034Z"
    },
    {
      "id": "3185ee58",
      "timestamp": "2025-12-10T16:26:12.381000+00:00",
      "name": "3185_tpu_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import cv2\nimport numpy as np\nfrom PIL import Image\nimport tflite_runtime.interpreter as tflite\nfrom pycoral.utils.edgetpu import make_interpreter\n\n# Configuration parameters\nmodel_path  = \"/home/mendel/tinyml_autopilot/models/edgetpu_detect.tflite\"\nlabel_path  = \"/home/mendel/tinyml_autopilot/models/labelmap.txt\"\ninput_path  = \"/home/mendel/tinyml_autopilot/data//sheeps.mp4\"\noutput_path  = \"/home/mendel/tinyml_autopilot/results/sheeps_detections.mp4\"\nconfidence_threshold  = 0.5\n\n# Load labels\ndef load_labels(filename):\n    with open(filename, 'r') as f:\n        return [line.strip() for line in f.readlines()]\n\nlabels = load_labels(label_path)\n\n# Setup interpreter\ninterpreter = make_interpreter(model_path)\ninterpreter.allocate_tensors()\n\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\n# Preprocessing function\ndef preprocess_image(image):\n    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n    image = Image.fromarray(image)\n    image = image.resize((input_details[0]['shape'][1], input_details[0]['shape'][2]))\n    image = np.array(image)\n    return image\n\n# Inference function\ndef run_inference(interpreter, image):\n    interpreter.set_tensor(input_details[0]['index'], [image])\n    interpreter.invoke()\n    \n    boxes = interpreter.get_tensor(output_details[0]['index'])[0]\n    class_ids = interpreter.get_tensor(output_details[1]['index'])[0]\n    scores = interpreter.get_tensor(output_details[2]['index'])[0]\n    count = int(interpreter.get_tensor(output_details[3]['index'])[0])\n    \n    results = []\n    for i in range(count):\n        if scores[i] >= confidence_threshold:\n            ymin, xmin, ymax, xmax = boxes[i]\n            results.append((xmin, ymin, xmax, ymax, class_ids[i], scores[i]))\n    return results\n\n# Draw detection boxes\ndef draw_detection_boxes(image, detections):\n    height, width, _ = image.shape\n    for (xmin, ymin, xmax, ymax, class_id, score) in detections:\n        xmin, xmax, ymin, ymax = int(xmin * width), int(xmax * width), int(ymin * height), int(ymax * height)\n        cv2.rectangle(image, (xmin, ymin), (xmax, ymax), (0, 255, 0), 2)\n        label = f'{labels[int(class_id)]}: {score:.2f}'\n        cv2.putText(image, label, (xmin, ymin - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n\n# Main pipeline\ncap = cv2.VideoCapture(input_path)\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\nfps = cap.get(cv2.CAP_PROP_FPS)\nwidth = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\nheight = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\nout = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n\nwhile cap.isOpened():\n    ret, frame = cap.read()\n    if not ret:\n        break\n    \n    processed_frame = preprocess_image(frame)\n    detections = run_inference(interpreter, processed_frame)\n    draw_detection_boxes(frame, detections)\n    \n    out.write(frame)\n\ncap.release()\nout.release()\ncv2.destroyAllWindows()"
      },
      "session_id": "qwen2.5-coder:32b_9e98_tpusg_batch",
      "metadata": {
        "num_run": 20
      },
      "tags": [
        "ablation",
        "benchmark",
        "qwen2.5-coder:32b",
        "tpu_sketch_generator"
      ],
      "latency": 26.932,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-18-26-12-382906_chatcmpl-e898f2b2-7d3a-4de7-b0c8-b882787488ab",
          "traceId": "3185ee58",
          "type": "GENERATION",
          "name": "3185_tpusg_gen_attempt#1",
          "startTime": "2025-12-10T16:26:12.382000+00:00",
          "endTime": "2025-12-10T16:26:39.314000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 26932.0,
          "promptTokens": 293,
          "costDetails": {},
          "environment": "default",
          "totalTokens": 1019,
          "completionTokens": 726,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "environment": "default",
      "createdAt": "2025-12-10T16:26:12.902Z",
      "updatedAt": "2025-12-10T16:27:37.238Z"
    },
    {
      "id": "f1367c7f",
      "timestamp": "2025-12-10T16:24:25.391000+00:00",
      "name": "f136_tpu_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import cv2\nimport numpy as np\nimport tflite_runtime.interpreter as tflite\nfrom PIL import Image\n\n# Configuration parameters\nmodel_path  = \"/home/mendel/tinyml_autopilot/models/edgetpu_detect.tflite\"\nlabel_path  = \"/home/mendel/tinyml_autopilot/models/labelmap.txt\"\ninput_path  = \"/home/mendel/tinyml_autopilot/data//sheeps.mp4\"\noutput_path  = \"/home/mendel/tinyml_autopilot/results/sheeps_detections.mp4\"\nconfidence_threshold  = 0.5\n\n# Load labels\nwith open(label_path, 'r') as f:\n    labels = [line.strip() for line in f.readlines()]\n\n# Load TFLite interpreter with TPU delegate\ninterpreter = tflite.Interpreter(model_path=model_path,\n                                  experimental_delegates=[tflite.load_delegate('/usr/lib/aarch64-linux-gnu/libedgetpu.so.1.0')])\ninterpreter.allocate_tensors()\n\n# Get input and output details\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\n# Load video capture\ncap = cv2.VideoCapture(input_path)\nif not cap.isOpened():\n    raise Exception(\"Error opening video file\")\n\n# Prepare video writer for output\nframe_width = int(cap.get(3))\nframe_height = int(cap.get(4))\nfps = cap.get(cv2.CAP_PROP_FPS)\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\nout = cv2.VideoWriter(output_path, fourcc, fps, (frame_width, frame_height))\n\ndef preprocess_image(image):\n    input_shape = input_details[0]['shape']\n    image_resized = Image.fromarray(cv2.cvtColor(image, cv2.COLOR_BGR2RGB)).resize((input_shape[1], input_shape[2]))\n    return np.expand_dims(np.array(image_resized), axis=0).astype(np.uint8)\n\ndef draw_boxes(frame, boxes, class_ids, scores):\n    for box, class_id, score in zip(boxes, class_ids, scores):\n        if score > confidence_threshold:\n            ymin = int(max(1, (box[0] * frame_height)))\n            xmin = int(max(1, (box[1] * frame_width)))\n            ymax = int(min(frame_height, (box[2] * frame_height)))\n            xmax = int(min(frame_width, (box[3] * frame_width)))\n\n            cv2.rectangle(frame, (xmin, ymin), (xmax, ymax), (0, 255, 0), 2)\n            label = f'{labels[int(class_id)]}: {score:.2f}'\n            cv2.putText(frame, label, (xmin, ymin - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (36, 255, 12), 2)\n\nwhile cap.isOpened():\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    # Preprocess image\n    input_tensor = preprocess_image(frame)\n    interpreter.set_tensor(input_details[0]['index'], input_tensor)\n\n    # Run inference\n    interpreter.invoke()\n\n    # Extract results\n    boxes = interpreter.get_tensor(output_details[0]['index'])[0]\n    class_ids = interpreter.get_tensor(output_details[1]['index'])[0]\n    scores = interpreter.get_tensor(output_details[2]['index'])[0]\n\n    # Draw bounding boxes and labels\n    draw_boxes(frame, boxes, class_ids, scores)\n\n    # Write the frame with bounding boxes to the output video file\n    out.write(frame)\n\n# Release resources\ncap.release()\nout.release()"
      },
      "session_id": "qwen2.5-coder:32b_9e98_tpusg_batch",
      "metadata": {
        "num_run": 19
      },
      "tags": [
        "ablation",
        "benchmark",
        "qwen2.5-coder:32b",
        "tpu_sketch_generator"
      ],
      "latency": 29.315,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-18-24-25-392758_chatcmpl-3625bb8c-8189-4b71-93d4-cc6d1ea018f2",
          "traceId": "f1367c7f",
          "type": "GENERATION",
          "name": "f136_tpusg_gen_attempt#1",
          "startTime": "2025-12-10T16:24:25.392000+00:00",
          "endTime": "2025-12-10T16:24:54.707000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 29315.0,
          "promptTokens": 293,
          "costDetails": {},
          "environment": "default",
          "totalTokens": 1083,
          "completionTokens": 790,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "environment": "default",
      "createdAt": "2025-12-10T16:24:25.913Z",
      "updatedAt": "2025-12-10T16:25:52.037Z"
    },
    {
      "id": "c6de8151",
      "timestamp": "2025-12-10T16:22:04.448000+00:00",
      "name": "c6de_tpu_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import cv2\nimport numpy as np\nfrom PIL import Image\nimport tflite_runtime.interpreter as tflite\nfrom pycoral.adapters import common\nfrom pycoral.adapters import detect\nfrom pycoral.utils.dataset import read_label_file\n\n# Configuration parameters\nmodel_path  = \"/home/mendel/tinyml_autopilot/models/edgetpu_detect.tflite\"\nlabel_path  = \"/home/mendel/tinyml_autopilot/models/labelmap.txt\"\ninput_path  = \"/home/mendel/tinyml_autopilot/data//sheeps.mp4\"\noutput_path  = \"/home/mendel/tinyml_autopilot/results/sheeps_detections.mp4\"\nconfidence_threshold  = 0.5\n\n# Load the TFLite interpreter for TPU\ninterpreter = tflite.Interpreter(model_path=model_path, experimental_delegates=[tflite.load_delegate('/usr/lib/aarch64-linux-gnu/libedgetpu.so.1.0')])\ninterpreter.allocate_tensors()\n\n# Get input and output details\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\n# Load all labels from the provided label map\nlabels = read_label_file(label_path)\n\n# Open video file for reading\ncap = cv2.VideoCapture(input_path)\nif not cap.isOpened():\n    raise IOError(\"Cannot open input video\")\n\n# Get video properties and set up output video writer\nframe_width = int(cap.get(3))\nframe_height = int(cap.get(4))\nfps = cap.get(cv2.CAP_PROP_FPS)\n\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\nout = cv2.VideoWriter(output_path, fourcc, fps, (frame_width, frame_height))\n\ndef draw_detection_boxes(image, detections):\n    for obj in detections:\n        bbox = obj.bbox\n        score = obj.score\n        class_id = int(obj.id)\n        if score > confidence_threshold:\n            label = labels.get(class_id, f'unknown:{class_id}')\n            cv2.rectangle(image, (bbox.xmin, bbox.ymin), (bbox.xmax, bbox.ymax), (0, 255, 0), 2)\n            cv2.putText(image, f'{label}: {score:.2f}', (bbox.xmin, bbox.ymin - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n\n# Process each frame in the video\nwhile cap.isOpened():\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    # Preprocess the image\n    image = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n    resized_image = image.resize((input_details[0]['shape'][1], input_details[0]['shape'][2]), Image.ANTIALIAS)\n    common.set_input(interpreter, resized_image)\n\n    # Run inference\n    interpreter.invoke()\n\n    # Get detection results\n    detections = detect.get_objects(interpreter, confidence_threshold)\n\n    # Draw detection boxes on the frame\n    draw_detection_boxes(frame, detections)\n\n    # Write the processed frame to output video\n    out.write(frame)\n\n# Release resources\ncap.release()\nout.release()"
      },
      "session_id": "qwen2.5-coder:32b_9e98_tpusg_batch",
      "metadata": {
        "num_run": 18
      },
      "tags": [
        "ablation",
        "benchmark",
        "qwen2.5-coder:32b",
        "tpu_sketch_generator"
      ],
      "latency": 59.135,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-18-22-04-449357_chatcmpl-f134777d-d94e-496e-a70a-e39210bb8059",
          "traceId": "c6de8151",
          "type": "GENERATION",
          "name": "c6de_tpusg_gen_attempt#1",
          "startTime": "2025-12-10T16:22:04.449000+00:00",
          "endTime": "2025-12-10T16:22:31.697000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 27248.0,
          "promptTokens": 293,
          "costDetails": {},
          "environment": "default",
          "totalTokens": 1027,
          "completionTokens": 734,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-18-22-39-397295_chatcmpl-5ee1ca5b-2c80-4975-8551-49b4b7420ddd",
          "traceId": "c6de8151",
          "type": "GENERATION",
          "name": "c6de_tpusg_gen_attempt#2",
          "startTime": "2025-12-10T16:22:39.397000+00:00",
          "endTime": "2025-12-10T16:23:03.584000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 24187.0,
          "promptTokens": 293,
          "costDetails": {},
          "environment": "default",
          "totalTokens": 947,
          "completionTokens": 654,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "environment": "default",
      "createdAt": "2025-12-10T16:22:04.968Z",
      "updatedAt": "2025-12-10T16:24:05.665Z"
    },
    {
      "id": "db1edfc1",
      "timestamp": "2025-12-10T16:18:01.529000+00:00",
      "name": "db1e_tpu_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import os\nimport cv2\nimport numpy as np\nfrom PIL import Image\nfrom pycoral.utils.edgetpu import make_interpreter\nfrom pycoral.adapters import common, detect\n\n# Configuration parameters\nmodel_path  = \"/home/mendel/tinyml_autopilot/models/edgetpu_detect.tflite\"\nlabel_path  = \"/home/mendel/tinyml_autopilot/models/labelmap.txt\"\ninput_path  = \"/home/mendel/tinyml_autopilot/data//sheeps.mp4\"\noutput_path  = \"/home/mendel/tinyml_autopilot/results/sheeps_detections.mp4\"\nconfidence_threshold  = 0.5\n\n# Load labels\ndef load_labels(label_path):\n    with open(label_path, 'r', encoding='utf-8') as f:\n        return {i: line.strip() for i, line in enumerate(f)}\n\nlabels = load_labels(label_path)\n\n# Initialize TFLite interpreter for TPU\ninterpreter = make_interpreter(model_path)\ninterpreter.allocate_tensors()\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\n# Read input video\ncap = cv2.VideoCapture(input_path)\nif not cap.isOpened():\n    raise IOError(\"Cannot open video file\")\n\n# Get video properties\nframe_width = int(cap.get(3))\nframe_height = int(cap.get(4))\nfps = cap.get(cv2.CAP_PROP_FPS)\n\n# Define codec and create VideoWriter object\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\nout = cv2.VideoWriter(output_path, fourcc, fps, (frame_width, frame_height))\n\ndef draw_detection_boxes(frame, detections, labels):\n    for obj in detections:\n        if obj.score > confidence_threshold:\n            bbox = obj.bbox\n            start_point = (int(bbox.xmin * frame.shape[1]), int(bbox.ymin * frame.shape[0]))\n            end_point = (int(bbox.xmax * frame.shape[1]), int(bbox.ymax * frame.shape[0]))\n            cv2.rectangle(frame, start_point, end_point, (0, 255, 0), 2)\n            label_text = f\"{labels.get(obj.id, 'Unknown')} {obj.score:.2f}\"\n            text_size = cv2.getTextSize(label_text, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 1)[0]\n            cv2.rectangle(frame, start_point, (start_point[0] + text_size[0], start_point[1] - 10), (0, 255, 0), -1)\n            cv2.putText(frame, label_text, (start_point[0], start_point[1] - 4), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 0), 1)\n\nwhile cap.isOpened():\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    # Preprocessing\n    img = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n    resized_img = img.resize((input_details[0]['shape'][2], input_details[0]['shape'][1]), Image.ANTIALIAS)\n    input_tensor = np.array(resized_img).astype(np.uint8)\n\n    # Inference\n    common.set_input(interpreter, input_tensor)\n    interpreter.invoke()\n    detections = detect.get_objects(interpreter, confidence_threshold)\n\n    # Draw detection boxes and labels\n    draw_detection_boxes(frame, detections, labels)\n\n    # Output handling\n    out.write(frame)\n\ncap.release()\nout.release()\ncv2.destroyAllWindows()"
      },
      "session_id": "qwen2.5-coder:32b_9e98_tpusg_batch",
      "metadata": {
        "num_run": 17
      },
      "tags": [
        "ablation",
        "benchmark",
        "qwen2.5-coder:32b",
        "tpu_sketch_generator"
      ],
      "latency": 161.259,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-18-18-01-530956_chatcmpl-d9644892-07ff-412d-9c78-d17aa7fd0416",
          "traceId": "db1edfc1",
          "type": "GENERATION",
          "name": "db1e_tpusg_gen_attempt#1",
          "startTime": "2025-12-10T16:18:01.530000+00:00",
          "endTime": "2025-12-10T16:18:25.509000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 23979.0,
          "promptTokens": 293,
          "costDetails": {},
          "environment": "default",
          "totalTokens": 939,
          "completionTokens": 646,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-18-18-33-621248_chatcmpl-dc987eec-e023-457c-87b0-76060bdc74af",
          "traceId": "db1edfc1",
          "type": "GENERATION",
          "name": "db1e_tpusg_gen_attempt#2",
          "startTime": "2025-12-10T16:18:33.621000+00:00",
          "endTime": "2025-12-10T16:19:00.911000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 27290.0,
          "promptTokens": 293,
          "costDetails": {},
          "environment": "default",
          "totalTokens": 1031,
          "completionTokens": 738,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-18-19-09-005589_chatcmpl-b8a6315c-c510-470a-ad2b-27d8f4159053",
          "traceId": "db1edfc1",
          "type": "GENERATION",
          "name": "db1e_tpusg_gen_attempt#3",
          "startTime": "2025-12-10T16:19:09.005000+00:00",
          "endTime": "2025-12-10T16:19:31.325000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 22320.0,
          "promptTokens": 293,
          "costDetails": {},
          "environment": "default",
          "totalTokens": 896,
          "completionTokens": 603,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-18-19-39-550416_chatcmpl-5a576635-baf4-4e3d-80d1-78408b6a4fcb",
          "traceId": "db1edfc1",
          "type": "GENERATION",
          "name": "db1e_tpusg_gen_attempt#4",
          "startTime": "2025-12-10T16:19:39.550000+00:00",
          "endTime": "2025-12-10T16:20:08.375000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 28825.0,
          "promptTokens": 293,
          "costDetails": {},
          "environment": "default",
          "totalTokens": 1072,
          "completionTokens": 779,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-18-20-15-899258_chatcmpl-3c99cf6e-da22-4795-8b9e-e9a04ae7ba92",
          "traceId": "db1edfc1",
          "type": "GENERATION",
          "name": "db1e_tpusg_gen_attempt#5",
          "startTime": "2025-12-10T16:20:15.899000+00:00",
          "endTime": "2025-12-10T16:20:42.789000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 26890.0,
          "promptTokens": 293,
          "costDetails": {},
          "environment": "default",
          "totalTokens": 1020,
          "completionTokens": 727,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "environment": "default",
      "createdAt": "2025-12-10T16:18:02.048Z",
      "updatedAt": "2025-12-10T16:21:44.451Z"
    },
    {
      "id": "64c17461",
      "timestamp": "2025-12-10T16:14:49.571000+00:00",
      "name": "64c1_tpu_sketch_generator",
      "input": null,
      "output": {
        "status": "failed",
        "last_code": "import numpy as np\nimport cv2\nfrom PIL import Image\nfrom pycoral.utils.edgetpu import make_interpreter\nfrom pycoral.adapters.common import input_size, set_input\nfrom pycoral.adapters.detect import get_objects\n\n# Configuration parameters\nmodel_path  = \"/home/mendel/tinyml_autopilot/models/edgetpu_detect.tflite\"\nlabel_path  = \"/home/mendel/tinyml_autopilot/models/labelmap.txt\"\ninput_path  = \"/home/mendel/tinyml_autopilot/data//sheeps.mp4\"\noutput_path  = \"/home/mendel/tinyml_autopilot/results/sheeps_detections.mp4\"\nconfidence_threshold  = 0.5\n\n# Load labels\ndef load_labels(filename):\n    with open(filename, 'r', encoding='utf-8') as f:\n        return {int(line.split(',')[0]): line.strip() for line in f.readlines()[1:]}\n\nlabels = load_labels(label_path)\n\n# Setup interpreter\ninterpreter = make_interpreter(model_path)\ninterpreter.allocate_tensors()\n\n# Preprocessing function\ndef preprocess_image(image, input_size):\n    image = Image.fromarray(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n    image = image.resize(input_size, Image.ANTIALIAS)\n    return np.array(image)\n\n# Function to draw detection boxes\ndef draw_detection_boxes(frame, detections, labels, confidence_threshold):\n    height, width, _ = frame.shape\n    for obj in detections:\n        if obj.score > confidence_threshold:\n            box = obj.bbox\n            start_point = (int(box.xmin * width), int(box.ymin * height))\n            end_point = (int(box.xmax * width), int(box.ymax * height))\n            cv2.rectangle(frame, start_point, end_point, (0, 255, 0), 2)\n            label = f\"{labels[obj.id]}: {obj.score:.2f}\"\n            cv2.putText(frame, label, (start_point[0], start_point[1] - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n\n# Main pipeline\ncap = cv2.VideoCapture(input_path)\nif not cap.isOpened():\n    raise IOError(\"Cannot open video\")\n\nframe_width = int(cap.get(3))\nframe_height = int(cap.get(4))\nout = cv2.VideoWriter(output_path, cv2.VideoWriter_fourcc(*'mp4v'), 10, (frame_width, frame_height))\n\nwhile True:\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    size = input_size(interpreter)\n    processed_image = preprocess_image(frame, size)\n\n    set_input(interpreter, processed_image)\n    interpreter.invoke()\n\n    detections = get_objects(interpreter, confidence_threshold)[:10]\n    draw_detection_boxes(frame, detections, labels, confidence_threshold)\n\n    out.write(frame)\n\ncap.release()\nout.release()\ncv2.destroyAllWindows()",
        "last_error": "Max retries reached with error. Last error: Traceback (most recent call last):\n  File \"script_786c0c7e_1765383448.py\", line 20, in <module>\n    labels = load_labels(label_path)\n  File \"script_786c0c7e_1765383448.py\", line 18, in load_labels\n    return {int(line.split(',')[0]): line.strip() for line in f.readlines()[1:]}\n  File \"script_786c0c7e_1765383448.py\", line 18, in <dictcomp>\n    return {int(line.split(',')[0]): line.strip() for line in f.readlines()[1:]}\nValueError: invalid literal for int() with base 10: 'bicycle\\n'."
      },
      "session_id": "qwen2.5-coder:32b_9e98_tpusg_batch",
      "metadata": {
        "num_run": 16
      },
      "tags": [
        "ablation",
        "benchmark",
        "qwen2.5-coder:32b",
        "tpu_sketch_generator"
      ],
      "latency": 166.478,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-18-14-49-572767_chatcmpl-c52b5aa7-57fe-40be-bc3e-b9511817482c",
          "traceId": "64c17461",
          "type": "GENERATION",
          "name": "64c1_tpusg_gen_attempt#1",
          "startTime": "2025-12-10T16:14:49.572000+00:00",
          "endTime": "2025-12-10T16:15:14.300000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 24728.0,
          "promptTokens": 293,
          "costDetails": {},
          "environment": "default",
          "totalTokens": 959,
          "completionTokens": 666,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-18-15-21-962751_chatcmpl-afd709c9-eb5f-46ba-abe6-ef893601b4b2",
          "traceId": "64c17461",
          "type": "GENERATION",
          "name": "64c1_tpusg_gen_attempt#2",
          "startTime": "2025-12-10T16:15:21.962000+00:00",
          "endTime": "2025-12-10T16:15:50.486000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 28524.0,
          "promptTokens": 293,
          "costDetails": {},
          "environment": "default",
          "totalTokens": 1063,
          "completionTokens": 770,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-18-15-58-028700_chatcmpl-d8a939d8-a679-4b23-bc4c-f2e739893314",
          "traceId": "64c17461",
          "type": "GENERATION",
          "name": "64c1_tpusg_gen_attempt#3",
          "startTime": "2025-12-10T16:15:58.028000+00:00",
          "endTime": "2025-12-10T16:16:21.775000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 23747.0,
          "promptTokens": 293,
          "costDetails": {},
          "environment": "default",
          "totalTokens": 935,
          "completionTokens": 642,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-18-16-30-138155_chatcmpl-e34a2a1f-5586-4e03-b9cd-389b64662861",
          "traceId": "64c17461",
          "type": "GENERATION",
          "name": "64c1_tpusg_gen_attempt#4",
          "startTime": "2025-12-10T16:16:30.138000+00:00",
          "endTime": "2025-12-10T16:16:56.146000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 26008.0,
          "promptTokens": 293,
          "costDetails": {},
          "environment": "default",
          "totalTokens": 996,
          "completionTokens": 703,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-18-17-03-876102_chatcmpl-f2df14b5-75b5-428a-b851-dafea49a206f",
          "traceId": "64c17461",
          "type": "GENERATION",
          "name": "64c1_tpusg_gen_attempt#5",
          "startTime": "2025-12-10T16:17:03.876000+00:00",
          "endTime": "2025-12-10T16:17:28.169000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 24293.0,
          "promptTokens": 293,
          "costDetails": {},
          "environment": "default",
          "totalTokens": 950,
          "completionTokens": 657,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "4f5f5818-c7b5-4d37-bc76-047cb3421ab8",
          "traceId": "64c17461",
          "type": "SPAN",
          "name": "error_64_tpusg_failure_signal_tpu_sketch_generator",
          "startTime": "2025-12-10T16:17:36.050000+00:00",
          "level": "ERROR",
          "statusMessage": "Failed: Max retries reached with error. Last error: Traceback (most recent call last):\n  File \"script_786c0c7e_1765383448.py\", line 20, in <module>\n    labels = load_labels(label_path)\n  File \"script_786c0c7e_1765383448.py\", line 18, in load_labels\n    return {int(line.split(',')[0]): line.strip() for line in f.readlines()[1:]}\n  File \"script_786c0c7e_1765383448.py\", line 18, in <dictcomp>\n    return {int(line.split(',')[0]): line.strip() for line in f.readlines()[1:]}\nValueError: invalid literal for int() with base 10: 'bicycle\\n'.",
          "calculatedTotalCost": 0.0,
          "promptTokens": 0,
          "costDetails": {},
          "environment": "default",
          "totalTokens": 0,
          "completionTokens": 0,
          "endTime": null,
          "model": null,
          "modelParameters": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null,
          "latency": null
        }
      ],
      "environment": "default",
      "createdAt": "2025-12-10T16:14:50.092Z",
      "updatedAt": "2025-12-10T16:17:41.273Z"
    },
    {
      "id": "602d7b31",
      "timestamp": "2025-12-10T16:11:10.262000+00:00",
      "name": "602d_tpu_sketch_generator",
      "input": null,
      "output": {
        "status": "failed",
        "last_code": "import os\nimport cv2\nimport numpy as np\nfrom PIL import Image\nfrom pycoral.utils.edgetpu import make_interpreter\nfrom pycoral.adapters.common import input_size, set_input\nfrom pycoral.adapters.detect import get_objects\n\n# Configuration parameters\nmodel_path  = \"/home/mendel/tinyml_autopilot/models/edgetpu_detect.tflite\"\nlabel_path  = \"/home/mendel/tinyml_autopilot/models/labelmap.txt\"\ninput_path  = \"/home/mendel/tinyml_autopilot/data//sheeps.mp4\"\noutput_path  = \"/home/mendel/tinyml_autopilot/results/sheeps_detections.mp4\"\nconfidence_threshold  = 0.5\n\n# Load labels\ndef load_labels(path):\n    with open(path, 'r', encoding='utf-8') as f:\n        lines = f.readlines()\n    return {int(line.split()[0]): line.strip() for line in lines}\n\nlabels = load_labels(label_path)\n\n# Initialize interpreter\ninterpreter = make_interpreter(model_path)\ninterpreter.allocate_tensors()\n\n# Get input and output details\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\nheight, width, _ = input_size(interpreter)\n\n# Read video\ncap = cv2.VideoCapture(input_path)\nfps = cap.get(cv2.CAP_PROP_FPS)\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\nout = cv2.VideoWriter(output_path, fourcc, fps, (int(cap.get(3)), int(cap.get(4))))\n\n# Function to draw bounding boxes\ndef draw_boxes(frame, objects, labels):\n    for obj in objects:\n        bbox = obj.bbox\n        label = f'{labels[obj.id]}: {obj.score:.2f}'\n        cv2.rectangle(frame, (int(bbox.xmin * width), int(bbox.ymin * height)),\n                      (int(bbox.xmax * width), int(bbox.ymax * height)), (0, 255, 0), 2)\n        cv2.putText(frame, label, (int(bbox.xmin * width), int(bbox.ymin * height) - 10),\n                    cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n\n# Process video frame by frame\nwhile cap.isOpened():\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    # Preprocessing\n    img = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n    img = img.resize((width, height), Image.ANTIALIAS)\n    scale = (img.width / width, img.height / height)\n\n    # Inference\n    set_input(interpreter, img)\n    interpreter.invoke()\n    objects = get_objects(interpreter, score_threshold=confidence_threshold, image_scale=scale)\n\n    # Draw bounding boxes and labels\n    draw_boxes(frame, objects, labels)\n\n    # Write the frame with detection to output video\n    out.write(frame)\n\n# Release resources\ncap.release()\nout.release()\ncv2.destroyAllWindows()",
        "last_error": "Max retries reached with error. Last error: Traceback (most recent call last):\n  File \"script_9efe7bf9_1765383255.py\", line 22, in <module>\n    labels = load_labels(label_path)\n  File \"script_9efe7bf9_1765383255.py\", line 20, in load_labels\n    return {int(line.split()[0]): line.strip() for line in lines}\n  File \"script_9efe7bf9_1765383255.py\", line 20, in <dictcomp>\n    return {int(line.split()[0]): line.strip() for line in lines}\nValueError: invalid literal for int() with base 10: 'person'."
      },
      "session_id": "qwen2.5-coder:32b_9e98_tpusg_batch",
      "metadata": {
        "num_run": 15
      },
      "tags": [
        "ablation",
        "benchmark",
        "qwen2.5-coder:32b",
        "tpu_sketch_generator"
      ],
      "latency": 193.335,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-18-11-10-264196_chatcmpl-95fb1bde-ec39-4b69-bcda-975040b07eb7",
          "traceId": "602d7b31",
          "type": "GENERATION",
          "name": "602d_tpusg_gen_attempt#1",
          "startTime": "2025-12-10T16:11:10.264000+00:00",
          "endTime": "2025-12-10T16:11:34.627000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 24363.0,
          "promptTokens": 293,
          "costDetails": {},
          "environment": "default",
          "totalTokens": 946,
          "completionTokens": 653,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-18-11-42-492980_chatcmpl-83c4b66a-5ae8-4495-824d-583a967f2719",
          "traceId": "602d7b31",
          "type": "GENERATION",
          "name": "602d_tpusg_gen_attempt#2",
          "startTime": "2025-12-10T16:11:42.492000+00:00",
          "endTime": "2025-12-10T16:12:28.874000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 46382.0,
          "promptTokens": 293,
          "costDetails": {},
          "environment": "default",
          "totalTokens": 1538,
          "completionTokens": 1245,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-18-12-36-793703_chatcmpl-234c6366-bab9-47db-ae68-3857881704e3",
          "traceId": "602d7b31",
          "type": "GENERATION",
          "name": "602d_tpusg_gen_attempt#3",
          "startTime": "2025-12-10T16:12:36.793000+00:00",
          "endTime": "2025-12-10T16:13:04.484000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 27691.0,
          "promptTokens": 293,
          "costDetails": {},
          "environment": "default",
          "totalTokens": 1042,
          "completionTokens": 749,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-18-13-12-063883_chatcmpl-3469057f-96f2-422c-9415-ef002926c8dc",
          "traceId": "602d7b31",
          "type": "GENERATION",
          "name": "602d_tpusg_gen_attempt#4",
          "startTime": "2025-12-10T16:13:12.063000+00:00",
          "endTime": "2025-12-10T16:13:42.996000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 30933.0,
          "promptTokens": 293,
          "costDetails": {},
          "environment": "default",
          "totalTokens": 1129,
          "completionTokens": 836,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-18-13-50-516096_chatcmpl-f620b14a-958f-456b-9822-ddbd26dbcba3",
          "traceId": "602d7b31",
          "type": "GENERATION",
          "name": "602d_tpusg_gen_attempt#5",
          "startTime": "2025-12-10T16:13:50.516000+00:00",
          "endTime": "2025-12-10T16:14:15.424000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 24908.0,
          "promptTokens": 293,
          "costDetails": {},
          "environment": "default",
          "totalTokens": 966,
          "completionTokens": 673,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "154bb8cf-08fd-43a9-ba87-42a024e84151",
          "traceId": "602d7b31",
          "type": "SPAN",
          "name": "error_60_tpusg_failure_signal_tpu_sketch_generator",
          "startTime": "2025-12-10T16:14:23.599000+00:00",
          "level": "ERROR",
          "statusMessage": "Failed: Max retries reached with error. Last error: Traceback (most recent call last):\n  File \"script_9efe7bf9_1765383255.py\", line 22, in <module>\n    labels = load_labels(label_path)\n  File \"script_9efe7bf9_1765383255.py\", line 20, in load_labels\n    return {int(line.split()[0]): line.strip() for line in lines}\n  File \"script_9efe7bf9_1765383255.py\", line 20, in <dictcomp>\n    return {int(line.split()[0]): line.strip() for line in lines}\nValueError: invalid literal for int() with base 10: 'person'.",
          "calculatedTotalCost": 0.0,
          "promptTokens": 0,
          "costDetails": {},
          "environment": "default",
          "totalTokens": 0,
          "completionTokens": 0,
          "endTime": null,
          "model": null,
          "modelParameters": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null,
          "latency": null
        }
      ],
      "environment": "default",
      "createdAt": "2025-12-10T16:11:10.782Z",
      "updatedAt": "2025-12-10T16:14:28.972Z"
    },
    {
      "id": "661fdccc",
      "timestamp": "2025-12-10T16:08:46.271000+00:00",
      "name": "661f_tpu_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import os\nimport cv2\nimport numpy as np\nfrom PIL import Image\nfrom tflite_runtime.interpreter import Interpreter\nfrom tflite_runtime.interpreter import load_delegate\n\n# Configuration parameters\nmodel_path  = \"/home/mendel/tinyml_autopilot/models/edgetpu_detect.tflite\"\nlabel_path  = \"/home/mendel/tinyml_autopilot/models/labelmap.txt\"\ninput_path  = \"/home/mendel/tinyml_autopilot/data//sheeps.mp4\"\noutput_path  = \"/home/mendel/tinyml_autopilot/results/sheeps_detections.mp4\"\nconfidence_threshold  = 0.5\n\n# Load labels\nwith open(label_path, 'r') as f:\n    labels = [line.strip() for line in f.readlines()]\n\n# Setup TFLite interpreter with Edge TPU delegate\ninterpreter = Interpreter(model_path=model_path,\n                          experimental_delegates=[load_delegate('/usr/lib/aarch64-linux-gnu/libedgetpu.so.1.0')])\ninterpreter.allocate_tensors()\n\n# Get input and output details\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\n# Preprocessing function\ndef preprocess_image(image):\n    image_resized = cv2.resize(image, (300, 300))\n    image_rgb = cv2.cvtColor(image_resized, cv2.COLOR_BGR2RGB)\n    input_data = np.expand_dims(image_rgb, axis=0).astype(np.uint8)\n    return input_data\n\n# Function to draw bounding boxes\ndef draw_boxes(frame, detections):\n    height, width, _ = frame.shape\n    for i in range(len(detections['detection_boxes'])):\n        score = detections['detection_scores'][i]\n        if score > confidence_threshold:\n            box = detections['detection_boxes'][i]\n            class_id = int(detections['detection_classes'][i])\n            label = labels[class_id] if class_id < len(labels) else 'Unknown'\n            \n            y_min, x_min, y_max, x_max = box\n            y_min *= height\n            x_min *= width\n            y_max *= height\n            x_max *= width\n            \n            cv2.rectangle(frame, (int(x_min), int(y_min)), (int(x_max), int(y_max)), (0, 255, 0), 2)\n            cv2.putText(frame, f'{label}: {score:.2f}', (int(x_min), int(y_min) - 10),\n                        cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n\n# Read video from input path\ncap = cv2.VideoCapture(input_path)\nif not cap.isOpened():\n    raise IOError(f\"Cannot open video file {input_path}\")\n\n# Prepare to write the output video\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\nfps = int(cap.get(cv2.CAP_PROP_FPS))\nframe_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\nframe_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\nout = cv2.VideoWriter(output_path, fourcc, fps, (frame_width, frame_height))\n\n# Process each frame\nwhile cap.isOpened():\n    ret, frame = cap.read()\n    if not ret:\n        break\n    \n    # Preprocess the image\n    input_data = preprocess_image(frame)\n    \n    # Run inference\n    interpreter.set_tensor(input_details[0]['index'], input_data)\n    interpreter.invoke()\n    \n    # Extract detection results\n    boxes = interpreter.get_tensor(output_details[0]['index'])[0]\n    classes = interpreter.get_tensor(output_details[1]['index'])[0].astype(np.int64)\n    scores = interpreter.get_tensor(output_details[2]['index'])[0]\n    \n    detections = {\n        'detection_boxes': boxes,\n        'detection_classes': classes,\n        'detection_scores': scores\n    }\n    \n    # Draw bounding boxes on the frame\n    draw_boxes(frame, detections)\n    \n    # Write the frame to output video\n    out.write(frame)\n\n# Release resources\ncap.release()\nout.release()\ncv2.destroyAllWindows()"
      },
      "session_id": "qwen2.5-coder:32b_9e98_tpusg_batch",
      "metadata": {
        "num_run": 14
      },
      "tags": [
        "ablation",
        "benchmark",
        "qwen2.5-coder:32b",
        "tpu_sketch_generator"
      ],
      "latency": 66.011,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-18-08-46-272471_chatcmpl-d2ae8530-4815-413f-9436-f3b4c9396e8c",
          "traceId": "661fdccc",
          "type": "GENERATION",
          "name": "661f_tpusg_gen_attempt#1",
          "startTime": "2025-12-10T16:08:46.272000+00:00",
          "endTime": "2025-12-10T16:09:10.263000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 23991.0,
          "promptTokens": 293,
          "costDetails": {},
          "environment": "default",
          "totalTokens": 939,
          "completionTokens": 646,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-18-09-18-748732_chatcmpl-c32687e5-dc57-4e13-8f37-04dba523d042",
          "traceId": "661fdccc",
          "type": "GENERATION",
          "name": "661f_tpusg_gen_attempt#2",
          "startTime": "2025-12-10T16:09:18.748000+00:00",
          "endTime": "2025-12-10T16:09:52.283000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 33535.0,
          "promptTokens": 293,
          "costDetails": {},
          "environment": "default",
          "totalTokens": 1199,
          "completionTokens": 906,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "environment": "default",
      "createdAt": "2025-12-10T16:08:46.790Z",
      "updatedAt": "2025-12-10T16:10:50.488Z"
    },
    {
      "id": "19757924",
      "timestamp": "2025-12-10T16:05:29.266000+00:00",
      "name": "1975_tpu_sketch_generator",
      "input": null,
      "output": {
        "status": "failed",
        "last_code": "import cv2\nimport numpy as np\nfrom tflite_runtime.interpreter import Interpreter\nfrom PIL import Image\nfrom pycocotools.coco import COCO\n\n# Configuration parameters\nmodel_path  = \"/home/mendel/tinyml_autopilot/models/edgetpu_detect.tflite\"\nlabel_path  = \"/home/mendel/tinyml_autopilot/models/labelmap.txt\"\ninput_path  = \"/home/mendel/tinyml_autopilot/data//sheeps.mp4\"\noutput_path  = \"/home/mendel/tinyml_autopilot/results/sheeps_detections.mp4\"\nconfidence_threshold  = 0.5\n\n# Load labels\nwith open(label_path, 'r') as f:\n    labels = [line.strip() for line in f.readlines()]\n\n# Load TFLite model and allocate tensors\ninterpreter = Interpreter(model_path=model_path,\n                          experimental_delegates=[load_delegate('/usr/lib/aarch64-linux-gnu/libedgetpu.so.1.0')])\ninterpreter.allocate_tensors()\n\n# Get input and output details\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\n# Read video file\ncap = cv2.VideoCapture(input_path)\nwidth = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\nheight = int(cap.get(cv2.CAP_PROP_HEIGHT))\nfps = cap.get(cv2.CAP_PROP_FPS)\n\n# Define codec and create VideoWriter object\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\nout = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n\ndef preprocess(image):\n    image_resized = Image.fromarray(image).resize((300, 300))\n    input_data = np.array(image_resized, dtype=np.float32)\n    input_data = np.expand_dims(input_data, axis=0)\n    return input_data\n\ndef draw_boxes(frame, boxes, classes, scores):\n    for i in range(len(scores)):\n        if scores[i] > confidence_threshold:\n            box = boxes[i]\n            class_id = int(classes[i])\n            label = labels[class_id] if class_id < len(labels) else 'Unknown'\n            score = scores[i]\n\n            top_left = (int(box[1] * width), int(box[0] * height))\n            bottom_right = (int(box[3] * width), int(box[2] * height))\n\n            cv2.rectangle(frame, top_left, bottom_right, (0, 255, 0), 2)\n            label_text = f'{label}: {score:.2f}'\n            cv2.putText(frame, label_text, (top_left[0], top_left[1] - 10),\n                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n\n# Process video frame by frame\nwhile cap.isOpened():\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    input_data = preprocess(frame)\n    interpreter.set_tensor(input_details[0]['index'], input_data)\n    interpreter.invoke()\n\n    boxes = interpreter.get_tensor(output_details[0]['index'])[0]\n    classes = interpreter.get_tensor(output_details[1]['index'])[0]\n    scores = interpreter.get_tensor(output_details[2]['index'])[0]\n\n    draw_boxes(frame, boxes, classes, scores)\n    out.write(frame)\n\ncap.release()\nout.release()\ncv2.destroyAllWindows()",
        "last_error": "Max retries reached with error. Last error: Traceback (most recent call last):\n  File \"script_00c690a6_1765382892.py\", line 5, in <module>\n    from pycocotools.coco import COCO\nModuleNotFoundError: No module named 'pycocotools'."
      },
      "session_id": "qwen2.5-coder:32b_9e98_tpusg_batch",
      "metadata": {
        "num_run": 13
      },
      "tags": [
        "ablation",
        "benchmark",
        "qwen2.5-coder:32b",
        "tpu_sketch_generator"
      ],
      "latency": 171.265,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-18-05-29-267377_chatcmpl-7e34b97f-d3d0-43e7-a2a3-e138d1ab6d55",
          "traceId": "19757924",
          "type": "GENERATION",
          "name": "1975_tpusg_gen_attempt#1",
          "startTime": "2025-12-10T16:05:29.267000+00:00",
          "endTime": "2025-12-10T16:06:00.453000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 31186.0,
          "promptTokens": 293,
          "costDetails": {},
          "environment": "default",
          "totalTokens": 1134,
          "completionTokens": 841,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-18-06-08-140840_chatcmpl-8027882e-096a-4b9d-9c39-9fdfa9139d49",
          "traceId": "19757924",
          "type": "GENERATION",
          "name": "1975_tpusg_gen_attempt#2",
          "startTime": "2025-12-10T16:06:08.140000+00:00",
          "endTime": "2025-12-10T16:06:32.284000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 24144.0,
          "promptTokens": 293,
          "costDetails": {},
          "environment": "default",
          "totalTokens": 946,
          "completionTokens": 653,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-18-06-39-978197_chatcmpl-23779160-507b-4f36-a164-f6982376a5de",
          "traceId": "19757924",
          "type": "GENERATION",
          "name": "1975_tpusg_gen_attempt#3",
          "startTime": "2025-12-10T16:06:39.978000+00:00",
          "endTime": "2025-12-10T16:07:04.378000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 24400.0,
          "promptTokens": 293,
          "costDetails": {},
          "environment": "default",
          "totalTokens": 953,
          "completionTokens": 660,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-18-07-11-932390_chatcmpl-03d2ffcc-3cad-4a53-afdc-f43e9c81648d",
          "traceId": "19757924",
          "type": "GENERATION",
          "name": "1975_tpusg_gen_attempt#4",
          "startTime": "2025-12-10T16:07:11.932000+00:00",
          "endTime": "2025-12-10T16:07:37.567000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 25635.0,
          "promptTokens": 293,
          "costDetails": {},
          "environment": "default",
          "totalTokens": 986,
          "completionTokens": 693,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-18-07-45-402689_chatcmpl-1de8635e-58e5-4c63-b74a-b7ffa4aa403b",
          "traceId": "19757924",
          "type": "GENERATION",
          "name": "1975_tpusg_gen_attempt#5",
          "startTime": "2025-12-10T16:07:45.402000+00:00",
          "endTime": "2025-12-10T16:08:12.812000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 27410.0,
          "promptTokens": 293,
          "costDetails": {},
          "environment": "default",
          "totalTokens": 1034,
          "completionTokens": 741,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "2915efc0-558e-4a41-a6ef-38af2228b56a",
          "traceId": "19757924",
          "type": "SPAN",
          "name": "error_19_tpusg_failure_signal_tpu_sketch_generator",
          "startTime": "2025-12-10T16:08:20.532000+00:00",
          "level": "ERROR",
          "statusMessage": "Failed: Max retries reached with error. Last error: Traceback (most recent call last):\n  File \"script_00c690a6_1765382892.py\", line 5, in <module>\n    from pycocotools.coco import COCO\nModuleNotFoundError: No module named 'pycocotools'.",
          "calculatedTotalCost": 0.0,
          "promptTokens": 0,
          "costDetails": {},
          "environment": "default",
          "totalTokens": 0,
          "completionTokens": 0,
          "endTime": null,
          "model": null,
          "modelParameters": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null,
          "latency": null
        }
      ],
      "environment": "default",
      "createdAt": "2025-12-10T16:05:29.785Z",
      "updatedAt": "2025-12-10T16:08:25.962Z"
    },
    {
      "id": "49498a55",
      "timestamp": "2025-12-10T16:03:07.268000+00:00",
      "name": "4949_tpu_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import cv2\nimport numpy as np\nfrom tflite_runtime.interpreter import Interpreter\nfrom tflite_runtime.interpreter import load_delegate\n\n# Configuration parameters\nmodel_path  = \"/home/mendel/tinyml_autopilot/models/edgetpu_detect.tflite\"\nlabel_path  = \"/home/mendel/tinyml_autopilot/models/labelmap.txt\"\ninput_path  = \"/home/mendel/tinyml_autopilot/data//sheeps.mp4\"\noutput_path  = \"/home/mendel/tinyml_autopilot/results/sheeps_detections.mp4\"\nconfidence_threshold  = 0.5\n\n# Load labels\nwith open(label_path, 'r') as f:\n    labels = [line.strip() for line in f.readlines()]\n\n# Setup TFLite interpreter with TPU delegate\ninterpreter = Interpreter(model_path=model_path,\n                          experimental_delegates=[load_delegate('/usr/lib/aarch64-linux-gnu/libedgetpu.so.1.0')])\ninterpreter.allocate_tensors()\n\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\n# Read video from input path\ncap = cv2.VideoCapture(input_path)\nif not cap.isOpened():\n    raise IOError(f\"Could not open video file: {input_path}\")\n\n# Get video properties\nframe_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\nframe_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\nfps = cap.get(cv2.CAP_PROP_FPS)\n\n# Setup video writer for output\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\nout = cv2.VideoWriter(output_path, fourcc, fps, (frame_width, frame_height))\n\ndef draw_detection_boxes(frame, boxes, classes, scores):\n    for i in range(len(scores)):\n        if scores[i] >= confidence_threshold:\n            ymin, xmin, ymax, xmax = boxes[i]\n            label = labels[int(classes[i])]\n            score = scores[i]\n\n            # Convert normalized coordinates to pixel values\n            im_height, im_width, _ = frame.shape\n            (xminn, xmaxx, yminn, ymaxx) = (xmin * im_width, xmax * im_width, ymin * im_height, ymax * im_height)\n\n            cv2.rectangle(frame, (int(xminn), int(yminn)), (int(xmaxx), int(ymaxx)), (0, 255, 0), 2)\n            cv2.putText(frame, f'{label}: {score:.2f}', (int(xminn), int(yminn) - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9,\n                        (0, 255, 0), 2)\n\nwhile cap.isOpened():\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    # Preprocessing\n    input_data = np.expand_dims(cv2.resize(frame, (300, 300)), axis=0).astype(np.uint8)\n    interpreter.set_tensor(input_details[0]['index'], input_data)\n\n    # Inference\n    interpreter.invoke()\n\n    # Extract output data\n    boxes = interpreter.get_tensor(output_details[0]['index'])[0]\n    classes = interpreter.get_tensor(output_details[1]['index'])[0]\n    scores = interpreter.get_tensor(output_details[2]['index'])[0]\n\n    # Draw detection boxes with labels\n    draw_detection_boxes(frame, boxes, classes, scores)\n\n    # Write the frame to the output file\n    out.write(frame)\n\n# Release video capture and writer objects\ncap.release()\nout.release()\ncv2.destroyAllWindows()"
      },
      "session_id": "qwen2.5-coder:32b_9e98_tpusg_batch",
      "metadata": {
        "num_run": 12
      },
      "tags": [
        "ablation",
        "benchmark",
        "qwen2.5-coder:32b",
        "tpu_sketch_generator"
      ],
      "latency": 64.256,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-18-03-07-269675_chatcmpl-bc87d300-6771-49af-ae0c-db8183851129",
          "traceId": "49498a55",
          "type": "GENERATION",
          "name": "4949_tpusg_gen_attempt#1",
          "startTime": "2025-12-10T16:03:07.269000+00:00",
          "endTime": "2025-12-10T16:03:37.173000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 29904.0,
          "promptTokens": 293,
          "costDetails": {},
          "environment": "default",
          "totalTokens": 1098,
          "completionTokens": 805,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-18-03-44-645659_chatcmpl-5b5b3e17-ed15-43a2-94c6-2ddc14315782",
          "traceId": "49498a55",
          "type": "GENERATION",
          "name": "4949_tpusg_gen_attempt#2",
          "startTime": "2025-12-10T16:03:44.645000+00:00",
          "endTime": "2025-12-10T16:04:11.525000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 26880.0,
          "promptTokens": 293,
          "costDetails": {},
          "environment": "default",
          "totalTokens": 1019,
          "completionTokens": 726,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "environment": "default",
      "createdAt": "2025-12-10T16:03:07.792Z",
      "updatedAt": "2025-12-10T16:05:09.440Z"
    },
    {
      "id": "ebd91dad",
      "timestamp": "2025-12-10T16:00:52.523000+00:00",
      "name": "ebd9_tpu_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import os\nimport cv2\nimport numpy as np\nfrom PIL import Image\nfrom pycoral.adapters.common import input_size\nfrom pycoral.adapters.detect import get_objects\nfrom pycoral.utils.dataset import read_label_file\nfrom pycoral.utils.edgetpu import make_interpreter\n\n# Configuration parameters\nmodel_path  = \"/home/mendel/tinyml_autopilot/models/edgetpu_detect.tflite\"\nlabel_path  = \"/home/mendel/tinyml_autopilot/models/labelmap.txt\"\ninput_path  = \"/home/mendel/tinyml_autopilot/data//sheeps.mp4\"\noutput_path  = \"/home/mendel/tinyml_autopilot/results/sheeps_detections.mp4\"\nconfidence_threshold  = 0.5\n\n# Phase 1: Setup\ninterpreter = make_interpreter(model_path, device=':0')\ninterpreter.allocate_tensors()\nlabels = read_label_file(label_path)\n\n# Get model details\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\nheight, width = input_size(interpreter)\n\n# Load video and prepare output video writer\ncap = cv2.VideoCapture(input_path)\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\nout = cv2.VideoWriter(output_path, fourcc, 30.0, (int(cap.get(3)), int(cap.get(4))))\n\n# Phase 2: Preprocessing and Inference\nwhile cap.isOpened():\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    # Resize the frame to match model input size\n    resized_frame = cv2.resize(frame, (width, height))\n    rgb_resized_frame = cv2.cvtColor(resized_frame, cv2.COLOR_BGR2RGB)\n    \n    # Prepare input tensor\n    input_tensor = np.expand_dims(rgb_resized_frame, axis=0).astype(np.uint8)\n    interpreter.set_tensor(input_details[0]['index'], input_tensor)\n\n    # Run inference\n    interpreter.invoke()\n    detected_objects = get_objects(interpreter, score_threshold=confidence_threshold, image_scale=(1.0, 1.0))\n\n    # Phase 3: Postprocessing - Draw bounding boxes and labels\n    for obj in detected_objects:\n        bbox = obj.bbox\n        x0, y0, x1, y1 = int(bbox.xmin * width), int(bbox.ymin * height), int(bbox.xmax * width), int(bbox.ymax * height)\n        label = f\"{labels.get(obj.id, 'Unknown')} {obj.score:.2f}\"\n        \n        # Draw bounding box\n        cv2.rectangle(frame, (x0, y0), (x1, y1), (0, 255, 0), 2)\n        # Put label text\n        cv2.putText(frame, label, (x0, y0 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (36, 255, 12), 2)\n\n    # Write the frame to output video file\n    out.write(frame)\n\n# Release resources\ncap.release()\nout.release()\n\nprint(f\"Processed video saved at {output_path}\")"
      },
      "session_id": "qwen2.5-coder:32b_9e98_tpusg_batch",
      "metadata": {
        "num_run": 11
      },
      "tags": [
        "ablation",
        "benchmark",
        "qwen2.5-coder:32b",
        "tpu_sketch_generator"
      ],
      "latency": 55.935,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-18-00-52-524991_chatcmpl-9ec726cb-7d93-4ffc-be1f-66117dcb6155",
          "traceId": "ebd91dad",
          "type": "GENERATION",
          "name": "ebd9_tpusg_gen_attempt#1",
          "startTime": "2025-12-10T16:00:52.524000+00:00",
          "endTime": "2025-12-10T16:01:15.093000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 22569.0,
          "promptTokens": 293,
          "costDetails": {},
          "environment": "default",
          "totalTokens": 898,
          "completionTokens": 605,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-18-01-23-018004_chatcmpl-8eaac506-0e47-4d4e-b64d-7dd1a62a9b58",
          "traceId": "ebd91dad",
          "type": "GENERATION",
          "name": "ebd9_tpusg_gen_attempt#2",
          "startTime": "2025-12-10T16:01:23.018000+00:00",
          "endTime": "2025-12-10T16:01:48.459000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 25441.0,
          "promptTokens": 293,
          "costDetails": {},
          "environment": "default",
          "totalTokens": 982,
          "completionTokens": 689,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "environment": "default",
      "createdAt": "2025-12-10T16:00:53.044Z",
      "updatedAt": "2025-12-10T16:02:46.231Z"
    },
    {
      "id": "08635d21",
      "timestamp": "2025-12-10T15:57:21.540000+00:00",
      "name": "0863_tpu_sketch_generator",
      "input": null,
      "output": {
        "status": "failed",
        "last_code": "import cv2\nimport numpy as np\nfrom PIL import Image\nimport tflite_runtime.interpreter as tflite\nfrom tflite_runtime.interpreter import load_delegate\nfrom collections import defaultdict\n\n# Configuration parameters\nmodel_path  = \"/home/mendel/tinyml_autopilot/models/edgetpu_detect.tflite\"\nlabel_path  = \"/home/mendel/tinyml_autopilot/models/labelmap.txt\"\ninput_path  = \"/home/mendel/tinyml_autopilot/data//sheeps.mp4\"\noutput_path  = \"/home/mendel/tinyml_autopilot/results/sheeps_detections.mp4\"\nconfidence_threshold  = 0.5\n\n# Load the label map\ndef load_labels(filename):\n    with open(filename, 'r', encoding='utf-8') as f:\n        lines = f.readlines()\n    labels = {}\n    for row_number, content in enumerate(lines):\n        pair = re.split(r'[:\\s]+', content.strip(), maxsplit=1)\n        if len(pair) == 2 and pair[0].strip().isdigit():\n            labels[int(pair[0])] = pair[1].strip()\n    return labels\n\nlabels = load_labels(label_path)\n\n# Load the TFLite model and allocate tensors\ninterpreter = tflite.Interpreter(\n    model_path=model_path,\n    experimental_delegates=[load_delegate('/usr/lib/aarch64-linux-gnu/libedgetpu.so.1.0')]\n)\ninterpreter.allocate_tensors()\n\n# Get input and output tensors.\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\nheight = input_details[0]['shape'][1]\nwidth  = input_details[0]['shape'][2]\n\n# Helper function to preprocess the image\ndef preprocess_image(image_path, target_height=300, target_width=300):\n    img = Image.open(image_path)\n    img = img.resize((target_width, target_height))\n    input_data = np.expand_dims(img, axis=0)\n    return input_data\n\n# Helper function to draw bounding boxes\ndef draw_boxes(frame, results, labels, confidence_threshold):\n    for result in results:\n        ymin, xmin, ymax, xmax = result[:4]\n        score = result[4]\n        label_id = int(result[5])\n        if score > confidence_threshold:\n            h, w, _ = frame.shape\n            cv2.rectangle(frame, (int(xmin*w), int(ymin*h)), (int(xmax*w), int(ymax*h)), (0, 255, 0), 2)\n            label_text = f'{labels[label_id]}: {score:.2f}'\n            cv2.putText(frame, label_text, (int(xmin*w), int(ymin*h)-10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n\n# Process video\ncap = cv2.VideoCapture(input_path)\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\nout = cv2.VideoWriter(output_path, fourcc, cap.get(cv2.CAP_PROP_FPS), (int(cap.get(3)), int(cap.get(4))))\n\nwhile cap.isOpened():\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    # Preprocess the frame\n    input_data = cv2.resize(frame, (width, height))\n    input_data = np.expand_dims(input_data, axis=0)\n    input_data = input_data.astype(np.uint8)\n\n    # Run inference\n    interpreter.set_tensor(input_details[0]['index'], input_data)\n    interpreter.invoke()\n\n    # Extract results\n    boxes = interpreter.get_tensor(output_details[1]['index'])[0]\n    classes = interpreter.get_tensor(output_details[3]['index'])[0]\n    scores = interpreter.get_tensor(output_details[2]['index'])[0]\n\n    results = []\n    for i in range(len(scores)):\n        if scores[i] > confidence_threshold:\n            ymin, xmin, ymax, xmax = boxes[i]\n            label_id = int(classes[i])\n            results.append((ymin, xmin, ymax, xmax, scores[i], label_id))\n\n    # Draw bounding boxes\n    draw_boxes(frame, results, labels, confidence_threshold)\n\n    # Write the frame to output video\n    out.write(frame)\n\ncap.release()\nout.release()\ncv2.destroyAllWindows()",
        "last_error": "Max retries reached with error. Last error: Traceback (most recent call last):\n  File \"script_2b1f3919_1765382419.py\", line 26, in <module>\n    labels = load_labels(label_path)\n  File \"script_2b1f3919_1765382419.py\", line 21, in load_labels\n    pair = re.split(r'[:\\s]+', content.strip(), maxsplit=1)\nNameError: name 're' is not defined."
      },
      "session_id": "qwen2.5-coder:32b_9e98_tpusg_batch",
      "metadata": {
        "num_run": 10
      },
      "tags": [
        "ablation",
        "benchmark",
        "qwen2.5-coder:32b",
        "tpu_sketch_generator"
      ],
      "latency": 185.45,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-17-57-21-541435_chatcmpl-1ca1f9e9-7723-4321-92d8-9bde9806e400",
          "traceId": "08635d21",
          "type": "GENERATION",
          "name": "0863_tpusg_gen_attempt#1",
          "startTime": "2025-12-10T15:57:21.541000+00:00",
          "endTime": "2025-12-10T15:57:47.912000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 26371.0,
          "promptTokens": 293,
          "costDetails": {},
          "environment": "default",
          "totalTokens": 1004,
          "completionTokens": 711,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-17-57-55-817531_chatcmpl-a25fbb6e-e4a5-441d-be9e-daa0104de43b",
          "traceId": "08635d21",
          "type": "GENERATION",
          "name": "0863_tpusg_gen_attempt#2",
          "startTime": "2025-12-10T15:57:55.817000+00:00",
          "endTime": "2025-12-10T15:58:18.495000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 22678.0,
          "promptTokens": 293,
          "costDetails": {},
          "environment": "default",
          "totalTokens": 907,
          "completionTokens": 614,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-17-58-26-599485_chatcmpl-ad286321-82ae-458b-acc2-067f7c723e04",
          "traceId": "08635d21",
          "type": "GENERATION",
          "name": "0863_tpusg_gen_attempt#3",
          "startTime": "2025-12-10T15:58:26.599000+00:00",
          "endTime": "2025-12-10T15:58:57.923000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 31324.0,
          "promptTokens": 293,
          "costDetails": {},
          "environment": "default",
          "totalTokens": 1140,
          "completionTokens": 847,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-17-59-05-619378_chatcmpl-bb46a735-5f8f-4086-8283-20bdedb0fdc4",
          "traceId": "08635d21",
          "type": "GENERATION",
          "name": "0863_tpusg_gen_attempt#4",
          "startTime": "2025-12-10T15:59:05.619000+00:00",
          "endTime": "2025-12-10T15:59:36.675000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 31056.0,
          "promptTokens": 293,
          "costDetails": {},
          "environment": "default",
          "totalTokens": 1132,
          "completionTokens": 839,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-17-59-44-348755_chatcmpl-67e611b1-f8f1-44bd-9fd7-ae5c11b3ba22",
          "traceId": "08635d21",
          "type": "GENERATION",
          "name": "0863_tpusg_gen_attempt#5",
          "startTime": "2025-12-10T15:59:44.348000+00:00",
          "endTime": "2025-12-10T16:00:19.286000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 34938.0,
          "promptTokens": 293,
          "costDetails": {},
          "environment": "default",
          "totalTokens": 1236,
          "completionTokens": 943,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "dba94e28-792c-47cb-bae2-d5749989d843",
          "traceId": "08635d21",
          "type": "SPAN",
          "name": "error_08_tpusg_failure_signal_tpu_sketch_generator",
          "startTime": "2025-12-10T16:00:26.991000+00:00",
          "level": "ERROR",
          "statusMessage": "Failed: Max retries reached with error. Last error: Traceback (most recent call last):\n  File \"script_2b1f3919_1765382419.py\", line 26, in <module>\n    labels = load_labels(label_path)\n  File \"script_2b1f3919_1765382419.py\", line 21, in load_labels\n    pair = re.split(r'[:\\s]+', content.strip(), maxsplit=1)\nNameError: name 're' is not defined.",
          "calculatedTotalCost": 0.0,
          "promptTokens": 0,
          "costDetails": {},
          "environment": "default",
          "totalTokens": 0,
          "completionTokens": 0,
          "endTime": null,
          "model": null,
          "modelParameters": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null,
          "latency": null
        }
      ],
      "environment": "default",
      "createdAt": "2025-12-10T15:57:22.062Z",
      "updatedAt": "2025-12-10T16:00:32.241Z"
    },
    {
      "id": "e30a75ab",
      "timestamp": "2025-12-10T15:54:55.558000+00:00",
      "name": "e30a_tpu_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import os\nimport cv2\nimport numpy as np\nfrom PIL import Image\nimport tflite_runtime.interpreter as tflite\nfrom pycoral.utils.edgetpu import make_interpreter\nfrom pycoral.adapters import common, detect\n\n# Configuration parameters\nmodel_path  = \"/home/mendel/tinyml_autopilot/models/edgetpu_detect.tflite\"\nlabel_path  = \"/home/mendel/tinyml_autopilot/models/labelmap.txt\"\ninput_path  = \"/home/mendel/tinyml_autopilot/data//sheeps.mp4\"\noutput_path  = \"/home/mendel/tinyml_autopilot/results/sheeps_detections.mp4\"\nconfidence_threshold  = 0.5\n\n# Load labels\ndef load_labels(filename):\n    with open(filename, 'r', encoding='utf-8') as f:\n        return {i: line.strip() for i, line in enumerate(f.readlines())}\n\nlabels = load_labels(label_path)\n\n# Setup interpreter\ninterpreter = make_interpreter(model_path)\ninterpreter.allocate_tensors()\n\n# Preprocessing function\ndef preprocess_image(image):\n    input_tensor = common.input_tensor(interpreter)\n    _, scale = common.set_resized_input(interpreter, image.size, lambda size: image.resize(size, Image.ANTIALIAS))\n    return np.array(input_tensor), scale\n\n# Inference function\ndef run_inference(interpreter, input_tensor):\n    interpreter.invoke()\n    return detect.get_objects(interpreter, confidence_threshold)\n\n# Draw boxes and labels on the frame\ndef draw_detections(frame, objs, scale, labels):\n    for obj in objs:\n        bbox = obj.bbox\n        bbox = (bbox.xmin * scale[0], bbox.ymin * scale[1], bbox.xmax * scale[0], bbox.ymax * scale[1])\n        cv2.rectangle(frame, (int(bbox[0]), int(bbox[1])), (int(bbox[2]), int(bbox[3])), (0, 255, 0), 2)\n        label = f'{labels[obj.id]}: {obj.score:.2f}'\n        cv2.putText(frame, label, (int(bbox[0]), int(bbox[1]) - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (36, 255, 12), 2)\n\n# Main pipeline\ncap = cv2.VideoCapture(input_path)\nif not cap.isOpened():\n    raise IOError(f\"Cannot open input video file {input_path}\")\n\nframe_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\nframe_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\nfps = cap.get(cv2.CAP_PROP_FPS)\n\nout = cv2.VideoWriter(output_path, cv2.VideoWriter_fourcc(*'mp4v'), fps, (frame_width, frame_height))\n\nwhile True:\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    pil_image = Image.fromarray(frame)\n    input_tensor, scale = preprocess_image(pil_image)\n    objs = run_inference(interpreter, input_tensor)\n    draw_detections(frame, objs, scale, labels)\n\n    out.write(frame)\n\ncap.release()\nout.release()\ncv2.destroyAllWindows()"
      },
      "session_id": "qwen2.5-coder:32b_9e98_tpusg_batch",
      "metadata": {
        "num_run": 9
      },
      "tags": [
        "ablation",
        "benchmark",
        "qwen2.5-coder:32b",
        "tpu_sketch_generator"
      ],
      "latency": 65.635,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-17-54-55-559768_chatcmpl-3098364c-4d36-49ec-9349-0d229565ebc0",
          "traceId": "e30a75ab",
          "type": "GENERATION",
          "name": "e30a_tpusg_gen_attempt#1",
          "startTime": "2025-12-10T15:54:55.559000+00:00",
          "endTime": "2025-12-10T15:55:27.700000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 32141.0,
          "promptTokens": 293,
          "costDetails": {},
          "environment": "default",
          "totalTokens": 1159,
          "completionTokens": 866,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-17-55-35-273384_chatcmpl-dd9d6c93-c7df-47fb-83d9-c9e6c2e558a5",
          "traceId": "e30a75ab",
          "type": "GENERATION",
          "name": "e30a_tpusg_gen_attempt#2",
          "startTime": "2025-12-10T15:55:35.273000+00:00",
          "endTime": "2025-12-10T15:56:01.194000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 25921.0,
          "promptTokens": 293,
          "costDetails": {},
          "environment": "default",
          "totalTokens": 994,
          "completionTokens": 701,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "environment": "default",
      "createdAt": "2025-12-10T15:54:56.080Z",
      "updatedAt": "2025-12-10T15:57:01.769Z"
    },
    {
      "id": "5388600a",
      "timestamp": "2025-12-10T15:51:11.603000+00:00",
      "name": "5388_tpu_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import cv2\nimport numpy as np\nfrom PIL import Image\nfrom tflite_runtime.interpreter import Interpreter\nfrom tflite_runtime.interpreter import load_delegate\n\n# Configuration parameters\nmodel_path  = \"/home/mendel/tinyml_autopilot/models/edgetpu_detect.tflite\"\nlabel_path  = \"/home/mendel/tinyml_autopilot/models/labelmap.txt\"\ninput_path  = \"/home/mendel/tinyml_autopilot/data//sheeps.mp4\"\noutput_path  = \"/home/mendel/tinyml_autopilot/results/sheeps_detections.mp4\"\nconfidence_threshold  = 0.5\n\n# Load labels\nwith open(label_path, 'r') as f:\n    labels = [line.strip() for line in f.readlines()]\n\n# Load TFLite model and allocate tensors.\ninterpreter = Interpreter(model_path=model_path,\n                          experimental_delegates=[load_delegate('/usr/lib/aarch64-linux-gnu/libedgetpu.so.1.0')])\ninterpreter.allocate_tensors()\n\n# Get input and output tensors.\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\n# Read video\ncap = cv2.VideoCapture(input_path)\nif not cap.isOpened():\n    raise IOError(\"Cannot open video\")\n\n# Video writer setup\nframe_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\nframe_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\nfps = cap.get(cv2.CAP_PROP_FPS)\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\nout = cv2.VideoWriter(output_path, fourcc, fps, (frame_width, frame_height))\n\ndef preprocess_image(image):\n    image = Image.fromarray(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n    image = image.resize((300, 300), Image.ANTIALIAS)\n    input_data = np.array(image)\n    input_data = np.expand_dims(input_data, axis=0).astype(np.uint8)\n    return input_data\n\ndef draw_detection_boxes(frame, boxes, classes, scores):\n    for i in range(len(scores)):\n        if scores[i] > confidence_threshold:\n            ymin, xmin, ymax, xmax = boxes[i]\n            label = labels[int(classes[i])]\n            score = scores[i]\n\n            # Convert normalized coordinates to absolute coordinates\n            height, width, _ = frame.shape\n            (xminn, xmaxx, yminn, ymaxx) = (xmin * width, xmax * width, ymin * height, ymax * height)\n            cv2.rectangle(frame, (int(xminn), int(yminn)), (int(xmaxx), int(ymaxx)), (0, 255, 0), 2)\n            label_text = f'{label}: {score:.2f}'\n            cv2.putText(frame, label_text, (int(xminn), int(yminn) - 10),\n                        cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n\n# Main loop\nwhile cap.isOpened():\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    # Preprocess image\n    input_data = preprocess_image(frame)\n\n    # Set the tensor to point to the input data to be inferred\n    interpreter.set_tensor(input_details[0]['index'], input_data)\n\n    # Run inference\n    interpreter.invoke()\n\n    # Extract results\n    boxes = interpreter.get_tensor(output_details[0]['index'])[0]\n    classes = interpreter.get_tensor(output_details[1]['index'])[0]\n    scores = interpreter.get_tensor(output_details[2]['index'])[0]\n\n    # Draw detection boxes on the frame\n    draw_detection_boxes(frame, boxes, classes, scores)\n\n    # Write the processed frame to the output video file\n    out.write(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n\n# Release resources\ncap.release()\nout.release()\ncv2.destroyAllWindows()"
      },
      "session_id": "qwen2.5-coder:32b_9e98_tpusg_batch",
      "metadata": {
        "num_run": 8
      },
      "tags": [
        "ablation",
        "benchmark",
        "qwen2.5-coder:32b",
        "tpu_sketch_generator"
      ],
      "latency": 141.885,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-17-51-11-604668_chatcmpl-96be0524-2422-42ca-b04f-11ca99574807",
          "traceId": "5388600a",
          "type": "GENERATION",
          "name": "5388_tpusg_gen_attempt#1",
          "startTime": "2025-12-10T15:51:11.604000+00:00",
          "endTime": "2025-12-10T15:51:35.356000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 23752.0,
          "promptTokens": 293,
          "costDetails": {},
          "environment": "default",
          "totalTokens": 932,
          "completionTokens": 639,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-17-51-43-524988_chatcmpl-0540446d-6ba4-490c-be3d-660b8fc661b9",
          "traceId": "5388600a",
          "type": "GENERATION",
          "name": "5388_tpusg_gen_attempt#2",
          "startTime": "2025-12-10T15:51:43.524000+00:00",
          "endTime": "2025-12-10T15:52:20.185000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 36661.0,
          "promptTokens": 293,
          "costDetails": {},
          "environment": "default",
          "totalTokens": 1281,
          "completionTokens": 988,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-17-52-27-797112_chatcmpl-26529f42-f5a7-484c-8986-cc12c68e8936",
          "traceId": "5388600a",
          "type": "GENERATION",
          "name": "5388_tpusg_gen_attempt#3",
          "startTime": "2025-12-10T15:52:27.797000+00:00",
          "endTime": "2025-12-10T15:52:52.799000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 25002.0,
          "promptTokens": 293,
          "costDetails": {},
          "environment": "default",
          "totalTokens": 969,
          "completionTokens": 676,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-17-53-01-617235_chatcmpl-10dd0c6a-f6e8-4109-8b32-5567076e2454",
          "traceId": "5388600a",
          "type": "GENERATION",
          "name": "5388_tpusg_gen_attempt#4",
          "startTime": "2025-12-10T15:53:01.617000+00:00",
          "endTime": "2025-12-10T15:53:33.489000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 31872.0,
          "promptTokens": 293,
          "costDetails": {},
          "environment": "default",
          "totalTokens": 1154,
          "completionTokens": 861,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "environment": "default",
      "createdAt": "2025-12-10T15:51:12.126Z",
      "updatedAt": "2025-12-10T15:54:35.337Z"
    },
    {
      "id": "a5215bc7",
      "timestamp": "2025-12-10T15:48:30.525000+00:00",
      "name": "a521_tpu_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import cv2\nimport numpy as np\nfrom tflite_runtime.interpreter import Interpreter\nfrom pycoral.adapters.common import input_size\nfrom pycoral.utils.dataset import read_label_file\nfrom pycoral.utils.edgetpu import make_interpreter\n\n# Configuration parameters\nmodel_path  = \"/home/mendel/tinyml_autopilot/models/edgetpu_detect.tflite\"\nlabel_path  = \"/home/mendel/tinyml_autopilot/models/labelmap.txt\"\ninput_path  = \"/home/mendel/tinyml_autopilot/data//sheeps.mp4\"\noutput_path  = \"/home/mendel/tinyml_autopilot/results/sheeps_detections.mp4\"\nconfidence_threshold  = 0.5\n\n# Phase 1: Setup\ninterpreter = make_interpreter(model_path, device=':0')\ninterpreter.allocate_tensors()\nlabels = read_label_file(label_path)\n_, height, width, _ = interpreter.get_input_details()[0]['shape']\ninference_size = (width, height)\n\n# Preprocessing function\ndef preprocess(image):\n    image_resized = cv2.resize(image, inference_size)\n    return image_resized.astype(np.uint8)\n\n# Inference function\ndef infer(image):\n    tensor_index = interpreter.tensor(interpreter.get_input_details()[0]['index'])\n    input_tensor = np.expand_dims(image, axis=0)\n    tensor_index()[0] = input_tensor\n    interpreter.invoke()\n    output_details = interpreter.get_output_details()\n    return [interpreter.get_tensor(output_details[i]['index']) for i in range(len(output_details))]\n\n# Function to draw bounding boxes and labels on the frame\ndef draw_detections(frame, boxes, scores, classes):\n    for i in range(boxes.shape[0]):\n        if scores[i] >= confidence_threshold:\n            ymin = int(max(1, (boxes[i][0] * height)))\n            xmin = int(max(1, (boxes[i][1] * width)))\n            ymax = int(min(height, (boxes[i][2] * height)))\n            xmax = int(min(width, (boxes[i][3] * width)))\n\n            cv2.rectangle(frame, (xmin, ymin), (xmax, ymax), (0, 255, 0), 2)\n            label = f'{labels.get(classes[i], \"N/A\")}: {scores[i]:.2f}'\n            cv2.putText(frame, label, (xmin, ymin - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n\n# Phase 3: Inference and Phase 4: Output handling\ncap = cv2.VideoCapture(input_path)\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\nout = cv2.VideoWriter(output_path, fourcc, 30.0, (width, height))\n\nwhile cap.isOpened():\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    image_resized = preprocess(frame)\n    boxes, scores, classes, num_detections = infer(image_resized)\n\n    draw_detections(frame, boxes[0], scores[0], classes[0])\n\n    out.write(frame)\n\ncap.release()\nout.release()\ncv2.destroyAllWindows()"
      },
      "session_id": "qwen2.5-coder:32b_9e98_tpusg_batch",
      "metadata": {
        "num_run": 7
      },
      "tags": [
        "ablation",
        "benchmark",
        "qwen2.5-coder:32b",
        "tpu_sketch_generator"
      ],
      "latency": 86.361,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-17-48-30-526477_chatcmpl-53d0b83c-4e91-4bb2-82c9-6a62a96d39a2",
          "traceId": "a5215bc7",
          "type": "GENERATION",
          "name": "a521_tpusg_gen_attempt#1",
          "startTime": "2025-12-10T15:48:30.526000+00:00",
          "endTime": "2025-12-10T15:48:50.529000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 20003.0,
          "promptTokens": 293,
          "costDetails": {},
          "environment": "default",
          "totalTokens": 832,
          "completionTokens": 539,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-17-48-59-187849_chatcmpl-a3e750a0-0ae8-45f3-92f9-d061f68892a5",
          "traceId": "a5215bc7",
          "type": "GENERATION",
          "name": "a521_tpusg_gen_attempt#2",
          "startTime": "2025-12-10T15:48:59.187000+00:00",
          "endTime": "2025-12-10T15:49:24.894000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 25707.0,
          "promptTokens": 293,
          "costDetails": {},
          "environment": "default",
          "totalTokens": 988,
          "completionTokens": 695,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-17-49-32-818312_chatcmpl-d9d1fdff-e3b1-4c08-8652-e6180f21b242",
          "traceId": "a5215bc7",
          "type": "GENERATION",
          "name": "a521_tpusg_gen_attempt#3",
          "startTime": "2025-12-10T15:49:32.818000+00:00",
          "endTime": "2025-12-10T15:49:56.887000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 24069.0,
          "promptTokens": 293,
          "costDetails": {},
          "environment": "default",
          "totalTokens": 944,
          "completionTokens": 651,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "environment": "default",
      "createdAt": "2025-12-10T15:48:31.047Z",
      "updatedAt": "2025-12-10T15:50:51.223Z"
    },
    {
      "id": "80d10233",
      "timestamp": "2025-12-10T15:44:54.576000+00:00",
      "name": "80d1_tpu_sketch_generator",
      "input": null,
      "output": {
        "status": "failed",
        "last_code": "import os\nimport cv2\nimport numpy as np\nfrom tflite_runtime.interpreter import Interpreter\nfrom tflite_runtime.interpreter import load_delegate\n\n# Configuration parameters\nmodel_path  = \"/home/mendel/tinyml_autopilot/models/edgetpu_detect.tflite\"\nlabel_path  = \"/home/mendel/tinyml_autopilot/models/labelmap.txt\"\ninput_path  = \"/home/mendel/tinyml_autopilot/data//sheeps.mp4\"\noutput_path  = \"/home/mendel/tinyml_autopilot/results/sheeps_detections.mp4\"\nconfidence_threshold  = 0.5\n\n# Load labels\nwith open(label_path, 'r') as f:\n    labels = [line.strip() for line in f.readlines()]\n\n# Setup TFLite interpreter with Edge TPU delegate\ninterpreter = Interpreter(\n    model_path=model_path,\n    experimental_delegates=[load_delegate('/usr/lib/aarch64-linux-gnu/libedgetpu.so.1.0')]\n)\ninterpreter.allocate_tensors()\n\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\n# Read video input\ncap = cv2.VideoCapture(input_path)\nif not cap.isOpened():\n    raise IOError(\"Cannot open video file\")\n\n# Get video properties to set up output video writer\nframe_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\nframe_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\nfps = cap.get(cv2.CAP_PROP_FPS)\n\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\nout = cv2.VideoWriter(output_path, fourcc, fps, (frame_width, frame_height))\n\ndef draw_detection_boxes(image, boxes, classes, scores):\n    for i in range(len(scores)):\n        if scores[i] > confidence_threshold:\n            box = boxes[i]\n            class_id = int(classes[i])\n            label = labels[class_id]\n            score = scores[i]\n\n            # Convert normalized coordinates to image dimensions\n            y_min, x_min, y_max, x_max = box\n            top_left = (int(x_min * frame_width), int(y_min * frame_height))\n            bottom_right = (int(x_max * frame_width), int(y_max * frame_height))\n\n            cv2.rectangle(image, top_left, bottom_right, (0, 255, 0), 2)\n            label_text = f'{label}: {score:.2f}'\n            cv2.putText(image, label_text, (top_left[0], top_left[1] - 10),\n                        cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n\n# Process video frames\nwhile True:\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    # Preprocessing: Resize and normalize image\n    input_shape = input_details[0]['shape']\n    resized_frame = cv2.resize(frame, (input_shape[1], input_shape[2]))\n    normalized_frame = np.array(resized_frame, dtype=np.float32) / 255.0\n    input_data = np.expand_dims(normalized_frame, axis=0)\n\n    # Run inference\n    interpreter.set_tensor(input_details[0]['index'], input_data)\n    interpreter.invoke()\n\n    # Extract results\n    boxes = interpreter.get_tensor(output_details[0]['index'])[0]\n    classes = interpreter.get_tensor(output_details[1]['index'])[0]\n    scores = interpreter.get_tensor(output_details[2]['index'])[0]\n\n    # Draw detection boxes on the frame\n    draw_detection_boxes(frame, boxes, classes, scores)\n\n    # Write the output frame\n    out.write(frame)\n\n# Release resources\ncap.release()\nout.release()\ncv2.destroyAllWindows()",
        "last_error": "Max retries reached with error. Last error: Traceback (most recent call last):\n  File \"script_4b7b36e5_1765381676.py\", line 72, in <module>\n    interpreter.set_tensor(input_details[0]['index'], input_data)\n  File \"/home/mendel/tinyml_autopilot/tinyml-env/lib/python3.7/site-packages/tflite_runtime/interpreter.py\", line 572, in set_tensor\n    self._interpreter.SetTensor(tensor_index, value)\nValueError: Cannot set tensor: Got value of type FLOAT32 but expected type UINT8 for input 175, name: normalized_input_image_tensor ."
      },
      "session_id": "qwen2.5-coder:32b_9e98_tpusg_batch",
      "metadata": {
        "num_run": 6
      },
      "tags": [
        "ablation",
        "benchmark",
        "qwen2.5-coder:32b",
        "tpu_sketch_generator"
      ],
      "latency": 190.309,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-17-44-54-577395_chatcmpl-b7215633-2b57-4afb-9433-2ab7d523d649",
          "traceId": "80d10233",
          "type": "GENERATION",
          "name": "80d1_tpusg_gen_attempt#1",
          "startTime": "2025-12-10T15:44:54.577000+00:00",
          "endTime": "2025-12-10T15:45:23.932000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 29355.0,
          "promptTokens": 293,
          "costDetails": {},
          "environment": "default",
          "totalTokens": 1083,
          "completionTokens": 790,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-17-45-32-417294_chatcmpl-d2ccd5bf-e894-4010-a51c-aea80272fdcc",
          "traceId": "80d10233",
          "type": "GENERATION",
          "name": "80d1_tpusg_gen_attempt#2",
          "startTime": "2025-12-10T15:45:32.417000+00:00",
          "endTime": "2025-12-10T15:46:05.982000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 33565.0,
          "promptTokens": 293,
          "costDetails": {},
          "environment": "default",
          "totalTokens": 1199,
          "completionTokens": 906,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-17-46-14-024794_chatcmpl-243a3a5d-dc2e-491b-93d4-baeeffaebc85",
          "traceId": "80d10233",
          "type": "GENERATION",
          "name": "80d1_tpusg_gen_attempt#3",
          "startTime": "2025-12-10T15:46:14.024000+00:00",
          "endTime": "2025-12-10T15:46:38.051000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 24027.0,
          "promptTokens": 293,
          "costDetails": {},
          "environment": "default",
          "totalTokens": 943,
          "completionTokens": 650,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-17-46-45-815156_chatcmpl-20116b13-c46f-4cfa-a8c9-44f734e812f9",
          "traceId": "80d10233",
          "type": "GENERATION",
          "name": "80d1_tpusg_gen_attempt#4",
          "startTime": "2025-12-10T15:46:45.815000+00:00",
          "endTime": "2025-12-10T15:47:20.901000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 35086.0,
          "promptTokens": 293,
          "costDetails": {},
          "environment": "default",
          "totalTokens": 1240,
          "completionTokens": 947,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-17-47-28-614699_chatcmpl-e9a1d28f-94da-4f00-9b22-992cf3a0bc36",
          "traceId": "80d10233",
          "type": "GENERATION",
          "name": "80d1_tpusg_gen_attempt#5",
          "startTime": "2025-12-10T15:47:28.614000+00:00",
          "endTime": "2025-12-10T15:47:56.782000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 28168.0,
          "promptTokens": 293,
          "costDetails": {},
          "environment": "default",
          "totalTokens": 1054,
          "completionTokens": 761,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "b319cbeb-1f45-47cc-9ccf-05203ae2fc0b",
          "traceId": "80d10233",
          "type": "SPAN",
          "name": "error_80_tpusg_failure_signal_tpu_sketch_generator",
          "startTime": "2025-12-10T15:48:04.886000+00:00",
          "level": "ERROR",
          "statusMessage": "Failed: Max retries reached with error. Last error: Traceback (most recent call last):\n  File \"script_4b7b36e5_1765381676.py\", line 72, in <module>\n    interpreter.set_tensor(input_details[0]['index'], input_data)\n  File \"/home/mendel/tinyml_autopilot/tinyml-env/lib/python3.7/site-packages/tflite_runtime/interpreter.py\", line 572, in set_tensor\n    self._interpreter.SetTensor(tensor_index, value)\nValueError: Cannot set tensor: Got value of type FLOAT32 but expected type UINT8 for input 175, name: normalized_input_image_tensor .",
          "calculatedTotalCost": 0.0,
          "promptTokens": 0,
          "costDetails": {},
          "environment": "default",
          "totalTokens": 0,
          "completionTokens": 0,
          "endTime": null,
          "model": null,
          "modelParameters": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null,
          "latency": null
        }
      ],
      "environment": "default",
      "createdAt": "2025-12-10T15:44:55.096Z",
      "updatedAt": "2025-12-10T15:48:10.338Z"
    },
    {
      "id": "b2a3120d",
      "timestamp": "2025-12-10T15:40:30.522000+00:00",
      "name": "b2a3_tpu_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import os\nimport numpy as np\nimport cv2\nfrom PIL import Image\nimport tflite_runtime.interpreter as tflite\nfrom pycoral.adapters.common import input_size\nfrom pycoral.utils.dataset import read_label_file\nfrom pycoral.utils.edgetpu import make_interpreter\n\n# Configuration parameters\nmodel_path  = \"/home/mendel/tinyml_autopilot/models/edgetpu_detect.tflite\"\nlabel_path  = \"/home/mendel/tinyml_autopilot/models/labelmap.txt\"\ninput_path  = \"/home/mendel/tinyml_autopilot/data//sheeps.mp4\"\noutput_path  = \"/home/mendel/tinyml_autopilot/results/sheeps_detections.mp4\"\nconfidence_threshold  = 0.5\n\n# Setup\ninterpreter = make_interpreter(model_path, device=':0')\ninterpreter.allocate_tensors()\nlabels = read_label_file(label_path)\nsize = input_size(interpreter)\n\n# Preprocessing function\ndef preprocess_image(image):\n    image = Image.fromarray(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n    image = image.resize(size, Image.ANTIALIAS)\n    return image\n\n# Inference function\ndef run_inference(interpreter, image):\n    input_details = interpreter.get_input_details()\n    output_details = interpreter.get_output_details()\n\n    # Set the tensor to point to the input data to be inferred\n    scale, zero_point = input_details[0]['quantization']\n    if isinstance(scale, np.floating) and isinstance(zero_point, np.integer):\n        image = (np.float32(image) - zero_point) * scale\n\n    interpreter.set_tensor(input_details[0]['index'], np.expand_dims(image, axis=0))\n    interpreter.invoke()\n\n    # Extract output data from the interpreter\n    boxes = interpreter.get_tensor(output_details[0]['index'])[0]\n    class_ids = interpreter.get_tensor(output_details[1]['index'])[0]\n    scores = interpreter.get_tensor(output_details[2]['index'])[0]\n    count = int(interpreter.get_tensor(output_details[3]['index'])[0])\n\n    return boxes, class_ids, scores, count\n\n# Drawing function\ndef draw_detections(image, boxes, class_ids, scores, labels):\n    height, width, _ = image.shape\n    for i in range(len(boxes)):\n        if scores[i] >= confidence_threshold:\n            ymin, xmin, ymax, xmax = boxes[i]\n            (xminn, xmaxx, yminn, ymaxx) = (int(xmin * width), int(xmax * width),\n                                            int(ymin * height), int(ymax * height))\n            cv2.rectangle(image, (xminn, yminn), (xmaxx, ymaxx), (10, 255, 0), 2)\n            class_name = labels.get(int(class_ids[i]), 'N/A')\n            label = f'{class_name}: {int(scores[i] * 100)}%'\n            labelSize, baseLine = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.7, 2)\n            label_ymin = max(yminn, labelSize[1] + 10)\n            cv2.rectangle(image, (xminn, label_ymin - labelSize[1] - 10),\n                          (xminn + labelSize[0], label_ymin + baseLine - 10), (255, 255, 255), cv2.FILLED)\n            cv2.putText(image, label, (xminn, label_ymin - 7),\n                        cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 0), 2)\n\n# Main processing loop\ncap = cv2.VideoCapture(input_path)\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\nfps = cap.get(cv2.CAP_PROP_FPS)\nwidth = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\nheight = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n\nout = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n\nwhile cap.isOpened():\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    preprocessed_image = preprocess_image(frame)\n    boxes, class_ids, scores, count = run_inference(interpreter, preprocessed_image)\n    draw_detections(frame, boxes, class_ids, scores, labels)\n\n    out.write(frame)\n\ncap.release()\nout.release()\ncv2.destroyAllWindows()"
      },
      "session_id": "qwen2.5-coder:32b_9e98_tpusg_batch",
      "metadata": {
        "num_run": 5
      },
      "tags": [
        "ablation",
        "benchmark",
        "qwen2.5-coder:32b",
        "tpu_sketch_generator"
      ],
      "latency": 182.009,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-17-40-30-523344_chatcmpl-efee28ec-b1a4-49f1-8a79-02fada03f631",
          "traceId": "b2a3120d",
          "type": "GENERATION",
          "name": "b2a3_tpusg_gen_attempt#1",
          "startTime": "2025-12-10T15:40:30.523000+00:00",
          "endTime": "2025-12-10T15:41:01.017000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 30494.0,
          "promptTokens": 293,
          "costDetails": {},
          "environment": "default",
          "totalTokens": 1115,
          "completionTokens": 822,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-17-41-08-733990_chatcmpl-d996b7ea-f40e-4356-91bf-be4c5f9e1739",
          "traceId": "b2a3120d",
          "type": "GENERATION",
          "name": "b2a3_tpusg_gen_attempt#2",
          "startTime": "2025-12-10T15:41:08.733000+00:00",
          "endTime": "2025-12-10T15:41:33.615000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 24882.0,
          "promptTokens": 293,
          "costDetails": {},
          "environment": "default",
          "totalTokens": 965,
          "completionTokens": 672,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-17-41-42-036417_chatcmpl-0e4683a2-05bb-47ee-b6c8-11ea8535156c",
          "traceId": "b2a3120d",
          "type": "GENERATION",
          "name": "b2a3_tpusg_gen_attempt#3",
          "startTime": "2025-12-10T15:41:42.036000+00:00",
          "endTime": "2025-12-10T15:42:17+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 34964.0,
          "promptTokens": 293,
          "costDetails": {},
          "environment": "default",
          "totalTokens": 1236,
          "completionTokens": 943,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-17-42-24-570465_chatcmpl-549fddfc-69cc-4c6f-a810-aca894d83fb2",
          "traceId": "b2a3120d",
          "type": "GENERATION",
          "name": "b2a3_tpusg_gen_attempt#4",
          "startTime": "2025-12-10T15:42:24.570000+00:00",
          "endTime": "2025-12-10T15:42:50.537000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 25967.0,
          "promptTokens": 293,
          "costDetails": {},
          "environment": "default",
          "totalTokens": 995,
          "completionTokens": 702,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-17-42-58-498875_chatcmpl-112f850e-36fd-460e-9bcf-cb41c55fee71",
          "traceId": "b2a3120d",
          "type": "GENERATION",
          "name": "b2a3_tpusg_gen_attempt#5",
          "startTime": "2025-12-10T15:42:58.498000+00:00",
          "endTime": "2025-12-10T15:43:32.532000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 34034.0,
          "promptTokens": 293,
          "costDetails": {},
          "environment": "default",
          "totalTokens": 1213,
          "completionTokens": 920,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "environment": "default",
      "createdAt": "2025-12-10T15:40:31.042Z",
      "updatedAt": "2025-12-10T15:44:34.944Z"
    },
    {
      "id": "c5a6efa4",
      "timestamp": "2025-12-10T15:38:10.579000+00:00",
      "name": "c5a6_tpu_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import numpy as np\nimport cv2\nfrom PIL import Image\nimport tflite_runtime.interpreter as tflite\nfrom pycoral.utils.edgetpu import make_interpreter\nfrom pycoral.adapters import common, detect\n\n# Configuration parameters\nmodel_path  = \"/home/mendel/tinyml_autopilot/models/edgetpu_detect.tflite\"\nlabel_path  = \"/home/mendel/tinyml_autopilot/models/labelmap.txt\"\ninput_path  = \"/home/mendel/tinyml_autopilot/data//sheeps.mp4\"\noutput_path  = \"/home/mendel/tinyml_autopilot/results/sheeps_detections.mp4\"\nconfidence_threshold  = 0.5\n\n# Load labels\ndef load_labels(filename):\n    with open(filename, 'r', encoding='utf-8') as f:\n        return {i: line.strip() for i, line in enumerate(f.readlines())}\n\nlabels = load_labels(label_path)\n\n# Initialize interpreter\ninterpreter = make_interpreter(model_path)\ninterpreter.allocate_tensors()\n\ncap = cv2.VideoCapture(input_path)\nif not cap.isOpened():\n    raise IOError(\"Cannot open video\")\n\nframe_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\nframe_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\nfps = cap.get(cv2.CAP_PROP_FPS)\n\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\nout = cv2.VideoWriter(output_path, fourcc, fps, (frame_width, frame_height))\n\ndef draw_detection_boxes(image, detections):\n    for obj in detections:\n        if obj.score > confidence_threshold:\n            bbox = obj.bbox\n            label = labels[obj.id]\n            score = obj.score\n            cv2.rectangle(image, (int(bbox.xmin), int(bbox.ymin)), \n                          (int(bbox.xmax), int(bbox.ymax)), (0, 255, 0), 2)\n            text = f'{label}: {score:.2f}'\n            cv2.putText(image, text, (int(bbox.xmin), int(bbox.ymin) - 10),\n                        cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n\nwhile cap.isOpened():\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    # Preprocessing\n    img = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n    resized_img = img.resize((300, 300), Image.ANTIALIAS)\n    \n    common.set_input(interpreter, np.array(resized_img))\n    \n    # Inference\n    interpreter.invoke()\n    detections = detect.get_objects(interpreter, confidence_threshold)\n    \n    # Draw bounding boxes and labels on the original frame\n    draw_detection_boxes(frame, detections)\n    \n    # Write processed frame to output video\n    out.write(frame)\n\ncap.release()\nout.release()\ncv2.destroyAllWindows()"
      },
      "session_id": "qwen2.5-coder:32b_9e98_tpusg_batch",
      "metadata": {
        "num_run": 4
      },
      "tags": [
        "ablation",
        "benchmark",
        "qwen2.5-coder:32b",
        "tpu_sketch_generator"
      ],
      "latency": 57.46,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-17-38-10-581076_chatcmpl-36750dbf-633c-49ae-a285-ef59ecb178c4",
          "traceId": "c5a6efa4",
          "type": "GENERATION",
          "name": "c5a6_tpusg_gen_attempt#1",
          "startTime": "2025-12-10T15:38:10.581000+00:00",
          "endTime": "2025-12-10T15:38:38.209000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 27628.0,
          "promptTokens": 293,
          "costDetails": {},
          "environment": "default",
          "totalTokens": 1038,
          "completionTokens": 745,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-17-38-45-993205_chatcmpl-ccb25e35-54b4-4f3c-b0f2-03c5ad785955",
          "traceId": "c5a6efa4",
          "type": "GENERATION",
          "name": "c5a6_tpusg_gen_attempt#2",
          "startTime": "2025-12-10T15:38:45.993000+00:00",
          "endTime": "2025-12-10T15:39:08.041000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 22048.0,
          "promptTokens": 293,
          "costDetails": {},
          "environment": "default",
          "totalTokens": 889,
          "completionTokens": 596,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "environment": "default",
      "createdAt": "2025-12-10T15:38:11.102Z",
      "updatedAt": "2025-12-10T15:40:10.276Z"
    },
    {
      "id": "76feadfa",
      "timestamp": "2025-12-10T15:35:50.527000+00:00",
      "name": "76fe_tpu_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import os\nimport numpy as np\nimport cv2\nfrom PIL import Image\nfrom tflite_runtime.interpreter import Interpreter\nfrom tflite_runtime.interpreter import load_delegate\n\n# Configuration parameters\nmodel_path  = \"/home/mendel/tinyml_autopilot/models/edgetpu_detect.tflite\"\nlabel_path  = \"/home/mendel/tinyml_autopilot/models/labelmap.txt\"\ninput_path  = \"/home/mendel/tinyml_autopilot/data//sheeps.mp4\"\noutput_path  = \"/home/mendel/tinyml_autopilot/results/sheeps_detections.mp4\"\nconfidence_threshold  = 0.5\n\n# Load labels\ndef load_labels(label_path):\n    with open(label_path, 'r', encoding='utf-8') as f:\n        return {i: line.strip() for i, line in enumerate(f.readlines())}\n\n# Initialize interpreter with TPU delegate\nlabels = load_labels(label_path)\ninterpreter = Interpreter(\n    model_path=model_path,\n    experimental_delegates=[load_delegate('/usr/lib/aarch64-linux-gnu/libedgetpu.so.1.0')]\n)\n\ninterpreter.allocate_tensors()\n\n# Get input and output details\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\n# Read video file\ncap = cv2.VideoCapture(input_path)\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\nout = None\n\ndef draw_detection_boxes(image, boxes, classes, scores, labels):\n    for i in range(len(scores)):\n        if scores[i] > confidence_threshold:\n            ymin, xmin, ymax, xmax = tuple(boxes[i])\n            label = labels[int(classes[i])]\n            score = scores[i]\n\n            # Convert normalized coordinates to image coordinates\n            height, width, _ = image.shape\n            left = int(xmin * width)\n            right = int(xmax * width)\n            top = int(ymin * height)\n            bottom = int(ymax * height)\n\n            cv2.rectangle(image, (left, top), (right, bottom), (0, 255, 0), 2)\n            label_text = f'{label}: {score:.2f}'\n            cv2.putText(image, label_text, (left, top - 10),\n                        cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n\nwhile cap.isOpened():\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    # Preprocess frame\n    image_resized = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)).resize((300, 300))\n    input_data = np.expand_dims(image_resized, axis=0)\n\n    # Set the tensor to point to the input data to be inferred\n    interpreter.set_tensor(input_details[0]['index'], input_data.astype(np.uint8))\n\n    # Run inference\n    interpreter.invoke()\n\n    # Extract output data from the results\n    boxes = np.squeeze(interpreter.get_tensor(output_details[0]['index']))\n    classes = np.squeeze(interpreter.get_tensor(output_details[1]['index']))\n    scores = np.squeeze(interpreter.get_tensor(output_details[2]['index']))\n\n    # Draw detection boxes on frame\n    draw_detection_boxes(frame, boxes, classes, scores, labels)\n\n    if out is None:\n        out = cv2.VideoWriter(output_path, fourcc, cap.get(cv2.CAP_PROP_FPS), (frame.shape[1], frame.shape[0]))\n\n    # Write the frame with detection to output video\n    out.write(frame)\n\ncap.release()\nif out is not None:\n    out.release()\ncv2.destroyAllWindows()"
      },
      "session_id": "qwen2.5-coder:32b_9e98_tpusg_batch",
      "metadata": {
        "num_run": 3
      },
      "tags": [
        "ablation",
        "benchmark",
        "qwen2.5-coder:32b",
        "tpu_sketch_generator"
      ],
      "latency": 62.32,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-17-35-50-528930_chatcmpl-d9598bae-b457-48e3-8df5-4acbb9252d98",
          "traceId": "76feadfa",
          "type": "GENERATION",
          "name": "76fe_tpusg_gen_attempt#1",
          "startTime": "2025-12-10T15:35:50.528000+00:00",
          "endTime": "2025-12-10T15:36:15.231000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 24703.0,
          "promptTokens": 293,
          "costDetails": {},
          "environment": "default",
          "totalTokens": 958,
          "completionTokens": 665,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-17-36-23-109730_chatcmpl-94caaee8-a848-463a-8923-4b653398b23c",
          "traceId": "76feadfa",
          "type": "GENERATION",
          "name": "76fe_tpusg_gen_attempt#2",
          "startTime": "2025-12-10T15:36:23.109000+00:00",
          "endTime": "2025-12-10T15:36:52.848000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 29739.0,
          "promptTokens": 293,
          "costDetails": {},
          "environment": "default",
          "totalTokens": 1096,
          "completionTokens": 803,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "environment": "default",
      "createdAt": "2025-12-10T15:35:51.047Z",
      "updatedAt": "2025-12-10T15:37:50.264Z"
    },
    {
      "id": "cfaead81",
      "timestamp": "2025-12-10T15:33:37.556000+00:00",
      "name": "cfae_tpu_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import os\nimport numpy as np\nimport cv2\nfrom PIL import Image\nimport tflite_runtime.interpreter as tflite\nfrom pycoral.utils.edgetpu import make_interpreter\nfrom pycoral.adapters import common, detect\n\n# Configuration parameters\nmodel_path  = \"/home/mendel/tinyml_autopilot/models/edgetpu_detect.tflite\"\nlabel_path  = \"/home/mendel/tinyml_autopilot/models/labelmap.txt\"\ninput_path  = \"/home/mendel/tinyml_autopilot/data//sheeps.mp4\"\noutput_path  = \"/home/mendel/tinyml_autopilot/results/sheeps_detections.mp4\"\nconfidence_threshold  = 0.5\n\n# Load labels\ndef load_labels(filename):\n    with open(filename, 'r', encoding='utf-8') as f:\n        return {i: line.strip() for i, line in enumerate(f.readlines())}\n\nlabels = load_labels(label_path)\n\n# Initialize TFLite interpreter with Edge TPU\ninterpreter = make_interpreter(model_path)\ninterpreter.allocate_tensors()\n\n# Get model details\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\n# Read video input\ncap = cv2.VideoCapture(input_path)\nif not cap.isOpened():\n    raise IOError(\"Cannot open video file\")\n\n# Define codec and create VideoWriter object\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\nfps = cap.get(cv2.CAP_PROP_FPS)\nwidth = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\nheight = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\nout = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n\n# Preprocessing and inference\nwhile True:\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    # Convert to PIL Image\n    image = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n    resized_image = image.resize((input_details[0]['shape'][1], input_details[0]['shape'][2]), Image.ANTIALIAS)\n\n    # Preprocess image\n    common.set_input(interpreter, resized_image)\n\n    # Run inference\n    interpreter.invoke()\n\n    # Get detection results\n    detections = detect.get_objects(interpreter, confidence_threshold)\n\n    # Draw bounding boxes and labels\n    for obj in detections:\n        bbox = obj.bbox\n        score = obj.score\n        label = labels[obj.id]\n\n        # Scale bounding box to original image dimensions\n        ymin, xmin, ymax, xmax = bbox\n        (left, right, top, bottom) = (xmin * width, xmax * width,\n                                      ymin * height, ymax * height)\n\n        cv2.rectangle(frame, (int(left), int(top)), (int(right), int(bottom)), (0, 255, 0), 2)\n        label_text = f'{label}: {score:.2f}'\n        cv2.putText(frame, label_text, (int(left), int(top) - 10),\n                    cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n\n    # Write the frame to output video\n    out.write(frame)\n\n# Release resources\ncap.release()\nout.release()\ncv2.destroyAllWindows()\n\nprint(f\"Output saved to {output_path}\")"
      },
      "session_id": "qwen2.5-coder:32b_9e98_tpusg_batch",
      "metadata": {
        "num_run": 2
      },
      "tags": [
        "ablation",
        "benchmark",
        "qwen2.5-coder:32b",
        "tpu_sketch_generator"
      ],
      "latency": 51.335,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-17-33-37-558164_chatcmpl-7e0b83de-df73-444e-aa10-8e390ca52238",
          "traceId": "cfaead81",
          "type": "GENERATION",
          "name": "cfae_tpusg_gen_attempt#1",
          "startTime": "2025-12-10T15:33:37.558000+00:00",
          "endTime": "2025-12-10T15:34:28.893000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 51335.0,
          "promptTokens": 293,
          "costDetails": {},
          "environment": "default",
          "totalTokens": 1023,
          "completionTokens": 730,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "environment": "default",
      "createdAt": "2025-12-10T15:33:38.078Z",
      "updatedAt": "2025-12-10T15:35:30.259Z"
    },
    {
      "id": "d611a970",
      "timestamp": "2025-12-10T15:28:10.551000+00:00",
      "name": "d611_tpu_sketch_generator",
      "input": null,
      "output": {
        "status": "failed",
        "last_code": "",
        "last_error": "[FATAL]: Ollama error: litellm.APIConnectionError: OllamaException - {\"error\":\"timed out waiting for llama runner to start - progress 0.00 - \"}"
      },
      "session_id": "qwen2.5-coder:32b_9e98_tpusg_batch",
      "metadata": {
        "num_run": 1
      },
      "tags": [
        "ablation",
        "benchmark",
        "qwen2.5-coder:32b",
        "tpu_sketch_generator"
      ],
      "latency": 0.0,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "72ada190-33f7-4c93-ba7e-fcb678124ee1",
          "traceId": "d611a970",
          "type": "SPAN",
          "name": "error_d6_tpusg_failure_signal_tpu_sketch_generator",
          "startTime": "2025-12-10T15:33:12.080000+00:00",
          "level": "ERROR",
          "statusMessage": "Failed: [FATAL]: Ollama error: litellm.APIConnectionError: OllamaException - {\"error\":\"timed out waiting for llama runner to start - progress 0.00 - \"}",
          "calculatedTotalCost": 0.0,
          "promptTokens": 0,
          "costDetails": {},
          "environment": "default",
          "totalTokens": 0,
          "completionTokens": 0,
          "endTime": null,
          "model": null,
          "modelParameters": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null,
          "latency": null
        }
      ],
      "environment": "default",
      "createdAt": "2025-12-10T15:28:11.072Z",
      "updatedAt": "2025-12-10T15:33:17.482Z"
    }
  ],
  "meta": {
    "total_items": 30
  }
}
{
  "data": [
    {
      "id": "573eef1b",
      "timestamp": "2025-12-11T02:26:42.592000+00:00",
      "name": "573e_py_sketch_generator",
      "input": null,
      "output": {
        "status": "failed",
        "last_code": "import cv2\nimport numpy as np\nfrom tflite_runtime.interpreter import Interpreter\n\n# Configuration parameters\nmodel_path = 'models/ssd-mobilenet_v1/detect.tflite'\nlabel_path = 'models/ssd-mobilenet_v1/labelmap.txt'\ninput_path = 'data/object_detection/sheeps.mp4'\noutput_path = 'results/object_detection/test_results/sheeps_detections.mp4'\nconfidence_threshold = 0.5\n\n# Load labels\nwith open(label_path, 'r') as f:\n    labels = {int(line.split(' ')[0]): line.strip().split(' ')[1] for line in f.readlines()}\n\n# Setup TFLite interpreter\ninterpreter = Interpreter(model_path=model_path)\ninterpreter.allocate_tensors()\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\n# Preprocess video input\ncap = cv2.VideoCapture(input_path)\nwidth = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\nheight = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\nfps = cap.get(cv2.CAP_PROP_FPS)\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\nout = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n\ndef preprocess_image(frame):\n    frame_resized = cv2.resize(frame, (input_details[0]['shape'][1], input_details[0]['shape'][2]))\n    frame_rgb = cv2.cvtColor(frame_resized, cv2.COLOR_BGR2RGB)\n    frame_expanded = np.expand_dims(frame_rgb, axis=0)\n    return frame_expanded\n\ndef draw_detection_boxes(image, boxes, classes, scores):\n    for i in range(len(scores)):\n        if scores[i] > confidence_threshold:\n            ymin, xmin, ymax, xmax = map(int, (boxes[i][0]*height, boxes[i][1]*width, boxes[i][2]*height, boxes[i][3]*width))\n            label = labels[int(classes[i])]\n            cv2.rectangle(image, (xmin, ymin), (xmax, ymax), (10, 255, 0), 2)\n            cv2.putText(image, f'{label}: {scores[i]:.2f}', (xmin, ymin-10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (36,255,12), 2)\n\n# Inference and output handling\nwhile cap.isOpened():\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    input_data = preprocess_image(frame)\n    interpreter.set_tensor(input_details[0]['index'], input_data)\n    interpreter.invoke()\n\n    boxes = interpreter.get_tensor(output_details[0]['index'])[0]\n    classes = interpreter.get_tensor(output_details[1]['index'])[0]\n    scores = interpreter.get_tensor(output_details[2]['index'])[0]\n\n    draw_detection_boxes(frame, boxes, classes, scores)\n\n    out.write(frame)\n\ncap.release()\nout.release()\ncv2.destroyAllWindows()",
        "last_error": "Max retries reached with failure. Last error from execution: Traceback (most recent call last):\n  File \"/home/wuguangh/Projects/03.12/tinyml-autopilot-pm.03.12.2025/results/object_detection/sketches/tmp_20251211042855_psg_qwen2.5-coder:32b/tmp_20251211042855_psg_qwen2.5-coder:32b.py\", line 14, in <module>\n    labels = {int(line.split(' ')[0]): line.strip().split(' ')[1] for line in f.readlines()}\n  File \"/home/wuguangh/Projects/03.12/tinyml-autopilot-pm.03.12.2025/results/object_detection/sketches/tmp_20251211042855_psg_qwen2.5-coder:32b/tmp_20251211042855_psg_qwen2.5-coder:32b.py\", line 14, in <dictcomp>\n    labels = {int(line.split(' ')[0]): line.strip().split(' ')[1] for line in f.readlines()}\nValueError: invalid literal for int() with base 10: 'person\\n'\n"
      },
      "session_id": "qwen2.5-coder:32b_a8a2_psg_batch",
      "metadata": {
        "num_run": 30
      },
      "tags": [
        "ablation",
        "benchmark",
        "py_sketch_generator",
        "qwen2.5-coder:32b"
      ],
      "latency": 133.424,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-04-26-42-592932_chatcmpl-faa04612-2e78-454e-bdd9-211d40984257",
          "traceId": "573eef1b",
          "type": "GENERATION",
          "name": "573e_psg_gen_attempt#1",
          "startTime": "2025-12-11T02:26:42.592000+00:00",
          "endTime": "2025-12-11T02:27:13.606000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 31014.0,
          "completionTokens": 838,
          "environment": "default",
          "totalTokens": 1102,
          "promptTokens": 264,
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-04-27-13-751667_chatcmpl-1e0c44f0-d435-44e2-bfd1-935a68659dbb",
          "traceId": "573eef1b",
          "type": "GENERATION",
          "name": "573e_psg_gen_attempt#2",
          "startTime": "2025-12-11T02:27:13.751000+00:00",
          "endTime": "2025-12-11T02:27:41.263000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 27512.0,
          "completionTokens": 746,
          "environment": "default",
          "totalTokens": 1010,
          "promptTokens": 264,
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-04-27-41-418513_chatcmpl-af906a08-1fb3-42d8-bb51-f52df933e315",
          "traceId": "573eef1b",
          "type": "GENERATION",
          "name": "573e_psg_gen_attempt#3",
          "startTime": "2025-12-11T02:27:41.418000+00:00",
          "endTime": "2025-12-11T02:28:06.290000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 24872.0,
          "completionTokens": 675,
          "environment": "default",
          "totalTokens": 939,
          "promptTokens": 264,
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-04-28-06-438324_chatcmpl-50137147-bb9e-402e-a2a6-747175acb514",
          "traceId": "573eef1b",
          "type": "GENERATION",
          "name": "573e_psg_gen_attempt#4",
          "startTime": "2025-12-11T02:28:06.438000+00:00",
          "endTime": "2025-12-11T02:28:32.792000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 26354.0,
          "completionTokens": 715,
          "environment": "default",
          "totalTokens": 979,
          "promptTokens": 264,
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-04-28-32-956479_chatcmpl-2be63f95-97f3-4d3e-856c-a67456c4de7e",
          "traceId": "573eef1b",
          "type": "GENERATION",
          "name": "573e_psg_gen_attempt#5",
          "startTime": "2025-12-11T02:28:32.956000+00:00",
          "endTime": "2025-12-11T02:28:55.888000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 22932.0,
          "completionTokens": 623,
          "environment": "default",
          "totalTokens": 887,
          "promptTokens": 264,
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "b086e2a9-50f1-4e79-a0dc-7ca150470dec",
          "traceId": "573eef1b",
          "type": "SPAN",
          "name": "error_57_psg_failure_signal_py_sketch_generator",
          "startTime": "2025-12-11T02:28:56.016000+00:00",
          "level": "ERROR",
          "statusMessage": "Failed. Last error: Max retries reached with failure. Last error from execution: Traceback (most recent call last):\n  File \"/home/wuguangh/Projects/03.12/tinyml-autopilot-pm.03.12.2025/results/object_detection/sketches/tmp_20251211042855_psg_qwen2.5-coder:32b/tmp_20251211042855_psg_qwen2.5-coder:32b.py\", line 14, in <module>\n    labels = {int(line.split(' ')[0]): line.strip().split(' ')[1] for line in f.readlines()}\n  File \"/home/wuguangh/Projects/03.12/tinyml-autopilot-pm.03.12.2025/results/object_detection/sketches/tmp_20251211042855_psg_qwen2.5-coder:32b/tmp_20251211042855_psg_qwen2.5-coder:32b.py\", line 14, in <dictcomp>\n    labels = {int(line.split(' ')[0]): line.strip().split(' ')[1] for line in f.readlines()}\nValueError: invalid literal for int() with base 10: 'person\\n'\n",
          "calculatedTotalCost": 0.0,
          "completionTokens": 0,
          "environment": "default",
          "totalTokens": 0,
          "promptTokens": 0,
          "costDetails": {},
          "endTime": null,
          "model": null,
          "modelParameters": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null,
          "latency": null
        }
      ],
      "createdAt": "2025-12-11T02:26:43.108Z",
      "updatedAt": "2025-12-11T02:29:01.999Z",
      "environment": "default"
    },
    {
      "id": "a77025f5",
      "timestamp": "2025-12-11T02:24:54.584000+00:00",
      "name": "a770_py_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import os\nimport cv2\nimport numpy as np\nfrom tflite_runtime.interpreter import Interpreter\n\n# Configuration parameters\nmodel_path = 'models/ssd-mobilenet_v1/detect.tflite'\nlabel_path = 'models/ssd-mobilenet_v1/labelmap.txt'\ninput_path = 'data/object_detection/sheeps.mp4'\noutput_path = 'results/object_detection/test_results/sheeps_detections.mp4'\nconfidence_threshold = 0.5\n\n# 1. Setup\ndef load_labels(label_path):\n    with open(label_path, 'r') as f:\n        labels = {i: line.strip() for i, line in enumerate(f.readlines())}\n    return labels\n\ninterpreter = Interpreter(model_path=model_path)\ninterpreter.allocate_tensors()\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\nlabels = load_labels(label_path)\n\n# 2. Preprocessing\ncap = cv2.VideoCapture(input_path)\nif not cap.isOpened():\n    raise IOError(\"Cannot open video file\")\n\nframe_width = int(cap.get(3))\nframe_height = int(cap.get(4))\nfps = cap.get(cv2.CAP_PROP_FPS)\nout = cv2.VideoWriter(output_path, cv2.VideoWriter_fourcc(*'mp4v'), fps, (frame_width, frame_height))\n\ndef preprocess_frame(frame):\n    input_data = cv2.resize(frame, (input_details[0]['shape'][1], input_details[0]['shape'][2]))\n    input_data = np.expand_dims(input_data, axis=0)\n    return input_data\n\n# 3. Inference\nwhile cap.isOpened():\n    ret, frame = cap.read()\n    if not ret:\n        break\n    \n    preprocessed_frame = preprocess_frame(frame)\n    interpreter.set_tensor(input_details[0]['index'], preprocessed_frame.astype(np.uint8))\n    interpreter.invoke()\n\n    boxes = interpreter.get_tensor(output_details[0]['index'])[0]\n    classes = interpreter.get_tensor(output_details[1]['index'])[0].astype(int)\n    scores = interpreter.get_tensor(output_details[2]['index'])[0]\n\n    # 4. Output handling\n    for i in range(len(scores)):\n        if scores[i] > confidence_threshold:\n            ymin, xmin, ymax, xmax = boxes[i]\n            im_height, im_width, _ = frame.shape\n            (left, right, top, bottom) = (xmin * im_width, xmax * im_width,\n                                          ymin * im_height, ymax * im_height)\n            \n            cv2.rectangle(frame, (int(left), int(top)), (int(right), int(bottom)), (0, 255, 0), 2)\n            label = f'{labels[classes[i]]}: {scores[i]:.2f}'\n            cv2.putText(frame, label, (int(left), int(top) - 10),\n                        cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n    \n    out.write(frame)\n\ncap.release()\nout.release()\ncv2.destroyAllWindows()\n\n# Note: Computing mAP is not part of the inference pipeline for a single video file.\n# It typically requires a dataset and ground truth annotations to compare against predictions."
      },
      "session_id": "qwen2.5-coder:32b_a8a2_psg_batch",
      "metadata": {
        "num_run": 29
      },
      "tags": [
        "ablation",
        "benchmark",
        "py_sketch_generator",
        "qwen2.5-coder:32b"
      ],
      "latency": 51.163,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-04-24-54-585065_chatcmpl-f2c1e97e-42dc-4f7b-a33a-badc2291b834",
          "traceId": "a77025f5",
          "type": "GENERATION",
          "name": "a770_psg_gen_attempt#1",
          "startTime": "2025-12-11T02:24:54.585000+00:00",
          "endTime": "2025-12-11T02:25:19.466000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 24881.0,
          "completionTokens": 673,
          "environment": "default",
          "totalTokens": 937,
          "promptTokens": 264,
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-04-25-19-615104_chatcmpl-f769051f-5267-43b9-874d-5820924d90cd",
          "traceId": "a77025f5",
          "type": "GENERATION",
          "name": "a770_psg_gen_attempt#2",
          "startTime": "2025-12-11T02:25:19.615000+00:00",
          "endTime": "2025-12-11T02:25:45.748000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 26133.0,
          "completionTokens": 709,
          "environment": "default",
          "totalTokens": 973,
          "promptTokens": 264,
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "createdAt": "2025-12-11T02:24:55.103Z",
      "updatedAt": "2025-12-11T02:26:22.711Z",
      "environment": "default"
    },
    {
      "id": "b5987b6b",
      "timestamp": "2025-12-11T02:21:41.658000+00:00",
      "name": "b598_py_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import cv2\nimport numpy as np\nfrom tflite_runtime.interpreter import Interpreter\nfrom PIL import Image\n\n# Configuration parameters\nmodel_path = 'models/ssd-mobilenet_v1/detect.tflite'\nlabel_path = 'models/ssd-mobilenet_v1/labelmap.txt'\ninput_path = 'data/object_detection/sheeps.mp4'\noutput_path = 'results/object_detection/test_results/sheeps_detections.mp4'\nconfidence_threshold = 0.5\n\n# Load labels\nwith open(label_path, 'r') as f:\n    labels = [line.strip() for line in f.readlines()]\n\n# Load TFLite model and allocate tensors.\ninterpreter = Interpreter(model_path=model_path)\ninterpreter.allocate_tensors()\n\n# Get input and output details\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\n# Read video from the specified path\ncap = cv2.VideoCapture(input_path)\n\nif not cap.isOpened():\n    raise Exception(f\"Error opening video file: {input_path}\")\n\n# Get video properties to save the output video with the same format\nframe_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\nframe_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\nfps = cap.get(cv2.CAP_PROP_FPS)\n\n# Define codec and create VideoWriter object\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\nout = cv2.VideoWriter(output_path, fourcc, fps, (frame_width, frame_height))\n\ndef draw_boxes(frame, boxes, classes, scores):\n    for i in range(len(scores)):\n        if scores[i] >= confidence_threshold:\n            ymin = int(max(1, boxes[i][0] * frame_height))\n            xmin = int(max(1, boxes[i][1] * frame_width))\n            ymax = int(min(frame_height, boxes[i][2] * frame_height))\n            xmax = int(min(frame_width, boxes[i][3] * frame_width))\n\n            label = f'{labels[int(classes[i])]}: {scores[i]:.2f}'\n            color = (0, 255, 0)  # Green color\n            cv2.rectangle(frame, (xmin, ymin), (xmax, ymax), color, 2)\n            cv2.putText(frame, label, (xmin, ymin - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, color, 2)\n\n# Process video frame by frame\nwhile cap.isOpened():\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    # Preprocess the image to fit the model's expected input size and data type\n    input_shape = input_details[0]['shape']\n    height, width = input_shape[1:3]\n    img = cv2.resize(frame, (width, height))\n    img = np.expand_dims(img, axis=0)\n    img = img.astype(np.uint8)\n\n    # Run inference\n    interpreter.set_tensor(input_details[0]['index'], img)\n    interpreter.invoke()\n\n    # Extract results\n    boxes = interpreter.get_tensor(output_details[0]['index'])[0]  # Bounding box coordinates of detected objects\n    classes = interpreter.get_tensor(output_details[1]['index'])[0]  # Class index of detected objects\n    scores = interpreter.get_tensor(output_details[2]['index'])[0]  # Confidence score of detections\n\n    # Draw detection boxes on the frame\n    draw_boxes(frame, boxes, classes, scores)\n\n    # Write the frame with detection to the output video file\n    out.write(frame)\n\n# Release everything when job is finished\ncap.release()\nout.release()\ncv2.destroyAllWindows()"
      },
      "session_id": "qwen2.5-coder:32b_a8a2_psg_batch",
      "metadata": {
        "num_run": 28
      },
      "tags": [
        "ablation",
        "benchmark",
        "py_sketch_generator",
        "qwen2.5-coder:32b"
      ],
      "latency": 136.014,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-04-21-41-659174_chatcmpl-130e4b04-8f8e-4b2d-9a65-64106b23c37b",
          "traceId": "b5987b6b",
          "type": "GENERATION",
          "name": "b598_psg_gen_attempt#1",
          "startTime": "2025-12-11T02:21:41.659000+00:00",
          "endTime": "2025-12-11T02:22:08.692000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 27033.0,
          "completionTokens": 731,
          "environment": "default",
          "totalTokens": 995,
          "promptTokens": 264,
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-04-22-08-859909_chatcmpl-07cb85f4-b919-49ac-bb04-443b4452a494",
          "traceId": "b5987b6b",
          "type": "GENERATION",
          "name": "b598_psg_gen_attempt#2",
          "startTime": "2025-12-11T02:22:08.859000+00:00",
          "endTime": "2025-12-11T02:22:38.912000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 30053.0,
          "completionTokens": 815,
          "environment": "default",
          "totalTokens": 1079,
          "promptTokens": 264,
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-04-22-39-059227_chatcmpl-9474b456-697b-4163-9b94-e148d8ce496b",
          "traceId": "b5987b6b",
          "type": "GENERATION",
          "name": "b598_psg_gen_attempt#3",
          "startTime": "2025-12-11T02:22:39.059000+00:00",
          "endTime": "2025-12-11T02:23:06.025000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 26966.0,
          "completionTokens": 732,
          "environment": "default",
          "totalTokens": 996,
          "promptTokens": 264,
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-04-23-06-386952_chatcmpl-afb537ca-ed17-4b65-9f54-c3f4c80cb1f7",
          "traceId": "b5987b6b",
          "type": "GENERATION",
          "name": "b598_psg_gen_attempt#4",
          "startTime": "2025-12-11T02:23:06.386000+00:00",
          "endTime": "2025-12-11T02:23:29.469000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 23083.0,
          "completionTokens": 627,
          "environment": "default",
          "totalTokens": 891,
          "promptTokens": 264,
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-04-23-29-626840_chatcmpl-af378029-f4b3-42db-888e-867aedbbcf4b",
          "traceId": "b5987b6b",
          "type": "GENERATION",
          "name": "b598_psg_gen_attempt#5",
          "startTime": "2025-12-11T02:23:29.626000+00:00",
          "endTime": "2025-12-11T02:23:57.673000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 28047.0,
          "completionTokens": 761,
          "environment": "default",
          "totalTokens": 1025,
          "promptTokens": 264,
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "createdAt": "2025-12-11T02:21:42.174Z",
      "updatedAt": "2025-12-11T02:24:34.926Z",
      "environment": "default"
    },
    {
      "id": "14ed492a",
      "timestamp": "2025-12-11T02:18:49.724000+00:00",
      "name": "14ed_py_sketch_generator",
      "input": null,
      "output": {
        "status": "failed",
        "last_code": "import cv2\nimport numpy as np\nimport tflite_runtime.interpreter as tflite\nfrom PIL import ImageDraw, ImageFont\n\n# Configuration parameters\nmodel_path = 'models/ssd-mobilenet_v1/detect.tflite'\nlabel_path = 'models/ssd-mobilenet_v1/labelmap.txt'\ninput_path = 'data/object_detection/sheeps.mp4'\noutput_path = 'results/object_detection/test_results/sheeps_detections.mp4'\nconfidence_threshold = 0.5\n\n# Phase 1: Setup\ndef load_labels(label_path):\n    with open(label_path, 'r') as f:\n        return {i: line.strip() for i, line in enumerate(f.readlines())}\n\nlabels = load_labels(label_path)\n\ninterpreter = tflite.Interpreter(model_path=model_path)\ninterpreter.allocate_tensors()\n\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\n# Phase 2: Preprocessing\ndef preprocess_image(image):\n    input_shape = input_details[0]['shape']\n    resized_image = cv2.resize(image, (input_shape[1], input_shape[2]))\n    normalized_image = np.array(resized_image, dtype=np.float32) / 255.0\n    input_data = np.expand_dims(normalized_image, axis=0)\n    return input_data\n\n# Phase 3: Inference\ndef detect_objects(interpreter, image):\n    interpreter.set_tensor(input_details[0]['index'], image)\n    interpreter.invoke()\n    \n    boxes = interpreter.get_tensor(output_details[0]['index'])[0]\n    classes = interpreter.get_tensor(output_details[1]['index'])[0]\n    scores = interpreter.get_tensor(output_details[2]['index'])[0]\n    num_detections = int(interpreter.get_tensor(output_details[3]['index'])[0])\n    \n    results = []\n    for i in range(num_detections):\n        if scores[i] > confidence_threshold:\n            class_id = int(classes[i])\n            label = labels.get(class_id, 'Unknown')\n            box = boxes[i]\n            results.append((label, float(scores[i]), (box[0], box[1], box[2], box[3])))\n    return results\n\n# Phase 4: Output Handling\ndef draw_boxes(image, detections):\n    draw = ImageDraw.Draw(image)\n    font = ImageFont.load_default()\n    \n    for label, score, box in detections:\n        y_min, x_min, y_max, x_max = box\n        image_height, image_width, _ = image.shape\n        (left, right, top, bottom) = (x_min * image_width, x_max * image_width,\n                                      y_min * image_height, y_max * image_height)\n        \n        draw.rectangle([(left, top), (right, bottom)], outline=(0, 255, 0), width=2)\n        label_text = f'{label}: {score:.2f}'\n        text_width, text_height = font.getsize(label_text)\n        label_ymin = max(top - text_height - 10, 10)\n        draw.rectangle([(left, label_ymin - text_height), (left + text_width, label_ymin)], fill=(0, 255, 0))\n        draw.text((left, label_ymin - text_height), label_text, font=font, fill=(0, 0, 0))\n\n# Main Pipeline\ncap = cv2.VideoCapture(input_path)\nif not cap.isOpened():\n    raise IOError(\"Cannot open video file\")\n\nframe_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\nframe_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\nfps = cap.get(cv2.CAP_PROP_FPS)\n\nout = cv2.VideoWriter(output_path, cv2.VideoWriter_fourcc(*'mp4v'), fps, (frame_width, frame_height))\n\nwhile True:\n    ret, frame = cap.read()\n    if not ret:\n        break\n    \n    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n    preprocessed_image = preprocess_image(rgb_frame)\n    \n    detections = detect_objects(interpreter, preprocessed_image)\n    draw_boxes(rgb_frame, detections)\n    \n    output_frame = cv2.cvtColor(np.array(rgb_frame), cv2.COLOR_RGB2BGR)\n    out.write(output_frame)\n\ncap.release()\nout.release()\ncv2.destroyAllWindows()",
        "last_error": "Max retries reached with failure. Last error from execution: INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\nTraceback (most recent call last):\n  File \"/home/wuguangh/Projects/03.12/tinyml-autopilot-pm.03.12.2025/results/object_detection/sketches/tmp_20251211042116_psg_qwen2.5-coder:32b/tmp_20251211042116_psg_qwen2.5-coder:32b.py\", line 90, in <module>\n    detections = detect_objects(interpreter, preprocessed_image)\n  File \"/home/wuguangh/Projects/03.12/tinyml-autopilot-pm.03.12.2025/results/object_detection/sketches/tmp_20251211042116_psg_qwen2.5-coder:32b/tmp_20251211042116_psg_qwen2.5-coder:32b.py\", line 36, in detect_objects\n    interpreter.set_tensor(input_details[0]['index'], image)\n  File \"/home/wuguangh/.conda/envs/tinyml/lib/python3.10/site-packages/tflite_runtime/interpreter.py\", line 720, in set_tensor\n    self._interpreter.SetTensor(tensor_index, value)\nValueError: Cannot set tensor: Got value of type FLOAT32 but expected type UINT8 for input 175, name: normalized_input_image_tensor \n"
      },
      "session_id": "qwen2.5-coder:32b_a8a2_psg_batch",
      "metadata": {
        "num_run": 27
      },
      "tags": [
        "ablation",
        "benchmark",
        "py_sketch_generator",
        "qwen2.5-coder:32b"
      ],
      "latency": 146.468,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-04-18-49-725962_chatcmpl-126cc3bb-6a01-4258-bac4-e9b954d8ae40",
          "traceId": "14ed492a",
          "type": "GENERATION",
          "name": "14ed_psg_gen_attempt#1",
          "startTime": "2025-12-11T02:18:49.725000+00:00",
          "endTime": "2025-12-11T02:19:19.714000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 29989.0,
          "completionTokens": 811,
          "environment": "default",
          "totalTokens": 1075,
          "promptTokens": 264,
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-04-19-20-073146_chatcmpl-de1978fb-0d82-4877-8c82-b36d80a85333",
          "traceId": "14ed492a",
          "type": "GENERATION",
          "name": "14ed_psg_gen_attempt#2",
          "startTime": "2025-12-11T02:19:20.073000+00:00",
          "endTime": "2025-12-11T02:19:51.703000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 31630.0,
          "completionTokens": 857,
          "environment": "default",
          "totalTokens": 1121,
          "promptTokens": 264,
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-04-19-51-873911_chatcmpl-a445f7a0-b766-4d5e-8a8e-1009ef0d8855",
          "traceId": "14ed492a",
          "type": "GENERATION",
          "name": "14ed_psg_gen_attempt#3",
          "startTime": "2025-12-11T02:19:51.873000+00:00",
          "endTime": "2025-12-11T02:20:17.478000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 25605.0,
          "completionTokens": 695,
          "environment": "default",
          "totalTokens": 959,
          "promptTokens": 264,
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-04-20-17-822257_chatcmpl-470df149-c761-48e3-949e-53064d273ad4",
          "traceId": "14ed492a",
          "type": "GENERATION",
          "name": "14ed_psg_gen_attempt#4",
          "startTime": "2025-12-11T02:20:17.822000+00:00",
          "endTime": "2025-12-11T02:20:42.451000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 24629.0,
          "completionTokens": 669,
          "environment": "default",
          "totalTokens": 933,
          "promptTokens": 264,
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-04-20-42-598392_chatcmpl-3bd9bada-0463-4296-82e8-bb9612b89d79",
          "traceId": "14ed492a",
          "type": "GENERATION",
          "name": "14ed_psg_gen_attempt#5",
          "startTime": "2025-12-11T02:20:42.598000+00:00",
          "endTime": "2025-12-11T02:21:16.023000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 33425.0,
          "completionTokens": 906,
          "environment": "default",
          "totalTokens": 1170,
          "promptTokens": 264,
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "ebb93f89-e9e0-4a08-b3de-600a0d33a0a2",
          "traceId": "14ed492a",
          "type": "SPAN",
          "name": "error_14_psg_failure_signal_py_sketch_generator",
          "startTime": "2025-12-11T02:21:16.193000+00:00",
          "level": "ERROR",
          "statusMessage": "Failed. Last error: Max retries reached with failure. Last error from execution: INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\nTraceback (most recent call last):\n  File \"/home/wuguangh/Projects/03.12/tinyml-autopilot-pm.03.12.2025/results/object_detection/sketches/tmp_20251211042116_psg_qwen2.5-coder:32b/tmp_20251211042116_psg_qwen2.5-coder:32b.py\", line 90, in <module>\n    detections = detect_objects(interpreter, preprocessed_image)\n  File \"/home/wuguangh/Projects/03.12/tinyml-autopilot-pm.03.12.2025/results/object_detection/sketches/tmp_20251211042116_psg_qwen2.5-coder:32b/tmp_20251211042116_psg_qwen2.5-coder:32b.py\", line 36, in detect_objects\n    interpreter.set_tensor(input_details[0]['index'], image)\n  File \"/home/wuguangh/.conda/envs/tinyml/lib/python3.10/site-packages/tflite_runtime/interpreter.py\", line 720, in set_tensor\n    self._interpreter.SetTensor(tensor_index, value)\nValueError: Cannot set tensor: Got value of type FLOAT32 but expected type UINT8 for input 175, name: normalized_input_image_tensor \n",
          "calculatedTotalCost": 0.0,
          "completionTokens": 0,
          "environment": "default",
          "totalTokens": 0,
          "promptTokens": 0,
          "costDetails": {},
          "endTime": null,
          "model": null,
          "modelParameters": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null,
          "latency": null
        }
      ],
      "createdAt": "2025-12-11T02:18:50.242Z",
      "updatedAt": "2025-12-11T02:21:21.433Z",
      "environment": "default"
    },
    {
      "id": "54470263",
      "timestamp": "2025-12-11T02:15:57.817000+00:00",
      "name": "5447_py_sketch_generator",
      "input": null,
      "output": {
        "status": "failed",
        "last_code": "import cv2\nimport numpy as np\nimport tflite_runtime.interpreter as tflite\nfrom PIL import ImageDraw, ImageFont\n\n# Configuration parameters\nmodel_path = 'models/ssd-mobilenet_v1/detect.tflite'\nlabel_path = 'models/ssd-mobilenet_v1/labelmap.txt'\ninput_path = 'data/object_detection/sheeps.mp4'\noutput_path = 'results/object_detection/test_results/sheeps_detections.mp4'\nconfidence_threshold = 0.5\n\n# Load labels\nwith open(label_path, 'r') as f:\n    labels = [line.strip() for line in f.readlines()]\n\n# Load TFLite model and allocate tensors\ninterpreter = tflite.Interpreter(model_path=model_path)\ninterpreter.allocate_tensors()\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\n# Read input video\ncap = cv2.VideoCapture(input_path)\nif not cap.isOpened():\n    raise Exception(\"Error opening video file\")\n\n# Get video properties\nframe_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\nframe_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\nfps = cap.get(cv2.CAP_PROP_FPS)\n\n# Define codec and create VideoWriter object\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\nout = cv2.VideoWriter(output_path, fourcc, fps, (frame_width, frame_height))\n\n# Load font for drawing labels\nfont = ImageFont.truetype(\"arial.ttf\", 16)\n\ndef draw_detection_boxes(image, boxes, classes, scores):\n    draw = ImageDraw.Draw(image)\n    for i in range(len(boxes)):\n        if scores[i] > confidence_threshold:\n            ymin, xmin, ymax, xmax = boxes[i]\n            (left, right, top, bottom) = (xmin * frame_width, xmax * frame_width,\n                                          ymin * frame_height, ymax * frame_height)\n            draw.rectangle([(left, top), (right, bottom)], outline=\"red\", width=2)\n            label = f\"{labels[int(classes[i])]}: {scores[i]:.2f}\"\n            text_size = draw.textsize(label, font=font)\n            draw.rectangle([(left, top - text_size[1] - 5), (left + text_size[0] + 5, top)], fill=\"red\")\n            draw.text((left + 3, top - text_size[1] - 2), label, font=font, fill=\"white\")\n\n# Process video frame by frame\nwhile cap.isOpened():\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    # Preprocess image\n    input_image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n    input_image = cv2.resize(input_image, (300, 300))\n    input_tensor = np.expand_dims(input_image, axis=0).astype(np.float32)\n\n    # Run inference\n    interpreter.set_tensor(input_details[0]['index'], input_tensor)\n    interpreter.invoke()\n\n    # Extract output data\n    boxes = interpreter.get_tensor(output_details[0]['index'])[0]\n    classes = interpreter.get_tensor(output_details[1]['index'])[0]\n    scores = interpreter.get_tensor(output_details[2]['index'])[0]\n\n    # Draw detection boxes on the frame\n    draw_detection_boxes(Image.fromarray(frame), boxes, classes, scores)\n\n    # Write the result to output video file\n    out.write(cv2.cvtColor(np.array(frame), cv2.COLOR_RGB2BGR))\n\n# Release resources\ncap.release()\nout.release()\ncv2.destroyAllWindows()",
        "last_error": "Max retries reached with failure. Last error from execution: INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\nTraceback (most recent call last):\n  File \"/home/wuguangh/Projects/03.12/tinyml-autopilot-pm.03.12.2025/results/object_detection/sketches/tmp_20251211041824_psg_qwen2.5-coder:32b/tmp_20251211041824_psg_qwen2.5-coder:32b.py\", line 65, in <module>\n    interpreter.set_tensor(input_details[0]['index'], input_tensor)\n  File \"/home/wuguangh/.conda/envs/tinyml/lib/python3.10/site-packages/tflite_runtime/interpreter.py\", line 720, in set_tensor\n    self._interpreter.SetTensor(tensor_index, value)\nValueError: Cannot set tensor: Got value of type FLOAT32 but expected type UINT8 for input 175, name: normalized_input_image_tensor \n"
      },
      "session_id": "qwen2.5-coder:32b_a8a2_psg_batch",
      "metadata": {
        "num_run": 26
      },
      "tags": [
        "ablation",
        "benchmark",
        "py_sketch_generator",
        "qwen2.5-coder:32b"
      ],
      "latency": 146.418,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-04-15-57-817765_chatcmpl-bb73284b-39ba-4024-a5df-4d806726dac6",
          "traceId": "54470263",
          "type": "GENERATION",
          "name": "5447_psg_gen_attempt#1",
          "startTime": "2025-12-11T02:15:57.817000+00:00",
          "endTime": "2025-12-11T02:16:26.543000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 28726.0,
          "completionTokens": 777,
          "environment": "default",
          "totalTokens": 1041,
          "promptTokens": 264,
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-04-16-26-912072_chatcmpl-c3e23136-b836-4bd4-b621-f2dedb99ee8f",
          "traceId": "54470263",
          "type": "GENERATION",
          "name": "5447_psg_gen_attempt#2",
          "startTime": "2025-12-11T02:16:26.912000+00:00",
          "endTime": "2025-12-11T02:16:55.750000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 28838.0,
          "completionTokens": 782,
          "environment": "default",
          "totalTokens": 1046,
          "promptTokens": 264,
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-04-16-56-115176_chatcmpl-61763555-024e-41bd-b25b-a3662d4ac1ab",
          "traceId": "54470263",
          "type": "GENERATION",
          "name": "5447_psg_gen_attempt#3",
          "startTime": "2025-12-11T02:16:56.115000+00:00",
          "endTime": "2025-12-11T02:17:22.973000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 26858.0,
          "completionTokens": 729,
          "environment": "default",
          "totalTokens": 993,
          "promptTokens": 264,
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-04-17-23-142372_chatcmpl-e3b8f648-f194-4824-8e13-32bb3ec88af4",
          "traceId": "54470263",
          "type": "GENERATION",
          "name": "5447_psg_gen_attempt#4",
          "startTime": "2025-12-11T02:17:23.142000+00:00",
          "endTime": "2025-12-11T02:17:54.246000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 31104.0,
          "completionTokens": 843,
          "environment": "default",
          "totalTokens": 1107,
          "promptTokens": 264,
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-04-17-54-430352_chatcmpl-1b21b2dc-280c-4d51-8a12-89b6fa5e3e8e",
          "traceId": "54470263",
          "type": "GENERATION",
          "name": "5447_psg_gen_attempt#5",
          "startTime": "2025-12-11T02:17:54.430000+00:00",
          "endTime": "2025-12-11T02:18:24.052000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 29622.0,
          "completionTokens": 803,
          "environment": "default",
          "totalTokens": 1067,
          "promptTokens": 264,
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "d61077f4-d4c5-4e51-9256-892cbf498cdd",
          "traceId": "54470263",
          "type": "SPAN",
          "name": "error_54_psg_failure_signal_py_sketch_generator",
          "startTime": "2025-12-11T02:18:24.235000+00:00",
          "level": "ERROR",
          "statusMessage": "Failed. Last error: Max retries reached with failure. Last error from execution: INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\nTraceback (most recent call last):\n  File \"/home/wuguangh/Projects/03.12/tinyml-autopilot-pm.03.12.2025/results/object_detection/sketches/tmp_20251211041824_psg_qwen2.5-coder:32b/tmp_20251211041824_psg_qwen2.5-coder:32b.py\", line 65, in <module>\n    interpreter.set_tensor(input_details[0]['index'], input_tensor)\n  File \"/home/wuguangh/.conda/envs/tinyml/lib/python3.10/site-packages/tflite_runtime/interpreter.py\", line 720, in set_tensor\n    self._interpreter.SetTensor(tensor_index, value)\nValueError: Cannot set tensor: Got value of type FLOAT32 but expected type UINT8 for input 175, name: normalized_input_image_tensor \n",
          "calculatedTotalCost": 0.0,
          "completionTokens": 0,
          "environment": "default",
          "totalTokens": 0,
          "promptTokens": 0,
          "costDetails": {},
          "endTime": null,
          "model": null,
          "modelParameters": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null,
          "latency": null
        }
      ],
      "createdAt": "2025-12-11T02:15:58.335Z",
      "updatedAt": "2025-12-11T02:18:29.532Z",
      "environment": "default"
    },
    {
      "id": "bc237ec3",
      "timestamp": "2025-12-11T02:14:08.396000+00:00",
      "name": "bc23_py_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import cv2\nimport numpy as np\nfrom tflite_runtime.interpreter import Interpreter\n\n# Configuration parameters\nmodel_path = 'models/ssd-mobilenet_v1/detect.tflite'\nlabel_path = 'models/ssd-mobilenet_v1/labelmap.txt'\ninput_path = 'data/object_detection/sheeps.mp4'\noutput_path = 'results/object_detection/test_results/sheeps_detections.mp4'\nconfidence_threshold = 0.5\n\n# Setup\ninterpreter = Interpreter(model_path=model_path)\ninterpreter.allocate_tensors()\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\n# Load labels\nwith open(label_path, 'r') as f:\n    labels = [line.strip() for line in f.readlines()]\n\n# Preprocessing\ncap = cv2.VideoCapture(input_path)\nif not cap.isOpened():\n    raise IOError(\"Cannot open video\")\n\nframe_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\nframe_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\nfps = cap.get(cv2.CAP_PROP_FPS)\n\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\nout = cv2.VideoWriter(output_path, fourcc, fps, (frame_width, frame_height))\n\n# Inference and Output Handling\nwhile True:\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    input_shape = input_details[0]['shape']\n    height, width, _ = frame.shape\n    resized_frame = cv2.resize(frame, (input_shape[1], input_shape[2]))\n    input_data = np.expand_dims(resized_frame, axis=0)\n\n    interpreter.set_tensor(input_details[0]['index'], input_data)\n    interpreter.invoke()\n\n    boxes = interpreter.get_tensor(output_details[0]['index'])[0]\n    classes = interpreter.get_tensor(output_details[1]['index'])[0]\n    scores = interpreter.get_tensor(output_details[2]['index'])[0]\n\n    for i in range(len(scores)):\n        if scores[i] > confidence_threshold:\n            box = boxes[i]\n            class_id = int(classes[i])\n            label = labels[class_id]\n            score = scores[i]\n\n            ymin, xmin, ymax, xmax = box\n            (left, right, top, bottom) = (xmin * width, xmax * width,\n                                          ymin * height, ymax * height)\n\n            cv2.rectangle(frame, (int(left), int(top)), (int(right), int(bottom)),\n                          (0, 255, 0), 2)\n            label_text = f'{label}: {score:.2f}'\n            cv2.putText(frame, label_text, (int(left), int(top) - 10),\n                        cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n\n    out.write(frame)\n\ncap.release()\nout.release()\ncv2.destroyAllWindows()"
      },
      "session_id": "qwen2.5-coder:32b_a8a2_psg_batch",
      "metadata": {
        "num_run": 25
      },
      "tags": [
        "ablation",
        "benchmark",
        "py_sketch_generator",
        "qwen2.5-coder:32b"
      ],
      "latency": 50.444,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-04-14-08-397645_chatcmpl-763df7f8-834d-4ac9-8ad2-bb12d6cb3e3d",
          "traceId": "bc237ec3",
          "type": "GENERATION",
          "name": "bc23_psg_gen_attempt#1",
          "startTime": "2025-12-11T02:14:08.397000+00:00",
          "endTime": "2025-12-11T02:14:36.164000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 27767.0,
          "completionTokens": 751,
          "environment": "default",
          "totalTokens": 1015,
          "promptTokens": 264,
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-04-14-36-642961_chatcmpl-ba4888dc-654c-452d-a14f-9b475df4d79e",
          "traceId": "bc237ec3",
          "type": "GENERATION",
          "name": "bc23_psg_gen_attempt#2",
          "startTime": "2025-12-11T02:14:36.642000+00:00",
          "endTime": "2025-12-11T02:14:58.841000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 22199.0,
          "completionTokens": 603,
          "environment": "default",
          "totalTokens": 867,
          "promptTokens": 264,
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "createdAt": "2025-12-11T02:14:08.920Z",
      "updatedAt": "2025-12-11T02:15:37.115Z",
      "environment": "default"
    },
    {
      "id": "ef3d5e2a",
      "timestamp": "2025-12-11T02:10:58.180000+00:00",
      "name": "ef3d_py_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import cv2\nimport numpy as np\nimport tflite_runtime.interpreter as tflite\nfrom pathlib import Path\n\n# Configuration parameters\nmodel_path = 'models/ssd-mobilenet_v1/detect.tflite'\nlabel_path = 'models/ssd-mobilenet_v1/labelmap.txt'\ninput_path = 'data/object_detection/sheeps.mp4'\noutput_path = 'results/object_detection/test_results/sheeps_detections.mp4'\nconfidence_threshold = 0.5\n\n# Phase 1: Setup\n# Load TFLite model and allocate tensors.\ninterpreter = tflite.Interpreter(model_path=model_path)\ninterpreter.allocate_tensors()\n\n# Get input and output details\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\n# Load labels\nwith open(label_path, 'r') as f:\n    labels = [line.strip() for line in f.readlines()]\n\n# Phase 2: Preprocessing\ncap = cv2.VideoCapture(input_path)\nif not cap.isOpened():\n    raise IOError(\"Cannot open video\")\n\nframe_width = int(cap.get(3))\nframe_height = int(cap.get(4))\nfps = cap.get(cv2.CAP_PROP_FPS)\n\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\nout = cv2.VideoWriter(output_path, fourcc, fps, (frame_width, frame_height))\n\n# Phase 3: Inference\nwhile True:\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    input_shape = input_details[0]['shape']\n    frame_resized = cv2.resize(frame, (input_shape[1], input_shape[2]))\n    input_data = np.array(frame_resized, dtype=np.uint8)\n    input_data = np.expand_dims(input_data, axis=0)\n\n    # Set the tensor to point to the input data to be inferred\n    interpreter.set_tensor(input_details[0]['index'], input_data)\n\n    # Run inference\n    interpreter.invoke()\n\n    # Extract results\n    boxes = interpreter.get_tensor(output_details[0]['index'])\n    classes = interpreter.get_tensor(output_details[1]['index'])\n    scores = interpreter.get_tensor(output_details[2]['index'])\n\n    # Phase 4: Output handling\n    for i in range(len(scores[0])):\n        if scores[0][i] > confidence_threshold:\n            ymin, xmin, ymax, xmax = boxes[0][i]\n            (left, right, top, bottom) = (xmin * frame_width, xmax * frame_width,\n                                          ymin * frame_height, ymax * frame_height)\n\n            cv2.rectangle(frame, (int(left), int(top)), (int(right), int(bottom)), (0, 255, 0), 2)\n            label = f\"{labels[int(classes[0][i])]}: {scores[0][i]:.2f}\"\n            cv2.putText(frame, label, (int(left), int(top) - 10),\n                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n\n    out.write(frame)\n\n# Release resources\ncap.release()\nout.release()\ncv2.destroyAllWindows()\n\n# Note: mAP computation is not included in this script as it typically requires a set of ground truth annotations."
      },
      "session_id": "qwen2.5-coder:32b_a8a2_psg_batch",
      "metadata": {
        "num_run": 24
      },
      "tags": [
        "ablation",
        "benchmark",
        "py_sketch_generator",
        "qwen2.5-coder:32b"
      ],
      "latency": 130.49,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-04-10-58-181075_chatcmpl-672691c8-4951-490d-b208-48a78285cf6c",
          "traceId": "ef3d5e2a",
          "type": "GENERATION",
          "name": "ef3d_psg_gen_attempt#1",
          "startTime": "2025-12-11T02:10:58.181000+00:00",
          "endTime": "2025-12-11T02:11:23.378000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 25197.0,
          "completionTokens": 682,
          "environment": "default",
          "totalTokens": 946,
          "promptTokens": 264,
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-04-11-23-528064_chatcmpl-47203ba9-e39f-470f-8db5-5c0d384639a9",
          "traceId": "ef3d5e2a",
          "type": "GENERATION",
          "name": "ef3d_psg_gen_attempt#2",
          "startTime": "2025-12-11T02:11:23.528000+00:00",
          "endTime": "2025-12-11T02:11:50.075000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 26547.0,
          "completionTokens": 720,
          "environment": "default",
          "totalTokens": 984,
          "promptTokens": 264,
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-04-11-50-204507_chatcmpl-b343d546-06c2-4a32-916f-eae3246c00b6",
          "traceId": "ef3d5e2a",
          "type": "GENERATION",
          "name": "ef3d_psg_gen_attempt#3",
          "startTime": "2025-12-11T02:11:50.204000+00:00",
          "endTime": "2025-12-11T02:12:13.976000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 23772.0,
          "completionTokens": 646,
          "environment": "default",
          "totalTokens": 910,
          "promptTokens": 264,
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-04-12-14-104537_chatcmpl-a41ef66f-e6af-42e9-a6ab-448d62eb2e7a",
          "traceId": "ef3d5e2a",
          "type": "GENERATION",
          "name": "ef3d_psg_gen_attempt#4",
          "startTime": "2025-12-11T02:12:14.104000+00:00",
          "endTime": "2025-12-11T02:12:38.623000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 24519.0,
          "completionTokens": 666,
          "environment": "default",
          "totalTokens": 930,
          "promptTokens": 264,
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-04-12-41-615021_chatcmpl-5fb057f8-3a26-4c2c-a20a-980cc0bc5ca6",
          "traceId": "ef3d5e2a",
          "type": "GENERATION",
          "name": "ef3d_psg_gen_attempt#5",
          "startTime": "2025-12-11T02:12:41.615000+00:00",
          "endTime": "2025-12-11T02:13:08.671000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 27056.0,
          "completionTokens": 734,
          "environment": "default",
          "totalTokens": 998,
          "promptTokens": 264,
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "createdAt": "2025-12-11T02:10:58.697Z",
      "updatedAt": "2025-12-11T02:13:48.459Z",
      "environment": "default"
    },
    {
      "id": "ff07955d",
      "timestamp": "2025-12-11T02:07:42.004000+00:00",
      "name": "ff07_py_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import cv2\nimport numpy as np\nfrom tflite_runtime.interpreter import Interpreter\n\n# Configuration parameters\nmodel_path = 'models/ssd-mobilenet_v1/detect.tflite'\nlabel_path = 'models/ssd-mobilenet_v1/labelmap.txt'\ninput_path = 'data/object_detection/sheeps.mp4'\noutput_path = 'results/object_detection/test_results/sheeps_detections.mp4'\nconfidence_threshold = 0.5\n\n# 1. Setup\n# Load the TFLite model and allocate tensors.\ninterpreter = Interpreter(model_path=model_path)\ninterpreter.allocate_tensors()\n\n# Get input and output details\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\n# Load labels\nwith open(label_path, 'r') as f:\n    labels = [line.strip() for line in f.readlines()]\n\n# 2. Preprocessing\n# Open the video file\ncap = cv2.VideoCapture(input_path)\nif not cap.isOpened():\n    raise SystemExit(\"Could not open video source\")\n\n# Get video properties to save output video with the same settings\nframe_width = int(cap.get(3))\nframe_height = int(cap.get(4))\nfps = cap.get(cv2.CAP_PROP_FPS)\n\n# Define codec and create VideoWriter object\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\nout = cv2.VideoWriter(output_path, fourcc, fps, (frame_width, frame_height))\n\n# 3. Inference\nwhile cap.isOpened():\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    # Preprocess the image to fit the model input requirements\n    input_shape = input_details[0]['shape']\n    img_resized = cv2.resize(frame, (input_shape[1], input_shape[2]))\n    img_resized = np.expand_dims(img_resized, axis=0)\n    img_resized = img_resized.astype(np.uint8)\n\n    # Set the tensor to point to the input data to be inferred\n    interpreter.set_tensor(input_details[0]['index'], img_resized)\n\n    # Run inference\n    interpreter.invoke()\n\n    # Extract output tensors\n    boxes = interpreter.get_tensor(output_details[0]['index'])\n    classes = interpreter.get_tensor(output_details[1]['index'])\n    scores = interpreter.get_tensor(output_details[2]['index'])\n\n    # 4. Output Handling\n    for i in range(len(scores[0])):\n        if scores[0][i] > confidence_threshold:\n            class_id = int(classes[0][i])\n            box = boxes[0][i]\n            label = labels[class_id]\n\n            # Convert the coordinates to the image dimensions\n            ymin, xmin, ymax, xmax = box\n            (left, right, top, bottom) = (xmin * frame_width, xmax * frame_width,\n                                          ymin * frame_height, ymax * frame_height)\n\n            # Draw bounding box and label\n            cv2.rectangle(frame, (int(left), int(top)), (int(right), int(bottom)), (0, 255, 0), 2)\n            label_text = f'{label}: {scores[0][i]:.2f}'\n            cv2.putText(frame, label_text, (int(left), int(top) - 8),\n                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n\n    # Write the frame to the output file\n    out.write(frame)\n\n# Release everything when job is finished\ncap.release()\nout.release()\ncv2.destroyAllWindows()"
      },
      "session_id": "qwen2.5-coder:32b_a8a2_psg_batch",
      "metadata": {
        "num_run": 23
      },
      "tags": [
        "ablation",
        "benchmark",
        "py_sketch_generator",
        "qwen2.5-coder:32b"
      ],
      "latency": 138.091,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-04-07-42-005766_chatcmpl-7d0a4f34-f671-4694-b041-b4ff9b50dcee",
          "traceId": "ff07955d",
          "type": "GENERATION",
          "name": "ff07_psg_gen_attempt#1",
          "startTime": "2025-12-11T02:07:42.005000+00:00",
          "endTime": "2025-12-11T02:08:08.314000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 26309.0,
          "completionTokens": 712,
          "environment": "default",
          "totalTokens": 976,
          "promptTokens": 264,
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-04-08-08-473311_chatcmpl-d8ea7c9e-bfe6-45b1-b0fb-0adc992038ed",
          "traceId": "ff07955d",
          "type": "GENERATION",
          "name": "ff07_psg_gen_attempt#2",
          "startTime": "2025-12-11T02:08:08.473000+00:00",
          "endTime": "2025-12-11T02:08:35.746000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 27273.0,
          "completionTokens": 740,
          "environment": "default",
          "totalTokens": 1004,
          "promptTokens": 264,
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-04-08-35-890665_chatcmpl-25cc5bee-ceab-438a-83b8-0dabc8c0a623",
          "traceId": "ff07955d",
          "type": "GENERATION",
          "name": "ff07_psg_gen_attempt#3",
          "startTime": "2025-12-11T02:08:35.890000+00:00",
          "endTime": "2025-12-11T02:09:01.073000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 25183.0,
          "completionTokens": 684,
          "environment": "default",
          "totalTokens": 948,
          "promptTokens": 264,
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-04-09-01-221701_chatcmpl-6d4247c8-12f9-49e7-935c-577ccc1589d8",
          "traceId": "ff07955d",
          "type": "GENERATION",
          "name": "ff07_psg_gen_attempt#4",
          "startTime": "2025-12-11T02:09:01.221000+00:00",
          "endTime": "2025-12-11T02:09:32.258000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 31037.0,
          "completionTokens": 841,
          "environment": "default",
          "totalTokens": 1105,
          "promptTokens": 264,
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-04-09-32-635746_chatcmpl-8599dc37-1db3-478f-bdec-120a1c5d436c",
          "traceId": "ff07955d",
          "type": "GENERATION",
          "name": "ff07_psg_gen_attempt#5",
          "startTime": "2025-12-11T02:09:32.635000+00:00",
          "endTime": "2025-12-11T02:10:00.096000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 27461.0,
          "completionTokens": 745,
          "environment": "default",
          "totalTokens": 1009,
          "promptTokens": 264,
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "createdAt": "2025-12-11T02:07:42.520Z",
      "updatedAt": "2025-12-11T02:10:37.278Z",
      "environment": "default"
    },
    {
      "id": "bd16e9d8",
      "timestamp": "2025-12-11T02:06:40.042000+00:00",
      "name": "bd16_py_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import cv2\nimport numpy as np\nimport tensorflow as tf\n\n# Configuration parameters\nmodel_path = 'models/ssd-mobilenet_v1/detect.tflite'\nlabel_path = 'models/ssd-mobilenet_v1/labelmap.txt'\ninput_path = 'data/object_detection/sheeps.mp4'\noutput_path = 'results/object_detection/test_results/sheeps_detections.mp4'\nconfidence_threshold = 0.5\n\n# Phase 1: Setup\n# Load TFLite interpreter and allocate tensors\ninterpreter = tf.lite.Interpreter(model_path=model_path)\ninterpreter.allocate_tensors()\n\n# Get input and output details\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\n# Read labels from the label map file\nwith open(label_path, 'r') as f:\n    labels = [line.strip() for line in f.readlines()]\n\n# Phase 2: Preprocessing\n# Load video\ncap = cv2.VideoCapture(input_path)\nframe_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\nframe_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\nfps = cap.get(cv2.CAP_PROP_FPS)\n\n# Define codec and create VideoWriter object\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\nout = cv2.VideoWriter(output_path, fourcc, fps, (frame_width, frame_height))\n\n# Phase 3: Inference\nwhile cap.isOpened():\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    # Preprocess image\n    input_shape = input_details[0]['shape']\n    image_resized = cv2.resize(frame, (input_shape[1], input_shape[2]))\n    image_rgb = cv2.cvtColor(image_resized, cv2.COLOR_BGR2RGB)\n    input_data = np.expand_dims(image_rgb, axis=0)\n\n    # Set tensor to point to the input data to be inferred\n    interpreter.set_tensor(input_details[0]['index'], input_data)\n\n    # Run inference\n    interpreter.invoke()\n\n    # Extract results\n    boxes = interpreter.get_tensor(output_details[0]['index'])[0]\n    classes = interpreter.get_tensor(output_details[1]['index'])[0]\n    scores = interpreter.get_tensor(output_details[2]['index'])[0]\n\n    # Phase 4: Output handling\n    # Draw detection boxes with labels and scores\n    for i in range(len(scores)):\n        if scores[i] > confidence_threshold:\n            box = boxes[i]\n            class_id = int(classes[i])\n            label = labels[class_id]\n            score = scores[i]\n\n            # Convert normalized coordinates to pixel values\n            top_left = (int(box[1] * frame_width), int(box[0] * frame_height))\n            bottom_right = (int(box[3] * frame_width), int(box[2] * frame_height))\n\n            # Draw rectangle and label\n            cv2.rectangle(frame, top_left, bottom_right, (0, 255, 0), 2)\n            label_text = f'{label}: {score:.2f}'\n            cv2.putText(frame, label_text, (top_left[0], top_left[1] - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n\n    # Write the frame to output video\n    out.write(frame)\n\n# Release everything when job is finished\ncap.release()\nout.release()\ncv2.destroyAllWindows()"
      },
      "session_id": "qwen2.5-coder:32b_a8a2_psg_batch",
      "metadata": {
        "num_run": 22
      },
      "tags": [
        "ablation",
        "benchmark",
        "py_sketch_generator",
        "qwen2.5-coder:32b"
      ],
      "latency": 28.319,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-04-06-40-044443_chatcmpl-cb96acb6-0e38-4039-8c19-b825450d83d4",
          "traceId": "bd16e9d8",
          "type": "GENERATION",
          "name": "bd16_psg_gen_attempt#1",
          "startTime": "2025-12-11T02:06:40.044000+00:00",
          "endTime": "2025-12-11T02:07:08.363000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 28319.0,
          "completionTokens": 766,
          "environment": "default",
          "totalTokens": 1030,
          "promptTokens": 264,
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "createdAt": "2025-12-11T02:06:40.558Z",
      "updatedAt": "2025-12-11T02:07:22.180Z",
      "environment": "default"
    },
    {
      "id": "c5301cc5",
      "timestamp": "2025-12-11T02:03:49.011000+00:00",
      "name": "c530_py_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import cv2\nimport numpy as np\nfrom tflite_runtime.interpreter import Interpreter\n\n# Configuration parameters\nmodel_path = 'models/ssd-mobilenet_v1/detect.tflite'\nlabel_path = 'models/ssd-mobilenet_v1/labelmap.txt'\ninput_path = 'data/object_detection/sheeps.mp4'\noutput_path = 'results/object_detection/test_results/sheeps_detections.mp4'\nconfidence_threshold = 0.5\n\n# Load labels\nwith open(label_path, 'r') as f:\n    labels = [line.strip() for line in f.readlines()]\n\n# Setup TFLite interpreter\ninterpreter = Interpreter(model_path=model_path)\ninterpreter.allocate_tensors()\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\n# Preprocessing function\ndef preprocess_image(image):\n    input_shape = input_details[0]['shape']\n    image_resized = cv2.resize(image, (input_shape[1], input_shape[2]))\n    image_rgb = cv2.cvtColor(image_resized, cv2.COLOR_BGR2RGB)\n    input_data = np.expand_dims(image_rgb, axis=0).astype(np.uint8)\n    return input_data\n\n# Inference function\ndef run_inference(interpreter, input_data):\n    interpreter.set_tensor(input_details[0]['index'], input_data)\n    interpreter.invoke()\n    boxes = interpreter.get_tensor(output_details[0]['index'])[0]\n    classes = interpreter.get_tensor(output_details[1]['index'])[0]\n    scores = interpreter.get_tensor(output_details[2]['index'])[0]\n    num_detections = int(interpreter.get_tensor(output_details[3]['index']))\n    return boxes, classes, scores, num_detections\n\n# Function to draw bounding boxes\ndef draw_boxes(image, boxes, classes, scores, height, width):\n    for i in range(len(scores)):\n        if scores[i] > confidence_threshold:\n            ymin = int(max(1, (boxes[i][0] * height)))\n            xmin = int(max(1, (boxes[i][1] * width)))\n            ymax = int(min(height, (boxes[i][2] * height)))\n            xmax = int(min(width, (boxes[i][3] * width)))\n            \n            label = labels[int(classes[i])]\n            score = scores[i]\n            cv2.rectangle(image, (xmin, ymin), (xmax, ymax), (10, 255, 0), 2)\n            cv2.putText(image, f'{label}: {score:.2f}', (xmin, ymin - 10),\n                        cv2.FONT_HERSHEY_SIMPLEX, 0.9, (10, 255, 0), 2)\n\n# Main pipeline\ncap = cv2.VideoCapture(input_path)\nif not cap.isOpened():\n    raise IOError(f\"Could not open video file {input_path}\")\n\nframe_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\nframe_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\nfps = cap.get(cv2.CAP_PROP_FPS)\n\nout = cv2.VideoWriter(output_path, cv2.VideoWriter_fourcc(*'mp4v'), fps, (frame_width, frame_height))\n\nwhile True:\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    input_data = preprocess_image(frame)\n    boxes, classes, scores, num_detections = run_inference(interpreter, input_data)\n\n    draw_boxes(frame, boxes, classes, scores, frame_height, frame_width)\n    \n    out.write(frame)\n\ncap.release()\nout.release()\ncv2.destroyAllWindows()"
      },
      "session_id": "qwen2.5-coder:32b_a8a2_psg_batch",
      "metadata": {
        "num_run": 21
      },
      "tags": [
        "ablation",
        "benchmark",
        "py_sketch_generator",
        "qwen2.5-coder:32b"
      ],
      "latency": 113.916,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-04-03-49-012322_chatcmpl-d6372972-9b0c-47f2-b0e2-1712cd254aef",
          "traceId": "c5301cc5",
          "type": "GENERATION",
          "name": "c530_psg_gen_attempt#1",
          "startTime": "2025-12-11T02:03:49.012000+00:00",
          "endTime": "2025-12-11T02:04:15.367000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 26355.0,
          "completionTokens": 713,
          "environment": "default",
          "totalTokens": 977,
          "promptTokens": 264,
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-04-04-15-727672_chatcmpl-2c24051f-98dc-42a5-a8cc-ae83cf19bd81",
          "traceId": "c5301cc5",
          "type": "GENERATION",
          "name": "c530_psg_gen_attempt#2",
          "startTime": "2025-12-11T02:04:15.727000+00:00",
          "endTime": "2025-12-11T02:04:42.592000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 26865.0,
          "completionTokens": 729,
          "environment": "default",
          "totalTokens": 993,
          "promptTokens": 264,
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-04-04-42-777814_chatcmpl-e690f001-90a6-4a72-aba9-e3d6ea638c6a",
          "traceId": "c5301cc5",
          "type": "GENERATION",
          "name": "c530_psg_gen_attempt#3",
          "startTime": "2025-12-11T02:04:42.777000+00:00",
          "endTime": "2025-12-11T02:05:15.301000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 32524.0,
          "completionTokens": 881,
          "environment": "default",
          "totalTokens": 1145,
          "promptTokens": 264,
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-04-05-15-639541_chatcmpl-2abe725f-0d8c-48a1-b64a-fd3d103ef816",
          "traceId": "c5301cc5",
          "type": "GENERATION",
          "name": "c530_psg_gen_attempt#4",
          "startTime": "2025-12-11T02:05:15.639000+00:00",
          "endTime": "2025-12-11T02:05:42.928000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 27289.0,
          "completionTokens": 741,
          "environment": "default",
          "totalTokens": 1005,
          "promptTokens": 264,
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "createdAt": "2025-12-11T02:03:49.527Z",
      "updatedAt": "2025-12-11T02:06:19.777Z",
      "environment": "default"
    },
    {
      "id": "82f989e3",
      "timestamp": "2025-12-11T02:01:39.947000+00:00",
      "name": "82f9_py_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import cv2\nimport numpy as np\nfrom tflite_runtime.interpreter import Interpreter\n\n# Configuration parameters\nmodel_path = 'models/ssd-mobilenet_v1/detect.tflite'\nlabel_path = 'models/ssd-mobilenet_v1/labelmap.txt'\ninput_path = 'data/object_detection/sheeps.mp4'\noutput_path = 'results/object_detection/test_results/sheeps_detections.mp4'\nconfidence_threshold = 0.5\n\n# 1. Setup\n# Load the TFLite model and allocate tensors.\ninterpreter = Interpreter(model_path=model_path)\ninterpreter.allocate_tensors()\n\n# Get input and output details\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\n# Load labels\nwith open(label_path, 'r') as f:\n    labels = [line.strip() for line in f.readlines()]\n\n# 2. Preprocessing\ncap = cv2.VideoCapture(input_path)\nframe_width = int(cap.get(3))\nframe_height = int(cap.get(4))\n\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\nout = cv2.VideoWriter(output_path, fourcc, 30, (frame_width, frame_height))\n\n# 3. Inference\nwhile cap.isOpened():\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    # Preprocess the image\n    input_shape = input_details[0]['shape']\n    img_resized = cv2.resize(frame, (input_shape[1], input_shape[2]))\n    img_rgb = cv2.cvtColor(img_resized, cv2.COLOR_BGR2RGB)\n    input_data = np.expand_dims(img_rgb, axis=0)\n\n    # Run inference\n    interpreter.set_tensor(input_details[0]['index'], input_data)\n    interpreter.invoke()\n\n    # Extract results\n    boxes = interpreter.get_tensor(output_details[0]['index'])[0]\n    classes = interpreter.get_tensor(output_details[1]['index'])[0].astype(int)\n    scores = interpreter.get_tensor(output_details[2]['index'])[0]\n\n    # 4. Output handling\n    for i in range(len(scores)):\n        if scores[i] > confidence_threshold:\n            ymin, xmin, ymax, xmax = boxes[i]\n            label = labels[int(classes[i])]\n            score = scores[i]\n\n            # Convert normalized coordinates to image dimensions\n            left, right, top, bottom = int(xmin * frame_width), int(xmax * frame_width), int(ymin * frame_height), int(ymax * frame_height)\n\n            # Draw bounding box and label\n            cv2.rectangle(frame, (left, top), (right, bottom), (0, 255, 0), 2)\n            cv2.putText(frame, f'{label}: {score:.2f}', (left, top - 8), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n\n    # Write the frame to the output video\n    out.write(frame)\n\n# Release resources\ncap.release()\nout.release()\ncv2.destroyAllWindows()"
      },
      "session_id": "qwen2.5-coder:32b_a8a2_psg_batch",
      "metadata": {
        "num_run": 20
      },
      "tags": [
        "ablation",
        "benchmark",
        "py_sketch_generator",
        "qwen2.5-coder:32b"
      ],
      "latency": 72.008,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-04-01-39-948184_chatcmpl-0e2e8222-f11f-4d67-bb9e-bb0b0cb24265",
          "traceId": "82f989e3",
          "type": "GENERATION",
          "name": "82f9_psg_gen_attempt#1",
          "startTime": "2025-12-11T02:01:39.948000+00:00",
          "endTime": "2025-12-11T02:02:04.235000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 24287.0,
          "completionTokens": 657,
          "environment": "default",
          "totalTokens": 921,
          "promptTokens": 264,
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-04-02-04-394638_chatcmpl-6591f444-324d-4832-b1e7-dccf7fde79d2",
          "traceId": "82f989e3",
          "type": "GENERATION",
          "name": "82f9_psg_gen_attempt#2",
          "startTime": "2025-12-11T02:02:04.394000+00:00",
          "endTime": "2025-12-11T02:02:28.229000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 23835.0,
          "completionTokens": 647,
          "environment": "default",
          "totalTokens": 911,
          "promptTokens": 264,
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-04-02-28-358280_chatcmpl-30820832-3dce-401a-8fd4-1fdf929c1996",
          "traceId": "82f989e3",
          "type": "GENERATION",
          "name": "82f9_psg_gen_attempt#3",
          "startTime": "2025-12-11T02:02:28.358000+00:00",
          "endTime": "2025-12-11T02:02:51.956000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 23598.0,
          "completionTokens": 641,
          "environment": "default",
          "totalTokens": 905,
          "promptTokens": 264,
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "createdAt": "2025-12-11T02:01:40.462Z",
      "updatedAt": "2025-12-11T02:03:28.675Z",
      "environment": "default"
    },
    {
      "id": "a7d30e42",
      "timestamp": "2025-12-11T01:59:47.004000+00:00",
      "name": "a7d3_py_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import cv2\nimport numpy as np\nfrom tflite_runtime.interpreter import Interpreter\nimport os\n\n# Configuration parameters\nmodel_path = 'models/ssd-mobilenet_v1/detect.tflite'\nlabel_path = 'models/ssd-mobilenet_v1/labelmap.txt'\ninput_path = 'data/object_detection/sheeps.mp4'\noutput_path = 'results/object_detection/test_results/sheeps_detections.mp4'\nconfidence_threshold = 0.5\n\n# Phase 1: Setup\n# Load the TFLite model and allocate tensors.\ninterpreter = Interpreter(model_path=model_path)\ninterpreter.allocate_tensors()\n\n# Get input and output details\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\n# Read labels\nwith open(label_path, 'r') as f:\n    labels = [line.strip() for line in f.readlines()]\n\n# Phase 2: Preprocessing\n# Open the video file\ncap = cv2.VideoCapture(input_path)\nif not cap.isOpened():\n    raise SystemExit('Could not open video source')\n\n# Get video properties to save output video with the same properties\nframe_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\nframe_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\nfps = cap.get(cv2.CAP_PROP_FPS)\n\n# Define codec and create VideoWriter object\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\nout = cv2.VideoWriter(output_path, fourcc, fps, (frame_width, frame_height))\n\n# Phase 3: Inference\nwhile True:\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    # Preprocess the frame to fit the model's input\n    input_shape = input_details[0]['shape']\n    image_resized = cv2.resize(frame, (input_shape[1], input_shape[2]))\n    image_rgb = cv2.cvtColor(image_resized, cv2.COLOR_BGR2RGB)\n    input_data = np.expand_dims(image_rgb, axis=0)\n\n    # Run the inference\n    interpreter.set_tensor(input_details[0]['index'], input_data)\n    interpreter.invoke()\n\n    # Extract results\n    boxes = interpreter.get_tensor(output_details[0]['index'])[0]\n    classes = interpreter.get_tensor(output_details[1]['index'])[0]\n    scores = interpreter.get_tensor(output_details[2]['index'])[0]\n\n    # Phase 4: Output handling\n    for i in range(len(scores)):\n        if scores[i] > confidence_threshold:\n            ymin, xmin, ymax, xmax = boxes[i]\n            (left, right, top, bottom) = (xmin * frame_width, xmax * frame_width,\n                                          ymin * frame_height, ymax * frame_height)\n\n            # Draw the bounding box and label\n            cv2.rectangle(frame, (int(left), int(top)), (int(right), int(bottom)), (0, 255, 0), 2)\n            label = f'{labels[int(classes[i])]}: {scores[i]:.2f}'\n            cv2.putText(frame, label, (int(left), int(top) - 10),\n                        cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n\n    # Write the frame with detections to the output video\n    out.write(frame)\n\n# Release everything when job is finished\ncap.release()\nout.release()\ncv2.destroyAllWindows()\n\n# Compute mAP would typically require a set of ground truth annotations which are not provided in this context."
      },
      "session_id": "qwen2.5-coder:32b_a8a2_psg_batch",
      "metadata": {
        "num_run": 19
      },
      "tags": [
        "ablation",
        "benchmark",
        "py_sketch_generator",
        "qwen2.5-coder:32b"
      ],
      "latency": 55.734,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-03-59-47-005560_chatcmpl-2c5f4c30-9e60-4294-94fa-605ebf391435",
          "traceId": "a7d30e42",
          "type": "GENERATION",
          "name": "a7d3_psg_gen_attempt#1",
          "startTime": "2025-12-11T01:59:47.005000+00:00",
          "endTime": "2025-12-11T02:00:12.772000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 25767.0,
          "completionTokens": 697,
          "environment": "default",
          "totalTokens": 961,
          "promptTokens": 264,
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-04-00-12-924793_chatcmpl-9ae736e7-677d-4575-8a15-6b41c214eb86",
          "traceId": "a7d30e42",
          "type": "GENERATION",
          "name": "a7d3_psg_gen_attempt#2",
          "startTime": "2025-12-11T02:00:12.924000+00:00",
          "endTime": "2025-12-11T02:00:42.739000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 29815.0,
          "completionTokens": 808,
          "environment": "default",
          "totalTokens": 1072,
          "promptTokens": 264,
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "createdAt": "2025-12-11T01:59:47.520Z",
      "updatedAt": "2025-12-11T02:01:19.722Z",
      "environment": "default"
    },
    {
      "id": "1e427b5a",
      "timestamp": "2025-12-11T01:58:23.987000+00:00",
      "name": "1e42_py_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import cv2\nimport numpy as np\nfrom tflite_runtime.interpreter import Interpreter\n\n# Configuration parameters\nmodel_path = 'models/ssd-mobilenet_v1/detect.tflite'\nlabel_path = 'models/ssd-mobilenet_v1/labelmap.txt'\ninput_path = 'data/object_detection/sheeps.mp4'\noutput_path = 'results/object_detection/test_results/sheeps_detections.mp4'\nconfidence_threshold = 0.5\n\n# 1. Setup\n# Load the TFLite model and allocate tensors.\ninterpreter = Interpreter(model_path=model_path)\ninterpreter.allocate_tensors()\n\n# Get input and output details\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\n# Load labels\nwith open(label_path, 'r') as f:\n    labels = [line.strip() for line in f.readlines()]\n\n# 2. Preprocessing\ncap = cv2.VideoCapture(input_path)\nif not cap.isOpened():\n    raise IOError(\"Cannot open video file\")\n\nframe_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\nframe_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\nfps = cap.get(cv2.CAP_PROP_FPS)\n\nout = cv2.VideoWriter(output_path, cv2.VideoWriter_fourcc(*'mp4v'), fps, (frame_width, frame_height))\n\n# 3. Inference\nwhile True:\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    # Preprocess the image\n    input_shape = input_details[0]['shape']\n    img_resized = cv2.resize(frame, (input_shape[1], input_shape[2]))\n    img_input = np.array(img_resized, dtype=np.uint8)\n    img_input = np.expand_dims(img_input, axis=0)\n\n    # Set the tensor to point to the input data to be inferred\n    interpreter.set_tensor(input_details[0]['index'], img_input)\n\n    # Run inference\n    interpreter.invoke()\n\n    # Extract results\n    boxes = interpreter.get_tensor(output_details[0]['index'])[0]\n    classes = interpreter.get_tensor(output_details[1]['index'])[0]\n    scores = interpreter.get_tensor(output_details[2]['index'])[0]\n\n    # 4. Output handling\n    for i in range(len(scores)):\n        if scores[i] > confidence_threshold:\n            box = boxes[i]\n            class_id = int(classes[i])\n            label = labels[class_id]\n            score = scores[i]\n\n            ymin, xmin, ymax, xmax = box\n\n            # Convert normalized coordinates to image dimensions\n            xmin *= frame_width\n            xmax *= frame_width\n            ymin *= frame_height\n            ymax *= frame_height\n\n            cv2.rectangle(frame, (int(xmin), int(ymin)), (int(xmax), int(ymax)), (0, 255, 0), 2)\n            cv2.putText(frame, f'{label}: {score:.2f}', (int(xmin), int(ymin) - 10),\n                        cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n\n    out.write(frame)\n\ncap.release()\nout.release()"
      },
      "session_id": "qwen2.5-coder:32b_a8a2_psg_batch",
      "metadata": {
        "num_run": 18
      },
      "tags": [
        "ablation",
        "benchmark",
        "py_sketch_generator",
        "qwen2.5-coder:32b"
      ],
      "latency": 26.483,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-03-58-23-988813_chatcmpl-3d2819d4-5a98-420e-9c4a-86edf2fa4192",
          "traceId": "1e427b5a",
          "type": "GENERATION",
          "name": "1e42_psg_gen_attempt#1",
          "startTime": "2025-12-11T01:58:23.988000+00:00",
          "endTime": "2025-12-11T01:58:50.471000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 26483.0,
          "completionTokens": 716,
          "environment": "default",
          "totalTokens": 980,
          "promptTokens": 264,
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "createdAt": "2025-12-11T01:58:24.503Z",
      "updatedAt": "2025-12-11T01:59:27.136Z",
      "environment": "default"
    },
    {
      "id": "6b7ac73b",
      "timestamp": "2025-12-11T01:56:54.036000+00:00",
      "name": "6b7a_py_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import cv2\nimport numpy as np\nfrom tflite_runtime.interpreter import Interpreter\nimport os\n\n# Configuration parameters\nmodel_path = 'models/ssd-mobilenet_v1/detect.tflite'\nlabel_path = 'models/ssd-mobilenet_v1/labelmap.txt'\ninput_path = 'data/object_detection/sheeps.mp4'\noutput_path = 'results/object_detection/test_results/sheeps_detections.mp4'\nconfidence_threshold = 0.5\n\n# Phase 1: Setup\ndef load_labels(label_path):\n    with open(label_path, 'r', encoding='utf-8') as f:\n        labels = [line.strip() for line in f.readlines()]\n    return {index: label for index, label in enumerate(labels)}\n\ndef load_model(model_path):\n    interpreter = Interpreter(model_path=model_path)\n    interpreter.allocate_tensors()\n    return interpreter\n\nlabels = load_labels(label_path)\ninterpreter = load_model(model_path)\n\n# Get model details\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\nheight = input_details[0]['shape'][1]\nwidth = input_details[0]['shape'][2]\n\nfloating_model = (input_details[0]['dtype'] == np.float32)\n\n# Phase 2: Preprocessing\ndef preprocess_image(frame):\n    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n    frame_resized = cv2.resize(frame_rgb, (width, height))\n    input_data = np.expand_dims(frame_resized, axis=0)\n    \n    if floating_model:\n        input_data = (np.float32(input_data) - 127.5) / 127.5\n    \n    return input_data\n\n# Phase 3: Inference\ndef run_inference(interpreter, input_data):\n    interpreter.set_tensor(input_details[0]['index'], input_data)\n    interpreter.invoke()\n    \n    boxes = interpreter.get_tensor(output_details[0]['index'])[0]\n    classes = interpreter.get_tensor(output_details[1]['index'])[0]\n    scores = interpreter.get_tensor(output_details[2]['index'])[0]\n    num_detections = int(interpreter.get_tensor(output_details[3]['index'])[0])\n    \n    return boxes, classes, scores, num_detections\n\n# Phase 4: Output Handling\ndef draw_detection_boxes(frame, boxes, classes, scores, num_detections):\n    for i in range(num_detections):\n        if scores[i] > confidence_threshold:\n            ymin = int(max(1, (boxes[i][0] * frame.shape[0])))\n            xmin = int(max(1, (boxes[i][1] * frame.shape[1])))\n            ymax = int(min(frame.shape[0], (boxes[i][2] * frame.shape[0])))\n            xmax = int(min(frame.shape[1], (boxes[i][3] * frame.shape[1])))\n            \n            cv2.rectangle(frame, (xmin, ymin), (xmax, ymax), (10, 255, 0), 2)\n            label = f'{labels[int(classes[i])]}: {int(scores[i]*100)}%'\n            cv2.putText(frame, label, (xmin, ymin-10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (36,255,12), 2)\n\n# Main execution\ncap = cv2.VideoCapture(input_path)\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\nfps = cap.get(cv2.CAP_PROP_FPS)\nout = cv2.VideoWriter(output_path, fourcc, fps, (int(cap.get(3)), int(cap.get(4))))\n\nwhile cap.isOpened():\n    ret, frame = cap.read()\n    if not ret:\n        break\n    \n    input_data = preprocess_image(frame)\n    boxes, classes, scores, num_detections = run_inference(interpreter, input_data)\n    draw_detection_boxes(frame, boxes, classes, scores, num_detections)\n    \n    out.write(frame)\n\ncap.release()\nout.release()\ncv2.destroyAllWindows()"
      },
      "session_id": "qwen2.5-coder:32b_a8a2_psg_batch",
      "metadata": {
        "num_run": 17
      },
      "tags": [
        "ablation",
        "benchmark",
        "py_sketch_generator",
        "qwen2.5-coder:32b"
      ],
      "latency": 33.268,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-03-56-54-036638_chatcmpl-bae6c299-02d3-4a1c-997d-d17acc51e36c",
          "traceId": "6b7ac73b",
          "type": "GENERATION",
          "name": "6b7a_psg_gen_attempt#1",
          "startTime": "2025-12-11T01:56:54.036000+00:00",
          "endTime": "2025-12-11T01:57:27.304000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 33268.0,
          "completionTokens": 899,
          "environment": "default",
          "totalTokens": 1163,
          "promptTokens": 264,
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "createdAt": "2025-12-11T01:56:54.554Z",
      "updatedAt": "2025-12-11T01:58:04.245Z",
      "environment": "default"
    },
    {
      "id": "712eedef",
      "timestamp": "2025-12-11T01:55:30.097000+00:00",
      "name": "712e_py_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import cv2\nimport numpy as np\nimport tflite_runtime.interpreter as tflite\nfrom PIL import ImageDraw, ImageFont, Image\n\n# Configuration parameters\nmodel_path = 'models/ssd-mobilenet_v1/detect.tflite'\nlabel_path = 'models/ssd-mobilenet_v1/labelmap.txt'\ninput_path = 'data/object_detection/sheeps.mp4'\noutput_path = 'results/object_detection/test_results/sheeps_detections.mp4'\nconfidence_threshold = 0.5\n\n# Load labels\nwith open(label_path, 'r') as f:\n    labels = [line.strip() for line in f.readlines()]\n\n# Load TFLite interpreter\ninterpreter = tflite.Interpreter(model_path=model_path)\ninterpreter.allocate_tensors()\n\n# Get input and output details\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\nheight = input_details[0]['shape'][1]\nwidth = input_details[0]['shape'][2]\n\n# Load video\ncap = cv2.VideoCapture(input_path)\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\nout = cv2.VideoWriter(output_path, fourcc, 30.0, (int(cap.get(3)), int(cap.get(4))))\n\ndef preprocess_image(frame):\n    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n    frame_resized = cv2.resize(frame_rgb, (width, height))\n    input_data = np.expand_dims(frame_resized, axis=0)\n    return input_data\n\nwhile cap.isOpened():\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    # Preprocess image\n    input_data = preprocess_image(frame)\n\n    # Run inference\n    interpreter.set_tensor(input_details[0]['index'], input_data)\n    interpreter.invoke()\n\n    # Extract results\n    boxes = interpreter.get_tensor(output_details[0]['index'])[0]\n    classes = interpreter.get_tensor(output_details[1]['index'])[0]\n    scores = interpreter.get_tensor(output_details[2]['index'])[0]\n\n    # Draw detection boxes with labels\n    for i in range(len(scores)):\n        if scores[i] > confidence_threshold:\n            ymin, xmin, ymax, xmax = boxes[i]\n            (left, right, top, bottom) = (xmin * frame.shape[1], xmax * frame.shape[1],\n                                          ymin * frame.shape[0], ymax * frame.shape[0])\n            label = labels[int(classes[i])]\n            confidence = scores[i]\n\n            # Draw rectangle\n            cv2.rectangle(frame, (int(left), int(top)), (int(right), int(bottom)), (0, 255, 0), 2)\n\n            # Draw label\n            font = cv2.FONT_HERSHEY_SIMPLEX\n            text = f'{label}: {confidence:.2f}'\n            cv2.putText(frame, text, (int(left), int(top) - 10), font, 0.9, (0, 255, 0), 2)\n\n    # Write the frame with detection boxes to the output video\n    out.write(frame)\n\ncap.release()\nout.release()"
      },
      "session_id": "qwen2.5-coder:32b_a8a2_psg_batch",
      "metadata": {
        "num_run": 16
      },
      "tags": [
        "ablation",
        "benchmark",
        "py_sketch_generator",
        "qwen2.5-coder:32b"
      ],
      "latency": 26.166,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-03-55-30-098606_chatcmpl-559778b8-2381-4a5d-85b9-68094c66203f",
          "traceId": "712eedef",
          "type": "GENERATION",
          "name": "712e_psg_gen_attempt#1",
          "startTime": "2025-12-11T01:55:30.098000+00:00",
          "endTime": "2025-12-11T01:55:56.264000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 26166.0,
          "completionTokens": 708,
          "environment": "default",
          "totalTokens": 972,
          "promptTokens": 264,
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "createdAt": "2025-12-11T01:55:30.616Z",
      "updatedAt": "2025-12-11T01:56:33.830Z",
      "environment": "default"
    },
    {
      "id": "f1a6affe",
      "timestamp": "2025-12-11T01:54:03.085000+00:00",
      "name": "f1a6_py_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import cv2\nimport numpy as np\nimport tflite_runtime.interpreter as tflite\nfrom collections import defaultdict\n\n# Configuration parameters\nmodel_path = 'models/ssd-mobilenet_v1/detect.tflite'\nlabel_path = 'models/ssd-mobilenet_v1/labelmap.txt'\ninput_path = 'data/object_detection/sheeps.mp4'\noutput_path = 'results/object_detection/test_results/sheeps_detections.mp4'\nconfidence_threshold = 0.5\n\n# Phase 1: Setup\ninterpreter = tflite.Interpreter(model_path=model_path)\ninterpreter.allocate_tensors()\n\n# Get input and output details\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\n# Load labels\nwith open(label_path, 'r') as f:\n    labels = [line.strip() for line in f.readlines()]\n\n# Phase 2: Preprocessing\ncap = cv2.VideoCapture(input_path)\nif not cap.isOpened():\n    raise IOError(f\"Cannot open input file {input_path}\")\n\nwidth = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\nheight = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\nfps = int(cap.get(cv2.CAP_PROP_FPS))\n\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\nout = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n\n# Phase 3: Inference\nwhile True:\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    # Preprocess the frame\n    input_data = cv2.resize(frame, (300, 300))\n    input_data = np.expand_dims(input_data, axis=0)\n    input_data = input_data.astype(np.uint8)\n\n    # Set tensor to point to the input data to be inferred\n    interpreter.set_tensor(input_details[0]['index'], input_data)\n\n    # Run inference\n    interpreter.invoke()\n\n    # Post-process the results\n    boxes = interpreter.get_tensor(output_details[0]['index'])[0]\n    classes = interpreter.get_tensor(output_details[1]['index'])[0]\n    scores = interpreter.get_tensor(output_details[2]['index'])[0]\n\n    for i in range(len(scores)):\n        if scores[i] > confidence_threshold:\n            # Get bounding box coordinates and draw the box\n            ymin, xmin, ymax, xmax = boxes[i]\n            (left, right, top, bottom) = (xmin * width, xmax * width,\n                                          ymin * height, ymax * height)\n            cv2.rectangle(frame, (int(left), int(top)), (int(right), int(bottom)), (0, 255, 0), 2)\n\n            # Get the label and draw it\n            object_name = labels[int(classes[i])]\n            label = f'{object_name}: {int(scores[i] * 100)}%'\n            cv2.putText(frame, label, (int(left), int(top) - 10),\n                        cv2.FONT_HERSHEY_SIMPLEX, 0.9, (36, 255, 12), 2)\n\n    # Phase 4: Output handling\n    out.write(frame)\n\ncap.release()\nout.release()"
      },
      "session_id": "qwen2.5-coder:32b_a8a2_psg_batch",
      "metadata": {
        "num_run": 15
      },
      "tags": [
        "ablation",
        "benchmark",
        "py_sketch_generator",
        "qwen2.5-coder:32b"
      ],
      "latency": 26.507,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-03-54-03-086927_chatcmpl-6c1a1837-7a38-406e-a497-4c83b6c0398f",
          "traceId": "f1a6affe",
          "type": "GENERATION",
          "name": "f1a6_psg_gen_attempt#1",
          "startTime": "2025-12-11T01:54:03.086000+00:00",
          "endTime": "2025-12-11T01:54:29.593000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 26507.0,
          "completionTokens": 716,
          "environment": "default",
          "totalTokens": 980,
          "promptTokens": 264,
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "createdAt": "2025-12-11T01:54:03.604Z",
      "updatedAt": "2025-12-11T01:55:09.244Z",
      "environment": "default"
    },
    {
      "id": "a95b304a",
      "timestamp": "2025-12-11T01:50:34.987000+00:00",
      "name": "a95b_py_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import cv2\nimport numpy as np\nfrom tflite_runtime.interpreter import Interpreter\nfrom PIL import Image\n\n# Configuration parameters\nmodel_path = 'models/ssd-mobilenet_v1/detect.tflite'\nlabel_path = 'models/ssd-mobilenet_v1/labelmap.txt'\ninput_path = 'data/object_detection/sheeps.mp4'\noutput_path = 'results/object_detection/test_results/sheeps_detections.mp4'\nconfidence_threshold = 0.5\n\n# Load labels\nwith open(label_path, 'r') as f:\n    labels = [line.strip() for line in f.readlines()]\n\n# Load TFLite interpreter\ninterpreter = Interpreter(model_path=model_path)\ninterpreter.allocate_tensors()\n\n# Get input and output details\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\nheight = input_details[0]['shape'][1]\nwidth = input_details[0]['shape'][2]\n\n# Setup video capture\ncap = cv2.VideoCapture(input_path)\n\n# Setup video writer\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\nout = cv2.VideoWriter(output_path, fourcc, cap.get(cv2.CAP_PROP_FPS), (int(cap.get(3)), int(cap.get(4))))\n\ndef preprocess_image(image):\n    image_resized = cv2.resize(image, (width, height))\n    input_data = np.expand_dims(image_resized, axis=0)\n    return input_data\n\ndef draw_boxes(frame, boxes, classes, scores):\n    for i in range(len(scores)):\n        if scores[i] > confidence_threshold:\n            box = boxes[i]\n            class_id = int(classes[i])\n            label = labels[class_id]\n            score = scores[i]\n\n            top_left = (int(box[1] * frame.shape[1]), int(box[0] * frame.shape[0]))\n            bottom_right = (int(box[3] * frame.shape[1]), int(box[2] * frame.shape[0]))\n\n            cv2.rectangle(frame, top_left, bottom_right, (0, 255, 0), 2)\n            label_text = f'{label}: {score:.2f}'\n            cv2.putText(frame, label_text, (top_left[0], top_left[1] - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n\nwhile cap.isOpened():\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    input_data = preprocess_image(frame)\n    interpreter.set_tensor(input_details[0]['index'], input_data.astype(np.uint8))\n\n    interpreter.invoke()\n\n    boxes = interpreter.get_tensor(output_details[0]['index'])\n    classes = interpreter.get_tensor(output_details[1]['index'])\n    scores = interpreter.get_tensor(output_details[2]['index'])\n\n    draw_boxes(frame, np.squeeze(boxes), np.squeeze(classes).astype(int), np.squeeze(scores))\n    out.write(frame)\n\ncap.release()\nout.release()\ncv2.destroyAllWindows()"
      },
      "session_id": "qwen2.5-coder:32b_a8a2_psg_batch",
      "metadata": {
        "num_run": 14
      },
      "tags": [
        "ablation",
        "benchmark",
        "py_sketch_generator",
        "qwen2.5-coder:32b"
      ],
      "latency": 148.378,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-03-50-34-988286_chatcmpl-bf2f1a5b-92ac-4037-afea-921e0d536061",
          "traceId": "a95b304a",
          "type": "GENERATION",
          "name": "a95b_psg_gen_attempt#1",
          "startTime": "2025-12-11T01:50:34.988000+00:00",
          "endTime": "2025-12-11T01:51:04.751000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 29763.0,
          "completionTokens": 805,
          "environment": "default",
          "totalTokens": 1069,
          "promptTokens": 264,
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-03-51-04-877787_chatcmpl-f2f8a1a7-303d-4f12-940d-b90b44d8ae90",
          "traceId": "a95b304a",
          "type": "GENERATION",
          "name": "a95b_psg_gen_attempt#2",
          "startTime": "2025-12-11T01:51:04.877000+00:00",
          "endTime": "2025-12-11T01:51:36.963000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 32086.0,
          "completionTokens": 869,
          "environment": "default",
          "totalTokens": 1133,
          "promptTokens": 264,
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-03-51-37-141174_chatcmpl-da81731f-5d9c-48ea-b5bc-72f17da71108",
          "traceId": "a95b304a",
          "type": "GENERATION",
          "name": "a95b_psg_gen_attempt#3",
          "startTime": "2025-12-11T01:51:37.141000+00:00",
          "endTime": "2025-12-11T01:52:05.727000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 28586.0,
          "completionTokens": 775,
          "environment": "default",
          "totalTokens": 1039,
          "promptTokens": 264,
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-03-52-08-968287_chatcmpl-ad0e71cd-50b3-4fa3-bfb5-59e635e42065",
          "traceId": "a95b304a",
          "type": "GENERATION",
          "name": "a95b_psg_gen_attempt#4",
          "startTime": "2025-12-11T01:52:08.968000+00:00",
          "endTime": "2025-12-11T01:52:39.939000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 30971.0,
          "completionTokens": 839,
          "environment": "default",
          "totalTokens": 1103,
          "promptTokens": 264,
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-03-52-40-093212_chatcmpl-db1e21b4-ff2b-4037-8e0d-3b8e4b92fed2",
          "traceId": "a95b304a",
          "type": "GENERATION",
          "name": "a95b_psg_gen_attempt#5",
          "startTime": "2025-12-11T01:52:40.093000+00:00",
          "endTime": "2025-12-11T01:53:03.366000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 23273.0,
          "completionTokens": 632,
          "environment": "default",
          "totalTokens": 896,
          "promptTokens": 264,
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "createdAt": "2025-12-11T01:50:35.504Z",
      "updatedAt": "2025-12-11T01:53:42.747Z",
      "environment": "default"
    },
    {
      "id": "ab7387d6",
      "timestamp": "2025-12-11T01:48:21.061000+00:00",
      "name": "ab73_py_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import cv2\nimport numpy as np\nimport tflite_runtime.interpreter as tflite\n\n# Configuration parameters\nmodel_path = 'models/ssd-mobilenet_v1/detect.tflite'\nlabel_path = 'models/ssd-mobilenet_v1/labelmap.txt'\ninput_path = 'data/object_detection/sheeps.mp4'\noutput_path = 'results/object_detection/test_results/sheeps_detections.mp4'\nconfidence_threshold = 0.5\n\n# Load labels\nwith open(label_path, 'r') as f:\n    labels = [line.strip() for line in f.readlines()]\n\n# Setup TFLite interpreter\ninterpreter = tflite.Interpreter(model_path=model_path)\ninterpreter.allocate_tensors()\n\n# Get input and output details\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\n# Preprocess video frames\ncap = cv2.VideoCapture(input_path)\nframe_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\nframe_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\nfps = cap.get(cv2.CAP_PROP_FPS)\n\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\nout = cv2.VideoWriter(output_path, fourcc, fps, (frame_width, frame_height))\n\ndef draw_detection_boxes(frame, boxes, classes, scores):\n    for box, cls, score in zip(boxes, classes, scores):\n        if score > confidence_threshold:\n            ymin, xmin, ymax, xmax = box\n            h, w, _ = frame.shape\n            left = int(xmin * w)\n            top = int(ymin * h)\n            right = int(xmax * w)\n            bottom = int(ymax * h)\n\n            label = f'{labels[int(cls)]}: {score:.2f}'\n            cv2.rectangle(frame, (left, top), (right, bottom), (0, 255, 0), 2)\n            cv2.putText(frame, label, (left, top - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n\n# Inference and output handling\nwhile cap.isOpened():\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    input_data = np.expand_dims(frame, axis=0)\n    input_data = cv2.resize(input_data[0], (300, 300))\n    input_data = input_data.reshape(1, 300, 300, 3).astype(np.uint8)\n\n    interpreter.set_tensor(input_details[0]['index'], input_data)\n    interpreter.invoke()\n\n    boxes = interpreter.get_tensor(output_details[0]['index'])[0]\n    classes = interpreter.get_tensor(output_details[1]['index'])[0]\n    scores = interpreter.get_tensor(output_details[2]['index'])[0]\n\n    draw_detection_boxes(frame, boxes, classes, scores)\n\n    out.write(frame)\n\ncap.release()\nout.release()\ncv2.destroyAllWindows()"
      },
      "session_id": "qwen2.5-coder:32b_a8a2_psg_batch",
      "metadata": {
        "num_run": 13
      },
      "tags": [
        "ablation",
        "benchmark",
        "py_sketch_generator",
        "qwen2.5-coder:32b"
      ],
      "latency": 77.244,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-03-48-21-062770_chatcmpl-832df11c-daaf-4a1a-8708-4ba15dd5d1ce",
          "traceId": "ab7387d6",
          "type": "GENERATION",
          "name": "ab73_psg_gen_attempt#1",
          "startTime": "2025-12-11T01:48:21.062000+00:00",
          "endTime": "2025-12-11T01:48:45.179000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 24117.0,
          "completionTokens": 652,
          "environment": "default",
          "totalTokens": 916,
          "promptTokens": 264,
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-03-48-45-542160_chatcmpl-2fa49170-af94-4a52-84dc-414d11e38d25",
          "traceId": "ab7387d6",
          "type": "GENERATION",
          "name": "ab73_psg_gen_attempt#2",
          "startTime": "2025-12-11T01:48:45.542000+00:00",
          "endTime": "2025-12-11T01:49:15.048000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 29506.0,
          "completionTokens": 800,
          "environment": "default",
          "totalTokens": 1064,
          "promptTokens": 264,
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-03-49-15-174189_chatcmpl-ce59f0d3-d109-4546-b710-943c9a5a57fe",
          "traceId": "ab7387d6",
          "type": "GENERATION",
          "name": "ab73_psg_gen_attempt#3",
          "startTime": "2025-12-11T01:49:15.174000+00:00",
          "endTime": "2025-12-11T01:49:38.306000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 23132.0,
          "completionTokens": 629,
          "environment": "default",
          "totalTokens": 893,
          "promptTokens": 264,
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "createdAt": "2025-12-11T01:48:21.578Z",
      "updatedAt": "2025-12-11T01:50:15.215Z",
      "environment": "default"
    },
    {
      "id": "d72c87be",
      "timestamp": "2025-12-11T01:46:30.108000+00:00",
      "name": "d72c_py_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import cv2\nimport numpy as np\nfrom tflite_runtime.interpreter import Interpreter\n\n# Configuration parameters\nmodel_path = 'models/ssd-mobilenet_v1/detect.tflite'\nlabel_path = 'models/ssd-mobilenet_v1/labelmap.txt'\ninput_path = 'data/object_detection/sheeps.mp4'\noutput_path = 'results/object_detection/test_results/sheeps_detections.mp4'\nconfidence_threshold = 0.5\n\n# Phase 1: Setup\n# Load the TFLite model and allocate tensors.\ninterpreter = Interpreter(model_path=model_path)\ninterpreter.allocate_tensors()\n\n# Get input and output details\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\n# Load labels from label map\nwith open(label_path, 'r') as f:\n    labels = [line.strip() for line in f.readlines()]\n\n# Phase 2: Preprocessing\ncap = cv2.VideoCapture(input_path)\nframe_width = int(cap.get(3))\nframe_height = int(cap.get(4))\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\nout = cv2.VideoWriter(output_path, fourcc, 20.0, (frame_width, frame_height))\n\n# Phase 3: Inference and Output Handling\ndef draw_detection_boxes(frame, boxes, classes, scores, confidence_threshold):\n    for i in range(len(scores)):\n        if scores[i] >= confidence_threshold:\n            box = boxes[i]\n            class_id = int(classes[i])\n            label = labels[class_id]\n            score = scores[i]\n            \n            # Convert normalized coordinates to image coordinates\n            top_left = (int(box[1] * frame_width), int(box[0] * frame_height))\n            bottom_right = (int(box[3] * frame_width), int(box[2] * frame_height))\n            \n            cv2.rectangle(frame, top_left, bottom_right, (0, 255, 0), 2)\n            label_text = f'{label}: {score:.2f}'\n            cv2.putText(frame, label_text, (top_left[0], top_left[1] - 10),\n                        cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n\nwhile cap.isOpened():\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    # Preprocess the frame\n    input_shape = input_details[0]['shape']\n    image_resized = cv2.resize(frame, (input_shape[1], input_shape[2]))\n    image_rgb = cv2.cvtColor(image_resized, cv2.COLOR_BGR2RGB)\n    input_data = np.expand_dims(image_rgb, axis=0)\n\n    # Set the tensor to point to the input data\n    interpreter.set_tensor(input_details[0]['index'], input_data.astype(np.uint8))\n\n    # Run inference\n    interpreter.invoke()\n\n    # Extract results\n    boxes = interpreter.get_tensor(output_details[0]['index'])[0]\n    classes = interpreter.get_tensor(output_details[1]['index'])[0]\n    scores = interpreter.get_tensor(output_details[2]['index'])[0]\n\n    # Draw detection boxes with labels\n    draw_detection_boxes(frame, boxes, classes, scores, confidence_threshold)\n\n    # Write the frame to the output file\n    out.write(frame)\n\ncap.release()\nout.release()"
      },
      "session_id": "qwen2.5-coder:32b_a8a2_psg_batch",
      "metadata": {
        "num_run": 12
      },
      "tags": [
        "ablation",
        "benchmark",
        "py_sketch_generator",
        "qwen2.5-coder:32b"
      ],
      "latency": 54.28,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-03-46-30-109483_chatcmpl-a68cf666-d939-47c3-a02e-85bc5d941e38",
          "traceId": "d72c87be",
          "type": "GENERATION",
          "name": "d72c_psg_gen_attempt#1",
          "startTime": "2025-12-11T01:46:30.109000+00:00",
          "endTime": "2025-12-11T01:46:55.581000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 25472.0,
          "completionTokens": 688,
          "environment": "default",
          "totalTokens": 952,
          "promptTokens": 264,
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-03-46-55-735399_chatcmpl-0ea49d24-847e-42df-9111-e6cd2e3e721c",
          "traceId": "d72c87be",
          "type": "GENERATION",
          "name": "d72c_psg_gen_attempt#2",
          "startTime": "2025-12-11T01:46:55.735000+00:00",
          "endTime": "2025-12-11T01:47:24.389000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 28654.0,
          "completionTokens": 777,
          "environment": "default",
          "totalTokens": 1041,
          "promptTokens": 264,
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "createdAt": "2025-12-11T01:46:30.627Z",
      "updatedAt": "2025-12-11T01:48:01.244Z",
      "environment": "default"
    },
    {
      "id": "eeb07204",
      "timestamp": "2025-12-11T01:43:07.072000+00:00",
      "name": "eeb0_py_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import cv2\nimport numpy as np\nfrom tflite_runtime.interpreter import Interpreter\n\n# Configuration parameters\nmodel_path = 'models/ssd-mobilenet_v1/detect.tflite'\nlabel_path = 'models/ssd-mobilenet_v1/labelmap.txt'\ninput_path = 'data/object_detection/sheeps.mp4'\noutput_path = 'results/object_detection/test_results/sheeps_detections.mp4'\nconfidence_threshold = 0.5\n\n# Load labels\nwith open(label_path, 'r') as f:\n    labels = [line.strip() for line in f.readlines()]\n\n# Load TFLite interpreter\ninterpreter = Interpreter(model_path=model_path)\ninterpreter.allocate_tensors()\n\n# Get input and output details\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\n# Read video from input path\ncap = cv2.VideoCapture(input_path)\nwidth = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\nheight = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\nfps = cap.get(cv2.CAP_PROP_FPS)\n\n# Define codec and create VideoWriter object\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\nout = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n\ndef preprocess_image(frame):\n    input_shape = input_details[0]['shape']\n    frame_resized = cv2.resize(frame, (input_shape[1], input_shape[2]))\n    input_data = np.expand_dims(frame_resized, axis=0)\n    return input_data\n\ndef draw_detection_boxes(frame, boxes, classes, scores, labels):\n    for i in range(len(scores)):\n        if scores[i] > confidence_threshold:\n            box = boxes[i]\n            class_id = int(classes[i])\n            score = scores[i]\n            label = labels[class_id]\n            ymin, xmin, ymax, xmax = box\n            h, w, _ = frame.shape\n            (left, right, top, bottom) = (int(xmin * w), int(xmax * w),\n                                          int(ymin * h), int(ymax * h))\n            cv2.rectangle(frame, (left, top), (right, bottom), (0, 255, 0), 2)\n            label_text = f'{label}: {score:.2f}'\n            cv2.putText(frame, label_text, (left, top - 10),\n                        cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n\nwhile cap.isOpened():\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    input_data = preprocess_image(frame)\n    interpreter.set_tensor(input_details[0]['index'], input_data)\n    \n    interpreter.invoke()\n\n    boxes = interpreter.get_tensor(output_details[0]['index'])[0]\n    classes = interpreter.get_tensor(output_details[1]['index'])[0]\n    scores = interpreter.get_tensor(output_details[2]['index'])[0]\n\n    draw_detection_boxes(frame, boxes, classes, scores, labels)\n\n    out.write(frame)\n\ncap.release()\nout.release()\ncv2.destroyAllWindows()"
      },
      "session_id": "qwen2.5-coder:32b_a8a2_psg_batch",
      "metadata": {
        "num_run": 11
      },
      "tags": [
        "ablation",
        "benchmark",
        "py_sketch_generator",
        "qwen2.5-coder:32b"
      ],
      "latency": 146.284,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-03-43-07-074006_chatcmpl-1075235a-ccfa-4ca2-8595-74502cc51508",
          "traceId": "eeb07204",
          "type": "GENERATION",
          "name": "eeb0_psg_gen_attempt#1",
          "startTime": "2025-12-11T01:43:07.074000+00:00",
          "endTime": "2025-12-11T01:43:32.635000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 25561.0,
          "completionTokens": 691,
          "environment": "default",
          "totalTokens": 955,
          "promptTokens": 264,
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-03-43-37-491943_chatcmpl-4c3d0e8c-999e-4de9-854d-fe11b0fcf80b",
          "traceId": "eeb07204",
          "type": "GENERATION",
          "name": "eeb0_psg_gen_attempt#2",
          "startTime": "2025-12-11T01:43:37.491000+00:00",
          "endTime": "2025-12-11T01:44:07.683000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 30192.0,
          "completionTokens": 819,
          "environment": "default",
          "totalTokens": 1083,
          "promptTokens": 264,
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-03-44-08-036749_chatcmpl-8b36b632-d69f-48c6-baa2-60e2bb50fe37",
          "traceId": "eeb07204",
          "type": "GENERATION",
          "name": "eeb0_psg_gen_attempt#3",
          "startTime": "2025-12-11T01:44:08.036000+00:00",
          "endTime": "2025-12-11T01:44:38.476000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 30440.0,
          "completionTokens": 825,
          "environment": "default",
          "totalTokens": 1089,
          "promptTokens": 264,
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-03-44-38-622214_chatcmpl-eec2c4b9-567a-48f0-bac1-a013f817fe1a",
          "traceId": "eeb07204",
          "type": "GENERATION",
          "name": "eeb0_psg_gen_attempt#4",
          "startTime": "2025-12-11T01:44:38.622000+00:00",
          "endTime": "2025-12-11T01:45:09.215000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 30593.0,
          "completionTokens": 829,
          "environment": "default",
          "totalTokens": 1093,
          "promptTokens": 264,
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-03-45-09-389886_chatcmpl-50c51c3a-ac2e-47dc-b997-edc50be1aad9",
          "traceId": "eeb07204",
          "type": "GENERATION",
          "name": "eeb0_psg_gen_attempt#5",
          "startTime": "2025-12-11T01:45:09.389000+00:00",
          "endTime": "2025-12-11T01:45:33.358000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 23969.0,
          "completionTokens": 651,
          "environment": "default",
          "totalTokens": 915,
          "promptTokens": 264,
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "createdAt": "2025-12-11T01:43:07.591Z",
      "updatedAt": "2025-12-11T01:46:10.257Z",
      "environment": "default"
    },
    {
      "id": "6209d13c",
      "timestamp": "2025-12-11T01:41:41.155000+00:00",
      "name": "6209_py_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import os\nimport numpy as np\nimport cv2\nfrom PIL import Image\nimport tflite_runtime.interpreter as tflite\n\n# Configuration parameters\nmodel_path = 'models/ssd-mobilenet_v1/detect.tflite'\nlabel_path = 'models/ssd-mobilenet_v1/labelmap.txt'\ninput_path = 'data/object_detection/sheeps.mp4'\noutput_path = 'results/object_detection/test_results/sheeps_detections.mp4'\nconfidence_threshold = 0.5\n\n# Phase 1: Setup\ndef load_labels(label_path):\n    with open(label_path, 'r', encoding='utf-8') as f:\n        labels = [line.strip() for line in f.readlines()]\n    return {i: label for i, label in enumerate(labels)}\n\ninterpreter = tflite.Interpreter(model_path=model_path)\ninterpreter.allocate_tensors()\n\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\nheight = input_details[0]['shape'][1]\nwidth = input_details[0]['shape'][2]\n\nlabels = load_labels(label_path)\n\n# Phase 2: Preprocessing\ncap = cv2.VideoCapture(input_path)\nif not cap.isOpened():\n    raise Exception(\"Error opening video file\")\n\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\nout = cv2.VideoWriter(output_path, fourcc, cap.get(cv2.CAP_PROP_FPS), (int(cap.get(3)), int(cap.get(4))))\n\n# Phase 3: Inference\nwhile cap.isOpened():\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n    frame_resized = cv2.resize(frame_rgb, (width, height))\n    input_data = np.expand_dims(frame_resized, axis=0)\n\n    interpreter.set_tensor(input_details[0]['index'], input_data)\n\n    interpreter.invoke()\n\n    boxes = interpreter.get_tensor(output_details[0]['index'])[0]\n    classes = interpreter.get_tensor(output_details[1]['index'])[0]\n    scores = interpreter.get_tensor(output_details[2]['index'])[0]\n    num_detections = int(interpreter.get_tensor(output_details[3]['index']))\n\n    for i in range(num_detections):\n        if scores[i] > confidence_threshold:\n            ymin, xmin, ymax, xmax = boxes[i]\n            h, w, _ = frame.shape\n            left = int(xmin * w)\n            right = int(xmax * w)\n            top = int(ymin * h)\n            bottom = int(ymax * h)\n\n            label = labels[int(classes[i])]\n            cv2.rectangle(frame, (left, top), (right, bottom), (0, 255, 0), 2)\n            cv2.putText(frame, f'{label}: {int(scores[i]*100)}%', (left, top - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n\n    # Phase 4: Output Handling\n    out.write(cv2.cvtColor(frame, cv2.COLOR_RGB2BGR))\n\ncap.release()\nout.release()"
      },
      "session_id": "qwen2.5-coder:32b_a8a2_psg_batch",
      "metadata": {
        "num_run": 10
      },
      "tags": [
        "ablation",
        "benchmark",
        "py_sketch_generator",
        "qwen2.5-coder:32b"
      ],
      "latency": 26.351,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-03-41-41-157498_chatcmpl-01297a2f-a9a4-418f-9520-5f3959197ff7",
          "traceId": "6209d13c",
          "type": "GENERATION",
          "name": "6209_psg_gen_attempt#1",
          "startTime": "2025-12-11T01:41:41.157000+00:00",
          "endTime": "2025-12-11T01:42:07.508000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 26351.0,
          "completionTokens": 713,
          "environment": "default",
          "totalTokens": 977,
          "promptTokens": 264,
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "createdAt": "2025-12-11T01:41:41.673Z",
      "updatedAt": "2025-12-11T01:42:47.342Z",
      "environment": "default"
    },
    {
      "id": "17cc616c",
      "timestamp": "2025-12-11T01:38:52.209000+00:00",
      "name": "17cc_py_sketch_generator",
      "input": null,
      "output": {
        "status": "failed",
        "last_code": "import cv2\nimport numpy as np\nfrom tflite_runtime.interpreter import Interpreter\n\n# Configuration parameters\nmodel_path = 'models/ssd-mobilenet_v1/detect.tflite'\nlabel_path = 'models/ssd-mobilenet_v1/labelmap.txt'\ninput_path = 'data/object_detection/sheeps.mp4'\noutput_path = 'results/object_detection/test_results/sheeps_detections.mp4'\nconfidence_threshold = 0.5\n\n# 1. Setup\ndef load_labels(label_path):\n    with open(label_path, 'r') as f:\n        labels = {i: line.strip() for i, line in enumerate(f.readlines())}\n    return labels\n\ninterpreter = Interpreter(model_path=model_path)\ninterpreter.allocate_tensors()\n\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\nlabels = load_labels(label_path)\n\n# 2. Preprocessing\ncap = cv2.VideoCapture(input_path)\nframe_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\nframe_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\nfps = cap.get(cv2.CAP_PROP_FPS)\n\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\nout = cv2.VideoWriter(output_path, fourcc, fps, (frame_width, frame_height))\n\ndef preprocess_image(frame):\n    input_shape = input_details[0]['shape']\n    frame_resized = cv2.resize(frame, (input_shape[1], input_shape[2]))\n    input_data = np.expand_dims(frame_resized, axis=0)\n    return input_data\n\n# 3. Inference\nwhile cap.isOpened():\n    ret, frame = cap.read()\n    if not ret:\n        break\n    \n    input_data = preprocess_image(frame)\n    \n    interpreter.set_tensor(input_details[0]['index'], input_data.astype(np.uint8))\n    interpreter.invoke()\n\n    boxes = interpreter.get_tensor(output_details[1]['index'])\n    classes = interpreter.get_tensor(output_details[3]['index'])\n    scores = interpreter.get_tensor(output_details[2]['index'])\n\n    # 4. Output handling\n    for i in range(len(scores[0])):\n        if scores[0][i] >= confidence_threshold:\n            ymin, xmin, ymax, xmax = boxes[0][i]\n            (left, right, top, bottom) = (xmin * frame_width, xmax * frame_width,\n                                          ymin * frame_height, ymax * frame_height)\n            \n            cv2.rectangle(frame, (int(left), int(top)), (int(right), int(bottom)), (0, 255, 0), 2)\n            label = f\"{labels[int(classes[0][i]) + 1]}: {scores[0][i]:.2f}\"\n            cv2.putText(frame, label, (int(left), int(top) - 5),\n                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n\n    out.write(frame)\n\ncap.release()\nout.release()\ncv2.destroyAllWindows()",
        "last_error": "Max retries reached with failure. Last error from execution: INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\nTraceback (most recent call last):\n  File \"/home/wuguangh/Projects/03.12/tinyml-autopilot-pm.03.12.2025/results/object_detection/sketches/tmp_20251211034115_psg_qwen2.5-coder:32b/tmp_20251211034115_psg_qwen2.5-coder:32b.py\", line 59, in <module>\n    ymin, xmin, ymax, xmax = boxes[0][i]\nTypeError: cannot unpack non-iterable numpy.float32 object\n"
      },
      "session_id": "qwen2.5-coder:32b_a8a2_psg_batch",
      "metadata": {
        "num_run": 9
      },
      "tags": [
        "ablation",
        "benchmark",
        "py_sketch_generator",
        "qwen2.5-coder:32b"
      ],
      "latency": 143.313,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-03-38-52-210769_chatcmpl-e4bd7416-4ce5-4583-8892-e8fa537c1539",
          "traceId": "17cc616c",
          "type": "GENERATION",
          "name": "17cc_psg_gen_attempt#1",
          "startTime": "2025-12-11T01:38:52.210000+00:00",
          "endTime": "2025-12-11T01:39:20.606000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 28396.0,
          "completionTokens": 768,
          "environment": "default",
          "totalTokens": 1032,
          "promptTokens": 264,
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-03-39-20-944617_chatcmpl-003b125f-bd37-425c-88c8-25cdcbaa9a7b",
          "traceId": "17cc616c",
          "type": "GENERATION",
          "name": "17cc_psg_gen_attempt#2",
          "startTime": "2025-12-11T01:39:20.944000+00:00",
          "endTime": "2025-12-11T01:39:55.047000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 34103.0,
          "completionTokens": 923,
          "environment": "default",
          "totalTokens": 1187,
          "promptTokens": 264,
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-03-39-55-222062_chatcmpl-34264ae4-2552-4006-8d48-4190838e907c",
          "traceId": "17cc616c",
          "type": "GENERATION",
          "name": "17cc_psg_gen_attempt#3",
          "startTime": "2025-12-11T01:39:55.222000+00:00",
          "endTime": "2025-12-11T01:40:22.476000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 27254.0,
          "completionTokens": 739,
          "environment": "default",
          "totalTokens": 1003,
          "promptTokens": 264,
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-03-40-22-661413_chatcmpl-0e1772c3-4c1b-413b-886a-dd8b153c9f02",
          "traceId": "17cc616c",
          "type": "GENERATION",
          "name": "17cc_psg_gen_attempt#4",
          "startTime": "2025-12-11T01:40:22.661000+00:00",
          "endTime": "2025-12-11T01:40:49.497000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 26836.0,
          "completionTokens": 728,
          "environment": "default",
          "totalTokens": 992,
          "promptTokens": 264,
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-03-40-49-669720_chatcmpl-a59e3f1d-369e-4806-b781-cd23ef634cf2",
          "traceId": "17cc616c",
          "type": "GENERATION",
          "name": "17cc_psg_gen_attempt#5",
          "startTime": "2025-12-11T01:40:49.669000+00:00",
          "endTime": "2025-12-11T01:41:15.175000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 25506.0,
          "completionTokens": 692,
          "environment": "default",
          "totalTokens": 956,
          "promptTokens": 264,
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "b80142a8-dafd-4934-9250-871cd9977e46",
          "traceId": "17cc616c",
          "type": "SPAN",
          "name": "error_17_psg_failure_signal_py_sketch_generator",
          "startTime": "2025-12-11T01:41:15.523000+00:00",
          "level": "ERROR",
          "statusMessage": "Failed. Last error: Max retries reached with failure. Last error from execution: INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\nTraceback (most recent call last):\n  File \"/home/wuguangh/Projects/03.12/tinyml-autopilot-pm.03.12.2025/results/object_detection/sketches/tmp_20251211034115_psg_qwen2.5-coder:32b/tmp_20251211034115_psg_qwen2.5-coder:32b.py\", line 59, in <module>\n    ymin, xmin, ymax, xmax = boxes[0][i]\nTypeError: cannot unpack non-iterable numpy.float32 object\n",
          "calculatedTotalCost": 0.0,
          "completionTokens": 0,
          "environment": "default",
          "totalTokens": 0,
          "promptTokens": 0,
          "costDetails": {},
          "endTime": null,
          "model": null,
          "modelParameters": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null,
          "latency": null
        }
      ],
      "createdAt": "2025-12-11T01:38:52.727Z",
      "updatedAt": "2025-12-11T01:41:20.847Z",
      "environment": "default"
    },
    {
      "id": "df64f3d7",
      "timestamp": "2025-12-11T01:35:34.187000+00:00",
      "name": "df64_py_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import cv2\nimport numpy as np\nfrom tflite_runtime.interpreter import Interpreter\n\n# Configuration parameters\nmodel_path = 'models/ssd-mobilenet_v1/detect.tflite'\nlabel_path = 'models/ssd-mobilenet_v1/labelmap.txt'\ninput_path = 'data/object_detection/sheeps.mp4'\noutput_path = 'results/object_detection/test_results/sheeps_detections.mp4'\nconfidence_threshold = 0.5\n\n# Load labels\nwith open(label_path, 'r') as f:\n    labels = [line.strip() for line in f.readlines()]\n\n# Setup TFLite interpreter\ninterpreter = Interpreter(model_path=model_path)\ninterpreter.allocate_tensors()\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\n# Read video input\ncap = cv2.VideoCapture(input_path)\nwidth = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\nheight = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\nfps = cap.get(cv2.CAP_PROP_FPS)\n\n# Define codec and create VideoWriter object\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\nout = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n\ndef preprocess_image(frame):\n    frame_resized = cv2.resize(frame, (300, 300))\n    frame_rgb = cv2.cvtColor(frame_resized, cv2.COLOR_BGR2RGB)\n    input_data = np.expand_dims(frame_rgb, axis=0)\n    return input_data\n\ndef draw_boxes_and_labels(image, boxes, classes, scores):\n    for i in range(len(scores)):\n        if scores[i] > confidence_threshold:\n            ymin, xmin, ymax, xmax = boxes[i]\n            (left, right, top, bottom) = (xmin * width, xmax * width,\n                                          ymin * height, ymax * height)\n            cv2.rectangle(image, (int(left), int(top)), (int(right), int(bottom)),\n                          (0, 255, 0), thickness=2)\n            label = '{}: {:.2f}'.format(labels[int(classes[i])], scores[i])\n            cv2.putText(image, label, (int(left), int(top) - 10),\n                        cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n\n# Main loop for processing video frames\nwhile cap.isOpened():\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    # Preprocessing\n    input_data = preprocess_image(frame)\n    \n    # Inference\n    interpreter.set_tensor(input_details[0]['index'], input_data)\n    interpreter.invoke()\n    boxes = interpreter.get_tensor(output_details[0]['index'])[0]\n    classes = interpreter.get_tensor(output_details[1]['index'])[0]\n    scores = interpreter.get_tensor(output_details[2]['index'])[0]\n    \n    # Draw detection results\n    draw_boxes_and_labels(frame, boxes, classes, scores)\n    \n    # Output handling\n    out.write(frame)\n\ncap.release()\nout.release()\ncv2.destroyAllWindows()"
      },
      "session_id": "qwen2.5-coder:32b_a8a2_psg_batch",
      "metadata": {
        "num_run": 8
      },
      "tags": [
        "ablation",
        "benchmark",
        "py_sketch_generator",
        "qwen2.5-coder:32b"
      ],
      "latency": 141.036,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-03-35-34-188907_chatcmpl-a467ead4-5c7d-44a2-99e5-f7252ba9d60d",
          "traceId": "df64f3d7",
          "type": "GENERATION",
          "name": "df64_psg_gen_attempt#1",
          "startTime": "2025-12-11T01:35:34.188000+00:00",
          "endTime": "2025-12-11T01:36:09.560000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 35372.0,
          "completionTokens": 956,
          "environment": "default",
          "totalTokens": 1220,
          "promptTokens": 264,
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-03-36-09-685617_chatcmpl-efd1090e-8693-4632-ad9f-32f862a864d0",
          "traceId": "df64f3d7",
          "type": "GENERATION",
          "name": "df64_psg_gen_attempt#2",
          "startTime": "2025-12-11T01:36:09.685000+00:00",
          "endTime": "2025-12-11T01:36:34.686000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 25001.0,
          "completionTokens": 679,
          "environment": "default",
          "totalTokens": 943,
          "promptTokens": 264,
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-03-36-34-856876_chatcmpl-cd2acae1-2959-49df-8b03-484fe01686bc",
          "traceId": "df64f3d7",
          "type": "GENERATION",
          "name": "df64_psg_gen_attempt#3",
          "startTime": "2025-12-11T01:36:34.856000+00:00",
          "endTime": "2025-12-11T01:37:05.232000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 30376.0,
          "completionTokens": 824,
          "environment": "default",
          "totalTokens": 1088,
          "promptTokens": 264,
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-03-37-05-364218_chatcmpl-d710e2ba-c7f1-4944-ac7f-64d538aaa026",
          "traceId": "df64f3d7",
          "type": "GENERATION",
          "name": "df64_psg_gen_attempt#4",
          "startTime": "2025-12-11T01:37:05.364000+00:00",
          "endTime": "2025-12-11T01:37:29.126000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 23762.0,
          "completionTokens": 645,
          "environment": "default",
          "totalTokens": 909,
          "promptTokens": 264,
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-03-37-29-314468_chatcmpl-d70be68f-d101-40e2-8e73-53e865e43e8e",
          "traceId": "df64f3d7",
          "type": "GENERATION",
          "name": "df64_psg_gen_attempt#5",
          "startTime": "2025-12-11T01:37:29.314000+00:00",
          "endTime": "2025-12-11T01:37:55.224000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 25910.0,
          "completionTokens": 703,
          "environment": "default",
          "totalTokens": 967,
          "promptTokens": 264,
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "createdAt": "2025-12-11T01:35:34.705Z",
      "updatedAt": "2025-12-11T01:38:32.351Z",
      "environment": "default"
    },
    {
      "id": "a55d48d3",
      "timestamp": "2025-12-11T01:33:14.238000+00:00",
      "name": "a55d_py_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import cv2\nimport numpy as np\nfrom tflite_runtime.interpreter import Interpreter\n\n# Configuration parameters\nmodel_path = 'models/ssd-mobilenet_v1/detect.tflite'\nlabel_path = 'models/ssd-mobilenet_v1/labelmap.txt'\ninput_path = 'data/object_detection/sheeps.mp4'\noutput_path = 'results/object_detection/test_results/sheeps_detections.mp4'\nconfidence_threshold = 0.5\n\n# 1. Setup\n# Load the TFLite model and allocate tensors.\ninterpreter = Interpreter(model_path=model_path)\ninterpreter.allocate_tensors()\n\n# Get input and output details\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\n# Read labels from label map file\nwith open(label_path, 'r') as f:\n    labels = [line.strip() for line in f.readlines()]\n\n# 2. Preprocessing\ncap = cv2.VideoCapture(input_path)\nframe_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\nframe_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\nfps = cap.get(cv2.CAP_PROP_FPS)\n\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\nout = cv2.VideoWriter(output_path, fourcc, fps, (frame_width, frame_height))\n\n# 3. Inference\nwhile cap.isOpened():\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    # Preprocess the image\n    input_shape = input_details[0]['shape']\n    height, width = input_shape[1], input_shape[2]\n    image_resized = cv2.resize(frame, (width, height))\n    image_rgb = cv2.cvtColor(image_resized, cv2.COLOR_BGR2RGB)\n    input_data = np.expand_dims(image_rgb, axis=0)\n\n    # Run inference\n    interpreter.set_tensor(input_details[0]['index'], input_data)\n    interpreter.invoke()\n\n    # Extract results\n    boxes = interpreter.get_tensor(output_details[0]['index'])[0]\n    classes = interpreter.get_tensor(output_details[1]['index'])[0]\n    scores = interpreter.get_tensor(output_details[2]['index'])[0]\n\n    # 4. Output handling\n    for i in range(len(scores)):\n        if scores[i] > confidence_threshold:\n            ymin, xmin, ymax, xmax = boxes[i]\n            (left, right, top, bottom) = (xmin * frame_width, xmax * frame_width,\n                                          ymin * frame_height, ymax * frame_height)\n            \n            # Draw the bounding box and label\n            cv2.rectangle(frame, (int(left), int(top)), (int(right), int(bottom)), (0, 255, 0), 2)\n            label = f'{labels[int(classes[i])]}: {scores[i]:.2f}'\n            cv2.putText(frame, label, (int(left), int(top) - 10),\n                        cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n\n    out.write(frame)\n\ncap.release()\nout.release()\ncv2.destroyAllWindows()"
      },
      "session_id": "qwen2.5-coder:32b_a8a2_psg_batch",
      "metadata": {
        "num_run": 7
      },
      "tags": [
        "ablation",
        "benchmark",
        "py_sketch_generator",
        "qwen2.5-coder:32b"
      ],
      "latency": 83.427,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-03-33-14-240000_chatcmpl-a3a159e9-d74b-40f7-bcba-2b1eab762a98",
          "traceId": "a55d48d3",
          "type": "GENERATION",
          "name": "a55d_psg_gen_attempt#1",
          "startTime": "2025-12-11T01:33:14.240000+00:00",
          "endTime": "2025-12-11T01:33:42.857000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 28617.0,
          "completionTokens": 774,
          "environment": "default",
          "totalTokens": 1038,
          "promptTokens": 264,
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-03-33-43-196086_chatcmpl-c7bb536e-54a2-44f7-a2cb-3de8c515f7ff",
          "traceId": "a55d48d3",
          "type": "GENERATION",
          "name": "a55d_psg_gen_attempt#2",
          "startTime": "2025-12-11T01:33:43.196000+00:00",
          "endTime": "2025-12-11T01:34:11.938000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 28742.0,
          "completionTokens": 779,
          "environment": "default",
          "totalTokens": 1043,
          "promptTokens": 264,
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-03-34-12-066503_chatcmpl-34e5bd8c-5645-4277-8a8f-698c4159ef04",
          "traceId": "a55d48d3",
          "type": "GENERATION",
          "name": "a55d_psg_gen_attempt#3",
          "startTime": "2025-12-11T01:34:12.066000+00:00",
          "endTime": "2025-12-11T01:34:37.667000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 25601.0,
          "completionTokens": 695,
          "environment": "default",
          "totalTokens": 959,
          "promptTokens": 264,
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "createdAt": "2025-12-11T01:33:14.758Z",
      "updatedAt": "2025-12-11T01:35:14.457Z",
      "environment": "default"
    },
    {
      "id": "bf753c6e",
      "timestamp": "2025-12-11T01:29:35.906000+00:00",
      "name": "bf75_py_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import cv2\nimport numpy as np\nfrom tflite_runtime.interpreter import Interpreter\n\n# Configuration parameters\nmodel_path = 'models/ssd-mobilenet_v1/detect.tflite'\nlabel_path = 'models/ssd-mobilenet_v1/labelmap.txt'\ninput_path = 'data/object_detection/sheeps.mp4'\noutput_path = 'results/object_detection/test_results/sheeps_detections.mp4'\nconfidence_threshold = 0.5\n\n# Phase 1: Setup\ndef load_labels(label_path):\n    with open(label_path, 'r', encoding='utf-8') as f:\n        labels = [line.strip() for line in f.readlines()]\n    return labels\n\ndef initialize_interpreter(model_path):\n    interpreter = Interpreter(model_path=model_path)\n    interpreter.allocate_tensors()\n    input_details = interpreter.get_input_details()\n    output_details = interpreter.get_output_details()\n    return interpreter, input_details, output_details\n\n# Load labels and initialize the interpreter\nlabels = load_labels(label_path)\ninterpreter, input_details, output_details = initialize_interpreter(model_path)\n\n# Phase 2: Preprocessing\ncap = cv2.VideoCapture(input_path)\nif not cap.isOpened():\n    raise IOError(\"Cannot open video file\")\n\nframe_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\nframe_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\nfps = cap.get(cv2.CAP_PROP_FPS)\n\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\nout = cv2.VideoWriter(output_path, fourcc, fps, (frame_width, frame_height))\n\n# Phase 3: Inference\nwhile True:\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    input_shape = input_details[0]['shape']\n    img = cv2.resize(frame, (input_shape[1], input_shape[2]))\n    input_data = np.expand_dims(img, axis=0)\n    interpreter.set_tensor(input_details[0]['index'], input_data.astype(np.uint8))\n\n    interpreter.invoke()\n\n    boxes = interpreter.get_tensor(output_details[0]['index'])[0]\n    classes = interpreter.get_tensor(output_details[1]['index'])[0].astype(int)\n    scores = interpreter.get_tensor(output_details[2]['index'])[0]\n\n    # Phase 4: Output Handling\n    for i in range(len(scores)):\n        if scores[i] > confidence_threshold:\n            ymin, xmin, ymax, xmax = boxes[i]\n            label = labels[int(classes[i]) + 1]  # Adjusting for labelmap.txt indexing starting at 1\n\n            imH, imW, _ = frame.shape\n            (xminn, xmaxx, yminn, ymaxx) = (xmin * imW, xmax * imW, ymin * imH, ymax * imH)\n            cv2.rectangle(frame, (int(xminn), int(yminn)), (int(xmaxx), int(ymaxx)), (10, 255, 0), 2)\n\n            font = cv2.FONT_HERSHEY_SIMPLEX\n            label_size, base_line = cv2.getTextSize('%s: %.2f' % (label, scores[i]), font, 0.7, 2)\n            label_ymin = max(yminn - label_size[1], 10) if yminn > 20 else yminn + 15\n            cv2.rectangle(frame, (int(xminn), int(label_ymin - label_size[1])),\n                          (int(xminn + label_size[0]), int(label_ymin + base_line)), (10, 255, 0), cv2.FILLED)\n            cv2.putText(frame, '%s: %.2f' % (label, scores[i]), (int(xminn), int(label_ymin)),\n                        font, 0.7, (0, 0, 0), 2)\n\n    out.write(frame)\n\ncap.release()\nout.release()\ncv2.destroyAllWindows()\n\n# Compute mAP would typically require a ground truth dataset and is not included in this script."
      },
      "session_id": "qwen2.5-coder:32b_a8a2_psg_batch",
      "metadata": {
        "num_run": 6
      },
      "tags": [
        "ablation",
        "benchmark",
        "py_sketch_generator",
        "qwen2.5-coder:32b"
      ],
      "latency": 155.847,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-03-29-35-908143_chatcmpl-9ccb2476-e408-4143-a2c9-c73d830f6fd8",
          "traceId": "bf753c6e",
          "type": "GENERATION",
          "name": "bf75_psg_gen_attempt#1",
          "startTime": "2025-12-11T01:29:35.908000+00:00",
          "endTime": "2025-12-11T01:30:10.597000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 34689.0,
          "completionTokens": 934,
          "environment": "default",
          "totalTokens": 1198,
          "promptTokens": 264,
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-03-30-10-749704_chatcmpl-aabe9055-96ed-4418-a26b-1484475d761c",
          "traceId": "bf753c6e",
          "type": "GENERATION",
          "name": "bf75_psg_gen_attempt#2",
          "startTime": "2025-12-11T01:30:10.749000+00:00",
          "endTime": "2025-12-11T01:30:39.933000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 29184.0,
          "completionTokens": 792,
          "environment": "default",
          "totalTokens": 1056,
          "promptTokens": 264,
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-03-30-40-111419_chatcmpl-2fd460f2-ea7a-434f-9dc9-ea0bc4aa7f0a",
          "traceId": "bf753c6e",
          "type": "GENERATION",
          "name": "bf75_psg_gen_attempt#3",
          "startTime": "2025-12-11T01:30:40.111000+00:00",
          "endTime": "2025-12-11T01:31:08.837000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 28726.0,
          "completionTokens": 779,
          "environment": "default",
          "totalTokens": 1043,
          "promptTokens": 264,
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-03-31-08-991737_chatcmpl-940d49e2-8230-41e0-9e77-872c63d32af3",
          "traceId": "bf753c6e",
          "type": "GENERATION",
          "name": "bf75_psg_gen_attempt#4",
          "startTime": "2025-12-11T01:31:08.991000+00:00",
          "endTime": "2025-12-11T01:31:36.392000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 27401.0,
          "completionTokens": 743,
          "environment": "default",
          "totalTokens": 1007,
          "promptTokens": 264,
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-03-31-36-541815_chatcmpl-2e624637-ef1b-47da-80c6-03905fe3ac83",
          "traceId": "bf753c6e",
          "type": "GENERATION",
          "name": "bf75_psg_gen_attempt#5",
          "startTime": "2025-12-11T01:31:36.541000+00:00",
          "endTime": "2025-12-11T01:32:11.755000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 35214.0,
          "completionTokens": 953,
          "environment": "default",
          "totalTokens": 1217,
          "promptTokens": 264,
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "createdAt": "2025-12-11T01:29:36.427Z",
      "updatedAt": "2025-12-11T01:32:52.161Z",
      "environment": "default"
    },
    {
      "id": "66d27e95",
      "timestamp": "2025-12-11T01:27:17.958000+00:00",
      "name": "66d2_py_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import cv2\nimport numpy as np\nimport tflite_runtime.interpreter as tflite\nfrom PIL import ImageDraw, ImageFont, Image\n\n# Configuration parameters\nmodel_path = 'models/ssd-mobilenet_v1/detect.tflite'\nlabel_path = 'models/ssd-mobilenet_v1/labelmap.txt'\ninput_path = 'data/object_detection/sheeps.mp4'\noutput_path = 'results/object_detection/test_results/sheeps_detections.mp4'\nconfidence_threshold = 0.5\n\n# Phase 1: Setup\ninterpreter = tflite.Interpreter(model_path=model_path)\ninterpreter.allocate_tensors()\n\n# Get input and output details\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\n# Load labels\nwith open(label_path, 'r') as f:\n    labels = [line.strip() for line in f.readlines()]\n\n# Phase 2: Preprocessing\ncap = cv2.VideoCapture(input_path)\nwidth = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\nheight = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\nfps = cap.get(cv2.CAP_PROP_FPS)\n\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\nout = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n\ndef preprocess_image(image):\n    image_resized = cv2.resize(image, (300, 300))\n    input_data = np.expand_dims(image_resized, axis=0)\n    return input_data\n\n# Phase 3: Inference\nwhile cap.isOpened():\n    ret, frame = cap.read()\n    if not ret:\n        break\n    \n    input_data = preprocess_image(frame)\n    \n    interpreter.set_tensor(input_details[0]['index'], input_data)\n    interpreter.invoke()\n    \n    boxes = interpreter.get_tensor(output_details[0]['index'])[0]\n    classes = interpreter.get_tensor(output_details[1]['index'])[0]\n    scores = interpreter.get_tensor(output_details[2]['index'])[0]\n    \n    # Draw detection boxes with labels\n    for i in range(len(scores)):\n        if scores[i] > confidence_threshold:\n            ymin, xmin, ymax, xmax = boxes[i]\n            im_height, im_width, _ = frame.shape\n            \n            (left, right, top, bottom) = (xmin * im_width, xmax * im_width,\n                                          ymin * im_height, ymax * im_height)\n            \n            cv2.rectangle(frame, (int(left), int(top)), (int(right), int(bottom)), (0, 255, 0), 2)\n            label = labels[int(classes[i])]\n            font = cv2.FONT_HERSHEY_SIMPLEX\n            cv2.putText(frame, f'{label}: {scores[i]:.2f}', (int(left), int(top) - 10),\n                        font, 0.5, (0, 255, 0), 2)\n    \n    # Phase 4: Output Handling\n    out.write(frame)\n\ncap.release()\nout.release()\ncv2.destroyAllWindows()"
      },
      "session_id": "qwen2.5-coder:32b_a8a2_psg_batch",
      "metadata": {
        "num_run": 5
      },
      "tags": [
        "ablation",
        "benchmark",
        "py_sketch_generator",
        "qwen2.5-coder:32b"
      ],
      "latency": 80.593,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-03-27-17-959735_chatcmpl-f91e3426-6122-4893-9afc-1d158aa27e24",
          "traceId": "66d27e95",
          "type": "GENERATION",
          "name": "66d2_psg_gen_attempt#1",
          "startTime": "2025-12-11T01:27:17.959000+00:00",
          "endTime": "2025-12-11T01:27:42.593000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 24634.0,
          "completionTokens": 667,
          "environment": "default",
          "totalTokens": 931,
          "promptTokens": 264,
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-03-27-42-993140_chatcmpl-ca4c2843-4c15-4895-861d-55ed02100786",
          "traceId": "66d27e95",
          "type": "GENERATION",
          "name": "66d2_psg_gen_attempt#2",
          "startTime": "2025-12-11T01:27:42.993000+00:00",
          "endTime": "2025-12-11T01:28:13.523000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 30530.0,
          "completionTokens": 828,
          "environment": "default",
          "totalTokens": 1092,
          "promptTokens": 264,
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-03-28-13-680565_chatcmpl-f454a131-695a-4d13-9829-26551f6ce0ac",
          "traceId": "66d27e95",
          "type": "GENERATION",
          "name": "66d2_psg_gen_attempt#3",
          "startTime": "2025-12-11T01:28:13.680000+00:00",
          "endTime": "2025-12-11T01:28:38.552000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 24872.0,
          "completionTokens": 675,
          "environment": "default",
          "totalTokens": 939,
          "promptTokens": 264,
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "createdAt": "2025-12-11T01:27:18.478Z",
      "updatedAt": "2025-12-11T01:29:15.600Z",
      "environment": "default"
    },
    {
      "id": "d69c7dd5",
      "timestamp": "2025-12-11T01:23:52.916000+00:00",
      "name": "d69c_py_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import cv2\nimport numpy as np\nfrom tflite_runtime.interpreter import Interpreter\n\n# Configuration parameters\nmodel_path = 'models/ssd-mobilenet_v1/detect.tflite'\nlabel_path = 'models/ssd-mobilenet_v1/labelmap.txt'\ninput_path = 'data/object_detection/sheeps.mp4'\noutput_path = 'results/object_detection/test_results/sheeps_detections.mp4'\nconfidence_threshold = 0.5\n\n# Phase 1: Setup\ninterpreter = Interpreter(model_path=model_path)\ninterpreter.allocate_tensors()\n\n# Get input and output details\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\n# Load labels\nwith open(label_path, 'r') as f:\n    labels = [line.strip() for line in f.readlines()]\n\n# Phase 2: Preprocessing\ncap = cv2.VideoCapture(input_path)\nwidth = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\nheight = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\nfps = cap.get(cv2.CAP_PROP_FPS)\n\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\nout = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n\n# Phase 3: Inference\nwhile cap.isOpened():\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    # Preprocess the image\n    input_shape = input_details[0]['shape']\n    img_resized = cv2.resize(frame, (input_shape[1], input_shape[2]))\n    input_data = np.expand_dims(img_resized, axis=0).astype(np.uint8)\n\n    # Run inference\n    interpreter.set_tensor(input_details[0]['index'], input_data)\n    interpreter.invoke()\n\n    # Extract detection results\n    boxes = interpreter.get_tensor(output_details[0]['index'])[0]\n    classes = interpreter.get_tensor(output_details[1]['index'])[0].astype(int)\n    scores = interpreter.get_tensor(output_details[2]['index'])[0]\n\n    # Phase 4: Output Handling\n    for i in range(len(scores)):\n        if scores[i] > confidence_threshold:\n            box = boxes[i]\n            class_id = classes[i]\n            label = labels[class_id]\n            score = scores[i]\n\n            y_min, x_min, y_max, x_max = box\n            y_min, x_min, y_max, x_max = int(y_min * height), int(x_min * width), int(y_max * height), int(x_max * width)\n\n            cv2.rectangle(frame, (x_min, y_min), (x_max, y_max), (0, 255, 0), 2)\n            label_text = f'{label}: {score:.2f}'\n            cv2.putText(frame, label_text, (x_min, y_min - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n\n    out.write(frame)\n\ncap.release()\nout.release()\ncv2.destroyAllWindows()\n\n# mAP computation is not included in this script as it requires a dataset with ground truth annotations."
      },
      "session_id": "qwen2.5-coder:32b_a8a2_psg_batch",
      "metadata": {
        "num_run": 4
      },
      "tags": [
        "ablation",
        "benchmark",
        "py_sketch_generator",
        "qwen2.5-coder:32b"
      ],
      "latency": 148.064,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-03-23-52-917936_chatcmpl-96cdac97-16e5-4cdb-8d2b-0a7d9b5c5042",
          "traceId": "d69c7dd5",
          "type": "GENERATION",
          "name": "d69c_psg_gen_attempt#1",
          "startTime": "2025-12-11T01:23:52.917000+00:00",
          "endTime": "2025-12-11T01:24:16.256000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 23339.0,
          "completionTokens": 631,
          "environment": "default",
          "totalTokens": 895,
          "promptTokens": 264,
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-03-24-16-595210_chatcmpl-4768f35e-f1df-455f-aedb-e9dbc32f9553",
          "traceId": "d69c7dd5",
          "type": "GENERATION",
          "name": "d69c_psg_gen_attempt#2",
          "startTime": "2025-12-11T01:24:16.595000+00:00",
          "endTime": "2025-12-11T01:24:49.874000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 33279.0,
          "completionTokens": 901,
          "environment": "default",
          "totalTokens": 1165,
          "promptTokens": 264,
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-03-24-50-042990_chatcmpl-95133fb2-003f-4375-9a15-b400d50f73cd",
          "traceId": "d69c7dd5",
          "type": "GENERATION",
          "name": "d69c_psg_gen_attempt#3",
          "startTime": "2025-12-11T01:24:50.042000+00:00",
          "endTime": "2025-12-11T01:25:23.402000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 33360.0,
          "completionTokens": 904,
          "environment": "default",
          "totalTokens": 1168,
          "promptTokens": 264,
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-03-25-23-757224_chatcmpl-54286254-83e5-4c19-9e8f-9c72e46d81a8",
          "traceId": "d69c7dd5",
          "type": "GENERATION",
          "name": "d69c_psg_gen_attempt#4",
          "startTime": "2025-12-11T01:25:23.757000+00:00",
          "endTime": "2025-12-11T01:25:56.574000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 32817.0,
          "completionTokens": 890,
          "environment": "default",
          "totalTokens": 1154,
          "promptTokens": 264,
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-03-25-56-729819_chatcmpl-759bce0f-f21e-4c2c-ab4e-84b9ab4beba6",
          "traceId": "d69c7dd5",
          "type": "GENERATION",
          "name": "d69c_psg_gen_attempt#5",
          "startTime": "2025-12-11T01:25:56.729000+00:00",
          "endTime": "2025-12-11T01:26:20.981000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 24252.0,
          "completionTokens": 659,
          "environment": "default",
          "totalTokens": 923,
          "promptTokens": 264,
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "createdAt": "2025-12-11T01:23:53.436Z",
      "updatedAt": "2025-12-11T01:26:58.113Z",
      "environment": "default"
    },
    {
      "id": "5a438f5b",
      "timestamp": "2025-12-11T01:20:27.958000+00:00",
      "name": "5a43_py_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import cv2\nimport numpy as np\nimport tflite_runtime.interpreter as tflite\n\n# Configuration parameters\nmodel_path = 'models/ssd-mobilenet_v1/detect.tflite'\nlabel_path = 'models/ssd-mobilenet_v1/labelmap.txt'\ninput_path = 'data/object_detection/sheeps.mp4'\noutput_path = 'results/object_detection/test_results/sheeps_detections.mp4'\nconfidence_threshold = 0.5\n\n# Load labels\nwith open(label_path, 'r') as f:\n    labels = [line.strip() for line in f.readlines()]\n\n# Load TFLite interpreter\ninterpreter = tflite.Interpreter(model_path=model_path)\ninterpreter.allocate_tensors()\n\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\n# Read input video\ncap = cv2.VideoCapture(input_path)\nwidth = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\nheight = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\nfps = cap.get(cv2.CAP_PROP_FPS)\n\n# Write output video\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\nout = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n\nwhile cap.isOpened():\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    # Preprocessing\n    input_shape = input_details[0]['shape']\n    image_resized = cv2.resize(frame, (input_shape[1], input_shape[2]))\n    input_data = np.expand_dims(image_resized, axis=0).astype(np.uint8)\n\n    # Inference\n    interpreter.set_tensor(input_details[0]['index'], input_data)\n    interpreter.invoke()\n\n    # Extract results\n    boxes = interpreter.get_tensor(output_details[0]['index'])[0]\n    classes = interpreter.get_tensor(output_details[1]['index'])[0]\n    scores = interpreter.get_tensor(output_details[2]['index'])[0]\n\n    # Draw detection boxes with labels\n    for i in range(len(scores)):\n        if scores[i] > confidence_threshold:\n            ymin, xmin, ymax, xmax = boxes[i]\n            (left, right, top, bottom) = (xmin * width, xmax * width,\n                                          ymin * height, ymax * height)\n            label = '%s: %.2f' % (labels[int(classes[i])], scores[i])\n            cv2.rectangle(frame, (int(left), int(top)), (int(right), int(bottom)), (0, 255, 0), 2)\n            cv2.putText(frame, label, (int(left), int(top) - 10),\n                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n\n    # Write the frame with detection boxes to the output video\n    out.write(frame)\n\n# Release resources\ncap.release()\nout.release()\ncv2.destroyAllWindows()"
      },
      "session_id": "qwen2.5-coder:32b_a8a2_psg_batch",
      "metadata": {
        "num_run": 3
      },
      "tags": [
        "ablation",
        "benchmark",
        "py_sketch_generator",
        "qwen2.5-coder:32b"
      ],
      "latency": 148.057,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-03-20-27-959839_chatcmpl-b8b1d944-26f9-4357-ac3a-699aaad69a27",
          "traceId": "5a438f5b",
          "type": "GENERATION",
          "name": "5a43_psg_gen_attempt#1",
          "startTime": "2025-12-11T01:20:27.959000+00:00",
          "endTime": "2025-12-11T01:21:04.892000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 36933.0,
          "completionTokens": 998,
          "environment": "default",
          "totalTokens": 1262,
          "promptTokens": 264,
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-03-21-05-046935_chatcmpl-ff5804d0-1052-4233-9b1e-0c248cc959e5",
          "traceId": "5a438f5b",
          "type": "GENERATION",
          "name": "5a43_psg_gen_attempt#2",
          "startTime": "2025-12-11T01:21:05.046000+00:00",
          "endTime": "2025-12-11T01:21:34.027000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 28981.0,
          "completionTokens": 786,
          "environment": "default",
          "totalTokens": 1050,
          "promptTokens": 264,
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-03-21-34-178431_chatcmpl-0acdfad9-c7f1-4a7e-82b7-bbf7bc13b517",
          "traceId": "5a438f5b",
          "type": "GENERATION",
          "name": "5a43_psg_gen_attempt#3",
          "startTime": "2025-12-11T01:21:34.178000+00:00",
          "endTime": "2025-12-11T01:22:02.890000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 28712.0,
          "completionTokens": 779,
          "environment": "default",
          "totalTokens": 1043,
          "promptTokens": 264,
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-03-22-03-063731_chatcmpl-ef1ca01b-790e-42e8-9241-f470578eee3c",
          "traceId": "5a438f5b",
          "type": "GENERATION",
          "name": "5a43_psg_gen_attempt#4",
          "startTime": "2025-12-11T01:22:03.063000+00:00",
          "endTime": "2025-12-11T01:22:33.318000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 30255.0,
          "completionTokens": 821,
          "environment": "default",
          "totalTokens": 1085,
          "promptTokens": 264,
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-03-22-33-526969_chatcmpl-13e640fa-30fc-4b54-8d69-ac2f3f37b34c",
          "traceId": "5a438f5b",
          "type": "GENERATION",
          "name": "5a43_psg_gen_attempt#5",
          "startTime": "2025-12-11T01:22:33.526000+00:00",
          "endTime": "2025-12-11T01:22:56.016000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 22490.0,
          "completionTokens": 611,
          "environment": "default",
          "totalTokens": 875,
          "promptTokens": 264,
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "createdAt": "2025-12-11T01:20:28.478Z",
      "updatedAt": "2025-12-11T01:23:33.211Z",
      "environment": "default"
    },
    {
      "id": "3158cbe6",
      "timestamp": "2025-12-11T01:18:39.987000+00:00",
      "name": "3158_py_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import os\nimport cv2\nimport numpy as np\nimport tflite_runtime.interpreter as tflite\nfrom PIL import Image\n\n# Configuration parameters\nmodel_path = 'models/ssd-mobilenet_v1/detect.tflite'\nlabel_path = 'models/ssd-mobilenet_v1/labelmap.txt'\ninput_path = 'data/object_detection/sheeps.mp4'\noutput_path = 'results/object_detection/test_results/sheeps_detections.mp4'\nconfidence_threshold = 0.5\n\n# Load labels\nwith open(label_path, 'r') as f:\n    labels = [line.strip() for line in f.readlines()]\n\n# Load TFLite model and allocate tensors\ninterpreter = tflite.Interpreter(model_path=model_path)\ninterpreter.allocate_tensors()\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\n# Read input video\ncap = cv2.VideoCapture(input_path)\nif not cap.isOpened():\n    raise IOError(\"Cannot open video file\")\n\n# Get video properties\nframe_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\nframe_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\nfps = cap.get(cv2.CAP_PROP_FPS)\n\n# Define the codec and create VideoWriter object\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\nout = cv2.VideoWriter(output_path, fourcc, fps, (frame_width, frame_height))\n\ndef draw_detection_boxes(image, boxes, classes, scores):\n    for box, cls, score in zip(boxes[0], classes[0], scores[0]):\n        if score > confidence_threshold:\n            ymin = int(max(1, box[0] * image.shape[0]))\n            xmin = int(max(1, box[1] * image.shape[1]))\n            ymax = int(min(image.shape[0], box[2] * image.shape[0]))\n            xmax = int(min(image.shape[1], box[3] * image.shape[1]))\n\n            cv2.rectangle(image, (xmin, ymin), (xmax, ymax), (255, 0, 0), 2)\n            label = f'{labels[int(cls)]}: {score:.2f}'\n            cv2.putText(image, label, (xmin, ymin - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (36,255,12), 2)\n\nwhile cap.isOpened():\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    # Preprocessing\n    input_shape = input_details[0]['shape']\n    image_resized = cv2.resize(frame, (input_shape[1], input_shape[2]))\n    image_rgb = cv2.cvtColor(image_resized, cv2.COLOR_BGR2RGB)\n    input_data = np.expand_dims(image_rgb, axis=0)\n\n    # Inference\n    interpreter.set_tensor(input_details[0]['index'], input_data.astype(np.uint8))\n    interpreter.invoke()\n\n    boxes = interpreter.get_tensor(output_details[0]['index'])\n    classes = interpreter.get_tensor(output_details[1]['index'])\n    scores = interpreter.get_tensor(output_details[2]['index'])\n\n    # Draw detection boxes\n    draw_detection_boxes(frame, boxes, classes, scores)\n\n    # Output handling\n    out.write(frame)\n\ncap.release()\nout.release()\ncv2.destroyAllWindows()"
      },
      "session_id": "qwen2.5-coder:32b_a8a2_psg_batch",
      "metadata": {
        "num_run": 2
      },
      "tags": [
        "ablation",
        "benchmark",
        "py_sketch_generator",
        "qwen2.5-coder:32b"
      ],
      "latency": 50.855,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-03-18-39-988202_chatcmpl-6c21b1cf-69ef-439b-8482-68d2cd81047c",
          "traceId": "3158cbe6",
          "type": "GENERATION",
          "name": "3158_psg_gen_attempt#1",
          "startTime": "2025-12-11T01:18:39.988000+00:00",
          "endTime": "2025-12-11T01:19:04.846000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 24858.0,
          "completionTokens": 673,
          "environment": "default",
          "totalTokens": 937,
          "promptTokens": 264,
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-03-19-04-997389_chatcmpl-b8187bd7-cf34-4d9c-a82e-94250b393fcb",
          "traceId": "3158cbe6",
          "type": "GENERATION",
          "name": "3158_psg_gen_attempt#2",
          "startTime": "2025-12-11T01:19:04.997000+00:00",
          "endTime": "2025-12-11T01:19:30.843000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 25846.0,
          "completionTokens": 702,
          "environment": "default",
          "totalTokens": 966,
          "promptTokens": 264,
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "createdAt": "2025-12-11T01:18:40.506Z",
      "updatedAt": "2025-12-11T01:20:07.702Z",
      "environment": "default"
    },
    {
      "id": "84a75dd5",
      "timestamp": "2025-12-11T01:16:06.043000+00:00",
      "name": "84a7_py_sketch_generator",
      "input": null,
      "output": {
        "status": "failed",
        "last_code": "import cv2\nimport numpy as np\nfrom tflite_runtime.interpreter import Interpreter\nfrom PIL import ImageDraw, ImageFont\n\n# Configuration parameters\nmodel_path = 'models/ssd-mobilenet_v1/detect.tflite'\nlabel_path = 'models/ssd-mobilenet_v1/labelmap.txt'\ninput_path = 'data/object_detection/sheeps.mp4'\noutput_path = 'results/object_detection/test_results/sheeps_detections.mp4'\nconfidence_threshold = 0.5\n\n# Phase 1: Setup\ninterpreter = Interpreter(model_path=model_path)\ninterpreter.allocate_tensors()\n\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\nheight, width = input_details[0]['shape'][1], input_details[0]['shape'][2]\n\nwith open(label_path, 'r') as f:\n    labels = [line.strip() for line in f.readlines()]\n\n# Load video\ncap = cv2.VideoCapture(input_path)\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\nout = cv2.VideoWriter(output_path, fourcc, cap.get(cv2.CAP_PROP_FPS), (int(cap.get(3)), int(cap.get(4))))\n\n# Phase 2: Preprocessing\nwhile cap.isOpened():\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    input_data = cv2.resize(frame, (width, height))\n    input_data = np.expand_dims(input_data, axis=0)\n    input_data = input_data.astype(np.uint8)\n\n    # Phase 3: Inference\n    interpreter.set_tensor(input_details[0]['index'], input_data)\n    interpreter.invoke()\n\n    boxes = interpreter.get_tensor(output_details[0]['index'])\n    classes = interpreter.get_tensor(output_details[1]['index']).astype(int)\n    scores = interpreter.get_tensor(output_details[2]['index'])\n\n    # Phase 4: Output Handling\n    img = Image.fromarray(frame)\n    draw = ImageDraw.Draw(img)\n    font = ImageFont.load_default()\n\n    for i in range(len(scores)):\n        if scores[i] > confidence_threshold:\n            ymin, xmin, ymax, xmax = boxes[0][i]\n            (left, right, top, bottom) = (xmin * width, xmax * width,\n                                          ymin * height, ymax * height)\n\n            label = f'{labels[int(classes[i])]}: {scores[i]:.2f}'\n            draw.rectangle([(left, top), (right, bottom)], outline='red', width=2)\n            text_width, text_height = font.getsize(label)\n            draw.rectangle([(left, top - text_height), (left + text_width, top)], fill='red')\n            draw.text((left, top - text_height), label, font=font, fill=(255, 255, 255))\n\n    frame_with_boxes = np.array(img)\n    out.write(frame_with_boxes)\n\ncap.release()\nout.release()",
        "last_error": "Max retries reached with failure. Last error from execution: INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\nTraceback (most recent call last):\n  File \"/home/wuguangh/Projects/03.12/tinyml-autopilot-pm.03.12.2025/results/object_detection/sketches/tmp_20251211031814_psg_qwen2.5-coder:32b/tmp_20251211031814_psg_qwen2.5-coder:32b.py\", line 49, in <module>\n    img = Image.fromarray(frame)\nNameError: name 'Image' is not defined\n"
      },
      "session_id": "qwen2.5-coder:32b_a8a2_psg_batch",
      "metadata": {
        "num_run": 1
      },
      "tags": [
        "ablation",
        "benchmark",
        "py_sketch_generator",
        "qwen2.5-coder:32b"
      ],
      "latency": 128.461,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-03-16-06-044492_chatcmpl-f1de3d70-a29e-43dc-9710-dac00ceabb96",
          "traceId": "84a75dd5",
          "type": "GENERATION",
          "name": "84a7_psg_gen_attempt#1",
          "startTime": "2025-12-11T01:16:06.044000+00:00",
          "endTime": "2025-12-11T01:16:33.553000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 27509.0,
          "completionTokens": 731,
          "environment": "default",
          "totalTokens": 995,
          "promptTokens": 264,
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-03-16-33-903079_chatcmpl-0e240040-2822-42e1-ace4-4a80d4af23c5",
          "traceId": "84a75dd5",
          "type": "GENERATION",
          "name": "84a7_psg_gen_attempt#2",
          "startTime": "2025-12-11T01:16:33.903000+00:00",
          "endTime": "2025-12-11T01:17:02.753000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 28850.0,
          "completionTokens": 783,
          "environment": "default",
          "totalTokens": 1047,
          "promptTokens": 264,
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-03-17-02-910464_chatcmpl-cae76828-9842-47f9-b2a4-9785d05f6fb6",
          "traceId": "84a75dd5",
          "type": "GENERATION",
          "name": "84a7_psg_gen_attempt#3",
          "startTime": "2025-12-11T01:17:02.910000+00:00",
          "endTime": "2025-12-11T01:17:26.931000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 24021.0,
          "completionTokens": 652,
          "environment": "default",
          "totalTokens": 916,
          "promptTokens": 264,
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-03-17-27-128039_chatcmpl-2f0c5354-258e-485a-9e3f-f440f7b21e3f",
          "traceId": "84a75dd5",
          "type": "GENERATION",
          "name": "84a7_psg_gen_attempt#4",
          "startTime": "2025-12-11T01:17:27.128000+00:00",
          "endTime": "2025-12-11T01:17:51.213000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 24085.0,
          "completionTokens": 654,
          "environment": "default",
          "totalTokens": 918,
          "promptTokens": 264,
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-03-17-51-416331_chatcmpl-8880a6ba-0129-4df9-aa80-f1703ce3945d",
          "traceId": "84a75dd5",
          "type": "GENERATION",
          "name": "84a7_psg_gen_attempt#5",
          "startTime": "2025-12-11T01:17:51.416000+00:00",
          "endTime": "2025-12-11T01:18:14.145000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 22729.0,
          "completionTokens": 617,
          "environment": "default",
          "totalTokens": 881,
          "promptTokens": 264,
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "55972833-7aff-495c-a171-d2d74d506456",
          "traceId": "84a75dd5",
          "type": "SPAN",
          "name": "error_84_psg_failure_signal_py_sketch_generator",
          "startTime": "2025-12-11T01:18:14.505000+00:00",
          "level": "ERROR",
          "statusMessage": "Failed. Last error: Max retries reached with failure. Last error from execution: INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\nTraceback (most recent call last):\n  File \"/home/wuguangh/Projects/03.12/tinyml-autopilot-pm.03.12.2025/results/object_detection/sketches/tmp_20251211031814_psg_qwen2.5-coder:32b/tmp_20251211031814_psg_qwen2.5-coder:32b.py\", line 49, in <module>\n    img = Image.fromarray(frame)\nNameError: name 'Image' is not defined\n",
          "calculatedTotalCost": 0.0,
          "completionTokens": 0,
          "environment": "default",
          "totalTokens": 0,
          "promptTokens": 0,
          "costDetails": {},
          "endTime": null,
          "model": null,
          "modelParameters": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null,
          "latency": null
        }
      ],
      "createdAt": "2025-12-11T01:16:06.573Z",
      "updatedAt": "2025-12-11T01:18:19.718Z",
      "environment": "default"
    }
  ],
  "meta": {
    "total_items": 30
  }
}